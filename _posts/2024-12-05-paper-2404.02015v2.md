---
title: "MuxServe:FlexibleSpatial-TemporalMultiplexingforMultipleLLMServing"
date: 2024-12-05
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2404.02015v2)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

### 논문의 강점과 독창성

**MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving**는 다중 대형 언어 모델(LLM)을 효율적으로 서빙하기 위한 공간-시간 다중화(spatial-temporal multiplexing) 시스템을 제안합니다. 이 논문의 주요 강점과 독창적인 점은 다음과 같습니다:

1. **공간-시간 다중화 모델의 제안**:
   - 기존의 공간 분할(spatial partitioning) 및 시간 분할(temporal multiplexing) 방식의 한계를 극복하기 위해 두 방식을 결합한 공간-시간 다중화 접근 방식을 도입하였습니다.
   - LLM의 prefill 단계와 decoding 단계의 계산 요구 사항 차이를 활용하여 GPU 자원을 동적으로 활용합니다.

2. **최적화된 배치 알고리즘 및 적응형 배치 스케줄링**:
   - 각 LLM의 인기도(popularity)를 고려한 LLM 병렬 배치 알고리즘과 적응형 배치 스케줄링(ADBS)을 통해 GPU 자원 활용도를 극대화합니다.
   - 이를 통해 계산 및 메모리 자원을 효율적으로 분배하고, 공정한 리소스 공유를 보장합니다.

3. **일관된 자원 관리 시스템 설계**:
   - CUDA MPS(Multi-Process Service)를 활용해 동적으로 GPU SM 자원을 관리하며, 여러 LLM 간의 메모리 공유를 위해 통합된 KV 캐시를 도입하였습니다.

4. **성능 개선**:
   - 기존 시스템 대비 최대 1.8배 높은 처리량과 2.9배 더 많은 요청을 99%의 SLO(Specific Latency Objective)를 유지하며 처리할 수 있음을 입증했습니다.

---

### 핵심 알고리즘 과정 및 예시

#### 1. **LLM 배치 문제 정의**
   - 주어진 클러스터와 워크로드에서 최적의 LLM 그룹 배치를 찾아 GPU 활용률(throughput)을 극대화합니다.
   - 수식으로는 다음과 같이 정의됩니다:
     \[
     B^* = \arg\max_{B \in \mathcal{B}} \sum_{b \in B} F(b, W_b)
     \]
     \(F(b, W_b)\)는 특정 LLM 유닛 \(b\)의 처리량(throughput)을 계산합니다.

#### 2. **알고리즘: Greedy LLM 배치**
   - 각 LLM을 그 요구되는 계산량에 따라 정렬하고, GPU 클러스터 그룹에 배치합니다.
   - 예를 들어, LLM A는 높은 계산 요구량이 있어 독립적으로 배치되며, LLM B와 C는 계산 요구량이 낮아 동일한 GPU 그룹에 배치될 수 있습니다.

#### 3. **적응형 배치 스케줄링 (ADBS)**
   - prefill 작업에 높은 우선순위를 부여하고, 남는 자원으로 decoding 작업을 배치합니다.
   - 예: 요청이 LLM A(10 req/s), B(5 req/s), C(2 req/s)로 들어올 경우, 각 단계의 GPU 사용량과 요구량을 기반으로 배치 스케줄을 동적으로 변경합니다.

#### 4. **실행 예시**
   - 입력: LLM A(요청 길이 150 tokens), LLM B(요청 길이 50 tokens).
   - prefill 단계에서는 LLM A와 B의 요청이 병렬 처리되며, decoding 단계에서는 각 요청이 나뉘어 순차적으로 처리됩니다.
   - GPU 자원의 활용도가 높아져 병목 현상이 완화됩니다.

---

### 한계점

1. **복잡성 증가**:
   - 공간-시간 다중화는 기존의 공간 또는 시간 다중화보다 복잡도가 높아지고, 시스템 구현 및 유지보수가 어려울 수 있습니다.

2. **추가적인 프로파일링 필요**:
   - 최적화된 성능을 위해 각 LLM의 계산 특성과 워크로드를 사전에 정확히 분석해야 하며, 이는 초기 설정 비용을 증가시킵니다.

3. **특정 워크로드 환경에 최적화**:
   - 제안된 시스템은 특정 GPU 하드웨어 및 클러스터 설정에 의존적일 수 있으며, 다양한 환경에서의 일반화를 보장하지 못할 가능성이 있습니다.

4. **LLM 동적 변화 대응 한계**:
   - 워크로드 패턴이 빠르게 변하거나 매우 다양할 경우, 시스템의 적응성이 부족할 수 있습니다.

---

### 결론

MuxServe는 대형 언어 모델의 동시 서빙 효율성을 크게 개선할 수 있는 혁신적인 시스템 설계를 제공합니다. 다만, 시스템 복잡성과 초기 설정 요구사항이 높은 점은 현실적인 제한으로 작용할 수 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

### 핵심 알고리즘: MuxServe의 배치 및 스케줄링 전략

MuxServe는 다중 LLM 서빙에서 GPU 자원을 효율적으로 사용하기 위해 **배치 알고리즘(Placement Algorithm)**과 **적응형 배치 스케줄링(ADBS: Adaptive Batch Scheduling)**을 사용합니다. 아래에서는 두 핵심 알고리즘을 예제를 통해 구체적으로 설명하겠습니다.

---

#### 1. **배치 알고리즘 (Placement Algorithm)**

**목표**: 주어진 GPU 클러스터와 여러 LLM 워크로드를 기반으로, 최적의 GPU 배치를 찾아 GPU 활용률을 극대화합니다.

**과정**:
1. **입력**:
   - 클러스터 \(C\): GPU 자원의 구성(예: 8개의 GPU가 각 노드에 존재).
   - LLM 목록 \(M\): 서로 다른 크기와 요구량의 LLM들(예: LLM A, B, C).
   - 워크로드 \(W\): 각 LLM의 요청 도착률(예: A=10 req/s, B=5 req/s, C=2 req/s).

2. **병렬 후보 생성**:
   - 각 LLM의 계산 및 메모리 요구량을 기반으로, 다양한 병렬 실행 구성을 생성.
   - 예: LLM A는 4개의 GPU가 필요하고, LLM B는 2개의 GPU만으로 충분.

3. **그리디 배치**:
   - 계산 요구량이 큰 LLM부터 GPU에 배치.
   - LLM A와 B를 분리된 GPU에 배치하되, 자원이 남으면 LLM C와 병렬로 배치.

**예제**:
- **입력**:
  - GPU 클러스터 \(C = \{GPU_1, GPU_2, GPU_3, GPU_4\}\)
  - LLM 목록: \(M = \{A, B, C\}\), 각각의 워크로드는 \(W_A = 10\), \(W_B = 5\), \(W_C = 2\).
  - A는 4개의 GPU가 필요, B는 2개의 GPU, C는 1개의 GPU만 필요.

- **출력**:
  - LLM A는 \(GPU_1, GPU_2, GPU_3, GPU_4\)에 배치.
  - LLM B는 \(GPU_5, GPU_6\)에 배치.
  - LLM C는 \(GPU_7\)에 배치.

---

#### 2. **적응형 배치 스케줄링 (ADBS)**

**목표**: 주어진 배치 구성에서 GPU 자원의 사용을 최적화하고, LLM 간 공정한 리소스 분배를 보장하며 전체 처리량을 최대화합니다.

**과정**:
1. **초기화**:
   - 각 LLM에 대해 **토큰 블록 할당량**(token block quota)을 설정합니다.
   - Prefill 작업(입력 토큰 생성)을 우선적으로 실행.

2. **스케줄링 전략**:
   - **Prefill 우선**: LLM의 초기 입력을 생성하는 prefill 작업을 우선 배치.
   - **남는 자원 활용**: 남는 GPU 자원에 decoding 작업(결과 토큰 생성)을 배치.
   - **적응형 할당**: 토큰 블록 사용량을 모니터링하여 필요에 따라 자원을 동적으로 재조정.

3. **정기적인 조정**:
   - LLM의 요청 도착률 변화에 따라 토큰 블록 할당량을 조정.
   - KV 캐시를 실시간으로 공유하여 메모리 활용을 최적화.

**예제**:
- **입력**:
  - LLM A, B, C의 요청 도착률: \(W_A = 10\), \(W_B = 5\), \(W_C = 2\).
  - GPU 클러스터: \(4\)개의 GPU 사용 가능.
  - LLM A는 prefill 단계에서 GPU의 80%를 사용하고, decoding 단계에서는 40%를 사용.

- **과정**:
  1. LLM A의 prefill 작업을 먼저 실행(GPU 1~4 모두 사용).
  2. LLM B의 prefill 작업을 실행(GPU 2~3 활용).
  3. LLM A의 decoding 작업과 LLM C의 prefill 작업을 병렬로 실행.

- **결과**:
  - 모든 GPU가 효율적으로 활용되며, LLM 간 자원 충돌 최소화.

---

### 요약: 알고리즘의 핵심 아이디어

1. **배치 알고리즘**:
   - LLM의 크기와 요청 도착률에 따라 GPU를 배치하여 계산 자원을 효율적으로 활용.

2. **ADBS 스케줄링**:
   - Prefill과 decoding 작업의 자원 요구량 차이를 활용해 GPU 자원을 동적으로 관리.
   - 요청 도착률과 KV 캐시 사용량을 기반으로 자원을 실시간 재조정.

---

### 핵심 이점

1. **효율성**:
   - GPU 자원의 활용도를 극대화하여 처리량(throughput)을 크게 개선.

2. **공정성**:
   - LLM 간 자원 분배를 균형 있게 조정하여 특정 모델이 자원을 독점하는 문제를 방지.

3. **적응성**:
   - 실시간 워크로드 변화에 대응 가능, 다양한 규모의 LLM 서빙에 적합.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### 논문에서 제시한 결과 분석

MuxServe는 공간-시간 다중화를 사용하여 다중 LLM 서빙에서 GPU 활용도를 극대화하고 처리량(throughput)과 지연 시간(SLO: Service Level Objective) 달성을 크게 개선했다고 주장합니다. 논문에서 제시한 주요 결과는 다음과 같습니다:

---

#### **1. 주요 결과**
1. **Throughput(처리량) 개선**:
   - 기존 방법(공간 분할 및 시간 분할) 대비 최대 **1.8배 높은 처리량**을 달성.
   - 동일한 자원에서 최대 **2.9배 더 많은 요청**을 99% SLO 내에서 처리 가능.

2. **SLO 달성률 개선**:
   - 다양한 워크로드 환경에서 SLO(응답 시간 목표) 달성률이 일관적으로 높음.
   - 특히 요청 도착률이 높은 경우에서도 효율성을 유지하며 SLO 목표를 달성.

3. **다양한 워크로드 시뮬레이션**:
   - **Synthetic Workload**와 **Real Workload** 모두에서 MuxServe의 성능이 검증됨.
   - Real Workload에서는 기존 대비 1.38배(SLO), 1.46배(처리량) 향상을 보임.

---

#### **2. 다른 방법론 대비 특출난 점**
MuxServe는 기존 방법론들과 비교하여 다음과 같은 혁신적인 특징이 있습니다:

1. **공간-시간 다중화의 조합**:
   - **공간 분할(spatial partitioning)**은 GPU 메모리와 계산 자원을 정적으로 나누지만, 이는 특정 LLM이 자원을 독점하거나 유휴 GPU가 발생할 수 있음.
   - **시간 분할(temporal multiplexing)**은 여러 LLM을 교대로 실행하지만, 계산 효율성이 낮아질 수 있음.
   - MuxServe는 **두 방식을 결합**하여 prefill 단계와 decoding 단계를 동적으로 나누고, 각 LLM의 요청 도착률을 고려한 최적화를 수행.

2. **적응형 배치 스케줄링(ADBS)**:
   - 자원의 실시간 모니터링 및 동적 재분배를 통해 **GPU 활용도를 극대화**.
   - 동일한 GPU에서 prefill과 decoding 작업을 병렬 실행 가능.

3. **통합 메모리 관리**:
   - **통합 KV 캐시(Unified KV Cache)** 설계를 통해 LLM 간 메모리 공유를 최적화하여 메모리 낭비를 줄임.
   - 메모리 파편화 문제를 해결하며, 동적 요청에 유연하게 대응.

---

#### **3. 성능 개선을 도출한 핵심 방법론**
논문에서는 다음 세 가지 주요 방법이 이러한 결과를 도출했다고 주장합니다:

1. **배치 알고리즘**:
   - LLM의 크기, 요청 도착률, 계산 요구량 등을 기반으로 GPU 그룹에 효율적으로 배치.
   - 워크로드에 따라 GPU 리소스를 그룹화하여 특정 LLM이 자원을 과점하는 문제를 방지.

2. **적응형 배치 스케줄링(ADBS)**:
   - prefill과 decoding의 계산 특성 차이를 활용해 GPU를 병렬로 활용.
   - 요청 도착률과 자원 사용량을 실시간으로 조정하여 병목 현상을 최소화.

3. **통합 KV 캐시 설계**:
   - 여러 LLM이 동일한 GPU 메모리를 효율적으로 공유.
   - 메모리 관리 비용을 줄이고, 대규모 모델에서도 높은 활용률을 유지.

---

### 너의 생각: 이러한 성능 개선의 이유

MuxServe의 성능 개선은 주로 GPU 자원을 효율적으로 사용한 데서 기인합니다. 구체적인 이유와 이에 대한 내 생각은 다음과 같습니다:

1. **GPU 자원 활용의 극대화**:
   - GPU는 계산 리소스(SM)와 메모리 리소스(KV 캐시)가 주요 병목 요소인데, MuxServe는 두 자원을 **동시에 고려한 다중화 전략**을 제안했습니다.
   - 특히, prefill 단계와 decoding 단계의 자원 요구 차이를 분리하여 GPU 유휴 시간을 최소화한 점이 효과적이었다고 생각합니다.

2. **실시간 자원 조정의 유효성**:
   - ADBS 알고리즘은 자원 사용량을 실시간으로 모니터링하며 요청 패턴에 따라 유연하게 GPU 자원을 재조정합니다.
   - 이는 LLM 서빙의 변동성이 높은 워크로드에서 특히 유리하게 작용했다고 봅니다.

3. **모듈화된 설계**:
   - MuxServe는 배치, 스케줄링, 메모리 관리의 세 가지 주요 요소를 독립적으로 최적화하여 시스템 전반의 복잡도를 낮추는 동시에 성능을 개선했습니다.
   - 이 점이 실질적인 워크로드에서도 성능 향상을 보장했다고 판단됩니다.

---

### 개선 결과에 대한 종합 평가

MuxServe는 기존의 공간 또는 시간 다중화 방식에서 발생하는 비효율성을 극복하며 GPU 활용도를 극대화하는 데 성공했습니다. 특히, **다양한 워크로드 시나리오에서의 강력한 성능 향상**이 이 시스템의 실질적인 가치를 증명합니다.

#### **추가적으로 생각할 점**:
1. **시스템 확장성**:
   - 더 많은 GPU를 사용하는 대규모 클러스터 환경에서 얼마나 확장 가능한지가 중요한 연구 과제입니다.
2. **워크로드 변화 대응**:
   - 실시간으로 급격하게 변동하는 워크로드에서 ADBS가 얼마나 효과적으로 작동하는지, 혹은 추가적인 최적화가 필요한지 검토가 필요합니다.

MuxServe의 설계는 강력하지만, 특정한 환경에 초점이 맞춰져 있어 더 일반적인 GPU 리소스 활용 시나리오로 확장하기 위한 추가 연구도 유효할 것입니다.



# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :

MuxServe의 한계를 극복하기 위해 발전시킬 수 있는 연구 흐름과 이에 대한 구체적인 방법론을 제안하면 다음과 같습니다:

---

### 1. **워크로드 다양성 및 동적 변화 대응**

#### 한계:
MuxServe는 LLM의 워크로드가 비교적 예측 가능하고, 시간이 지남에 따라 점진적으로 변화한다고 가정합니다. 하지만 실제 환경에서는 워크로드가 급격히 변동하거나 새로운 LLM이 추가되는 경우가 많습니다.

#### 극복 방안:
1. **강화 학습 기반 동적 스케줄링**:
   - 워크로드 변동성을 실시간으로 학습하여 최적의 배치와 스케줄링 정책을 제안.
   - 강화 학습 알고리즘(DQN, PPO 등)을 통해 워크로드 특성과 GPU 활용률을 연속적으로 최적화.

2. **온라인 프로파일링 시스템**:
   - 각 LLM의 워크로드 특성을 실시간으로 모니터링하여 배치 및 자원 재할당.
   - 예: 특정 LLM의 요청량이 급증하면 다른 LLM을 저자원 상태로 전환하여 우선 처리.

3. **AutoML 기반 적응형 워크로드 관리**:
   - AutoML 기법을 사용해 다양한 워크로드 환경에서 배치 및 스케줄링 전략을 자동 탐색.

#### 연구 흐름:
- **실시간 워크로드 학습 및 적응**: 현재 많은 연구가 온라인 학습 기법을 GPU 리소스 스케줄링에 적용하려 하고 있으며, 이를 기반으로 MuxServe의 실시간 적응성을 강화할 수 있음.

---

### 2. **대규모 클러스터 확장성**

#### 한계:
MuxServe는 중소 규모의 클러스터(최대 32 GPU 노드)에서 검증되었습니다. 하지만 초대규모 클러스터 환경에서는 통신 오버헤드와 자원 관리의 복잡도가 크게 증가합니다.

#### 극복 방안:
1. **분산 최적화 알고리즘 개발**:
   - 분산 클러스터 환경에서 GPU 간 통신 비용을 줄이는 최적화 알고리즘.
   - 예: Horovod와 같은 분산 학습 프레임워크의 스케줄링 방식 활용.

2. **하이브리드 병렬화 기법 적용**:
   - 파이프라인 병렬화와 데이터 병렬화를 결합하여 대규모 모델의 효율적 분산 실행.
   - 예: Megatron-LM에서 사용하는 병렬화 방식을 MuxServe의 스케줄링에 적용.

3. **트래픽 예측 기반 배치**:
   - 대규모 클러스터에서는 모델의 요청 도착률을 예측하여 클러스터 자원을 사전에 할당.
   - 예: 딥러닝 기반의 워크로드 트래픽 예측 모델 적용.

#### 연구 흐름:
- **초대규모 LLM 배치 최적화**: GPT-4와 같은 초대규모 LLM을 효율적으로 배치하고 실행하기 위한 연구가 활발히 진행 중.

---

### 3. **모델 간 리소스 충돌 완화**

#### 한계:
MuxServe는 동일 GPU에 여러 LLM을 병렬 배치하여 리소스를 공유하지만, 모델 간 리소스 충돌로 인해 성능 저하가 발생할 수 있습니다.

#### 극복 방안:
1. **메모리 분리 관리**:
   - 모델마다 독립된 메모리 공간을 동적으로 조정하는 방식으로 충돌을 완화.
   - NVIDIA MIG(Multi-Instance GPU)와 같은 기술을 활용.

2. **자원 격리 및 우선순위 정책**:
   - LLM의 중요도(예: 요청 빈도나 응답 시간 목표)에 따라 우선순위를 설정하여 자원 분배.
   - 중요도가 낮은 LLM의 자원을 제한하여 고우선 LLM에 더 많은 자원을 할당.

3. **토큰 블록 기반 분할 최적화**:
   - KV 캐시의 세부 분할 단위를 줄이고, 모델별 동적 재조정 빈도를 최적화.
   - Paged Attention(vLLM)이 제안한 페이지 기반 캐시 관리 방식을 결합.

#### 연구 흐름:
- **모델 자원 격리**: 멀티 테넌트 환경에서 GPU를 더 나은 분리 관리로 사용하는 연구와 밀접한 연관.

---

### 4. **비용 효율성과 에너지 소비 최적화**

#### 한계:
MuxServe는 성능 최적화에 중점을 두지만, 대규모 LLM 서빙에서 에너지 소비와 비용 최적화는 점점 더 중요한 요소가 됩니다.

#### 극복 방안:
1. **에너지 효율 스케줄링**:
   - GPU의 에너지 소비량을 실시간으로 측정하고, 사용량을 줄이는 스케줄링 정책.
   - 낮은 우선순위 작업은 저전력 모드로 실행하거나 비활성화.

2. **프리엠터블 인스턴스 활용**:
   - 스팟 인스턴스 또는 프리엠터블 인스턴스에서 LLM을 실행하여 비용을 절감.
   - SpotServe(2024)의 방법론을 참고하여 비핵심 작업을 저비용 인스턴스에서 실행.

3. **자원 통합 및 컨테이너화**:
   - Kubernetes 기반의 GPU 자원 통합 관리로 비용 및 에너지 최적화.
   - GPU를 컨테이너로 가상화하여 필요에 따라 동적으로 할당.

#### 연구 흐름:
- **그린 AI와 LLM 서빙 최적화**: AI 서빙의 탄소 배출을 줄이기 위한 연구가 최근 활발히 진행 중.

---

### 5. **메모리와 계산 자원의 세밀한 다중화**

#### 한계:
MuxServe의 통합 메모리 관리와 GPU 다중화는 효과적이지만, 더 세밀한 제어 및 병렬화가 가능할 여지가 있음.

#### 극복 방안:
1. **헤드 단위 캐시 최적화**:
   - KV 캐시를 레이어별 또는 헤드 단위로 더 세밀히 관리.
   - 예: 특정 레이어의 활성화 빈도가 낮은 경우 캐시를 삭제하거나 압축.

2. **멀티 모달 LLM 서빙 지원**:
   - 텍스트뿐 아니라 멀티모달 데이터(이미지, 비디오 등)까지 확장 가능한 GPU 다중화 방법론.
   - 다양한 데이터 유형의 리소스 요구를 혼합 처리.

3. **GPU와 CPU의 협업 계산**:
   - 비핵심 계산은 CPU로 오프로드하여 GPU의 부하를 줄임.
   - NVIDIA Triton과 같은 프레임워크와 결합.

#### 연구 흐름:
- **다양한 LLM 특화 서빙 시스템**: 모델별 최적화와 GPU-CPU 혼합 서빙 전략을 개발하는 연구가 주목받고 있음.

---

### 결론: 종합적 발전 방향

MuxServe는 GPU 자원 활용을 최적화하는 데 있어 뛰어난 성능을 보였지만, 다음과 같은 연구 방향에서 추가 개선이 가능합니다:

1. **동적 워크로드 대응**: 강화 학습 및 온라인 프로파일링을 통해 실시간 적응성을 강화.
2. **확장성**: 초대규모 클러스터에서 GPU 자원 사용 최적화를 위한 분산 시스템 설계.
3. **리소스 격리**: GPU 메모리 및 계산 자원의 충돌 문제를 해결하기 위한 정교한 자원 관리.
4. **비용 효율성**: 에너지 소비와 운영 비용을 줄이기 위한 지속 가능한 서빙 시스템.
5. **멀티모달 확장**: 멀티모달 LLM 서빙 지원을 통해 다양한 애플리케이션에 적용.

이러한 흐름은 MuxServe와 같은 시스템이 다양한 환경에서도 효과적이고 확장 가능하도록 만드는 데 기여할 것입니다.