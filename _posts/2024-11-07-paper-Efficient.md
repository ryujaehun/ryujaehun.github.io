---
title: Efficient Streaming Language Models with Attention Sinks
categories:
 - paper-review
 - with-gpt
tags:
---


논문 : https://arxiv.org/abs/2309.17453


아래글은 Scholar GPT와 대화를 나눈 내용입니다.

# Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점

# A : 

네, 물론입니다. 

### 논문의 강점과 독창성
1. **어텐션 싱크(Attention Sinks) 개념 도입**: 이 논문은 “어텐션 싱크”라는 새로운 개념을 도입하여, 초기 토큰이 중요한 의미를 갖지 않더라도 어텐션 점수를 흡수하여 모델의 성능을 안정적으로 유지하는 역할을 한다고 설명합니다. 이는 긴 문장에서도 성능 저하 없이 모델이 예측을 유지하도록 돕는 독창적인 접근입니다.
   
2. **메모리를 최소화하는 효율적인 스트리밍**: StreamingLLM은 최근의 슬라이딩 윈도우와 초기 토큰의 어텐션 싱크를 결합해, 메모리 사용량을 줄이면서 모델이 무한히 긴 시퀀스를 처리할 수 있도록 합니다. 이 방법은 모델 재훈련 없이 메모리 효율성을 크게 향상시킵니다.

3. **다양한 LLM과의 호환성**: 이 프레임워크는 Llama-2, MPT, Falcon, Pythia와 같은 인기 모델에서 실험되었으며, 모델의 추론 속도와 효율성에서 큰 개선을 보였습니다. 상대 위치 인코딩을 사용하는 모델과 호환되므로, 실제 응용에서 활용할 수 있는 가능성이 큽니다.

4. **성능 가속화**: StreamingLLM은 기존 방법 대비 최대 22.2배의 속도 향상을 이루었으며, 메모리 사용량도 일정하게 유지하여 대용량 데이터 스트리밍 환경에 적합합니다.

### 핵심 알고리즘 설명 (예시와 함께)
StreamingLLM의 핵심 알고리즘은 어텐션 메커니즘에서 초기 토큰의 **어텐션 싱크 토큰**과 **최근 토큰들의 롤링 캐시(Rolling KV Cache)**를 결합하여 동작합니다. 

예를 들어, 10개의 토큰이 있는 경우, 11번째 토큰을 생성할 때 모델은 초기 몇 개의 토큰을 어텐션 싱크로 유지하고 최근 토큰들을 롤링 캐시에 저장합니다. 이를 통해 모델은 메모리 절약을 위해 전체 시퀀스를 저장하지 않고도 안정적으로 초기 문맥과 최신 정보에 집중할 수 있게 됩니다.

### 논문의 한계점
1. **한정된 문맥 확장**: StreamingLLM은 성능과 메모리 문제를 해결하지만, 모델의 문맥 창(context window)을 실제로 확장하거나 장기 기억 기능을 강화하지는 못합니다. 따라서 문서 전체의 내용을 다뤄야 하는 요약 같은 작업에는 한계가 있습니다.

2. **캐시 크기에 대한 의존성**: 캐시 용량을 초과하는 문맥이 필요한 경우 성능이 떨어질 수 있습니다. 이는 StreamingLLM이 일정 범위 내의 문맥을 다루는 작업에 가장 적합함을 의미합니다.

3. **모델 호환성 필요**: 이 프레임워크는 상대 위치 인코딩을 사용하는 모델에 최적화되어 있어, 다른 인코딩 방식을 사용하는 모델과는 호환성이 떨어질 수 있습니다.

요약하자면, StreamingLLM은 긴 문장 스트리밍 환경에서 메모리와 성능 문제를 해결하기 위한 혁신적이고 효율적인 솔루션을 제공하지만, 장기 문맥 처리가 필요한 응용에는 적합하지 않을 수 있습니다.

# Q : 어텐션 싱크 토큰에 대해서 아주 자세하게 설명해 줄 수 있니?

# A : 

네, 어텐션 싱크 토큰(Attention Sink Token)에 대해 자세히 설명드리겠습니다.

### 어텐션 싱크 토큰의 개념
어텐션 싱크 토큰은 모델의 초기 토큰들이 **어텐션 점수의 “싱크(sink)” 역할**을 하도록 설정한 것입니다. 일반적으로 Transformer 기반의 언어 모델들은 긴 문장을 처리할 때 모든 이전 토큰의 키-값(Key-Value) 쌍을 저장하고 각 토큰이 전체 문맥을 참고하도록 합니다. 이때, **초기 토큰들은 의미와 상관없이 높은 어텐션 점수를 흡수**하는 경향이 있습니다. 이러한 현상을 “어텐션 싱크”라고 부르며, 초기 토큰이 어텐션의 고정 포인트가 되어 모델의 성능을 안정화시키는 역할을 합니다.

### 어텐션 싱크 토큰의 필요성
Transformer 모델이 긴 시퀀스를 처리할 때, 시간이 지남에 따라 특정 토큰이 계속 어텐션을 받는 경향이 있습니다. 특히 **초기 토큰들이 어텐션 점수를 많이 받는 이유**는 어텐션 메커니즘의 소프트맥스(SoftMax) 함수 특성 때문입니다. 소프트맥스는 전체 점수를 1로 만드는 함수인데, 의미 있는 정보가 부족해도 어디엔가 어텐션 점수를 할당해야 하기 때문에, 가장 처음부터 보이는 초기 토큰에 그 점수를 몰아주는 경향이 생깁니다. 이를 통해 모델이 길이가 매우 긴 시퀀스에서도 성능을 유지할 수 있습니다.

### 어텐션 싱크의 동작 방식
StreamingLLM에서는 초기 토큰을 몇 개 유지하여 이들이 **어텐션 싱크로 작용**하도록 합니다. 구체적으로, 초기의 4개 토큰만 남겨두고 나머지 이전의 토큰들은 롤링 캐시(Rolling KV Cache)로 대체하는데, 이 캐시에 담긴 최근의 토큰들이 실제로 언어 생성에 의미 있는 문맥 정보를 제공합니다.

이 초기 4개 토큰은 특별히 중요한 의미를 갖지 않아도, 모델이 자연스럽게 그들에게 많은 어텐션을 할당하여 점수를 분배하는 효과를 얻습니다. 이를 통해 모델은 긴 시퀀스에서도 초기 문맥을 안정적으로 유지하면서도 필요한 최신 문맥에 초점을 맞춰 성능 저하 없이 길고 일관된 응답을 생성할 수 있습니다.

### 어텐션 싱크 토큰의 추가 개선 방법
논문에서는 어텐션 싱크의 역할을 보다 효과적으로 수행하도록 **학습 가능한 “싱크 토큰(Sink Token)”**을 추가하는 방법도 제안합니다. 이 토큰을 통해 모델은 불필요한 어텐션 점수를 이 토큰에 집중하도록 유도할 수 있습니다. 학습 중에 특정 토큰이 항상 어텐션 싱크 역할을 하도록 지정하면, 모델이 필요 없는 어텐션 점수를 무의미한 초기 토큰 대신 이 토큰에 분배하게 됩니다. 이를 통해 더욱 안정적이고 효율적인 스트리밍 성능을 얻을 수 있습니다.

### 어텐션 싱크 토큰의 장점
- **안정성 향상**: 초기 몇 개의 토큰이 어텐션 싱크로 작용함으로써 모델이 긴 시퀀스에서도 일관된 성능을 유지할 수 있습니다.
- **메모리 효율성**: 모든 이전 토큰의 키-값을 저장할 필요 없이 일부 초기 토큰만 남기고 최신 토큰의 슬라이딩 윈도우만 저장하여 메모리 효율이 크게 향상됩니다.
- **확장성**: 학습 가능한 싱크 토큰을 추가할 경우, 모델은 학습 과정에서 이 싱크 토큰에 불필요한 어텐션을 자연스럽게 집중하도록 훈련되어 성능을 더욱 안정화할 수 있습니다.

### 한계점
하지만 어텐션 싱크는 **장기 문맥을 유지하는 데에는 한계**가 있습니다. 초기 토큰을 남겨두어 어텐션을 안정화할 수 있지만, 장기 문맥을 필요로 하는 작업에서는 이러한 접근법만으로는 한계가 있으며, 오랜 문맥을 전체적으로 유지하는 다른 방법과의 결합이 필요할 수 있습니다.

요약하면, 어텐션 싱크 토큰은 Transformer 모델의 어텐션 메커니즘을 개선하여 긴 문장에서도 메모리 효율성과 성능을 유지하는 중요한 역할을 하며, StreamingLLM에서 매우 효과적으로 활용됩니다.

# Q : Rolling KV Cache에 대해서 자세하게 설명해 주겠니?

# A : 

네, Rolling KV Cache에 대해 자세히 설명드리겠습니다.

### Rolling KV Cache의 개념
Rolling KV Cache는 **언어 모델이 긴 텍스트를 스트리밍 방식으로 처리할 때 필요한 정보만 효율적으로 저장하고 사용하는 메커니즘**입니다. Transformer 모델에서는 일반적으로 각 토큰의 키(Key)와 값(Value) 정보를 저장하여 어텐션 계산에 활용합니다. 하지만 긴 텍스트의 모든 토큰을 저장하려면 메모리 사용량이 급증하기 때문에 효율적인 처리가 어려워집니다.

Rolling KV Cache는 이를 해결하기 위해, **가장 최근의 토큰들만 저장하는 슬라이딩 윈도우 방식의 캐시**입니다. 즉, 모델이 새 토큰을 생성할 때마다 이전 토큰 중 가장 오래된 것들을 버리고 새롭게 생성된 토큰의 정보를 추가함으로써, 메모리 사용량을 일정하게 유지하면서도 필요한 최신 문맥을 계속 활용할 수 있습니다.

### Rolling KV Cache의 동작 방식
1. **초기 설정**: 모델은 처음 시퀀스를 처리할 때 모든 토큰의 키-값(Key-Value) 정보를 캐시에 저장합니다. 초기에는 캐시에 여유가 있어 모든 토큰의 정보를 저장할 수 있지만, 이후 시퀀스가 길어지면서 캐시의 용량을 초과할 경우, 오래된 정보를 지우고 새로운 토큰 정보를 추가하게 됩니다.

2. **슬라이딩 윈도우 방식**: 캐시에 저장된 **키-값 쌍의 최대 용량**이 정해져 있으므로, 새로운 토큰이 생성될 때마다 가장 오래된 키-값 쌍을 제거하고, 최신 토큰의 키-값 쌍을 추가합니다. 이렇게 하면 캐시의 크기를 일정하게 유지하면서 최신 문맥에 집중할 수 있게 됩니다.

3. **어텐션 싱크와의 조합**: Rolling KV Cache는 최신 정보만 저장하기 때문에, 어텐션 싱크 토큰과 조합하여 사용합니다. 어텐션 싱크는 초기의 몇 개 토큰을 유지하여 모델이 안정적으로 초기 문맥을 유지하도록 돕고, Rolling KV Cache는 최신 문맥을 담아 최신 정보를 기반으로 언어 생성 작업을 수행합니다.

### 예시를 통한 설명
예를 들어, 캐시의 최대 용량이 4라고 가정하고 모델이 8번째 토큰을 처리 중이라고 해봅시다. 
1. 처음 토큰 1, 2, 3, 4를 처리하면서 키-값 쌍을 캐시에 저장합니다.
2. 5번째 토큰을 처리할 때 캐시가 가득 찼으므로, 가장 오래된 1번 토큰의 키-값을 제거하고, 5번 토큰의 키-값을 추가합니다.
3. 이후 6, 7, 8번 토큰을 처리할 때도 동일한 방식으로 가장 오래된 정보를 삭제하고 새 정보를 추가해 갑니다.

이 과정을 통해 캐시는 항상 최근 4개의 토큰 정보만 유지하게 되며, 모델은 최신 문맥에 대한 정보를 활용하여 예측을 이어갑니다.

### Rolling KV Cache의 장점
1. **메모리 효율성**: 모든 토큰의 키-값을 저장할 필요 없이 최신 토큰의 정보만 유지하기 때문에, 메모리 사용량이 크게 줄어듭니다.
2. **성능 유지**: 어텐션 싱크와 함께 사용하여, 초기 문맥을 유지하면서도 최신 문맥에 집중할 수 있어 긴 텍스트에서도 안정적인 성능을 제공합니다.
3. **실시간 응답 가능**: 모델이 실시간으로 텍스트를 스트리밍 방식으로 처리할 때, 캐시가 오래된 정보를 자동으로 삭제해주기 때문에, 빠른 반응성을 유지할 수 있습니다.

### 한계점
Rolling KV Cache는 최신 문맥을 중심으로 동작하기 때문에, **긴 시간에 걸친 문맥을 유지하는 데 한계**가 있습니다. 이전의 오래된 정보가 필요할 경우, 캐시에 남아있지 않으므로 모델이 이를 참조할 수 없습니다. 따라서 긴 문서를 전체적으로 요약하거나, 긴 시간 동안의 대화 내용을 모두 기억해야 하는 작업에는 적합하지 않습니다.

### 요약
Rolling KV Cache는 모델이 긴 시퀀스를 처리할 때 최근 문맥에 집중하면서도 메모리 효율을 극대화할 수 있도록 돕는 슬라이딩 윈도우 기반의 캐시 메커니즘입니다. 어텐션 싱크와 결합하여 사용될 때, 초기와 최신 문맥을 효과적으로 활용할 수 있어 긴 텍스트 스트리밍 처리에 매우 유용합니다.


# Q: 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘


**MagicPIG 논문**은 Locality Sensitive Hashing (LSH) 기반 샘플링과 이기종 시스템 설계를 통해 LLM의 주의(attention) 계산 효율을 높였지만, 몇 가지 한계도 지니고 있습니다. 이를 해결하기 위한 연구 흐름을 정리하여 설명드리겠습니다.

### 1. LSH 샘플링의 정확도 개선
MagicPIG에서 LSH 샘플링은 유사한 문맥을 가진 토큰을 효율적으로 선택하여 계산량을 줄이지만, 일부 작업에서는 기존의 TopK 방식이 더 높은 정확도를 제공할 수 있습니다. LSH 샘플링의 정확도를 개선하기 위한 연구 방향은 다음과 같습니다.

- **동적 샘플링 기법 개발**: 모든 문맥에서 동일한 수의 샘플을 추출하는 대신, **문맥에 따라 샘플링 비율을 동적으로 조절**하는 연구가 필요합니다. 예를 들어, 복잡한 문장이나 정보가 중요한 문맥에서는 더 많은 샘플을 추출하고, 간단한 문장에서는 적은 샘플을 추출하는 방식으로 정확도를 높일 수 있습니다.
  
- **하이브리드 샘플링 기법**: LSH와 TopK 방식을 결합한 하이브리드 샘플링 기법을 연구할 수 있습니다. 예를 들어, LSH로 후보군을 좁힌 후 TopK 방식으로 가장 중요한 토큰들을 다시 선택하여, 정확도와 효율성 모두를 강화하는 방식입니다.

- **적응형 해시 함수 최적화**: LSH 해시 함수의 파라미터를 자동으로 최적화하거나, 문맥에 따라 적응적으로 해시 함수를 조절하는 방식도 고려할 수 있습니다. 이를 통해 다양한 문맥과 도메인에서 최적의 샘플링 성능을 제공할 수 있습니다.

### 2. 이기종 시스템 설계의 단순화 및 범용화
MagicPIG는 이기종 시스템(CPU-GPU)을 활용하여 연산 자원을 분리하고 최적화하지만, 이로 인해 시스템 구현이 복잡해지고 하드웨어 의존성이 높아지는 단점이 있습니다. 이를 극복하기 위한 연구 흐름은 다음과 같습니다.

- **단일 하드웨어 최적화 연구**: CPU와 GPU를 분리해 사용하지 않고도 동일한 성능을 달성할 수 있는 **단일 하드웨어 최적화**를 연구하는 방향입니다. 예를 들어, GPU만으로도 연산 및 샘플링 작업을 모두 수행할 수 있는 최적화 기법을 개발하면 하드웨어 의존성을 낮출 수 있습니다. 이를 위해 GPU에서 LSH 샘플링과 주의 계산을 동시에 수행할 수 있는 고효율 커널을 개발하는 연구가 필요합니다.
  
- **FPGA 및 ASIC 기반 최적화**: LSH 샘플링과 주의 계산을 동시에 수행할 수 있는 FPGA(Field-Programmable Gate Array)나 ASIC(Application-Specific Integrated Circuit)을 활용한 특화 하드웨어 설계를 연구할 수 있습니다. 이러한 하드웨어는 GPU보다 전력 효율이 높고 대규모 병렬 처리가 가능하므로, LSH 기반 시스템의 범용성과 효율성을 높이는 데 기여할 수 있습니다.

- **가상화 및 클라우드 기반 시스템 지원**: LSH 샘플링과 이기종 시스템 설계를 클라우드 환경에 적합하도록 설계하는 연구도 필요합니다. 예를 들어, 가상 머신(VM)이나 컨테이너를 활용하여 하드웨어 종속성을 줄이고, 클라우드 기반에서 손쉽게 확장할 수 있는 구조를 개발할 수 있습니다.

### 3. 메모리 사용량 최적화
긴 문맥에서 MagicPIG의 LSH 샘플링은 메모리 사용을 효율화하지만, 여전히 대규모 데이터에서 메모리 부담이 존재합니다. 이를 줄이기 위한 메모리 최적화 연구가 필요합니다.

- **압축 알고리즘을 통한 메모리 최적화**: LSH 해시 테이블의 메모리 사용을 줄이기 위해 **벡터 압축 알고리즘**을 적용할 수 있습니다. 예를 들어, 중요도가 낮은 키-값 쌍을 압축하거나, 유사한 키-값 쌍을 하나의 벡터로 통합하여 메모리 사용량을 줄일 수 있습니다.
  
- **온디맨드 메모리 할당**: 사용자가 요청한 특정 문맥에 대해서만 메모리를 할당하고, 필요하지 않은 경우 메모리를 해제하는 **온디맨드 메모리 관리 방식**을 연구할 수 있습니다. 이는 특히 긴 문맥에서의 메모리 사용량을 효율적으로 관리하는 데 기여할 수 있습니다.

- **외부 메모리와의 하이브리드 처리**: 긴 문맥을 필요로 하는 대규모 LLM 작업에서는 GPU 메모리 한계를 넘어서는 경우가 많으므로, **외부 메모리(DRAM 또는 디스크)를 활용하는 하이브리드 시스템**을 구축할 수 있습니다. 외부 메모리에 저장된 키-값 쌍을 필요할 때만 불러와 사용하면 메모리 부담을 크게 줄일 수 있습니다.

### 4. 실시간 성능 최적화
MagicPIG는 효율적인 샘플링과 주의 계산을 통해 성능을 높였지만, 실제 실시간 응용에서의 성능을 최적화하기 위한 추가 연구가 필요합니다.

- **지연 최소화를 위한 캐시 최적화**: 자주 사용되는 샘플이나 키-값 쌍을 캐시에 저장하여 **반복적인 계산을 줄이고 응답 시간을 단축**할 수 있습니다. 예를 들어, 캐시 계층을 구축하여 자주 사용되는 문맥을 빠르게 불러오는 방식으로 실시간 성능을 높일 수 있습니다.
  
- **빠른 전송을 위한 데이터 전송 최적화**: CPU와 GPU 간의 데이터 전송 지연을 줄이기 위해, **빠른 전송 기법을 적용**하여 실시간 성능을 높일 수 있습니다. 데이터 패킹이나 압축을 통해 전송 속도를 높이거나, CPU와 GPU가 직접적으로 데이터를 공유할 수 있는 구조를 구축할 수 있습니다.

- **프리페칭(pre-fetching) 기법 적용**: 사용자가 자주 요청할 것으로 예상되는 토큰이나 문맥을 사전에 GPU 메모리에 불러오는 프리페칭 기법을 통해, **실시간 성능을 높이는 연구**가 유망할 수 있습니다.

### 5. 다양화된 샘플링 기법 연구
LSH 기반 샘플링은 특정 문맥에 적합한 유사한 토큰을 선택하는 데 효과적이지만, 다양한 작업이나 도메인에서는 다른 샘플링 기법이 더 효율적일 수 있습니다.

- **적응형 샘플링 기법 개발**: 모델이 문맥에 따라 **동적으로 샘플링 방식을 선택**할 수 있는 적응형 샘플링 기법을 개발할 수 있습니다. 예를 들어, 특정 작업에서는 TopK 방식을 사용하고, 다른 작업에서는 LSH를 사용하는 방식으로 문맥에 맞는 최적의 샘플링 방법을 활용할 수 있습니다.
  
- **다층 해시 테이블 설계**: LSH의 기본 해시 테이블을 다층으로 구성하여, 상위 해시 테이블에서는 거친 샘플링을 수행하고 하위 해시 테이블에서는 정밀한 샘플링을 수행하는 구조를 연구할 수 있습니다. 이를 통해 다양한 문맥과 도메인에서 효율적인 샘플링이 가능하게 됩니다.

- **문맥 인식 샘플링 연구**: 특정 작업에 따라 문맥을 인식하고, 필요에 따라 더 많은 정보를 포함하거나 더 적은 정보를 포함하는 샘플링 방식을 개발할 수 있습니다. 이를 통해 다양한 도메인과 상황에서의 응답 품질을 높일 수 있습니다.

### 6. 다중 도메인 확장을 위한 학습 연구
MagicPIG는 현재 특정 도메인과 문맥에 최적화된 방식으로 작동하기 때문에 다중 도메인에 대응하기 위한 학습 연구가 필요합니다.

- **다중 도메인 적응 모델**: MagicPIG가 각 도메인에 맞는 최적의 샘플링과 해시 설정을 학습할 수 있는 **다중 도메인 적응 모델**을 개발할 수 있습니다. 이를 통해 다양한 도메인에서 유연하게 사용할 수 있습니다.

- **도메인 간 전이 학습**: 특정 도메인에서 학습된 샘플링 기법과 주의 계산 결과를 다른 도메인에 전이하여 활용하는 연구도 필요합니다. 이를 통해 다양한 도메인에 걸쳐 MagicPIG를 적용할 수 있습니다.

---

### 요약
MagicPIG의 한계를 극복하기 위한 주요 연구 흐름은 다음과 같습니다:
1. **LSH 샘플링의 정확도 개선**: 동적 샘플링, 하이브리드 기법, 적응형 해시 함수를 통한 성능 향상.
