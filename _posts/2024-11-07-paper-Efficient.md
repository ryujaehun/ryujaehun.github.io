---
title: Efficient Streaming Language Models with Attention Sinks
categories:
 - paper-review
 - with-gpt
tags:
---


논문 : https://arxiv.org/abs/2309.17453


아래글은 Scholar GPT와 대화를 나눈 내용입니다.

# Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점

# A : 

네, 물론입니다. 

### 논문의 강점과 독창성
1. **어텐션 싱크(Attention Sinks) 개념 도입**: 이 논문은 “어텐션 싱크”라는 새로운 개념을 도입하여, 초기 토큰이 중요한 의미를 갖지 않더라도 어텐션 점수를 흡수하여 모델의 성능을 안정적으로 유지하는 역할을 한다고 설명합니다. 이는 긴 문장에서도 성능 저하 없이 모델이 예측을 유지하도록 돕는 독창적인 접근입니다.
   
2. **메모리를 최소화하는 효율적인 스트리밍**: StreamingLLM은 최근의 슬라이딩 윈도우와 초기 토큰의 어텐션 싱크를 결합해, 메모리 사용량을 줄이면서 모델이 무한히 긴 시퀀스를 처리할 수 있도록 합니다. 이 방법은 모델 재훈련 없이 메모리 효율성을 크게 향상시킵니다.

3. **다양한 LLM과의 호환성**: 이 프레임워크는 Llama-2, MPT, Falcon, Pythia와 같은 인기 모델에서 실험되었으며, 모델의 추론 속도와 효율성에서 큰 개선을 보였습니다. 상대 위치 인코딩을 사용하는 모델과 호환되므로, 실제 응용에서 활용할 수 있는 가능성이 큽니다.

4. **성능 가속화**: StreamingLLM은 기존 방법 대비 최대 22.2배의 속도 향상을 이루었으며, 메모리 사용량도 일정하게 유지하여 대용량 데이터 스트리밍 환경에 적합합니다.

### 핵심 알고리즘 설명 (예시와 함께)
StreamingLLM의 핵심 알고리즘은 어텐션 메커니즘에서 초기 토큰의 **어텐션 싱크 토큰**과 **최근 토큰들의 롤링 캐시(Rolling KV Cache)**를 결합하여 동작합니다. 

예를 들어, 10개의 토큰이 있는 경우, 11번째 토큰을 생성할 때 모델은 초기 몇 개의 토큰을 어텐션 싱크로 유지하고 최근 토큰들을 롤링 캐시에 저장합니다. 이를 통해 모델은 메모리 절약을 위해 전체 시퀀스를 저장하지 않고도 안정적으로 초기 문맥과 최신 정보에 집중할 수 있게 됩니다.

### 논문의 한계점
1. **한정된 문맥 확장**: StreamingLLM은 성능과 메모리 문제를 해결하지만, 모델의 문맥 창(context window)을 실제로 확장하거나 장기 기억 기능을 강화하지는 못합니다. 따라서 문서 전체의 내용을 다뤄야 하는 요약 같은 작업에는 한계가 있습니다.

2. **캐시 크기에 대한 의존성**: 캐시 용량을 초과하는 문맥이 필요한 경우 성능이 떨어질 수 있습니다. 이는 StreamingLLM이 일정 범위 내의 문맥을 다루는 작업에 가장 적합함을 의미합니다.

3. **모델 호환성 필요**: 이 프레임워크는 상대 위치 인코딩을 사용하는 모델에 최적화되어 있어, 다른 인코딩 방식을 사용하는 모델과는 호환성이 떨어질 수 있습니다.

요약하자면, StreamingLLM은 긴 문장 스트리밍 환경에서 메모리와 성능 문제를 해결하기 위한 혁신적이고 효율적인 솔루션을 제공하지만, 장기 문맥 처리가 필요한 응용에는 적합하지 않을 수 있습니다.

# Q : 어텐션 싱크 토큰에 대해서 아주 자세하게 설명해 줄 수 있니?

# A : 

네, 어텐션 싱크 토큰(Attention Sink Token)에 대해 자세히 설명드리겠습니다.

### 어텐션 싱크 토큰의 개념
어텐션 싱크 토큰은 모델의 초기 토큰들이 **어텐션 점수의 “싱크(sink)” 역할**을 하도록 설정한 것입니다. 일반적으로 Transformer 기반의 언어 모델들은 긴 문장을 처리할 때 모든 이전 토큰의 키-값(Key-Value) 쌍을 저장하고 각 토큰이 전체 문맥을 참고하도록 합니다. 이때, **초기 토큰들은 의미와 상관없이 높은 어텐션 점수를 흡수**하는 경향이 있습니다. 이러한 현상을 “어텐션 싱크”라고 부르며, 초기 토큰이 어텐션의 고정 포인트가 되어 모델의 성능을 안정화시키는 역할을 합니다.

### 어텐션 싱크 토큰의 필요성
Transformer 모델이 긴 시퀀스를 처리할 때, 시간이 지남에 따라 특정 토큰이 계속 어텐션을 받는 경향이 있습니다. 특히 **초기 토큰들이 어텐션 점수를 많이 받는 이유**는 어텐션 메커니즘의 소프트맥스(SoftMax) 함수 특성 때문입니다. 소프트맥스는 전체 점수를 1로 만드는 함수인데, 의미 있는 정보가 부족해도 어디엔가 어텐션 점수를 할당해야 하기 때문에, 가장 처음부터 보이는 초기 토큰에 그 점수를 몰아주는 경향이 생깁니다. 이를 통해 모델이 길이가 매우 긴 시퀀스에서도 성능을 유지할 수 있습니다.

### 어텐션 싱크의 동작 방식
StreamingLLM에서는 초기 토큰을 몇 개 유지하여 이들이 **어텐션 싱크로 작용**하도록 합니다. 구체적으로, 초기의 4개 토큰만 남겨두고 나머지 이전의 토큰들은 롤링 캐시(Rolling KV Cache)로 대체하는데, 이 캐시에 담긴 최근의 토큰들이 실제로 언어 생성에 의미 있는 문맥 정보를 제공합니다.

이 초기 4개 토큰은 특별히 중요한 의미를 갖지 않아도, 모델이 자연스럽게 그들에게 많은 어텐션을 할당하여 점수를 분배하는 효과를 얻습니다. 이를 통해 모델은 긴 시퀀스에서도 초기 문맥을 안정적으로 유지하면서도 필요한 최신 문맥에 초점을 맞춰 성능 저하 없이 길고 일관된 응답을 생성할 수 있습니다.

### 어텐션 싱크 토큰의 추가 개선 방법
논문에서는 어텐션 싱크의 역할을 보다 효과적으로 수행하도록 **학습 가능한 “싱크 토큰(Sink Token)”**을 추가하는 방법도 제안합니다. 이 토큰을 통해 모델은 불필요한 어텐션 점수를 이 토큰에 집중하도록 유도할 수 있습니다. 학습 중에 특정 토큰이 항상 어텐션 싱크 역할을 하도록 지정하면, 모델이 필요 없는 어텐션 점수를 무의미한 초기 토큰 대신 이 토큰에 분배하게 됩니다. 이를 통해 더욱 안정적이고 효율적인 스트리밍 성능을 얻을 수 있습니다.

### 어텐션 싱크 토큰의 장점
- **안정성 향상**: 초기 몇 개의 토큰이 어텐션 싱크로 작용함으로써 모델이 긴 시퀀스에서도 일관된 성능을 유지할 수 있습니다.
- **메모리 효율성**: 모든 이전 토큰의 키-값을 저장할 필요 없이 일부 초기 토큰만 남기고 최신 토큰의 슬라이딩 윈도우만 저장하여 메모리 효율이 크게 향상됩니다.
- **확장성**: 학습 가능한 싱크 토큰을 추가할 경우, 모델은 학습 과정에서 이 싱크 토큰에 불필요한 어텐션을 자연스럽게 집중하도록 훈련되어 성능을 더욱 안정화할 수 있습니다.

### 한계점
하지만 어텐션 싱크는 **장기 문맥을 유지하는 데에는 한계**가 있습니다. 초기 토큰을 남겨두어 어텐션을 안정화할 수 있지만, 장기 문맥을 필요로 하는 작업에서는 이러한 접근법만으로는 한계가 있으며, 오랜 문맥을 전체적으로 유지하는 다른 방법과의 결합이 필요할 수 있습니다.

요약하면, 어텐션 싱크 토큰은 Transformer 모델의 어텐션 메커니즘을 개선하여 긴 문장에서도 메모리 효율성과 성능을 유지하는 중요한 역할을 하며, StreamingLLM에서 매우 효과적으로 활용됩니다.

# Q : Rolling KV Cache에 대해서 자세하게 설명해 주겠니?

# A : 
네, Rolling KV Cache에 대해 자세히 설명드리겠습니다.

### Rolling KV Cache의 개념
Rolling KV Cache는 **언어 모델이 긴 텍스트를 스트리밍 방식으로 처리할 때 필요한 정보만 효율적으로 저장하고 사용하는 메커니즘**입니다. Transformer 모델에서는 일반적으로 각 토큰의 키(Key)와 값(Value) 정보를 저장하여 어텐션 계산에 활용합니다. 하지만 긴 텍스트의 모든 토큰을 저장하려면 메모리 사용량이 급증하기 때문에 효율적인 처리가 어려워집니다.

Rolling KV Cache는 이를 해결하기 위해, **가장 최근의 토큰들만 저장하는 슬라이딩 윈도우 방식의 캐시**입니다. 즉, 모델이 새 토큰을 생성할 때마다 이전 토큰 중 가장 오래된 것들을 버리고 새롭게 생성된 토큰의 정보를 추가함으로써, 메모리 사용량을 일정하게 유지하면서도 필요한 최신 문맥을 계속 활용할 수 있습니다.

### Rolling KV Cache의 동작 방식
1. **초기 설정**: 모델은 처음 시퀀스를 처리할 때 모든 토큰의 키-값(Key-Value) 정보를 캐시에 저장합니다. 초기에는 캐시에 여유가 있어 모든 토큰의 정보를 저장할 수 있지만, 이후 시퀀스가 길어지면서 캐시의 용량을 초과할 경우, 오래된 정보를 지우고 새로운 토큰 정보를 추가하게 됩니다.

2. **슬라이딩 윈도우 방식**: 캐시에 저장된 **키-값 쌍의 최대 용량**이 정해져 있으므로, 새로운 토큰이 생성될 때마다 가장 오래된 키-값 쌍을 제거하고, 최신 토큰의 키-값 쌍을 추가합니다. 이렇게 하면 캐시의 크기를 일정하게 유지하면서 최신 문맥에 집중할 수 있게 됩니다.

3. **어텐션 싱크와의 조합**: Rolling KV Cache는 최신 정보만 저장하기 때문에, 어텐션 싱크 토큰과 조합하여 사용합니다. 어텐션 싱크는 초기의 몇 개 토큰을 유지하여 모델이 안정적으로 초기 문맥을 유지하도록 돕고, Rolling KV Cache는 최신 문맥을 담아 최신 정보를 기반으로 언어 생성 작업을 수행합니다.

### 예시를 통한 설명
예를 들어, 캐시의 최대 용량이 4라고 가정하고 모델이 8번째 토큰을 처리 중이라고 해봅시다. 
1. 처음 토큰 1, 2, 3, 4를 처리하면서 키-값 쌍을 캐시에 저장합니다.
2. 5번째 토큰을 처리할 때 캐시가 가득 찼으므로, 가장 오래된 1번 토큰의 키-값을 제거하고, 5번 토큰의 키-값을 추가합니다.
3. 이후 6, 7, 8번 토큰을 처리할 때도 동일한 방식으로 가장 오래된 정보를 삭제하고 새 정보를 추가해 갑니다.

이 과정을 통해 캐시는 항상 최근 4개의 토큰 정보만 유지하게 되며, 모델은 최신 문맥에 대한 정보를 활용하여 예측을 이어갑니다.

### Rolling KV Cache의 장점
1. **메모리 효율성**: 모든 토큰의 키-값을 저장할 필요 없이 최신 토큰의 정보만 유지하기 때문에, 메모리 사용량이 크게 줄어듭니다.
2. **성능 유지**: 어텐션 싱크와 함께 사용하여, 초기 문맥을 유지하면서도 최신 문맥에 집중할 수 있어 긴 텍스트에서도 안정적인 성능을 제공합니다.
3. **실시간 응답 가능**: 모델이 실시간으로 텍스트를 스트리밍 방식으로 처리할 때, 캐시가 오래된 정보를 자동으로 삭제해주기 때문에, 빠른 반응성을 유지할 수 있습니다.

### 한계점
Rolling KV Cache는 최신 문맥을 중심으로 동작하기 때문에, **긴 시간에 걸친 문맥을 유지하는 데 한계**가 있습니다. 이전의 오래된 정보가 필요할 경우, 캐시에 남아있지 않으므로 모델이 이를 참조할 수 없습니다. 따라서 긴 문서를 전체적으로 요약하거나, 긴 시간 동안의 대화 내용을 모두 기억해야 하는 작업에는 적합하지 않습니다.

### 요약
Rolling KV Cache는 모델이 긴 시퀀스를 처리할 때 최근 문맥에 집중하면서도 메모리 효율을 극대화할 수 있도록 돕는 슬라이딩 윈도우 기반의 캐시 메커니즘입니다. 어텐션 싱크와 결합하여 사용될 때, 초기와 최신 문맥을 효과적으로 활용할 수 있어 긴 텍스트 스트리밍 처리에 매우 유용합니다.