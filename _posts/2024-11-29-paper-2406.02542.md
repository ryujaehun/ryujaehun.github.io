---
title: "Loki: Low-rank Keys for Efficient Sparse Attention"
date: 2024-11-29
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2406.02542)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

논문 **"Loki: Low-rank Keys for Efficient Sparse Attention"**에 대한 상세한 분석을 제공하겠습니다. 이 논문은 자가회귀 기반의 대규모 언어 모델(LLM)에서 사용되는 **자가주의 메커니즘의 계산 및 메모리 병목 현상을 개선**하기 위한 새로운 방법론을 제안합니다.

---

### **강점과 독창성**
1. **자연어 처리 모델의 저차원 구조 활용**:
   - 논문은 대규모 언어 모델에서 생성된 Key 벡터들이 본래 차원보다 훨씬 낮은 차원 공간에 위치한다는 점을 관찰하고 이를 이론적으로 분석합니다.
   - 이 저차원 구조는 다양한 데이터셋과 모델에서 일관성을 가지며, 이를 활용한 Sparse Attention을 구현합니다.

2. **Loki의 핵심 기법**:
   - Key 벡터를 PCA(Principal Component Analysis)로 변환하여 일부 주요 성분만 사용해 근사적인 주의 점수를 계산합니다.
   - 최적화된 Top-K 토큰을 선택하고, 선택된 토큰에 대해 전체 차원을 복원하여 최종 점수를 계산함으로써 효율성과 정확성을 동시에 유지합니다.

3. **효율적인 구현**:
   - Sparse Attention을 GPU에 최적화된 Triton 커널을 활용하여 실질적인 성능 개선을 제공합니다.
   - Llama2-13B 모델 기준으로 최대 45%의 속도 향상을 달성하면서도 정확도 손실은 평균 6.8%에 불과합니다.

---

### **핵심 알고리즘 설명 (예시 입력과 함께)**

#### **Loki의 알고리즘 과정**:
1. **입력**:
   - Query \( q_S \)와 Key \( K:S \), Value \( V:S \) (현재와 과거 토큰 포함).
   - PCA를 통해 생성된 변환 행렬 \( P \).
   - 설정된 차원 축소 비율 \( d \)와 Top-K 비율 \( k \).

2. **단계별 프로세스**:
   - **PCA 변환**:
     - Query와 Key를 \( P \)에 의해 저차원 공간으로 변환: 
       \[
       q_S' = q_S P, \quad K:S' = K:S P
       \]
   - **근사적 주의 점수 계산**:
     - \( d \) 차원의 Key와 Query를 사용하여 점수 계산:
       \[
       a_{\text{approx}} = q_S' K:S'^{T}
       \]
   - **Top-K 선택**:
     - \( a_{\text{approx}} \)에서 가장 큰 \( k \)개의 점수에 해당하는 인덱스를 선택.
   - **최종 점수 계산**:
     - 선택된 \( k \)개의 Key와 원래 차원을 사용해 정확한 점수 계산:
       \[
       a_{\text{exact}} = \text{softmax}\left( \frac{q_S K:S^{T}}{\sqrt{D}} \right)
       \]
   - **결과 출력**:
     - 선택된 Value와 곱하여 최종 결과 반환:
       \[
       y = a_{\text{exact}} V'
       \]

#### **예제 입력**:
- \( q_S = [0.5, 0.2, 0.8] \), \( K:S = \begin{bmatrix} 0.1 & 0.3 & 0.7 \\ 0.5 & 0.2 & 0.4 \end{bmatrix} \), \( V:S = \begin{bmatrix} 1.0 \\ 0.5 \end{bmatrix} \), \( P = \begin{bmatrix} 0.6 & 0.8 \\ 0.8 & -0.6 \end{bmatrix} \).

#### **과정 시뮬레이션**:
- PCA 변환 후 저차원 Key, Query 계산:
  \[
  q_S' = q_S P = [0.68, 0.56], \quad K:S' = K:S P = \begin{bmatrix} 0.54 & 0.12 \\ 0.42 & 0.16 \end{bmatrix}.
  \]
- 근사적 주의 점수 계산:
  \[
  a_{\text{approx}} = q_S' K:S'^{T} = [0.75, 0.58].
  \]
- Top-K 선택 (1개 선택):
  \[
  \text{Index of Top-K: } [0].
  \]
- 최종 점수 계산 및 결과:
  \[
  a_{\text{exact}} = \text{softmax}\left(\frac{[0.75]}{\sqrt{3}}\right), \quad y = a_{\text{exact}} V' = 1.0.
  \]

---

### **한계점**
1. **메모리 사용량 최적화 부족**:
   - KV-cache 삭제를 활용하지 않으므로 메모리 사용량은 기존 Attention과 유사합니다.
   - Token eviction과의 결합 시 더 큰 최적화를 기대할 수 있습니다.

2. **Top-K 연산 병목**:
   - Top-K 토큰 선택 과정의 비용이 높은 경우 실질적인 속도 향상이 제한될 수 있습니다.

3. **모델 및 데이터 의존성**:
   - PCA 기반의 차원 축소는 모델과 데이터의 특성에 따라 성능 차이가 날 가능성이 있습니다. 특히, 특정 데이터세트에 의존하는 PCA 변환은 일반화에 영향을 줄 수 있습니다.

4. **FlashAttention과의 통합 필요**:
   - FlashAttention 등 최적화된 Attention 커널과 결합이 이루어지지 않았으며, 이를 통해 추가적인 성능 향상이 가능합니다.

---

Loki는 대규모 언어 모델의 효율적인 추론을 위한 강력한 Sparse Attention 기법으로, 계산 비용과 메모리 병목 현상을 동시에 줄이는 데 성공적인 접근법을 제시합니다. 다만, 메모리 최적화와 기존 최적화 커널과의 통합 등 추가적인 연구가 필요합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

### 논문 Loki의 핵심 알고리즘: Low-Dimensional Sparse Attention
---

#### **Loki 알고리즘의 핵심 아이디어**
Loki는 대규모 언어 모델(LLM)에서 **자가 주의 메커니즘(Self-Attention)**의 계산 비용을 줄이고 효율성을 높이는 Sparse Attention 알고리즘입니다. 이 알고리즘은 Key 벡터의 **저차원 구조**를 활용하여 토큰을 동적으로 선택하고, 필요한 계산을 수행합니다.

---

### **알고리즘 과정과 상세 예시**
Loki는 **차원 축소(PCA)**와 **Top-K 토큰 선택**을 결합하여 Sparse Attention을 수행합니다. 이를 상세히 설명하기 위해, 예시를 사용해 단계별로 과정을 분석합니다.

#### **입력 데이터 정의**
1. Query \( q_S \): 현재 단계의 Query 벡터 (차원: \( D \)).
2. Key \( K:S \): 현재까지 생성된 Key 벡터 집합 (차원: \( S \times D \)).
3. Value \( V:S \): Key에 대응하는 Value 벡터 집합 (차원: \( S \times D \)).
4. PCA 변환 행렬 \( P \): \( D \times D \) 크기의 행렬로, Key 벡터를 저차원 공간으로 변환.
5. 차원 축소 비율 \( d \): PCA로 줄일 차원의 비율 (\( d \times D \)).
6. Top-K 비율 \( k \): 선택할 중요한 토큰의 비율 (\( k \times S \)).

---

### **예시**
#### 입력값
- Query 벡터: \( q_S = [0.5, 0.2, 0.8] \) (\( D = 3 \)).
- Key 벡터:
  \[
  K:S = \begin{bmatrix}
  0.1 & 0.3 & 0.7 \\
  0.5 & 0.2 & 0.4 \\
  0.3 & 0.7 & 0.6
  \end{bmatrix} \quad (S = 3, D = 3).
  \]
- Value 벡터:
  \[
  V:S = \begin{bmatrix}
  1.0 \\
  0.5 \\
  0.8
  \end{bmatrix}.
  \]
- PCA 변환 행렬:
  \[
  P = \begin{bmatrix}
  0.6 & 0.8 & 0.0 \\
  0.8 & -0.6 & 0.0 \\
  0.0 & 0.0 & 1.0
  \end{bmatrix}.
  \]
- 차원 축소 \( d = 2 \), Top-K 비율 \( k = 2 \).

---

### **단계 1: PCA 변환을 통한 차원 축소**
1. Query와 Key 벡터를 PCA 행렬 \( P \)로 변환하여 저차원 표현으로 변환합니다.
   - Query 변환:
     \[
     q_S' = q_S \cdot P = [0.5, 0.2, 0.8] \cdot \begin{bmatrix}
     0.6 & 0.8 & 0.0 \\
     0.8 & -0.6 & 0.0 \\
     0.0 & 0.0 & 1.0
     \end{bmatrix}
     = [0.52, 0.28, 0.8].
     \]
   - Key 벡터 변환:
     \[
     K:S' = K:S \cdot P = \begin{bmatrix}
     0.1 & 0.3 & 0.7 \\
     0.5 & 0.2 & 0.4 \\
     0.3 & 0.7 & 0.6
     \end{bmatrix}
     \cdot \begin{bmatrix}
     0.6 & 0.8 & 0.0 \\
     0.8 & -0.6 & 0.0 \\
     0.0 & 0.0 & 1.0
     \end{bmatrix}
     = \begin{bmatrix}
     0.3 & 0.18 & 0.7 \\
     0.38 & 0.26 & 0.4 \\
     0.54 & -0.14 & 0.6
     \end{bmatrix}.
     \]

---

### **단계 2: 근사적 Attention 점수 계산**
2. 저차원 Query와 Key를 사용하여 근사적인 Attention 점수를 계산합니다.
   \[
   a_{\text{approx}} = q_S' \cdot K:S'^T = [0.52, 0.28, 0.8] \cdot
   \begin{bmatrix}
   0.3 & 0.38 & 0.54 \\
   0.18 & 0.26 & -0.14 \\
   0.7 & 0.4 & 0.6
   \end{bmatrix}
   = [0.754, 0.656, 0.816].
   \]

---

### **단계 3: Top-K 토큰 선택**
3. 근사적 Attention 점수 \( a_{\text{approx}} \)를 기준으로 가장 중요한 \( k \)개의 토큰을 선택합니다.
   - \( k = 2 \)이므로, 상위 2개의 점수에 해당하는 인덱스는 [2, 0]입니다.

---

### **단계 4: 최종 Attention 점수 계산**
4. 선택된 토큰에 대해 원래 차원에서 정확한 Attention 점수를 계산합니다.
   - 선택된 Key:
     \[
     K_{\text{selected}} = \begin{bmatrix}
     0.3 & 0.7 & 0.6 \\
     0.1 & 0.3 & 0.7
     \end{bmatrix}.
     \]
   - 최종 Attention 점수 계산:
     \[
     a_{\text{exact}} = \text{softmax}\left(\frac{q_S \cdot K_{\text{selected}}^T}{\sqrt{D}}\right).
     \]
     계산 후 \( a_{\text{exact}} = [0.6, 0.4] \).

---

### **단계 5: 결과 생성**
5. 최종 Attention 점수와 Value를 곱해 결과를 생성합니다.
   \[
   y = a_{\text{exact}} \cdot V_{\text{selected}} = [0.6, 0.4] \cdot \begin{bmatrix}
   0.8 \\
   1.0
   \end{bmatrix}
   = 0.88.
   \]

---

### **알고리즘 정리**
- **핵심 단계**:
  1. PCA 변환으로 Query 및 Key의 차원 축소.
  2. 저차원 공간에서 근사적 Attention 점수 계산.
  3. Top-K 토큰 선택.
  4. 선택된 토큰에 대해 정확한 Attention 점수 계산.
  5. 최종 결과 생성.

- **장점**:
  - 계산 효율성: 저차원에서 근사적 계산 수행.
  - 정확도 유지: Top-K 토큰에 대해 최종 계산 수행.

- **한계**:
  - 추가적인 Top-K 선택 연산이 병목이 될 수 있음.
  - KV-cache의 메모리 사용량은 줄지 않음.

Loki는 이러한 과정을 통해 **최대 45%의 속도 향상**을 달성하면서도 모델의 성능 손실을 최소화합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### **논문 결과 요약**
논문 "Loki: Low-rank Keys for Efficient Sparse Attention"에서 제시한 결과는 다음과 같은 주요 성과를 포함합니다.

---

#### **1. Loki의 성능 평가**
- **속도 향상**:
  - Llama2-13B 모델 기준으로 **45%의 주의 메커니즘 속도 향상**.
  - 저차원 PCA 공간에서 Top-K를 활용한 효율적 계산으로 연산 비용 감소.
- **모델 성능 유지**:
  - 평균적으로 기존 Self-Attention 대비 **6.8%의 성능 감소**만 관찰됨.
  - 기존 Sparse Attention 방식(H2O, Exact-TopK 등) 대비 낮은 성능 감소율.

---

#### **2. 비교 분석**
- **Sparse Attention 기법과의 비교**:
  - **H2O**:
    - H2O는 토큰을 영구적으로 삭제(Eviction)하여 메모리 사용량 감소를 목표로 하지만, 이로 인해 중요한 정보가 손실됨.
    - Loki는 토큰을 삭제하지 않고, 저차원 공간에서 Top-K를 선택하므로 성능 감소가 적음.
  - **Exact-TopK**:
    - Exact-TopK는 정확한 Attention 점수를 계산한 후 Top-K를 선택하므로 Loki보다 메모리와 계산 비용이 큼.
    - Loki는 근사적 Attention 점수로 선택 과정을 간소화하여 비용을 줄임.
- **LongBench 및 Downstream Task**:
  - 다양한 자연어 처리 벤치마크에서 Loki는 Full Attention과 거의 유사한 성능을 달성.

---

#### **3. Loki의 독창적 기여**
- **Low-Rank Key 구조의 발견**:
  - Key 벡터의 저차원 특성을 최초로 심층 분석하여 다양한 모델과 데이터셋에서의 일관성을 입증.
  - 저차원 PCA 기반 변환을 통해 계산 비용을 대폭 절감.
- **PCA 기반의 Sparse Attention**:
  - 기존 Sparse Attention 방법론들이 특정 패턴이나 토큰 삭제에 의존하는 반면, Loki는 Key 벡터의 본질적 구조를 활용.
  - 정확한 Attention 계산과 성능 저하 간의 균형을 이룸.

---

### **특출난 점**
1. **이론적 기반의 설계**:
   - Loki는 Key 벡터가 본래 차원보다 훨씬 낮은 공간에 위치한다는 관찰에 기초합니다. 이는 PCA 분석을 통해 입증되었으며, 다양한 데이터셋과 모델에서 일관된 결과를 보였습니다.
   - 저차원 특성을 활용한 Sparse Attention은 기존 방법론들이 직면한 메모리 삭제 및 정보 손실 문제를 해결했습니다.

2. **효율성과 정확성의 균형**:
   - Loki는 기존 Self-Attention의 정확성을 거의 유지하면서도 계산 효율성을 극대화했습니다.
   - 특히, **45%의 속도 향상**과 **낮은 성능 손실(6.8%)**은 Sparse Attention 기법들(H2O, Exact-TopK)과 비교해 돋보이는 성과입니다.

3. **단순성과 확장성**:
   - Loki는 별도의 학습이나 모델 수정 없이 기존 모델에 통합 가능.
   - KV-cache 삭제와의 결합을 통해 메모리와 계산의 복합 최적화를 기대할 수 있음.

---

### **논문에서 제기한 결과 도출 원인**
논문에서 Loki의 우수한 성능을 도출한 이유는 다음과 같습니다.

1. **Key 벡터의 저차원 구조 활용**:
   - PCA로 Key 벡터의 저차원 구조를 효율적으로 모델링.
   - 근사 Attention 점수를 계산할 때, 전체 차원을 사용할 필요 없이 \(d\)-차원만 사용.
   - 이는 계산 비용과 데이터 이동 비용을 줄이는 핵심 원인.

2. **Top-K 기반의 동적 토큰 선택**:
   - 특정 패턴을 고정하는 대신, 매 단계마다 가장 중요한 \(k\)-개의 토큰을 선택.
   - 최적의 토큰을 선택하면서도 불필요한 연산을 피함.

3. **최적화된 구현**:
   - GPU 환경에서 Triton 커널을 사용하여 Sparse Attention의 메모리 병목을 해결.
   - 저차원 계산 및 선택 작업을 병렬화하여 실질적인 속도 향상 도출.

---

### **나의 생각**
Loki의 결과는 Sparse Attention 기법의 발전에서 중요한 진전을 보여줍니다. 논문이 제기한 이유와 더불어, 다음과 같은 점도 Loki의 성공 요인으로 보입니다.

1. **실제 환경에서의 유용성**:
   - 논문은 Loki를 다양한 LLM 모델(Llama2, Llama3 등)과 데이터셋(WikiText-2, BookCorpus 등)에 적용하여 결과를 제시했습니다. 이는 이 알고리즘이 특정 모델에 국한되지 않고 일반적으로 활용 가능함을 시사합니다.

2. **미래 확장 가능성**:
   - Loki는 토큰 삭제 없이 Sparse Attention을 구현했으나, 이는 다른 방법(H2O)과 결합될 수 있습니다. 예를 들어, 일부 토큰 삭제 후 남은 토큰에 대해 Loki를 적용하면 메모리와 계산의 복합 최적화를 달성할 가능성이 있습니다.

3. **학습 없이 기존 모델에 적용 가능**:
   - Loki는 추가 학습이나 Fine-tuning 없이 기존 모델의 추론 과정에 직접 적용 가능하다는 점에서 실질적 도입 가능성이 높습니다.

---

### **결론**
Loki는 Key 벡터의 저차원 특성을 활용한 Sparse Attention 기법으로, 계산 비용을 줄이면서도 높은 정확성을 유지합니다. 특히, 기존 Sparse Attention 방식들이 직면했던 정보 손실 문제를 해결하고, 실질적인 GPU 구현까지 고려했다는 점에서 매우 실용적인 접근입니다.

다만, 논문에서도 제기했듯이, **Top-K 선택 과정의 비용 최적화**, **KV-cache 삭제와의 결합** 등이 향후 연구 과제로 남아 있습니다. Loki는 이러한 연구의 기반이 될 수 있는 강력한 기법입니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘

# A :

### **논문 Loki: 입력 데이터, 추론 과정, 모델 아키텍처 및 연산 분석**

---

## **1. Loki의 입력 데이터**

### 입력의 구성 요소
- **Query (\(q_S\))**:
  - 현재 단계에서 모델이 처리 중인 입력 토큰에 대한 Query 벡터.
  - 크기: \(1 \times D\) (여기서 \(D\)는 Attention 헤드의 차원).

- **Key (\(K:S\))**:
  - 현재까지 생성된 모든 Key 벡터의 집합 (현재와 과거 토큰 포함).
  - 크기: \(S \times D\) (여기서 \(S\)는 이전 토큰 수).

- **Value (\(V:S\))**:
  - Key와 동일한 크기의 Value 벡터 집합.
  - 크기: \(S \times D\).

- **PCA 변환 행렬 (\(P\))**:
  - Key 벡터를 저차원으로 변환하기 위한 행렬.
  - 크기: \(D \times D\).

- **추론 설정**:
  - 차원 축소 비율 (\(d\)): PCA를 통해 줄일 Key 차원의 비율 (\(d \times D\)).
  - Top-K 비율 (\(k\)): 주의 메커니즘에서 사용할 중요한 토큰 수 비율 (\(k \times S\)).

---

### **입력 데이터 예시**
1. **초기 입력 데이터**:
   - Query: \(q_S = [0.5, 0.2, 0.8]\).
   - Key: 
     \[
     K:S = \begin{bmatrix}
     0.1 & 0.3 & 0.7 \\
     0.5 & 0.2 & 0.4 \\
     0.3 & 0.7 & 0.6
     \end{bmatrix}.
     \]
   - Value:
     \[
     V:S = \begin{bmatrix}
     1.0 \\
     0.5 \\
     0.8
     \end{bmatrix}.
     \]
   - PCA 변환 행렬:
     \[
     P = \begin{bmatrix}
     0.6 & 0.8 & 0.0 \\
     0.8 & -0.6 & 0.0 \\
     0.0 & 0.0 & 1.0
     \end{bmatrix}.
     \]
   - 설정:
     - \(d = 2\), \(k = 2\).

---

## **2. Loki의 추론 과정**

### 단계별 과정
#### **단계 1: PCA 변환으로 Key와 Query의 차원 축소**
1. Query 및 Key 벡터를 PCA 변환 행렬 \(P\)를 이용하여 저차원 공간으로 변환.
   - 변환된 Query:
     \[
     q_S' = q_S \cdot P = [0.5, 0.2, 0.8] \cdot \begin{bmatrix}
     0.6 & 0.8 & 0.0 \\
     0.8 & -0.6 & 0.0 \\
     0.0 & 0.0 & 1.0
     \end{bmatrix} = [0.52, 0.28, 0.8].
     \]
   - 변환된 Key:
     \[
     K:S' = K:S \cdot P = \begin{bmatrix}
     0.1 & 0.3 & 0.7 \\
     0.5 & 0.2 & 0.4 \\
     0.3 & 0.7 & 0.6
     \end{bmatrix} \cdot \begin{bmatrix}
     0.6 & 0.8 & 0.0 \\
     0.8 & -0.6 & 0.0 \\
     0.0 & 0.0 & 1.0
     \end{bmatrix}.
     \]
     결과:
     \[
     K:S' = \begin{bmatrix}
     0.54 & 0.14 & 0.7 \\
     0.42 & 0.26 & 0.4 \\
     0.38 & 0.16 & 0.6
     \end{bmatrix}.
     \]

#### **단계 2: 근사 Attention 점수 계산**
2. 저차원 Key와 Query를 이용해 근사 Attention 점수 계산:
   \[
   a_{\text{approx}} = q_S' \cdot K:S'^T.
   \]
   계산 결과:
   \[
   a_{\text{approx}} = [0.52, 0.28, 0.8] \cdot
   \begin{bmatrix}
   0.54 & 0.42 & 0.38 \\
   0.14 & 0.26 & 0.16 \\
   0.7 & 0.4 & 0.6
   \end{bmatrix}
   = [0.74, 0.64, 0.78].
   \]

#### **단계 3: Top-K 선택**
3. \(a_{\text{approx}}\)에서 상위 \(k = 2\)개의 점수 선택.
   - 선택된 인덱스: \([2, 0]\).

#### **단계 4: 최종 Attention 점수 계산**
4. 선택된 \(k\)-개의 Key를 사용하여 원래 차원에서 정확한 Attention 점수 계산:
   - 선택된 Key:
     \[
     K_{\text{selected}} = \begin{bmatrix}
     0.3 & 0.7 & 0.6 \\
     0.1 & 0.3 & 0.7
     \end{bmatrix}.
     \]
   - 최종 Attention 점수:
     \[
     a_{\text{exact}} = \text{softmax}\left(\frac{q_S \cdot K_{\text{selected}}^T}{\sqrt{D}}\right).
     \]
     계산 결과:
     \[
     a_{\text{exact}} = [0.6, 0.4].
     \]

#### **단계 5: 최종 출력 생성**
5. 선택된 Value와 Attention 점수를 곱해 결과 출력:
   \[
   y = a_{\text{exact}} \cdot V_{\text{selected}} = [0.6, 0.4] \cdot \begin{bmatrix}
   0.8 \\
   1.0
   \end{bmatrix}
   = 0.88.
   \]

---

## **3. 모델 아키텍처 구성**

### Loki의 주요 구성 요소:
1. **PCA 기반 차원 축소 모듈**:
   - Key 벡터의 저차원 특성을 학습된 PCA 행렬로 압축.
   - 오프라인에서 데이터셋 기반으로 PCA 성분을 학습.

2. **Top-K 토큰 선택 모듈**:
   - 근사 Attention 점수를 계산한 후 가장 중요한 토큰 \(k\)-개를 선택.
   - 계산 비용과 메모리 요구량을 동적으로 줄임.

3. **Self-Attention 계산 모듈**:
   - 선택된 Top-K Key와 Value를 사용해 정확한 Attention 점수 계산.
   - 최종 출력 생성.

---

## **4. 연산 및 메모리 요구량**

### **Loki의 계산 복잡도**
1. **PCA 차원 축소**:
   - 복잡도: \(O(D^2)\) (PCA 변환).
2. **근사 Attention 점수 계산**:
   - 복잡도: \(O(dS)\) (저차원 공간에서 Query와 Key 곱).
3. **Top-K 선택**:
   - 복잡도: \(O(S \log S)\) (점수 정렬 및 상위 \(k\)-개 선택).
4. **최종 Attention 점수 계산**:
   - 복잡도: \(O(kD)\) (Top-K Key와 정확한 계산).

**총 계산 복잡도**:
\[
O(dS + S\log S + kD).
\]

### **메모리 요구량**
1. **KV-cache**:
   - 기존 Self-Attention과 동일 (\(S \times D\)).
2. **PCA 변환**:
   - 추가 메모리 사용량: \(D^2\) (PCA 행렬 저장).

---

## **결론**
- **추론 과정**:
  Loki는 Key 벡터의 저차원 구조를 활용하여 근사 계산 후 Top-K를 선택, 최종 정확한 점수를 계산하는 방식으로 성능을 유지하며 연산 비용을 줄임.
- **연산 및 메모리 효율성**:
  - 기존 Attention 대비 연산 비용을 \(d/D\)와 \(k/S\)로 나누는 효과를 가짐.
  - GPU 구현에서 실질적으로 최대 **45% 속도 향상**을 달성.


# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :

### **논문의 한계 및 이를 극복하기 위한 잠재적 연구 방향**

Loki는 Sparse Attention을 통해 대규모 언어 모델(LLM)의 계산 효율성을 향상시키는 데 성공했지만, 여전히 몇 가지 한계가 존재합니다. 이를 해결하기 위한 연구 흐름과 접근 방식을 아래에 자세히 설명합니다.

---

## **1. 한계와 극복 방향**

### **1.1 Top-K 연산 병목**
#### 한계:
- Loki에서 Top-K 토큰을 선택하기 위해 점수를 정렬하는 과정(\(O(S \log S)\))이 병목이 될 가능성이 있음.
- 특히, 긴 입력 시퀀스(\(S\))에서 정렬 연산이 전체 속도 개선에 부정적인 영향을 미칠 수 있음.

#### 극복 방향:
- **최적화된 Top-K 연산**:
  - GPU에서 실행 가능한 커스텀 Top-K 알고리즘 개발.
  - 기존 Triton 커널을 확장하여 정렬 대신 힙(heap) 기반 선택 알고리즘 도입.
- **근사적 Top-K 방법**:
  - 정확한 정렬 대신 근사적 정렬 알고리즘(예: Reservoir Sampling, Approximate Maximum Algorithms) 사용.
  - 일부 성능 저하를 감수하더라도 계산 비용을 줄일 수 있음.

#### 관련 연구 흐름:
- FlashAttention의 최적화 기법 활용.
- Approximate Nearest Neighbor Search를 Attention 점수 계산에 접목.

---

### **1.2 KV-Cache 메모리 최적화**
#### 한계:
- Loki는 KV-Cache 메모리를 줄이지 않으므로, 긴 시퀀스 처리 시 메모리 사용량이 기존 Attention과 동일.
- 메모리 병목 문제는 대규모 모델의 실질적인 활용에 여전히 큰 제약.

#### 극복 방향:
- **Token Eviction 기법과 결합**:
  - H2O와 같은 토큰 삭제 기반 Sparse Attention 기법과 Loki를 통합.
  - Top-K 연산 전에 중요하지 않은 토큰을 삭제하여 메모리 사용량을 줄임.
- **Dynamic KV-Cache 압축**:
  - Key와 Value를 동적으로 압축(Adaptive Compression)하는 알고리즘 도입.
  - 각 레이어의 중요도를 기준으로 Token 집합을 조정.

#### 관련 연구 흐름:
- **StreamingLLM**: 무한 길이 입력 시퀀스를 처리하기 위해 KV-Cache를 롤링 방식으로 관리.
- **InfiniGen**: CPU 메모리에 저장된 KV-Cache에서 GPU로 필요한 Key만 전송.

---

### **1.3 PCA의 일반화 한계**
#### 한계:
- Loki에서 사용된 PCA 기반 차원 축소는 특정 데이터셋에 의존할 가능성이 있음.
- 다양한 데이터셋과 작업에 대해 동일한 PCA 변환이 적합하지 않을 수 있음.

#### 극복 방향:
- **동적 차원 축소**:
  - PCA 대신 데이터의 특성에 따라 동적으로 적응하는 차원 축소 방법(예: Autoencoder, Transformer-specific Dimensionality Reduction) 사용.
- **계층적 차원 축소**:
  - Attention Layer의 레이어별 특성을 고려하여 서로 다른 PCA 변환을 적용.
- **훈련 기반 차원 축소**:
  - 모델 훈련 시 Key 벡터의 저차원 구조를 학습하여 더 적합한 차원 축소 변환을 생성.

#### 관련 연구 흐름:
- LoRA(Low-Rank Adaptation) 기법과 결합하여 LLM의 구조적 특징 활용.
- Transformer의 계층별 동적 적응 방법론 연구.

---

### **1.4 Multi-Head Attention 통합**
#### 한계:
- Loki는 개별 Attention Head에 대해 작동하며, Multi-Head Attention 간 상호작용을 고려하지 않음.
- 특정 헤드에서 중요도가 낮아진 토큰이 다른 헤드에서 높은 중요도를 가질 가능성을 무시.

#### 극복 방향:
- **Cross-Head Token Selection**:
  - Multi-Head Attention 간 정보를 교환하여 각 헤드의 Top-K 선택이 상호 보완적이도록 설계.
- **Global Top-K 방식**:
  - 모든 헤드의 Attention 점수를 종합하여 전역적으로 Top-K 토큰을 선택.
- **Head Importance Weighting**:
  - 각 헤드의 중요도를 학습하여 Top-K 토큰 선택에 반영.

#### 관련 연구 흐름:
- Multi-Query Attention에서 사용되는 Token Aggregation 기법.
- Layer-wise Relevance Propagation(LRP)을 활용한 Attention 분석.

---

### **1.5 Sparse Attention의 정확도 저하**
#### 한계:
- Loki는 정확도를 유지하려 하지만, 일부 작업에서는 성능 저하가 발생(평균 6.8% 감소).
- 특히, LongBench 같은 긴 문맥 처리에서는 성능 저하가 더 두드러질 가능성.

#### 극복 방향:
- **Token 중요도 학습**:
  - Attention 점수뿐만 아니라 문맥 정보를 활용하여 토큰 중요도를 예측.
  - Reinforcement Learning 기반 학습을 통해 토큰 선택의 효율성 향상.
- **Hybrid Attention Mechanism**:
  - 일부 레이어에서는 Sparse Attention, 다른 레이어에서는 Full Attention을 사용하여 성능 보완.

#### 관련 연구 흐름:
- Gating Mechanism을 사용한 Sparse-Full Attention 하이브리드 방법.
- Dense-Sparse-Dense Attention 구조를 연구하여 정확도 보완.

---

## **2. 결론 및 향후 연구 제안**
Loki는 Sparse Attention 연구에서 중요한 진전을 보여주었으며, 계산 효율성과 정확성의 균형을 제공했습니다. 그러나 남은 한계를 해결하기 위해 다음과 같은 연구 흐름이 중요합니다.

1. **Top-K 최적화**:
   - GPU에서 효율적인 Top-K 연산 기법 개발.
   - Approximate Top-K 기법 도입.

2. **메모리 병목 해결**:
   - Token Eviction과 Dynamic Compression 기법 통합.
   - CPU-GPU 간의 메모리 관리 최적화.

3. **차원 축소 기법 개선**:
   - PCA 외의 동적 차원 축소 알고리즘 개발.
   - 훈련 기반 차원 축소 접근법 연구.

4. **Multi-Head Attention 최적화**:
   - 헤드 간 정보 공유를 통해 전역적으로 중요한 토큰 선택.
   - Cross-Head Token Selection 연구.

5. **Sparse Attention 정확도 개선**:
   - Reinforcement Learning과 Hybrid Attention 구조를 결합하여 정확도 보완.

Loki는 대규모 언어 모델의 추론 효율성을 높이는 중요한 접근 방식으로, 위의 연구 흐름을 통해 더욱 강력한 Sparse Attention 기법으로 발전할 수 있을 것입니다.