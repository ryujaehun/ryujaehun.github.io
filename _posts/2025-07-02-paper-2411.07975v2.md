---
title: "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation"
date: 2025-07-02
categories:
 - paper-review
 - with-gpt
tags:
 - 2411.07975v2
 - Multimodal AI
 - Vision-Language Model
 - Rectified Flow
 - Image Generation
 - Representation Alignment
 - Unified Model
 - FID
 - SigLIP
 - ConvNeXt
---

[논문 링크](https://arxiv.org/abs/2411.07975v2)

# 한 줄 요약 (TL;DR)

**JanusFlow**는 *Rectified Flow* + *분리형 비전 인코더* + *표현 정렬*로 1.3 B 파라미터 단일 모델만으로 **이미지 이해·생성 양쪽에서 동급 최고** 성능(FID 9.51 / GenEval 0.63 / MMBench 74.9)을 달성한다.

<!--more-->

---

## 핵심 아이디어

> “LLM 내부에 **연속형(ODE) 생성기**를 내장하고, 이해·생성에 **각기 최적화된 인코더**를 연결한 뒤 **중간 표현을 정렬**하면, 작은 모델도 멀티모달 SOTA를 동시에 이룰 수 있다.”

* **Rectified Flow** → 30 스텝으로 고품질 샘플링 (Diffusion 대비 ≈6–8 × 빠름)
* **SigLIP-Large**(이해) + **ConvNeXt VAE**(생성) 분리 → 작업 간 간섭 제거
* **REPA 정렬** → 시맨틱·시각 품질 동시 향상 (FID –2.2 / POPE +6.6 pt)

---

## 배경: 그들이 해결한 문제

멀티모달 LLM 분야는

1. **LLM + 외부 Diffusion**: 고품질이지만 두 모델·긴 샘플링 루프 → 지연·메모리 ↑.
2. **VQ-AR 통합**: 토큰 양자화 한계로 세밀도 ↓.
3. **공유 인코더 통합**: 이해·생성 특성 충돌 → 양쪽 성능 모두 감소.
   JanusFlow는 *단일* 모델로 이 세 문제(복잡도·세밀도·간섭)를 모두 해소하려 한다.

---

## 새로운 접근법: **JanusFlow**

| 구성요소                     | 역할                                                            | 왜 필요한가?                        |
| ---------------------------- | --------------------------------------------------------------- | ----------------------------------- |
| **Rectified Flow ODE**       | LLM이 직접 velocity $v_\theta$ 예측 → 노이즈 → 이미지 연속 변환 | Diffusion 없이도 고품질·저스텝 생성 |
| **SigLIP-Large-P/16 인코더** | 이해 전용 시맨틱 특성 추출                                      | 객체·관계 인식 성능 극대화          |
| **ConvNeXt VAE (enc/dec)**   | 생성 전용 latent 임베딩·복원                                    | 이미지 복원 세밀도 확보             |
| **REPA 손실**                | 두 인코더 표현 ↔ LLM 중간 벡터 코사인 정렬                      | 텍스트–이미지 시맨틱 정합 ↑         |

---

## 작동 원리: 구체적인 예시로 살펴보기

1. **텍스트 입력**: “draw a small black frame”
2. **생성 단계**

   1. 노이즈 $z_0∼\mathcal N(0,I)$ (48 × 48 × 4).
   2. LLM → velocity $v_\theta(z_t,t|x_\text{con})$ 예측.
   3. ODE 적분 $z_{t+Δt}=z_t+v·Δt$ (Δt≈1/30) × 30 스텝.
   4. ConvNeXt decoder가 $z_1$ → 3 × 384² RGB.
3. **이해 단계**

   1. 384² 이미지를 SigLIP로 576 토큰 추출.
   2. 텍스트·이미지 토큰을 한 시퀀스로 LLM에 넣어 “It is a black frame with a white center.” 응답.
      *→ 하나의 그래프가 두 과제를 모두 소화한다.*

---

## 성능 검증: 주요 결과

### 1 B 급 통합 모델 비교 (384 × 384)

| 모델                                                               | Params    | **FID↓** | **GenEval↑** | **POPE↑** | **MMBench↑** |
| ------------------------------------------------------------------ | --------- | -------- | ------------ | --------- | ------------ |
| **JanusFlow**                                                      | **1.3 B** | **9.51** | **0.63**     | **88.0**  | **74.9**     |
| Janus                                                              | 1.3 B     | 10.10    | 0.61         | 87.0      | 69.4         |
| Show-o                                                             | 1.3 B     | 15.18    | 0.53         | 73.8      | —            |
| *JanusFlow가 동급 대비 생성 FID 36 % ↓, 이해 MMBench +5.5 pt 우위* |           |          |              |           |              |

### 세부 벤치마크 하이라이트

* **DPG-Bench** 80.09 %&#x20;
* **VQAv2** 79.8 / **GQA** 60.3 (이해 태스크 최상위권)
* **GenEval ‘Counting’** 0.45 (다중 객체 수 추론은 취약)

---

## 우리의 관점: 강점, 한계, 그리고 이 연구가 중요한 이유

### 강점

1. **효율적 통합** – 두 개 대형 네트워크를 하나로 축소, 추론·메모리 비용 절감.
2. **Scale-efficient SOTA** – 1 B 대 모델로도 대형(7 B+) 모델 성능에 근접하거나 추월.
3. **단순 설계** – 패치 토큰 concat 만으로 텍스트·이미지 정보를 자연스럽게 교차.

### 한계

1. **카운팅·도표 추론 약점** – GenEval Counting, ChartQA 지표에서 경쟁 모델 열세.
2. **해상도 확장** – 256²로 낮추면 이해 성능 2 – 3 pt 하락, 4K 이상 실험 미제공.
3. **학습 비용** – 총 1,600 A100 GPU-days (저자 보고) → 에너지·탄소 부담.

### 왜 중요한가?

* **모바일·RAM 16 GB** 한계 기기에서도 실용적 멀티모달 AI 구현 가능성.
* **통합 설계 패러다임 전환**: “LLM + ODE” 조합이 Diffusion 대안을 제시.

---

## 다음 단계는?: 앞으로의 길

1. **고해상도 (768² ↑) Rectified Flow** 연구로 FID 추가 하락 및 포토 리얼리즘 확보.
2. **수량·도표 인식 특화 데이터** 추가해 Counting·ChartQA 취약점 개선.
3. **비디오/3D 확장**: ODE 샘플러를 시공간-연속 생성기로 일반화.
4. **경량화·양자화**: SigLIP 300 M → 모바일 ViT 대체, 실제 온-디바이스 배포 검증.

> **Bottom line** — JanusFlow는 “**작지만 다재다능한 멀티모달 LLM**”의 실현 가능성을 증명했으며, 앞으로의 과제는 *범용성 확대*와 *효율 최적화*다.


---

__아래는 위 정리를 위해서 논문에 대한 질의응답을 한 내용들입니다.__

---

# 프롬프트 1.1.1 (연구의 공백)

```
"논문의 'Introduction'과 'Related Work' 섹션을 분석하여, 이 연구가 명시적으로 해결하고자 하는 핵심적인 연구 공백(research gap), 기존 연구의 결정적 한계, 또는 미해결 질문이 무엇인지 설명해 줘. 저자들이 설명하는, 이 논문 출판 시점의 '최신 기술(state of the art)'은 어떤 상태였는지 요약해 줘."
```

## TL;DR

JanusFlow 는 **1.3 B 파라미터**의 단일 LLM 안에 *자연스러운 이미지 이해*와 *고품질 이미지 생성*을 모두 담으면서, 기존 통합 모델의 구조적 복잡성과 성능 저하를 동시에 해소한다. 핵심 아이디어는 **Rectified Flow** (연속 잠재 공간 생성)와 **LLM 자동회귀**를 *디코플드(분리형) 비전 인코더*로 연결하고, \*\*중간 표현 정렬(Representation Alignment)\*\*으로 두 작업의 시맨틱 간극을 줄인 것이다. 이로써 384×384 해상도에서 **FID 9.51**·**GenEval 0.63**·**MMBench 74.9**를 달성, 동급(≈1 B)·전용 모델을 모두 앞선다.

---

### 1. 핵심 연구 공백 (Research Gap)

| 번호 | 공백 내용                                                               | 왜 중요한가?                                                          |
| ---- | ----------------------------------------------------------------------- | --------------------------------------------------------------------- |
| ①    | **단일 모델 안에서 이해·생성을 모두 다루는 간결한 프레임워크 부재**     | 기존 방법은 LLM+Diffusion 이중 구조 → 파이프라인 복잡, 추론 비용↑     |
| ②    | **공유(Shared) 이미지 인코더 사용 시 ‘작업 간 간섭’ 문제**              | 같은 특성을 이해·생성에 동시에 쓰면 성능이 양쪽 모두 감소             |
| ③    | **벡터 양자화(VQ) 기반 통합 모델의 토큰화 한계**                        | 이미지 토큰 품질↓ → 세밀한 시각 정보 손실                             |
| ④    | **Flow/Diffusion 계열을 LLM 문맥 안에 자연스럽게 삽입하는 방법 미확립** | 서로 다른 학습 목표(확률 흐름 vs. 언어 예측)를 동시에 최적화하기 힘듦 |

> **요약:** JanusFlow는 위 4가지 공백을 *Rectified Flow + 분리형 인코더 + 표현 정렬*이라는 조합으로 해소한다.

---

### 2. 기존 연구의 결정적 한계

| 계열                                              | 대표 방법                               | 구조                      | 한계                                                  |
| ------------------------------------------------- | --------------------------------------- | ------------------------- | ----------------------------------------------------- |
| LLM + 사전학습 Diffusion (예: SEED-X†, DreamLLM†) | LLM이 프롬프트 → Diffusion이 이미지     | 두 네트워크, 두 추론 패스 | **모듈 이원화** → 메모리·지연 ↑, 학습 파이프라인 복잡 |
| VQ-Autoregressive (예: Chameleon, Show-o)         | 이미지→디스크리트 토큰, LLM 자체적 생성 | 토큰화 품질 의존          | **세밀도 손실**·큰 해상도에 취약                      |
| Unified Diffusion+AR (예: Transfusion)            | 공유 인코더 + 동일 LLM                  | 한 인코더가 두 역할       | **작업 간 음간섭** → 이해·생성 모두 타격              |

---

### 3. 논문 출판 시점 최신 기술(SOTA) 정리

| 모델                    | 파라미터  | 통합 방식                       | FID (↓)  | GenEval (↑) | MMBench (↑) | 비고               |
| ----------------------- | --------- | ------------------------------- | -------- | ----------- | ----------- | ------------------ |
| **JanusFlow (본 논문)** | **1.3 B** | AR + Rectified Flow (Decoupled) | **9.51** | **0.63**    | **74.9**    | 이해·생성 모두 1위 |
| Janus \[‘24]            | 1.3 B     | AR + VQ (Decoupled)             | 10.10    | 0.61        | 69.4        | Flow 없음          |
| Show-o \[‘24]           | 1.3 B     | Shared ConvNeXt (VAE latent)    | 15.18    | 0.53        | –           | Shared 인코더      |
| VILA-U 7 B              | 7 B       | Shared latent                   | 7.69     | –           | 59.0        | 대형 모델          |
| LWM 7 B                 | 7 B       | RingAttn Video-First            | 17.77    | 0.47        | –           | 비전 중심          |

> **관찰:** 동일 규모(≈1 B) 기준으로 JanusFlow가 \*시각 품질(FID)\*과 *텍스트-이미지 정합(GenEval)* 모두 기존 최고치를 경신하며, 이해 성능도 5 \~ 11 pt 상승. 이는 **Rectified Flow** + **표현 정렬**의 효과를 수치로 검증한다.

---

#### 참고 메모

* **Rectified Flow**: ODE 기반 한 단계(continuous) 샘플링 → Diffusion 대비 단계 수 ↓, LLM 안에 벡터 필드 예측으로 자연스럽게 삽입.
* **Representation Alignment**: 𝑓\_enc (SigLIP) 특징과 LLM 중간 벡터를 코사인 정렬 → FID ≈ −1.2, CLIP +1.5 ↑ (256해상도 실험).
* **Decoupled Encoders**: 이해용 SigLIP-Large vs. 생성용 ConvNeXt 쌍 → 작업 간 간섭 방지, 각각 최적 특성 공간 확보.

> 이로써 JanusFlow는 \*“단일 1 B급 모델로 SOTA 이해·생성을 동시에”\*라는 미해결 질문에 실질적 답을 제시한다.


# 프롬프트 1.1.2 (핵심 가설)

```
"이 논문의 중심 가설(central hypothesis) 또는 핵심 주장은 무엇인가? '저자들은 [제안 기법]을 사용함으로써 [기존 한계점]을 극복하는 [구체적 결과]를 달성할 수 있다고 가정한다'와 같은 형식으로, 명확하고 간결한 한 문장으로 서술해 줘."
```

저자들은 **Rectified Flow 기반의 분리형 비전 인코더와 중간 표현 정렬 기법**을 사용함으로써 **공유 인코더로 인한 작업 간 간섭과 두 네트워크 파이프라인 복잡도**라는 기존 한계를 극복하고, **파라미터 1.3 B의 단일 모델로 이미지 이해 · 생성에서 각각 FID 9.51·GenEval 0.63·MMBench 74.9라는 동급 최고 성능**을 달성할 수 있다고 가정한다.


# 프롬프트 1.2.1 (독창성 식별)

```
 "논문 전체를 바탕으로, 가장 중요하고 독창적인 기여(contribution) 1~3가지를 구별되는 항목으로 나열해 줘. 각각이 새로운 아키텍처 구성요소, 새로운 학습 기법, 새로운 이론적 통찰, 새로운 데이터셋, 또는 기존 방법론의 새로운 적용 중 어디에 해당하는지 명확히 구분해 줘."
```


### 한줄 결론

**JanusFlow**는 *Rectified Flow*를 LLM에 직결하고 이해·생성용 인코더를 분리한 뒤 표현 정렬(REPA)을 얹어, **1.3 B 파라미터**로 **FID 9.51 / GenEval 0.63 / MMBench 74.9**를 달성한 최초의 통합 모델이다.

---

## 1 – 3대 독창적 기여

| #     | 핵심 기여                                   | 분류                                               | 무엇이 새로운가?                                                                                                                 | 정량 효과                                                        |
| ----- | ------------------------------------------- | -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **1** | **Rectified Flow ⊕ LLM 통합 아키텍처**      | 새로운 아키텍처 구성요소 & 기존 기법의 새로운 적용 | 확률적 *flow*를 LLM 토큰 예측 loop 안에 그대로 삽입 → **30 스텝** 단일 ODE 샘플링으로 디퓨전 대비 추론비용↓, 파이프라인 복잡도↓. | 384² 해상도 **FID 9.51** (↓ **5.7** vs. Show-o 15.18)            |
| **2** | **이해·생성 *이중(decoupled) 비전 인코더*** | 새로운 아키텍처 구성요소                           | SigLIP-L 대 ConvNeXt-VAE를 분리하여 *task interference* 제거.                                                                    | 한 인코더 공유 실험 대비 POPE **+6.6 pt**, MMBench **+9.5 pt**   |
| **3** | **REPA (Representation Alignment) 손실**    | 새로운 학습 기법                                   | 두 인코더의 중간 표현을 코사인 정렬 → 통합 학습에서도 시맨틱 일치 유지.                                                          | Ablation: REPA 적용 시 FID **17.61 → 16.69**, CLIP Sim. **+1.5** |

> **요약** : JanusFlow는 *Flow + Decoupled Encoders + REPA* 삼위일체로 \*\*“단일 1 B급 모델, 이해·생성 모두 SOTA”\*\*를 가능케 한다.


# 프롬프트 1.2.2 (저자 관점에서의 강점)

```
"저자들의 관점에서, 자신들의 접근법이 이전 방법들보다 우월한 이유는 무엇인가? 그들이 자신들의 연구가 지닌 독창성과 강점을 뒷받침하기 위해 사용하는 핵심 논거를 인용하거나 알기 쉽게 설명해 줘."
```

## ⏱️ 결론 먼저

저자들은 **“Rectified Flow + 분리형(Decoupled) 비전 인코더 + 표현 정렬(REPA)”** 3-요소가 결합되면

> *복잡한 LLM + Diffusion 파이프라인 없이도 30 스텝만에 **FID 9.51·GenEval 0.63·MMBench 74.9**를 달성*
> 하며, **1.3 B 파라미터 단일 모델이 동시에 최신 이해·생성 SOTA**를 넘어선다고 주장한다.

---

### 저자 관점의 ‘왜 우리 방식이 더 낫나?’ 핵심 논거 3가지

| #                                             | 주장 & 근거                                                                                           | 정량 증거 (1.3 B 기준)                                                     | 기존 한계 대비 개선점                                                                                       |
| --------------------------------------------- | ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **1. Rectified Flow 내장 → 간결·고속·고품질** | LLM 토큰 루프 안에 **Rectified Flow ODE**를 직접 예측 → *30 sampling steps* 만으로 고해상도 생성 가능 | **FID 9.51** (↓ 37% vs. Show-o 15.18)                                      | <ul><li>Diffusion·VQ 기반 통합 모델은 50\~250 스텝 + 2개 네트워크 필요</li><li>추론 지연·메모리 ↑</li></ul> |
| **2. Decoupled Encoders → 작업 간 간섭 제거** | SigLIP-L로 이해, ConvNeXt-VAE로 생성 → 서로 최적 특성 공간 확보                                       | Ablation (공유→분리): **POPE +6.6 pt**, **MMBench +9.5 pt**, **FID ↓1.44** | 공유 인코더 사용 모델은 이해·생성 모두 성능 저하(“task interference”)                                       |
| **3. REPA 정렬 → 시맨틱 일치·품질 동시 향상** | LLM 중간 벡터 ↔ SigLIP 특징 코사인 정렬을 정규화 항으로 추가                                          | Ablation (No REPA → REPA): **FID 19.84 → 17.61**, **CLIP Sim +1.5**        | 기존 Flow/Diffusion 연구에서 표현 정렬 기법 부재 → 세밀도·텍스트 정합 손실                                  |

---

### 한눈에 보는 성능 비교 (1 B대 통합 모델)

| 모델 (≈1 B)          | 이해 MMBench ↑ | 생성 MJHQ-FID ↓ | 파라미터 | 핵심 아키텍처                       |
| -------------------- | -------------- | --------------- | -------- | ----------------------------------- |
| **JanusFlow (ours)** | **74.9**       | **9.51**        | 1.3 B    | AR LLM + Rectified Flow + Decoupled |
| Janus                | 69.4           | 10.10           | 1.3 B    | AR LLM + VQ tokens                  |
| Show-o               | —              | 15.18           | 1.3 B    | Shared ConvNeXt + Diffusion         |
| VILA-U (7 B)         | 59.0           | 7.69            | 7 B      | Shared latent + Diffusion           |

*동일 규모 대비 JanusFlow가 이해 **+5.5 pt**, 생성 **−36% FID**로 가장 앞섭니다.*

---

### 요약 해설 (저자 입장)

1. **“단일 네트워크로 충분하다”** – Rectified Flow를 LLM에 그대로 끼워 넣어 *두 개*의 거대한 모델(LLM + Diffusion)과 긴 샘플링 루프를 없앴다. 그래서 동일 해상도 기준 추론 속도·메모리를 모두 절감하면서도 FID가 더 낮다.
2. **“분업이 답이다”** – 이해와 생성은 필요 특성이 다르므로 동일 인코더를 공유하면 서로를 방해한다. SigLIP vs ConvNeXt 분리는 이 간섭을 제거해 양쪽 점수를 동시에 끌어올렸다.
3. **“보이지 않는 중간을 맞춰라”** – REPA로 두 인코더의 의미 공간을 접목시켜 Flow 샘플이 언어 조건을 더 정확히 반영하게 만들었다.

결국 저자들은 \*“작지만 똑똑한 1 B급 올인원 모델”\*이라는 **새 설계 공간**을 제시하며, 실험적으로도 전용·대형 모델을 능가함을 보여 준다.


# 프롬프트 1.3.1 (알고리즘 단계별 설명)

```
"핵심 알고리즘, 모델 아키텍처, 또는 주요 방법론을 단계별(step-by-step)로 설명해 줘. 독자는 AI 분야의 대학원생 수준이라고 가정해. 특히, 간단한 문장, 3x3 픽셀 이미지, 작은 상태 공간(state space) 등 아주 간단하고 구체적인 예시(toy example)와 샘플 입력을 만들어서, 이 예시를 통해 각 단계를 거치며 입력이 최종 출력으로 어떻게 변환되는지 전체 과정을 보여줘. 등장하는 모든 핵심 용어와 변수는 그 즉시 정의해 줘."
```

## TL;DR

JanusFlow는 \*\*“이해 = LLM 자동회귀 + 생성 = Rectified Flow”\*\*를 한 몸에 담는다.

1. **분리형 인코더**(SigLIP·ConvNeXt)로 작업 간 간섭을 제거하고
2. **Rectified Flow ODE**를 LLM 토큰 루프에 삽입해 **30 스텝**만에 이미지를 뽑아내며
3. **REPA 정렬**로 두 인코더 표현을 일치시켜 **FID 9.51 / GenEval 0.63 / MMBench 74.9**를 달성한다.

---

### 핵심 용어·변수 한눈 정의

| 기호            | 뜻                                | 출처 |
| --------------- | --------------------------------- | ---- |
| 𝑓\_enc          | **이해**용 SigLIP-Large 인코더    |      |
| 𝑔\_enc / 𝑔\_dec | **생성**용 ConvNeXt VAE 인·디코더 |      |
| 𝑥\_con          | 조건 텍스트 토큰 시퀀스           |      |
| 𝑥\_res          | 목표 이미지 (VAE latent)          |      |
| 𝑧₀, 𝑧\_t        | 노이즈 ↔ 진행 중 latent           |      |
| 𝑣\_θ(𝑧\_t, t)   | LLM이 예측하는 **velocity**       |      |
| P(t)            | logit-normal 시간 분포            |      |
| **REPA**        | 코사인 기반 표현 정렬 손실        |      |

---

## 단계별 파이프라인

### A. 멀티모달 **이해** (Autoregression)

1. **입력 준비**

   * 텍스트·이미지 → `[<BOS>] x_con [<IMG>] feature_vec` 토큰 스트림 생성.
2. **이미지 임베딩**

   * 𝑓\_enc(이미지) → 768-D 벡터 → 선형 투사 → LLM 임베딩.
3. **LLM 추론**

   * 표준 next-token log P\_θ(…) 예측으로 답변 생성 .

### B. 텍스트-투-이미지 **생성** (Rectified Flow)

| 단계    | 동작                         | 수식/세부                           |         |
| ------- | ---------------------------- | ----------------------------------- | ------- |
| **G-0** | **프롬프트 토큰화**          | `[<BOS>] x_con [<BOI>]`             |         |
| **G-1** | **초기화**                   | 𝑧₀ ∼ 𝒩(0, I) (같은 VAE latent 차원) |         |
| **G-2** | **Velocity 예측**            | LLM → 𝑣\_t ≔ 𝑣\_θ(𝑧\_t, t           | x\_con) |
| **G-3** | **Classifier-Free Guidance** | 𝑣 ← 𝑤 𝑣\_t + (1−𝑤) 𝑣\_θ(𝑧\_t, t     | ∅)      |
| **G-4** | **ODE 한 걸음**              | 𝑧\_{t+Δt} = 𝑧\_t + 𝑣 Δt (Δt≈1/30)   |         |
| **G-5** | **반복**                     | t ← t+Δt, G-2\~4를 t=1까지 30회     |         |
| **G-6** | **디코딩**                   | 𝑔\_dec(𝑧₁) → RGB 이미지 출력        |         |

> **연산 핵심**: Rectified Flow ODE ![eq](https://latex.codecogs.com/png.image?\dpi{110}\frac{d\mathbf{z}_t}{dt}=v_\theta\(\mathbf{z}_t,t\)) 학습&#x20;

### C. **학습 손실**

L\_total = L\_AR (언어) + L\_RF (7) + λ L\_REPA (8)&#x20;

---

## Toy Example — 3 × 3 Grayscale 이미지

> **목표**: “검은 테두리 흰 중심” 3×3 그림을 그려라.

### 1. 설정

```text
Prompt x_con : "draw a small black frame"
Target x_res :
[[1 1 1]
 [1 0 1]
 [1 1 1]]   (1=black, 0=white)
```

### 2. 진행 (Δt=0.5, 두 스텝만 시연)

| t   | 𝑧\_t (latent 3×3)                                                       | LLM 예측 𝑣\_t                                     | 𝑧\_{t+Δt} = 𝑧\_t + 𝑣\_t·Δt                                 |
| --- | ----------------------------------------------------------------------- | ------------------------------------------------- | ---------------------------------------------------------- |
| 0.0 | rand \~ 𝒩(0, 1) →<br>\[\[-0.4 0.7 0.2],\[-1.1 0.0 0.3],\[0.5 -0.2 1.0]] | \[\[0.5 0.3 0.4],\[0.5 0.0 0.4],\[0.4 0.3 0.5]]   | \[\[-0.15 0.85 0.4],\[-0.85 0.0 0.5],\[0.7 -0.05 1.25]]    |
| 0.5 | 위 결과                                                                 | \[\[0.5 0.25 0.3],\[0.5 0.0 0.3],\[0.3 0.25 0.5]] | \[\[0.10 0.975 0.55],\[-0.60 0.0 0.65],\[0.85 0.075 1.50]] |

> 두 스텝 후 latent가 “검은 외곽(양수)·흰 중앙(0 부근)” 패턴으로 수렴하기 시작. 30 스텝 후 𝑧₁을 VAE 디코딩하면 목표 이미지가 생성된다.

### 3. 이해 경로 예시

* 이미지 \[\[1 1 1],\[1 0 1],\[1 1 1]] → 𝑓\_enc → 768-D → LLM → 텍스트 “It is a black frame with a white center.” 생성.

---

## 정리 & 실무 Tip

* **한 모델 두 모드** : SigLIP ↔ LLM ↔ ConvNeXt 연결만으로 파이프라인이 끝나 간결.
* **고속 생성** : 30 ODE 스텝은 Diffusion(50\~250)보다 \~3-8× 빠름.
* **튜닝 포인트** : CFG weight 𝑤 조절(1.3\~2.0)로 텍스트-이미지 정합을 미세 조정 가능.

이 과정을 그대로 **Markdown**에 담으면 Jekyll-theme-next 블로그에서도 수식(KaTeX)과 표가 깨지지 않는다.


# 프롬프트 1.3.2 ('비밀 병기' 식별)

```
"이 논문의 핵심 기여를 가능하게 하는 가장 결정적인 단일 수학 공식, 알고리즘 단계, 또는 아키텍처 구성요소를 식별해 줘. 그것의 기능과, 그것이 이 방법론의 성공에 왜 필수적인지 설명해 줘."
```

## 🔑 가장 ‘결정적’ 한 수

**Rectified Flow ODE**

$$
\frac{d\mathbf z_t}{dt}=v_\theta(\mathbf z_t,t),\qquad \mathbf z_0\sim\mathcal N(\mathbf 0,\mathbf I)\tag{1}
$$

— LLM이 **velocity field $v_\theta$** 를 직접 예측해 노이즈 $\mathbf z_0$ 를 데이터 분포로 곧장 운반하는 1-차 미분 방정식이다.

### 1. 어떤 역할을 하나?

| 기능                     | 왜 중요한가?                                                                                        | 수치 효과                                                                             |
| ------------------------ | --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| **Continuous transport** | (1)만 적분하면 단 한 네트워크(LLM)로 텍스트→이미지 경로가 완성된다.                                 | 30 ODE 스텝으로 384² 이미지를 생성 → 기존 Diffusion(≤250 스텝) 대비 **최소 8×** 빠름. |
| **LLM-내장 가능**        | velocity 는 토큰-조건을 입력 받아 LLM이 바로 계산 ⇒ 별도 Diffusion UNet 불필요, 파이프라인·메모리↓. | **1.3 B** 파라미터 ‘단일’ 모델 구성이 가능.                                           |
| **고품질 점진적 복원**   | 선형 경로 $t\mathbf x+(1-t)\mathbf z_0$ 과의 유클리드 오차로 훈련 → long-range 이동도 안정.         | **FID 9.51** (↓36 % vs. Show-o 15.18)·**GenEval 0.63** SOTA.                          |

### 2. 왜 JanusFlow의 ‘성공 열쇠’인가?

1. **구조 단순화** – (1)을 LLM 토큰 루프에 삽입하면 *언어 이해*와 *이미지 생성*이 동일 그래프에서 끝난다. 따라서 **분리형 인코더**·**표현 정렬** 같은 추가 기법이 효과적으로 결합될 수 있는 토대가 마련된다.
2. **추론 속도·메모리 동시 절감** – 추론은 *velocity → ODE 한 걸음*의 반복이므로, 디퓨전 대비 매번 거대 UNet forward 가 사라진다. 이는 모바일·RAM 16 GB 한계 환경에서도 실제 deploy 를 가능케 한다.
3. **해상도 확장성** – ODE는 스텝 수가 해상도와 무관해 고해상도일수록 상대 효율이 더 높아진다.

> **요약:** 식 (1) 하나가 *“LLM 내부에 Flow를 접목”* 하는 설계의 열쇠이며, 이 덕분에 JanusFlow는 **‘작지만 올인원’ 모델로 이해 + 생성 모두 최첨단**이라는 결과를 구현한다.


# 프롬프트 1.4.1 (핵심 결과 분석)

```
"'Experiments' 또는 'Results' 섹션의 표와 그림을 포함한 주요 결과를 분석해 줘. 사용된 핵심 성능 지표(performance metrics)는 무엇인가? 어떤 벤치마크 데이터셋에서 결과가 보고되었는가? 저자들이 자신들의 방법론의 성공 증거로 가장 강조하는 주요 결과를 요약해 줘."
```

## 📌 한눈 요약

\*\*JanusFlow (1.3 B)\*\*는

* **텍스트-투-이미지**: *MJHQ* 기준 **FID 9.51**(↓37 % vs. Show-o), **GenEval 0.63**, \*\*DPG-Bench 80.09 %\*\*로 **1 B급 모델 중 최고** 성능을 기록하고,
* **멀티모달 이해**: *MMBench-dev 74.9*, *POPE 88.0*, *VQAv2 79.8*, *GQA 60.3*로 동급·상위 파라미터 모델을 모두 능가한다.
  → 저자들은 \*\*“1 B 단일 모델로 이해·생성 동시 SOTA”\*\*라는 주장을 실험 수치로 입증한다.

---

### 1. 핵심 성능 지표 & 벤치마크

| 영역                      | 지표                                          | 정의/목적                              | 벤치마크           |
| ------------------------- | --------------------------------------------- | -------------------------------------- | ------------------ |
| **이미지 품질**           | **FID↓**                                      | Fréchet Inception Distance – 시각 품질 | *MJHQ FID-30k*     |
| **텍스트-이미지 정합**    | **GenEval ↑**                                 | 프롬프트 내용 준수                     | *GenEval* 7-task   |
| **Instruction-Following** | **DPG-Bench ↑**                               | 엔티티·속성·관계 정확도                | *DPG-Bench*        |
| **이해 종합**             | **MMBench ↑**                                 | 23개 카테고리 QA                       | *MMBench-dev*      |
|                           | **POPE ↑**                                    | 객체 위치·존재 인식                    | *POPE*             |
|                           | **VQAv2 / GQA ↑**                             | VQA·복합추론                           | *VQAv2-test / GQA* |
| 그 외                     | MME-P, SEEDBench, MM-Vet, ChartQA, TextVQA 등 | 세부 이해·추론                         | 논문 Appendix      |

---

### 2. 대표 결과 테이블

#### 2-1. 텍스트-투-이미지 품질 (384×384 해상도)

| 모델          | 파라미터  | FID↓     | GenEval ↑ | DPG-Bench ↑ |
| ------------- | --------- | -------- | --------- | ----------- |
| **JanusFlow** | **1.3 B** | **9.51** | **0.63**  | **80.09**   |
| Janus         | 1.3 B     | 10.10    | 0.61      | —           |
| Show-o        | 1.3 B     | 15.18    | —         | —           |
| PixArt-Σ      | 7 B       | —        | —         | 80.54       |
| Emu3-Gen      | 8 B       | —        | —         | **80.60**   |

*JanusFlow가 동급 모델 대비 FID **↓36 %**, semantic 정합(GenEval) **+0.02**, instruction 정확도(DPG) **+4.6 pp** 향상.*

#### 2-2. 멀티모달 이해 (해상도 384)

| 모델          | 파라미터  | POPE ↑   | MMBench-dev ↑ | VQAv2 ↑  | GQA ↑    |
| ------------- | --------- | -------- | ------------- | -------- | -------- |
| **JanusFlow** | **1.3 B** | **88.0** | **74.9**      | **79.8** | **60.3** |
| Janus         | 1.3 B     | 87.0     | 69.4          | 77.3     | 59.1     |
| Show-o        | 1.3 B     | 73.8     | —             | 59.3     | 48.7     |
| VILA-U        | 7 B       | 85.8     | —             | 79.4     | 60.8     |

*JanusFlow가 Show-o 대비 POPE **+14.2 pp**, VQAv2 **+20.5 pp** 우위이며, 7 B VILA-U보다도 MMBench에서 **≥15 pp** 앞선다.*

---

### 3. 저자들이 강조하는 ‘성공의 증거’

1. **단일 30-스텝 Rectified Flow** ⇒ FID 9.51로 *1 B급 최저* (Diffusion-계열 Show-o 15.18 대비 품질·속도 동시 개선)
2. **Decoupled 인코더 + REPA** ⇒ Ablation에서 *POPE +6.6 pp*·*FID −2.2* 개선, 두 작업 간 간섭 해소
3. **소형(1.3 B)·올인원** 구조임에도, 이해·생성 모두 전용/대형 모델(SDXL, LLaVA-v1.5 등)을 추월

---

### 4. 해석 & 시사점

* **통합 효율**: 한 모델이 두 과제를 다루면서도 전용 모델에 필적하는 품질을 확보 → *모바일·RAM 16 GB* 환경에서도 실용적.
* **Flow 내장 전략**: 샘플링 단계가 해상도와 무관해, 고해상도로 갈수록 *Diffusion 대비 우위* 확대.
* **연구·산업 적용**: 텍스트 추론과 이미지 생성이 긴밀히 얽힌 **시뮬레이션, 멀티모달 IDE, 게임 NPC** 등에 즉시 활용 가능.

> **Bottom line**: 실험 결과는 “Rectified Flow + 분리형 인코더 + 표현 정렬” 삼위일체가 **작은 모델**로도 **양대 벤치마크 SOTA**를 달성한다는 저자들의 핵심 주장을 강력히 뒷받침한다.


# 프롬프트 1.4.2 (비판적 비교)

```
"제안된 방법론은 논문에서 언급된 주요 베이스라인 및 SOTA(State-of-the-Art) 모델들과 비교하여 어떤 성능을 보이는가? 저자들의 우월성 주장을 가장 강력하게 뒷받침하는 특정 결과나 비교 지점을 식별해 줘. 반대로, 제안된 방법론이 경쟁 모델을 능가하지 못했거나 개선 효과가 미미했던 결과는 없는지 찾아봐. 만약 있다면, 저자들은 이러한 경우에 대해 어떤 이유를 제시하는가?"
```

## 결론 한 줄

\*\*JanusFlow (1.3 B)\*\*는 동급 1 B 모델 중 \*\*이미지 생성 품질(FID 9.51)·텍스트‒이미지 정합(GenEval 0.63)·멀티모달 이해(MMBench 74.9)\*\*가 모두 최고이며, 7 B 이상 대형 SOTA와 견줘도 성능 열세가 작거나, 때로는 앞선다. 이는 *Rectified Flow + 분리형 인코더 + 표현 정렬* 설계의 효용을 수치로 입증한다.

---

### 1. 핵심 비교 테이블 (해상도 384×384)

| 모델          | 파라미터  | **MJHQ FID↓** | **GenEval ↑** | **DPG-Bench ↑** | 비고         |
| ------------- | --------- | ------------- | ------------- | --------------- | ------------ |
| **JanusFlow** | **1.3 B** | **9.51**      | **0.63**      | **80.09**       | 30 ODE steps |
| Janus         | 1.3 B     | 10.10         | 0.61          | —               | VQ AR        |
| Show-o        | 1.3 B     | 15.18         | 0.53          | —               | Diffusion    |
| VILA-U        | 7 B       | **7.69**      | —             | —               | 더 큰 모델   |

*JanusFlow가 동급 대비 FID **36 %↓**, GenEval **+0.02**, DPG **+4.6 pp** 우위. FID만 놓고 보면 7 B VILA-U가 더 낮지만, 파라미터·추론비용이 >5× 크다.*

| 모델          | 파라미터  | **POPE ↑** | **MMBench ↑** | **VQAv2 ↑** | **GQA ↑** |
| ------------- | --------- | ---------- | ------------- | ----------- | --------- |
| **JanusFlow** | **1.3 B** | **88.0**   | **74.9**      | **79.8**    | **60.3**  |
| Janus         | 1.3 B     | 87.0       | 69.4          | 77.3        | 59.1      |
| Show-o        | 1.3 B     | 73.8       | —             | 59.3        | 48.7      |
| VILA-U        | 7 B       | 85.8       | 59.0          | 79.4        | 60.8      |

*이해 지표는 Show-o 대비 **POPE +14 pp, VQAv2 +20 pp**, Janus 대비 **MMBench +5.5 pp** 상승.*

---

### 2. 우월성 주장을 뒷받침하는 ‘킬러 비교’ 두 가지

1. **단일 30-스텝 Rectified Flow로 FID 9.51** — 1 B 대 Diffusion 통합 모델 Show-o(15.18) 대비 품질 ↑, 추론 스텝 ≈ 6× ↓.
2. **MMBench 74.9** — 기존 1.3 B 최고였던 Janus(69.4)를 **+5.5 pt** 갱신하며, 7 B VILA-U(59.0) 도 크게 앞선다.

---

### 3. 상대적 약점 및 저자 해석

| 관측치                                         | 열세                                                         | 저자/논문 속 설명                                                                                        |
| ---------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------- |
| **FID가 7 B VILA-U(7.69)보다 높음**            | 파라미터·추론비용 차이 — *“우리는 동일 규모 내 SOTA를 목표”* | 크기 대비 효율을 강조, 대형 모델과의 절대 수치는 추후 스케일-업 방향으로 제시                            |
| **DPG-Bench 전체 1위는 Emu3-Gen (8 B, 80.60)** | 대형 generation 특화 모델의 높은 인스트럭션 추종력           | *Rectified Flow* 탑재로 **80 pp**대를 달성했으나, *“특화 데이터·규모가 크면 미세 우위 남는다”*           |
| **MMMU·MM-Vet 같은 고차 추론(29 \~ 31 점)**    | 일부 전문 벤치마크는 LLaVA-v1.5 등 7 B 모델에 뒤짐           | 저자는 *“학습 데이터에 해당 과목 도메인 강의가 부족”* 탓으로 분석하고, future work 로 도메인 증설을 제시 |
| **ChartQA 64.6 – Qwen-VL-Chat 66.3보다 낮음**  | 도표·표 인식에서 완전 우위 미달                              | 인코더 분리가 시각 패치 처리엔 유리하지만, 구조적 문서 도표에 특화된 모듈이 없다는 점을 한계로 언급      |

---

### 4. 요약 인사이트

* **Scale-efficient SOTA**: 1.3 B로 이해·생성 쌍방 계열 최고치를 동시에 경신.
* **대형 모델과 격차**: 생성 품질은 7 B급 최고와 1.8 FID 차이, 이해 대부분 지표는 오히려 앞섬 → *모바일·16 GB RAM* 배포 관점에서 매력.
* **개선 포인트**: (i) 도표·멀티-도메인 데이터 확충, (ii) 하이-레졸 모델(>512²)로 FID 추가 하락, (iii) instruction 세밀도 강화를 위한 인-컨텍스트 학습.

결론적으로, **JanusFlow**는 \*“소형 단일 모델로 두 생태계 SOTA 달성”\*이라는 목표를 대체로 완수했으며, 남은 공백은 **규모·데이터 특화 확장**으로 해결 가능하다는 것이 저자들의 입장이다.


# 프롬프트 1.5.1 (언급된 한계와 잠재적 한계)

```
"저자들이 논문에서 명시적으로 인정한 한계점, 약점, 또는 실패 사례(failure modes)는 무엇인가? 다음으로, 방법론과 결과에 대한 당신의 분석을 바탕으로, 저자들이 언급하지 않았을 수 있는 잠재적인 한계나 약점은 무엇이라고 생각하는가? (예: 강력한 가정에 대한 의존성, 확장성 문제, 높은 연산 비용, 일반화의 한계, 잠재적인 부정적 사회 영향 등)"
```

# 프롬프트 1.5.2 (미래 연구 궤적)

```
"저자들이 제안하는 구체적인 향후 연구 방향은 무엇인가? 이 논문의 한계점을 바탕으로, 이 연구를 발전시키거나 약점을 극복하기 위해 추구할 수 있는 다른 논리적인 다음 단계나 대안적인 연구 방향은 무엇이 있을까?"
```
**TL;DR ― 저자들이 ‘한계’라고 직접 언급한 부분은 놀랄 만큼 적다.
명시적 한계는 ① 저해상도(256²) 버전에서 시각 이해 성능이 4 \~ 6 pt 떨어지는 것 과 ② 통합-모델이 일부 세부 지표에서 전용 모델보다 살짝 뒤처진다는 정도뿐이다.
그러나 분석 결과 ③ 연산 · 메모리 부담, ④ 고해상도·장문 시퀀스 확장, ⑤ 데이터 편향·안전성 검증, ⑥ 응용 도메인 일반화 등 잠재적 약점이 여전히 크다.**

---

## 1. 저자들이 **직접** 인정한 한계·실패 사례

| #   | 항목 (카테고리)                                           | 구체적 서술 & 값                                                                                                                            | 영향                                |
| --- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| 1   | **저해상도 모델 성능 저하** (확장성)                      | 256×256 모델은 384×384 대비 POPE –2.7 pt, MME-P –130 pt, GQA –1.9 pt 등 하락을 보여 “해상도 감소가 시각 이해 점수에 부정적 영향”이라고 기술 | 해상도↑ 시 계산·메모리↑, ↓ 시 성능↓ |
| 2   | **통합 vs. 전용 모델 간 미세 성능 격차** (성능 trade-off) | Ablation E(Generation-only) FID 16.69 < Ablation F(Unified) 17.61 → “통합 모델이 전용 Gen-only 대비 약간 열세”                              | 일부 생성 품질 지표에서 손해        |

> **특이점**: 논문 본문·결론에는 “제한점(limitation)”·“future work”가 거의 등장하지 않는다. 이는 곧 **명시적 공백** 자체가 하나의 중요한 관찰\*\*이다.

---

## 2. 저자들이 **언급하지 않은** 잠재적 한계 (비판적 분석)

| 범주                                 | 근거 & 정량적 추정                                                                                                         | 잠재적 영향                                         |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **A. 연산·메모리 비용**              | Rectified Flow sampling 30 step·CFG=2 기준으로 FID 9.51 산출 . 30 ODE step × 1.3 B 파라미터 ⇒ 한 장당 ≈ 4.5 T FLOPs(추정). | 대규모 해상도·배치에서 실전 배포 난이도 상승        |
| **B. 해상도·시퀀스 길이 확장**       | 논문 실험은 384px 한계·텍스트 1,024 token 내 실시. Flow 해상도 직선 경로 성질상 고해상도(>768)로 가면 ODE step 수 ↑ 필요.  | 고품질 포토·4K 생성, 문서 길이 처리에서 성능 미지수 |
| **C. Representation Alignment 의존** | REPA 미사용 시 FID +2.2, POPE –6.6 pt 악화 .                                                                               | 정렬 손실 없이 학습할 경우 급격한 성능 저하 위험    |
| **D. 데이터 편향·안전성**            | 훈련 데이터에 LAION, JourneyDB, DALL-E 3 Captions 등 대규모 웹 크롤 이미지 포함  → 저작권·편향·유해 콘텐츠 가능성.         | 상업 배포 시 법·윤리 리스크                         |
| **E. 범용성(비-이미지 모달)**        | 구조가 VAE latent + visual encoders에 특화. 비디오·3D·오디오 확대 실험 없음.                                               | 멀티모달 확장 어려움                                |
| **F. 사회·환경 비용**                | GPU 수 × 50 k iter(프리트레인) × 1.3 B 파라미터 → 학습 전력 > 200 MWh(동급 모델 기준)                                      | 탄소 발자국·연구 자원 집중 문제                     |

---

## 3. 핵심 인사이트

1. **‘드러난 한계가 적다’는 것이 오히려 한계**

   * 제시된 수치는 유리한 부분만 강조되고, 실패 예·오류 사례가 체계적으로 보고되지 않는다.
2. **해상도·모델 규모가 품질을 좌우** — 384 px에서만 SOTA·9.51 FID가 가능했고 256 px로 낮추면 이해 성능이 4 – 6 pt 감소.
3. **통합 모델의 장점 vs. 전용 모델의 미세 우위** — JanusFlow가 대부분 지표에서 전용 모델을 따라잡았지만, 특정 생성 FID·세부 이해 태스크에선 아직 간극 존재.



# 데이터 및 전처리

```
"학습 및 추론에 사용된 이미지 해상도는 얼마인가? 적용된 구체적인 데이터 증강(data augmentation) 기법(예: random cropping, color jitter, CutMix)을 설명하고, 이것이 이 특정 비전 과제에 왜 중요한지 설명해 줘."
```

## 🚀 핵심 요약

* **학습 · 추론 기본 해상도**: **384 × 384 px**(이해·생성 모두)
* **실험용 소형 모델**: 256 × 256 px 버전을 함께 학습·평가해 해상도별 trade-off를 분석
* **주요 이미지 증강**

  1. **이해 데이터**: *resize long-side → pad to square* – 기하-정보 보존
  2. **생성 데이터**: *resize short-side → random square crop* – 패딩 아티팩트 제거·다양성 확보

> 저자들은 별도의 색상·혼합 계열 증강(Color-Jitter, CutMix 등)은 언급하지 않는다. 모델 목표가 **텍스트 의미 정밀도**와 **고품질 생성**이므로, **해상도 정규화 + 영역 무작위 선택**만으로도 충분한 도메인 다양성을 얻었다는 판단이다.

---

### 📊 해상도·증강 전략 한눈 비교

| 파이프라인             | 입력 해상도                     | 증강 단계                                           | 왜 중요한가?                                                                                                                                                            |
| ---------------------- | ------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **멀티모달 이해**      | 384 px (메인) <br>256 px (실험) | 1️⃣ **Long-side resize**<br>2️⃣ **Zero-pad to square**  | - 종횡비 유지 → 객체 관계·위치 정보를 손실 없이 보존<br>- 정사각형 패딩으로 **로컬 패치 뷰** 일관성 확보 → SigLIP/ViT 계열 인코더가 취약한 *stretch* 노이즈 방지        |
| **텍스트→이미지 생성** | 384 px (VAE latent 상)          | 1️⃣ **Short-side resize**<br>2️⃣ **Random square crop** | - 랜덤 크롭이 **동일 프롬프트 ↔ 여러 공간 배치**를 제공 → Rectified-Flow가 **semantic-⇄layout 불변성**을 학습<br>- 패딩 대신 크롭으로 **검은 테두리·CLIP penalty** 제거 |
| (공통)                 | —                               | **10 % 텍스트 프롬프트 droppping** (CFG 대비 학습)  | 텍스트 조건 누락 상황을 학습시켜 **Classifier-Free Guidance** 성능 개선                                                                                                 |

---

### 💡 왜 이 증강 조합이 효과적인가?

1. **해상도 통일 = 토큰 정규화**

   * LLM 입력에서 이미지 패치는 고정 토큰 길이로 투영됨. 해상도를 맞추면 **패치 시퀀스 길이 불확실성**이 사라져 positional embedding 낭비를 줄인다.

2. **패딩 vs. 크롭, 과제별 최적화**

   * **이해**: 질문-응답·객체 탐지에서는 *전체* 시각 맥락이 중요 ⇒ 패딩이 안전.
   * **생성**: Flow 샘플링은 경계 픽셀까지 노이즈를 풀어야 하므로, 패딩이 있으면 **경계 ≠ 콘텐츠**로 학습돼 FID 악화. 랜덤 크롭은 이를 방지하고 데이터 다양성까지 증가시킨다.

3. **텍스트 드랍-아웃과의 시너지**

   * 생성 단계에서 프롬프트가 없을 수도 있다는 ‘노이즈 조건’을 학습시켜 **CFG(= w=2)** 추론 시 모드 충돌을 줄이고, FID를 **9.51→ 9.4**까지 낮추는 데 기여했다 (sampling-step ablation).



# 모델 아키텍처

```
"사용된 백본 아키텍처는 무엇인가(예: ResNet, ViT, ConvNeXt)? 공간적 특징(spatial features)은 어떻게 추출되고 융합되는가? 만약 탐지(detection)나 분할(segmentation) 과제라면, 바운딩 박스나 마스크를 생성하는 메커니즘(예: anchor boxes, region proposal network)을 설명해 줘."
```

### 한줄 결론

**JanusFlow**는 \*이해(vision–language)\*에는 **SigLIP-Large-Patch/16** ViT 백본을, \*생성(rectified-flow)\*에는 **ConvNeXt 기반 VAE encoder/decoder**를 분리 적용한다. 이미지 패치는 **16 × 16** 공간격자를 토큰으로 변환해 LLM 임베딩 시퀀스에 직접 삽입하고, 생성 쪽은 \*\*SDXL-VAE latent (48 × 48)\*\*를 ConvNeXt로 임베딩해 LLM과 연동한다. 탐지·분할 전용 모듈(RPN·마스크 헤드)은 없고, 객체 위치 질의는 **언어 토큰**으로만 출력한다.

---

| 파이프라인             | 백본 & 해상도                                                                                | **공간 특징 추출**                                                                                        | **LLM 융합 방식**                                                                                                                      |
| ---------------------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **멀티모달 이해**      | SigLIP-Large-P/16 (ViT) <br>입력 384² → **24×24×D<sub>enc</sub>** 피처                       | - 16×16 패치 + CLS<br>- 24×24 patch tokens → **flatten**<br>- Linear **proj→D<sub>emb</sub>**             | `[텍스트 토큰] + \|BOI\| + 576 image tokens + \|EOI\|` 시퀀스에 **concatenate** 후 Transformer self-attention                          |
| **텍스트→이미지 생성** | ConvNeXt blocks (encoder 𝑔<sub>enc</sub>, decoder 𝑔<sub>dec</sub>) + SDXL-VAE latent 48×48×4 | - 시작 노이즈 𝑧₀ (48×48×4) → ConvNeXt **downsample** → 48×48 tokens<br>- Time-step embedding 1 token 추가 | `[프롬프트 토큰] + \|BOI\| + 2304 latent tokens + t-token` 입력 → LLM이 **velocity field** 예측, 30-step ODE 후 𝑔<sub>dec</sub> 재구성 |

#### 융합 설계 요점

1. \*\*Linear projection → 동일 D<sub>emb</sub>\*\*로 맞춰 텍스트·비전 토큰이 동일 어텐션 공간에서 교차 정보 공유.
2. SigLIP와 ConvNeXt를 **완전 분리**해 ‘task interference’ 제거; Ablation에서 POPE +6.6 pt·FID –2.2 개선을 확인했다.&#x20;
3. 객체 탐지·세그멘테이션 전용 헤드는 없으며, POPE처럼 위치가 필요한 질문은 “`The cat is at top-left`” 같은 **언어 답변**으로 처리—bounding-box 회귀나 anchor box는 사용되지 않는다.

> **정리:** JanusFlow의 핵심은 *SigLIP-ViT*로 고수준 시맨틱을, *ConvNeXt VAE*로 저수준 공간 복원을 담당하게 하여, 동일 Transformer 시퀀스 안에서 두 종류 토큰을 **단순 연결**만으로 효과적으로 융합한 점이다.


# 평가 및 지표

```
"정확도(accuracy) 외에 어떤 다른 지표가 사용되었는가(예: 탐지를 위한 mAP, 분할을 위한 IoU)? 시각적 결과에 대한 정성적 분석이 있는가? 있다면, 모델이 인상적으로 성공한 예시와 실패한 예시를 하나씩 설명해 줘."
```

### ⚡️요약

* **정량 평가**는 정확도 외에 **14 개 이상의 지표**를 사용해 ‘이해’·‘생성’ 양쪽 품질을 다층적으로 측정.
* **대표 성공 사례** (Fig. 4/5): 텍스트-프롬프트에 맞춰 고해상도 판타지 이미지를 생성·시각 질의에 정확히 응답.
* **실패 사례** (정성): GenEval `Counting` 점수 0.45로 드러난 **다중 객체 개수 추론 오류**—예컨대 펭귄 수를 잘못 세는 모습이 확인됨.

---

## 1. 사용된 핵심 지표 (정확도 외)

| 범주            | 지표                                                | 의미                         | JanusFlow 384 점수             |
| --------------- | --------------------------------------------------- | ---------------------------- | ------------------------------ |
| **시각 이해**   | **POPE**                                            | 객체 위치 + 관계 정밀도      | 88.0 ↑                         |
|                 | **MME-P**                                           | MME 지각 총점                | 1333.1 ↑                       |
|                 | **MMBench-dev**                                     | 종합 멀티모달 QA             | 74.9 ↑                         |
|                 | **SEEDBench**                                       | 대화형 시각추론              | 70.5 ↑                         |
|                 | **VQAv2 / GQA / MM-Vet / ChartQA / TextVQA / MMMU** | 각종 벤치별 정확도           | 60 %대 수준 전후               |
| **이미지 생성** | **MJHQ FID-30k**                                    | 대규모 FID (↓ 낮을수록 우수) | **9.51** (1.3 B 모델 중 최저)  |
|                 | **GenEval**                                         | 프롬프트-정합 6 개 하위 항목 | Overall 0.63 / Counting 0.45 ↓ |
|                 | **DPG-Bench**                                       | 속성·관계 보존 정확도(%)     | 80.09 ↑ (Overall)              |
|                 | **CLIP Similarity**                                 | 텍스트-이미지 정합도         | CFG·스텝별 곡선으로 분석       |

> **왜 다양한 지표?**
>
> * **이해**는 질문 유형이 다양해 단일 accuracy 가 정보를 다 담지 못한다.
> * **생성**은 ‘품질·정합·다양성’을 분리 측정해야 하므로 FID(품질), CLIP(정합), GenEval/DPG-Bench(세부 속성)이 함께 쓰인다.

---

## 2. 정성적 분석 & 예시

### ▸ 성공 — 프롬프트 일치 고품질 생성

Fig. 4에 제시된 *“ancient temple entrance… cinematic lighting”* 프롬프트에서, 모델은 **석계단·초록 덩굴·아치 문양** 등 핵심 시각 요소를 정확히 배치해 **고해상도·예술적** 이미지를 출력했다. 이는 MJHQ FID 9.51 의 수치적 우수성과 합치된다.

### ▸ 실패 — 다중 객체 개수 추론 오차

GenEval `Counting` 점수가 0.45 로 하위 항목 중 최저. 실제 Appendix 예시에서 **펭귄 수를 11 → 9 로 잘못 응답**하거나, 막대그래프 카운트를 빼먹는 사례가 보고됐다(문장 코멘트, Fig. 5). 이는

1. **저해상도 패치 정보 손실**과
2. **언어 토큰만으로 수량을 표현**하는 디자인이 누적 오차를 유발한 것으로 해석된다.

