---
title: "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language
 Models Gains More"
date: 2024-12-09
categories:
 - paper-review
 - with-gpt
 - LLM-Inference
---

[논문 링크](https://arxiv.org/abs/2402.12065)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### **강점과 독창적인 지점**
**WKVQuant**은 대규모 언어 모델(LLM)의 메모리 사용 문제를 해결하기 위해 설계된 **Post-Training Quantization (PTQ)** 프레임워크입니다. 주요 강점과 독창적 특징은 다음과 같습니다:

1. **효율성과 정확성의 균형**:
   - 기존 방식(Weight-Only Quantization 또는 Weight-Activation Quantization)의 단점을 보완하며, 모델 정확도와 메모리 절약의 균형을 맞췄습니다.
   - 특히, KV 캐시(Key/Value Cache)와 가중치(Weights)만을 선택적으로 양자화하는 전략으로 메모리 사용량을 대폭 줄이면서도 성능 저하를 최소화했습니다.

2. **혁신적 양자화 기술**:
   - **Past-Only Quantization (POQ)**: 현재 생성된 KV 값을 양자화하지 않고 원래 정밀도를 유지하면서 이전 단계의 KV 캐시만 양자화합니다. 이는 Attention 계산 정확도를 향상시킵니다.
   - **2D-Quantization**: 채널 간 변동성을 줄이는 **Static Channel Smoothing**과 토큰 간 변동성을 처리하는 **Dynamic Token-wise Quantization**을 결합하여 양자화 오류를 줄였습니다.
   - **Cross-Block Reconstruction Regularization (CRR)**: 블록 간 손실을 줄이기 위한 정규화를 도입하여 최종 결과와 더 가까운 오류 계산을 가능하게 했습니다.

3. **실제 활용 가능성**:
   - LLaMA 및 LLaMA-2와 같은 대규모 모델에서 테스트한 결과, 높은 정확도와 메모리 절약을 동시에 달성하며, 기존 방법보다 우수한 성능을 보였습니다.

---

### **핵심 알고리즘: 예시와 과정**
#### 1. **Past-Only Quantization (POQ)**
   - **입력 예시**:
     - 현재 입력: "The cat"
     - 이전 KV 캐시: [Quantized KV] 저장된 4-bit 정밀도의 값들

   - **과정**:
     1. 현재 입력에서 생성된 Key/Value(KV) 값은 **Full-Precision**으로 유지.
     2. 이전 단계의 KV 캐시는 4-bit로 양자화된 값을 사용.
     3. Attention 계산 시, 현재의 정밀도 높은 KV와 양자화된 과거 KV 값을 결합하여 예측.

   - **결과**:
     - 메모리 절약(과거 KV 캐시의 양자화)과 높은 계산 정확도(현재 KV의 Full-Precision 유지)를 동시에 달성.

#### 2. **2D-Quantization**
   - **입력 예시**:
     - KV 캐시 값의 분포: 채널 간 큰 변동성, 토큰 간 비균일성
     - `Y = XW + B`로 표현된 Key/Value 계산

   - **과정**:
     1. **Static Channel Smoothing**: 채널 값의 중심을 조정하고 스케일링하여 채널 간 변동성 감소.
     2. **Dynamic Token-wise Quantization**: 토큰의 평균값과 최대값에 따라 각 토큰 값을 조정.
     3. 값들을 그룹 단위(예: 128)로 세분화하여 세밀하게 양자화.

   - **결과**:
     - 양자화 오류 감소, 특히 큰 값의 아웃라이어에 대한 민감성 완화.

#### 3. **Cross-Block Reconstruction Regularization (CRR)**
   - **입력 예시**:
     - 블록 i에서의 출력 \( y_i \), 블록 \( i+k-1 \)에서의 출력 \( \hat{y}_{i+k-1} \)

   - **과정**:
     1. \( k \)개의 연속 블록에서 정량화된 출력과 정밀 출력 간의 차이를 계산.
     2. 평균 절대 오차(Mean Absolute Error, MAE)를 사용하여 최적화.

   - **결과**:
     - 네트워크 최종 출력과 더 밀접한 손실 계산을 통해 정밀도 향상.

---

### **논문의 한계**
1. **임시 활성화(Temporary Activations)의 미양자화**:
   - 임시 활성화는 양자화되지 않아 메모리 사용량이 증가할 가능성이 있습니다. 특히, 대규모 배치나 긴 시퀀스에서 문제가 될 수 있습니다.

2. **가속 하드웨어의 활용 부족**:
   - Nvidia의 INT8 가속과 같은 고속 계산 장치를 완전히 활용하지 못합니다.

3. **최적화 시간 소요**:
   - CRR로 인한 최적화 시간 증가(7B 모델 약 3시간, 13B 모델 약 4시간)가 단점으로 작용할 수 있습니다.

---

**결론적으로**, WKVQuant은 대규모 언어 모델의 메모리 효율성과 정확성을 개선한 혁신적인 프레임워크이지만, 특정 활용 환경에서 제한이 있을 수 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



논문의 핵심 알고리즘인 **WKVQuant**의 주요 구성 요소는 다음 세 가지입니다. 각각의 과정과 예시를 들어 상세히 설명하고, 종합적으로 정리하겠습니다.

---

## 1. **Past-Only Quantization (POQ)**
### **알고리즘 개요**
- 현재 입력으로 생성된 Key/Value(KV) 값은 원본의 **Full-Precision** 값을 유지하고, 이전 단계에서 캐시된 KV 값만 양자화합니다.
- Attention 계산에서 고정밀과 양자화된 값을 병합하여 메모리 사용을 줄이면서도 정확도를 유지합니다.

### **과정**
#### **입력**
- 현재 입력: `"The cat"`
- 이전 KV 캐시: 
  - Key: \([1.2, 0.8, -0.5, 1.7]\) (Full-Precision)
  - Value: \([0.9, -0.3, 0.6, 0.2]\) (Full-Precision)
  - 양자화된 값 (4-bit): Key → \([1.0, 0.5, -0.5, 1.5]\), Value → \([0.8, -0.2, 0.5, 0.1]\)

#### **단계**
1. 현재 입력 `"The cat"`로 생성된 Key/Value는 **원본 정밀도(Full-Precision)**로 유지:
   - Key: \([1.3, 0.7, -0.6, 1.6]\)
   - Value: \([0.95, -0.25, 0.65, 0.15]\)

2. 이전 단계의 KV 캐시는 4-bit로 양자화된 값을 사용:
   - Key: \([1.0, 0.5, -0.5, 1.5]\)
   - Value: \([0.8, -0.2, 0.5, 0.1]\)

3. Attention 계산 시, 양자화된 과거 KV와 현재의 Full-Precision KV 병합:
   - Attention 값: 
     \[
     \text{Attention} = \text{Softmax}(QK^T)V
     \]
     \( Q \): 현재 Key, \( K^T \): 이전 Key (양자화된 값), \( V \): 이전 Value (양자화된 값)

#### **결과**
- 정확도를 유지하면서 메모리 사용량이 감소.
- 양자화된 이전 KV 캐시를 통해 저장 공간 최적화.

---

## 2. **Two-Dimensional Quantization (2D-Quantization)**
### **알고리즘 개요**
- **Static Channel Smoothing**: 채널 간 값의 중심을 정렬하고 스케일링하여 채널 간 변동성을 감소시킴.
- **Dynamic Token-wise Quantization**: 토큰별로 평균 및 최대값을 기준으로 정규화하여 아웃라이어의 영향을 완화.

### **과정**
#### **입력**
- Key/Value 캐시: \[
Y = XW + B
\]
  - \( Y \): KV 캐시, \( X \): 입력 토큰, \( W \): 가중치, \( B \): 바이어스
  - 초기 값: \( Y = [[10, 200], [50, 100], [20, 400]] \)

#### **단계**
1. **Static Channel Smoothing**:
   - 채널별로 이동 매개변수 \(\delta\)와 스케일링 매개변수 \(s\)를 학습.
   - 조정:
     \[
     Y = (Y - \delta) \oslash s \quad \text{(스케일링)}
     \]
   - 조정 후 \( Y \): \[
     Y_{\text{정렬}} = [[0.1, 0.2], [0.5, 0.1], [0.2, 0.4]]
     \]

2. **Dynamic Token-wise Quantization**:
   - 토큰 단위로 최대값, 평균값 계산.
   - 정규화:
     \[
     \text{Quantized}(Y) = \text{round}\left(\frac{Y - \text{mean}(Y)}{\text{max}(|Y - \text{mean}(Y)|)}\right)
     \]
   - 조정 후 \( Y \): \[
     Y_{\text{정규화}} = [[0, 1], [1, 0], [0, 1]]
     \]

#### **결과**
- KV 캐시의 양자화 오류 감소.
- 아웃라이어로 인한 양자화 오류를 완화.

---

## 3. **Cross-Block Reconstruction Regularization (CRR)**
### **알고리즘 개요**
- 여러 블록의 결과를 비교하여 전역적인 관점에서 최적의 양자화 매개변수를 학습.
- 기존의 MSE 기반 손실 대신, MAE(Mean Absolute Error)를 사용하여 아웃라이어의 영향을 완화.

### **과정**
#### **입력**
- 블록 \(i\)의 출력: \( y_i \)
- 블록 \(i+k-1\)의 출력: \( \hat{y}_{i+k-1} \) (양자화된 값)

#### **단계**
1. 블록 \(i\)부터 \(i+k-1\)까지 양자화된 출력 \( \hat{y}_{i+k-1} \)와 원본 출력 \( y_{i+k-1} \) 비교.
2. 손실 계산:
   \[
   \text{Loss} = \text{MAE}(\hat{y}_{i+k-1}, y_{i+k-1})
   \]

3. 손실에 따라 매개변수 \( \gamma, \beta, s, \delta \) 업데이트.

#### **결과**
- 정량화 매개변수를 최적화하여 양자화 오류를 줄임.
- 더 높은 정밀도를 보장.

---

### **정리**
- **WKVQuant**는 대규모 언어 모델의 메모리 사용량을 줄이면서 성능을 유지하는 혁신적인 PTQ 방법입니다.
  - **Past-Only Quantization**은 현재 KV를 원본 정밀도로 유지하며 과거 KV만 양자화해 메모리 효율성을 높임.
  - **2D-Quantization**은 채널 및 토큰별 정렬로 양자화 오류를 최소화.
  - **CRR**은 여러 블록을 통한 전역적 손실 계산으로 최적화된 양자화 매개변수를 학습.

이 방법은 모델의 성능과 효율성을 모두 극대화하는 강력한 양자화 기술로 평가됩니다.

# Q : 이 논문에서 사용하는 방법을 학습하기 위하여 어떤 데이터셋이 필요할까? 그리고 어떻게 학습을 진행하면 될지 예시를 들어서 아주 자세하게 설명해줘



### **WKVQuant 학습에 필요한 데이터셋**
이 논문에서 제안된 **WKVQuant**는 Post-Training Quantization (PTQ) 기법으로, 모델을 재학습하는 것이 아니라 양자화를 위한 매개변수를 최적화하는 과정에 중점을 둡니다. 학습에 필요한 데이터셋은 **모델의 특성과 양자화 대상에 따라 선택**되며, 다음과 같은 특징을 가진 데이터셋이 적합합니다.

#### **필요한 데이터셋의 특징**
1. **다양한 토큰 분포**:
   - 모델이 처리할 텍스트의 다양한 길이와 구조를 포함해야 합니다.
   - 문장 길이, 주제, 언어적 복잡성이 다양한 데이터셋 필요.
   
2. **대규모 텍스트 데이터**:
   - 모델이 사용하는 어휘와 토큰 분포를 반영할 수 있는 데이터셋.
   - 예: WikiText2, C4, OpenWebText.

3. **캘리브레이션 데이터**:
   - 학습 데이터는 모델의 양자화 매개변수를 최적화하기 위한 소규모 샘플로 구성.
   - WikiText2와 같은 데이터에서 **128개의 랜덤 2048-토큰 샘플**을 추출하여 활용.

---

### **학습 과정과 예시**
WKVQuant 학습 과정은 크게 3단계로 나뉩니다: **캘리브레이션 데이터 준비, 양자화 매개변수 초기화, 매개변수 최적화 및 평가**. 아래에 구체적인 학습 과정을 설명합니다.

#### 1. **캘리브레이션 데이터 준비**
양자화 매개변수를 학습하기 위해 캘리브레이션 데이터셋을 준비합니다.

- **데이터셋 준비**:
  - WikiText2 데이터셋에서 2048-토큰 길이의 128개 샘플을 무작위로 선택.
  - 예: 
    ```
    "The cat sat on the mat. It was a sunny day, and the world felt calm. ..."
    ```

- **목적**:
  - 가중치 및 KV 캐시의 분포를 모델링하여 매개변수를 학습.

---

#### 2. **양자화 매개변수 초기화**
양자화를 위한 매개변수 \(\gamma\), \(\beta\), \(s\), \(\delta\)를 초기화합니다.

- **초기화 방법**:
  - 각 채널 및 토큰별 최대값, 평균값 계산.
  - 초기 값:
    - \(\gamma = \text{max}(\text{Weights})\)
    - \(\beta = \text{min}(\text{Weights})\)
    - \(s = \text{max}(|Y|)\)
    - \(\delta = \text{mean}(Y)\)

- **초기화 후 예시**:
  - Key/Value 캐시의 채널별 값: \([10, 200], [50, 100], [20, 400]\)
  - 초기 매개변수:
    - \(s = 200\), \(\delta = 90\)

---

#### 3. **매개변수 최적화**
Cross-block Reconstruction Regularization (CRR) 기법을 사용해 매개변수를 최적화합니다.

##### **단계**
1. **출력 계산**:
   - 각 블록의 원본 출력 \(y_i\)와 양자화된 출력 \(\hat{y}_i\) 비교.
   - 예: 
     - 원본 출력 \(y_i = [0.9, 0.3, -0.2]\)
     - 양자화된 출력 \(\hat{y}_i = [1.0, 0.2, -0.3]\)

2. **손실 계산**:
   - Mean Absolute Error (MAE) 손실:
     \[
     \text{Loss} = \text{MAE}(\hat{y}_{i+k-1}, y_{i+k-1})
     \]
   - 예:
     \[
     \text{Loss} = \frac{|1.0 - 0.9| + |0.2 - 0.3| + |-0.3 - (-0.2)|}{3} = 0.1
     \]

3. **매개변수 업데이트**:
   - AdamW 옵티마이저를 사용해 매개변수 업데이트.
   - 학습률 설정: \( \text{lr}_{s,\delta} = 5e^{-4} \), \( \text{lr}_{\gamma,\beta} = 1e^{-2} \).

4. **최적화 반복**:
   - 한 번의 업데이트 후, 동일한 데이터로 5 에포크 반복.

---

#### **학습 완료 후 평가**
최적화된 모델을 평가하여 성능과 메모리 절감을 확인합니다.

- **평가 데이터**:
  - WikiText2, PTB(Penn Treebank), C4 등의 데이터셋 사용.
  - Perplexity(PPL)와 LongBench 점수를 활용하여 성능 비교.

- **결과**:
  - W4KV4(가중치 및 KV 캐시 양자화)는 원본(FP16) 모델에 근접한 정확도를 유지하면서 메모리 사용량을 대폭 절감.

---

### **종합 정리**
#### **학습 프로세스 요약**
1. **캘리브레이션 데이터 준비**: WikiText2 데이터셋에서 샘플을 추출.
2. **초기 매개변수 설정**: 채널 및 토큰의 분포를 기반으로 초기화.
3. **CRR을 사용한 최적화**: MAE 손실을 통해 양자화 매개변수 업데이트.

#### **결과**
- WKVQuant는 **KV 캐시와 가중치를 양자화**하여 메모리를 절약하고, **최적화된 양자화 매개변수**를 통해 모델의 정확도를 유지합니다.
- 학습은 비교적 적은 데이터와 계산으로 수행되므로 실제 환경에서 효율적으로 적용 가능합니다.

# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### **논문에서 제시한 결과**
이 논문은 **WKVQuant**가 대규모 언어 모델(LLMs)의 메모리 사용량을 줄이면서 높은 정확도를 유지하는 데 뛰어난 성능을 발휘한다고 주장합니다. 주요 결과는 다음과 같습니다:

---

#### **1. 정확도와 메모리 효율성**
- **WKVQuant (W4KV4)**는 FP16 (Full-Precision)과 비슷한 정확도를 유지하면서도 메모리 소비를 크게 줄였습니다.
- 다른 방법론 (OmniQuant의 W4A4 등)에 비해 높은 정확도를 유지합니다.
  
| **모델**             | **Method** | **Longtext avg** | **Zero-shot avg** | **Memory (GB)** |
| -------------------- | ---------- | ---------------- | ----------------- | --------------- |
| FP16                 | -          | 34.12            | 61.32%            | 27.1            |
| GPTQ (W4)            | -          | 34.06            | 60.55%            | 8.0             |
| OmniQuant (W4A4)     | -          | 16.35            | 47.46%            | 6.8             |
| **WKVQuant (W4KV4)** | -          | 32.52            | 60.34%            | 6.8             |

- **Longtext avg**는 긴 입력 텍스트에서의 성능 평가이며, FP16에 비해 약간의 정확도 감소가 있지만 **OmniQuant보다 월등히 우수**합니다.
- **Memory**는 동일한 메모리 소비량(6.8GB)을 유지하며 정확도를 극대화한 것이 특징입니다.

---

#### **2. 다른 방법론과의 비교**
- **GPTQ (W4)**: 높은 정확도를 유지하지만 KV 캐시를 양자화하지 않아 메모리 절약 효과가 부족합니다.
- **OmniQuant (W4A4)**: 메모리 절약 효과는 크지만, 활성화(Activation) 양자화로 인해 정확도가 크게 감소.
- **WKVQuant (W4KV4)**:
  - 정확도: FP16에 근접하며 GPTQ를 초월.
  - 메모리 사용량: OmniQuant 수준으로 절약.

---

### **특출난 점**
#### **1. Past-Only Quantization (POQ)의 효과**
- **논문에서 제시하는 이유**:
  - 기존 양자화 방법에서 현재의 Key/Value 값을 양자화하면 Attention 계산 정확도가 크게 손상됩니다.
  - POQ는 현재 값은 Full-Precision 상태로 유지하고 과거 값만 양자화하여, 메모리 절약과 정확도 모두를 달성.

- **내 생각**:
  - POQ는 매우 설득력 있는 방식으로, 양자화의 주요 문제인 **계산 정확도의 손실**을 최소화하면서 **메모리 효율성**을 극대화합니다.
  - 특히, LLMs의 Attention 메커니즘이 Key/Value 계산에 매우 민감하므로, POQ는 성능 저하 없이 양자화를 구현하는 효과적인 접근 방식입니다.

---

#### **2. Two-Dimensional Quantization (2D-Quantization)**
- **논문에서 제시하는 이유**:
  - KV 캐시의 채널 간 및 토큰 간 값 변동성이 매우 크기 때문에, 단일한 양자화 매개변수를 사용하는 경우 양자화 오류가 증가.
  - 2D-Quantization은 채널 정렬(Static Channel Smoothing)과 토큰 단위 동적 양자화(Dynamic Token-wise Quantization)를 결합하여 오류를 최소화.

- **내 생각**:
  - LLM에서 Outlier(극단 값)의 문제를 해결하려는 기존 방식들(예: SmoothQuant)과 비교할 때, 2D-Quantization은 보다 세밀하게 값을 조정하여 **양자화 오류를 체계적으로 줄이는 방식**입니다.
  - 특히, Fine-grained Quantization(그룹 단위로 매개변수를 조정)이 추가적으로 정밀도를 보장하는 중요한 요소로 보입니다.

---

#### **3. Cross-block Reconstruction Regularization (CRR)의 기여**
- **논문에서 제시하는 이유**:
  - 기존 방법들은 양자화된 출력과 원본 출력 간의 지역적 손실(예: 블록 단위의 MSE)을 계산했지만, 이는 최종 모델 출력과의 차이를 적절히 반영하지 못함.
  - CRR은 여러 블록의 출력 간 전역적 손실을 계산하고, MAE를 사용해 아웃라이어로 인한 오류를 줄임.

- **내 생각**:
  - CRR은 양자화된 출력이 모델의 **최종 성능과 더 직접적으로 연결**되도록 하는 중요한 정규화 방식입니다.
  - 특히, **MAE**를 사용해 아웃라이어의 영향을 완화한 점은 대규모 모델의 특성을 잘 이해한 설계로 보입니다.

---

### **왜 이러한 결과가 도출되었는가?**
1. **POQ**: 현재 값의 Full-Precision 유지로 Attention 계산의 핵심 정보를 보존.
2. **2D-Quantization**: KV 캐시의 특성을 반영하여 채널 및 토큰 간 양자화 오류를 최소화.
3. **CRR**: 전역적 손실 계산과 아웃라이어 완화로 정밀한 양자화 매개변수 학습.

---

### **내 생각**
- 이 논문은 대규모 모델의 양자화에서 **현실적이고 효과적인 해결책**을 제시했다고 생각합니다.
- 특히, **메모리 효율성과 정확성 간의 균형**을 맞춘 점에서 GPTQ나 OmniQuant 같은 기존 방법론보다 우수합니다.
- 다만, POQ로 인해 임시 활성화(Temporary Activations)가 양자화되지 않는 부분은 메모리 사용에서 한계를 남길 수 있지만, 대부분의 LLM 사용 사례에서는 큰 문제가 되지 않을 것입니다. 

---

### **결론**
**WKVQuant**는 **대규모 언어 모델의 메모리 사용량을 최소화하면서 정확도를 유지**하는 데 뛰어난 기법입니다. POQ, 2D-Quantization, CRR의 조합이 이러한 결과를 가능하게 했으며, 실제 적용 가능한 혁신적인 방법론으로 평가됩니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### **1. 입력 데이터와 추론 과정**
이 논문에서 제안된 **WKVQuant**는 대규모 언어 모델(LLM)에서 양자화된 Key/Value(KV) 캐시를 활용합니다. 이를 기반으로 입력 데이터와 추론 과정을 자세히 설명합니다.

---

#### **입력 데이터**
- **입력 형식**:
  - 토큰 시퀀스, 예: `"The cat sits on the mat."`
  - 모델에 입력되는 시퀀스는 토큰화된 숫자 배열로 변환됩니다.
    - 예: \[101, 203, 402, 78, 512\] (각 숫자는 어휘에 매핑된 토큰 ID)

- **특징**:
  - 입력 데이터의 길이(시퀀스 길이)가 중요하며, 메모리 사용량은 길이에 따라 선형적으로 증가.
  - 예: 시퀀스 길이가 2048인 경우, 각 토큰의 Key와 Value를 저장해야 함.

---

#### **추론 과정**
##### **1) 입력 처리**
- 입력 시퀀스 \(X = \{x_1, x_2, ..., x_T\}\)를 모델에 전달.
  - 예: 입력 `"The cat"` → 토큰 시퀀스 \[101, 203\].

##### **2) Transformer 블록 계산**
- 각 블록은 다음 연산을 수행합니다:
  1. **Attention Mechanism**:
     - Key (\(K\)), Value (\(V\)), Query (\(Q\)) 계산.
       \[
       Q = XW_Q, \quad K = XW_K, \quad V = XW_V
       \]
       예: 
       - \(W_Q, W_K, W_V\): 선형 변환을 위한 가중치.
       - \(Q = [0.8, 0.5]\), \(K = [0.7, 0.6]\), \(V = [0.4, 0.3]\).

  2. **Scaled Dot-Product Attention**:
     \[
     \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
     \]
     예: 
     - \(QK^T = [0.56]\), Softmax 결과 = \([1.0]\), 출력 = \(V = [0.4, 0.3]\).

  3. **Feedforward Layer**:
     - 출력에 비선형 변환 적용.
       \[
       \text{Output} = \text{ReLU}(XW_1 + b_1)W_2 + b_2
       \]

##### **3) Key/Value 캐시 관리**
- **Prefill 단계** (초기 입력 처리):
  - 입력 데이터의 Key와 Value를 생성하여 캐시에 저장.
  - 예: KV 캐시 = \([K: 0.7, 0.6], [V: 0.4, 0.3]\) (양자화).

- **Decode 단계** (새 토큰 생성):
  - 이전 Key/Value 캐시를 활용하여 새로운 입력 \(Q\)와 결합해 계산.
  - 예:
    - 현재 \(Q = [0.9, 0.4]\), 과거 \(K\)와 \(V\) 사용.

##### **4) 출력 생성**
- 모델의 최종 출력은 다음 토큰의 확률 분포:
  \[
  \text{Output Probabilities} = \text{Softmax}(WX + b)
  \]
- 예: 확률 분포 \([0.1, 0.7, 0.2]\) → 토큰 203("cat") 생성.

---

### **2. 모델 아키텍처 구성**
#### **1) Transformer 블록**
- **구성 요소**:
  - Multi-Head Attention: Key, Query, Value를 사용한 주요 연산.
  - Feedforward Layer: 비선형 변환을 수행하는 MLP.
  - Layer Normalization: 안정적 학습을 위한 정규화.

#### **2) WKVQuant의 변형**
- 기존 Transformer 모델에 KV 캐시 양자화를 추가:
  1. **Past-Only Quantization (POQ)**: 현재 \(K, V\)는 Full-Precision 유지, 이전 \(K, V\)만 양자화.
  2. **Two-Dimensional Quantization (2D-Quantization)**: KV 캐시의 채널 및 토큰 간 정밀도 조정.

---

### **3. 연산 및 메모리 요구량**
#### **1) 주요 연산**
- **Attention 계산**:
  - \(QK^T\) 연산은 \(O(T^2d)\) (시퀀스 길이 \(T\), 차원 \(d\)).
  - 메모리 사용량은 \(T \times d\)에 비례.
  
- **양자화 연산**:
  - 양자화된 \(K, V\)는 \(4\)-bit로 저장되므로 메모리 요구량 대폭 감소.

#### **2) 메모리 요구량**
- LLaMA-13B 모델 (FP16, 2048-토큰 입력):
  - **Weights**: 13B * 2 bytes = 26GB.
  - **KV Cache**: 2 * \(2048 \times d\) = 약 2.6GB (FP16 기준).
  - **양자화 후**:
    - KV 캐시는 \(2048 \times d\)의 4-bit 값으로 저장 → 약 0.65GB로 감소.

#### **3) 컴퓨팅 요구량**
- **Attention 중심 병목**:
  - Scaled Dot-Product Attention에서의 \(QK^T\) 연산이 병목.
  - 양자화로 인해 KV 캐시 메모리 접근 시간이 단축되어 병목 완화.
  
- **WKVQuant의 영향**:
  - POQ와 2D-Quantization을 사용하여 계산 정확도는 유지하면서 메모리 접근 및 전송 속도를 향상.

---

### **종합**
1. **입력 데이터와 추론 과정**:
   - 입력 토큰은 Transformer 블록을 거쳐 출력 확률로 변환되며, KV 캐시를 통해 효율적으로 연산.
   - POQ로 현재 KV 값을 Full-Precision 상태로 유지하여 정확도를 보장.

2. **모델 아키텍처**:
   - WKVQuant는 기존 Transformer 구조에 KV 캐시 양자화와 추가 최적화를 결합.

3. **연산 및 메모리 효율성**:
   - KV 캐시 양자화로 메모리 사용량을 기존 대비 4배 감소.
   - Attention 병목 완화를 통해 추론 속도 향상.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **1. 논문의 한계**
논문에서 제시한 WKVQuant의 한계는 다음과 같습니다:

1. **임시 활성화(Temporary Activations)의 미양자화**:
   - POQ 전략은 Key/Value 캐시만 양자화하고 임시 활성화는 Full-Precision으로 유지.
   - 긴 시퀀스나 대규모 배치에서 임시 활성화의 메모리 사용량 증가 가능성.
   - 이는 메모리 제한이 있는 환경에서 문제가 될 수 있음.

2. **하드웨어 가속 최적화 부족**:
   - 임시 활성화를 양자화하지 않음으로써 Nvidia INT8과 같은 고속 계산 유닛의 사용 제한.
   - 결과적으로 계산 효율성에서 손실이 있을 가능성.

3. **CRR(Cross-Block Reconstruction Regularization)의 시간 비용**:
   - CRR을 활용한 매개변수 최적화는 학습 시간 증가를 초래.
   - 13B 모델 기준 약 4시간의 추가 최적화 시간이 필요.

---

### **2. 한계를 극복하기 위한 연구 흐름**
이러한 한계를 극복하기 위해 다음과 같은 연구 흐름이 가능합니다:

---

#### **1) 임시 활성화의 양자화**
- **문제**:
  - 임시 활성화를 양자화하지 않아 메모리 사용량이 여전히 높음.
- **해결 가능성**:
  - **Dynamic Precision Quantization**:
    - 임시 활성화의 중요도에 따라 가변적인 정밀도를 적용.
    - 중요도가 낮은 활성화에는 8-bit 또는 4-bit 정밀도를 적용.
  - **Adaptive Quantization**:
    - 임시 활성화의 민감도를 동적으로 평가하여 양자화 수준을 조정.
    - 예: Layer-wise Sensitivity Analysis를 통해 민감도를 분석.
- **관련 연구**:
  - ZeroQuant (Yao et al., 2022): Fine-grained 양자화를 활용하여 활성화와 가중치를 동시에 양자화.

---

#### **2) 하드웨어 가속 최적화**
- **문제**:
  - POQ는 Full-Precision 연산이 포함되어 INT8과 같은 가속 유닛 사용이 제한됨.
- **해결 가능성**:
  - **Low-bit Arithmetic Compatibility**:
    - INT8 가속 유닛에서 POQ를 활용할 수 있도록 현재 값의 양자화를 정밀하게 조정.
  - **Unified Quantization Framework**:
    - 가중치, KV 캐시, 임시 활성화를 동일한 비트 폭으로 양자화해 하드웨어 친화적 설계.
- **관련 연구**:
  - SmoothQuant (Xiao et al., 2022): 활성화와 가중치 간의 양자화 변환을 통해 하드웨어 가속 호환성 향상.
  - GPTQ (Frantar et al., 2022): INT8 환경에서의 연산 최적화를 목표로 설계된 PTQ 기법.

---

#### **3) CRR의 시간 비용 감소**
- **문제**:
  - CRR을 통한 매개변수 최적화가 학습 시간 증가를 초래.
- **해결 가능성**:
  - **Gradient-Free Optimization**:
    - CRR 대신 비경사 기반 최적화 기법(예: 진화 알고리즘) 활용.
  - **Parallel Block Optimization**:
    - CRR을 병렬화하여 여러 블록에서 동시에 최적화 수행.
  - **Reduced Block-wise Sampling**:
    - 최적화에 필요한 블록 샘플 수를 줄여 계산 비용 감소.
- **관련 연구**:
  - OmniQuant (Shao et al., 2023): 계층 간 병렬 최적화를 활용해 양자화 시간을 단축.
  - RPTQ (Yuan et al., 2023): 양자화 매개변수를 클러스터링하여 최적화 속도 향상.

---

#### **4) 모델 구조 최적화**
- **문제**:
  - Attention 메커니즘의 계산 비용이 높고 메모리 사용량이 시퀀스 길이에 따라 선형적으로 증가.
- **해결 가능성**:
  - **Sparse Attention Mechanism**:
    - 중요한 토큰 간의 상호작용만 계산하여 연산 비용 감소.
  - **Efficient Transformer Variants**:
    - Linear Transformer나 Performer와 같은 효율적인 구조를 활용.
- **관련 연구**:
  - BigBird (Zaheer et al., 2020): 희소 Attention으로 긴 시퀀스를 처리.
  - FlashAttention (Dao et al., 2022): 메모리 사용량을 줄이고 계산 효율성을 향상.

---

### **3. 결론 및 새로운 연구 방향**
1. **임시 활성화 양자화**:
   - 임시 활성화 양자화의 적용은 메모리 사용량 문제를 해결할 중요한 방향.

2. **하드웨어 친화적 양자화**:
   - POQ와 같은 정밀도 유지 기법을 INT8 가속과 통합하는 연구가 필요.

3. **CRR 최적화 시간 감소**:
   - 병렬화 또는 경량화된 정규화 기법을 통해 최적화 시간을 단축.

4. **모델 구조 개선**:
   - Sparse Attention 또는 Linear Transformer 기법으로 계산 비용을 줄이는 방향이 유망.

---

### **요약**
- WKVQuant는 혁신적인 양자화 프레임워크지만, 임시 활성화 양자화, 하드웨어 가속 최적화, CRR 시간 비용 등의 문제가 여전히 존재.
- 이를 해결하기 위해 Dynamic Precision, Sparse Attention, Parallel Optimization 같은 기법이 적용될 수 있으며, 관련 연구 흐름이 이미 활발히 진행 중.
- 이러한 방향은 WKVQuant를 더욱 확장하고, 메모리 및 계산 효율성을 극대화할 잠재력을 제공합니다.