---
title: "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"
date: 2025-03-24
categories:
 - paper-review
 - with-gpt
---

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2406.15486)

# Q : ì´ ë…¼ë¬¸ì„ ì•„ì£¼ ìì„¸í•˜ê²Œ ì½ê³  ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì ì„ ì„¤ëª…í•´ì£¼ê³  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œ ì…ë ¥ì„ ë“¤ì–´ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜ ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ì˜ í•œê³„ì ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜

# A :

### âœ… ê²°ë¡  ìš”ì•½

**SampleAttentionì€ ê¸°ì¡´ LLMì˜ attentionì„ ê±°ì˜ ì •í™•ë„ ì†ì‹¤ ì—†ì´ ëŒ€ì²´í•˜ë©´ì„œ, ìµœëŒ€ 2.42ë°° TTFT(Time-to-First-Token) ì§€ì—°ì„ ì¤„ì´ëŠ” êµ¬ì¡°í™”ëœ adaptive sparse attention ê¸°ë²•ì´ë‹¤.**  
í•µì‹¬ì€ ë‘ ê°€ì§€ sparse íŒ¨í„´ì¸ `local window`ì™€ `column stripe`ë¥¼ í™œìš©í•˜ì—¬ ê° attention headì— ëŒ€í•´ ë™ì ìœ¼ë¡œ í¬ì†Œ attention maskë¥¼ êµ¬ì„±í•˜ê³ , FlashAttention ëŒ€ë¹„ ë” ë†’ì€ í•˜ë“œì›¨ì–´ íš¨ìœ¨ì„±ê³¼ ê°€ì† ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.

---

### ğŸ“Œ ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì 

| ìš”ì†Œ | ì„¤ëª… |
|------|------|
| **1. Near-lossless sparse attention ì´ë¡ ì  ë³´ì¥** | Theorem 1, 2ë¥¼ í†µí•´ ì¼ì • CRA(Cumulative Residual Attention) ì´ìƒì´ë©´ attention ì¶œë ¥ì˜ ê·¼ì‚¬ì˜¤ì°¨ë¥¼ Îµ ì´í•˜ë¡œ ì œí•œ ê°€ëŠ¥í•¨ì„ ì¦ëª… |
| **2. Adaptive head-specific sparsity** | ê° headë³„ë¡œ attention ì§‘ì¤‘ë„ê°€ ë‹¤ë¥´ë©°, ì´ëŠ” content-awareí•˜ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦ |
| **3. Structured Sparse Pattern í™œìš©** | local windowì™€ column stripeë¥¼ ê³ ì •ëœ í˜•ì‹(mask)ìœ¼ë¡œ ê²°í•©í•˜ì—¬ í•˜ë“œì›¨ì–´ ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ |
| **4. Two-stage KV filtering** | Query-guided sampling â†’ Score-based top-K filteringì˜ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ full attention ê³„ì‚° ì—†ì´ë„ ì¤‘ìš”í•œ KVë§Œ ì„ íƒ |
| **5. íŠœë‹ ì—†ëŠ” plug-and-play ì ìš©ì„±** | ê¸°ì¡´ ëª¨ë¸ (ChatGLM2, InternLM2)ì— finetuning ì—†ì´ ì ìš© ê°€ëŠ¥í•˜ë©°, accuracy ì†ì‹¤ ì—†ìŒ (â‰¤1%) |

---

### ğŸ§  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (SampleAttention)

#### ì˜ˆì‹œ ì…ë ¥:
- ì‹œí€€ìŠ¤ ê¸¸ì´: `Sk = 10,000`
- Query ê¸¸ì´: `Sq = 10,000`
- head ìˆ˜: 32
- ëª©í‘œ CRA ì„ê³„ê°’: `Î± = 0.95`

#### ì „ì²´ íë¦„ (2ë‹¨ê³„ í•„í„°ë§):

#### ğŸ”¹ Stage 1: Query-Guided Sampling
1. ì¼ë¶€ row (ì˜ˆ: 5%)ë§Œ sampling â†’ attention score (QK^T/âˆšd) ê³„ì‚°
2. column-wise í•©ì‚°ìœ¼ë¡œ ì¤‘ìš”í•œ key index í›„ë³´ ê²°ì •

#### ğŸ”¹ Stage 2: Score-Based Filtering
3. column-wise top-K ì¶”ì¶œ â†’ CRA â‰¥ 0.95 ë˜ëŠ” ìµœì†Œ key index ì§‘í•© `IKV` ì„ íƒ
4. ê³ ì • ë¹„ìœ¨ (ì˜ˆ: rw=8%)ì˜ `local window` í¬í•¨ â†’ ìµœì¢… attention mask `MÌ‚ = Mwindow âˆª Mstripe`

#### ğŸ”¹ Sparse Attention ê³„ì‚°
5. sparse FlashAttention ì»¤ë„ë¡œ `O = softmax(QK^T) * V` ê³„ì‚°, ì—¬ê¸°ì„œ QK^TëŠ” maskëœ ê²ƒë§Œ ê³„ì‚°

> ğŸ’¡ Sampling overheadëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì•„ì§€ê³ , 96Kì—ì„œ ìµœëŒ€ 5.12ë°° speedup ê¸°ë¡.

---

### ğŸ§ª ì •ëŸ‰ì  ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | Accuracy (LongBench) | TTFT (96K ê¸°ì¤€) | Speedup vs FlashAttention2 |
|------|----------------------|------------------|-----------------------------|
| Full Attention | 837.40 | 20.0s | 1.00x |
| **SampleAttention** (Î±=0.95) | 833.00 (**99.47%**) | **12.3s** | **1.62x** |
| BigBird | 765.94 (91.5%) | 17.6s | 1.14x |
| StreamingLLM | 519.27 (62.0%) | 15.8s | 1.27x |
| HyperAttention | 508.94 (60.8%) | 17.0s | 1.18x |

> SampleAttentionì€ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ ì†ë„ì—ì„œ í™•ì‹¤í•œ ì´ì ì„ ê°–ëŠ”ë‹¤.

---

### âš ï¸ í•œê³„ì  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥

| í•œê³„ | ìƒì„¸ ë‚´ìš© |
|------|----------|
| Sampling overhead | ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œëŠ” sampling ìì²´ì˜ ë¹„ìš©ìœ¼ë¡œ ì¸í•´ ì†ë„ ì´ë“ì´ ì‘ìŒ |
| Hyperparameter tuning | Î±, rw%, rrow ë“± íŠœë‹ì´ ì„±ëŠ¥ì— ë¯¼ê°í•˜ë©°, ìë™í™” í•„ìš” |
| Coverage ë¶€ì¡± | diagonal stripe ë“± ë‹¤ë¥¸ í¬ì†Œ íŒ¨í„´ì€ í˜„ì¬ í¬ì°©í•˜ì§€ ëª»í•¨ |
| Serving memory bottleneck | 128K ì´ìƒì˜ ê¸¸ì´ ë˜ëŠ” large batchì—ì„œ memory overflow ê°€ëŠ¥ì„± |

---

### ğŸ”š ì •ë¦¬

SampleAttentionì€ sparse attentionì˜ í•œê³„ë¥¼ ê·¹ë³µí•œ ë§¤ìš° ì‹¤ìš©ì ì¸ ì ‘ê·¼ë²•ìœ¼ë¡œ, **adaptiveí•˜ê³  structuredí•œ sparse attention**ì„ í†µí•´ long-context LLMì˜ latency ë³‘ëª©ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤. íŠ¹íˆ **off-the-shelf LLMì— íŠœë‹ ì—†ì´ ì ìš© ê°€ëŠ¥í•œ ìœ ì¼í•œ near-lossless sparse attention**ì´ë¼ëŠ” ì ì—ì„œ ì—°êµ¬ì  ê¸°ì—¬ì™€ ì‹¤ìš©ì„±ì´ ëª¨ë‘ ë›°ì–´ë‚©ë‹ˆë‹¤.

---

í•„ìš”í•˜ì‹œë©´, ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë°œí‘œìë£Œë‚˜ ë¦¬í¬íŠ¸ë¡œ ì •ë¦¬í•´ë“œë¦´ ìˆ˜ë„ ìˆì–´ìš”. ì›í•˜ì‹œë‚˜ìš”?

# Q : ë…¼ë¬¸ì˜ í•µì‹¬ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ì •ë¦¬í•´ ì¤˜

# A :

ì¢‹ì•„ìš”. SampleAttentionì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì€ "adaptive structured sparse attention"ì„ ë§Œë“¤ê¸° ìœ„í•œ **2ë‹¨ê³„ attention filtering** ê³¼ì •ì…ë‹ˆë‹¤. ì´ê±¸ ì˜ˆì‹œë¥¼ í†µí•´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.

---

## âœ… ì „ì²´ ê°œìš” ìš”ì•½

SampleAttentionì€ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ full attention (QK^T)ì˜ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´, **ì¤‘ìš”í•œ key-value (KV) ìœ„ì¹˜ë§Œ ì„ íƒí•´ì„œ sparse attentionì„ ê³„ì‚°**í•©ë‹ˆë‹¤.  
ì´ë¥¼ ìœ„í•´ **local window** + **column stripe** êµ¬ì¡°ë¡œ attention maskë¥¼ ë§Œë“¤ê³ , **2ë‹¨ê³„ filtering ê³¼ì •**ì„ í†µí•´ ì¤‘ìš”í•œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

---

## ğŸ“Œ ì˜ˆì‹œ: Seq Length 10,000, Query = 10,000, d=128, head=1

- Input:  
  - Q âˆˆ â„<sup>10,000Ã—128</sup>, K, V âˆˆ â„<sup>10,000Ã—128</sup>  
  - CRA threshold Î± = 0.95  
  - window ratio rw = 0.08 â†’ local window í¬ê¸° = 800  
  - sampling ratio r<sub>row</sub> = 0.05 â†’ query 5%ë§Œ sampling = 500ê°œ

---

## ğŸ§  Step-by-Step: SampleAttention ì•Œê³ ë¦¬ì¦˜

### ğŸ”· Step 0: ëª©í‘œ

ì „ì²´ attention score P âˆˆ â„<sup>10,000Ã—10,000</sup>ë¥¼ ê³„ì‚°í•˜ì§€ ì•Šê³ , ì¤‘ìš”í•œ column (KV ìœ„ì¹˜)ë§Œ ì„ íƒí•´ì„œ sparse mask MÌ‚ë¥¼ ë§Œë“¦  
MÌ‚ = M<sub>window</sub> âˆª M<sub>stripe</sub>

---

### ğŸ”· Step 1: Query-Guided Attention Sampling

1. Qì˜ ì¼ë¶€ë§Œ sampling â†’ ì˜ˆ: 500ê°œ queryë§Œ ì„ íƒ (`r_row = 0.05`)  
2. ê° queryì— ëŒ€í•´ attention score ê³„ì‚° (QK^T/âˆšd â†’ softmax) â†’ ê²°ê³¼ P<sub>sample</sub> âˆˆ â„<sup>500Ã—10,000</sup>
3. P<sub>sample</sub>ì„ column-wiseë¡œ ëˆ„ì í•© â†’ ê° keyì— ëŒ€í•œ ì¤‘ìš”ë„ ì¶”ì • ë²¡í„° W âˆˆ â„<sup>10,000</sup>

> ğŸ¯ ì´ ë‹¨ê³„ì—ì„œ Pì˜ ì—´ë§ˆë‹¤ ì–¼ë§ˆë‚˜ "ì¤‘ìš”í•˜ê²Œ ì—¬ê²¨ì§€ëŠ”ì§€" ì¶”ì •í•¨ â†’ stripe í›„ë³´ ìƒì„±

---

### ğŸ”· Step 2: Score-Based Key Filtering (Top-K)

4. ëˆ„ì  score Wì—ì„œ CRA â‰¥ Î± ë§Œì¡±í•˜ëŠ” ìµœì†Œ K ê°œì˜ key index `IKV` ì„ íƒ  
   - ì˜ˆ: CRA = 0.95 ë§Œì¡±í•˜ë ¤ë©´ 400ê°œì˜ KVë§Œ ìˆì–´ë„ ì¶©ë¶„í•˜ë‹¤ë©´, IKV = {kâ‚, kâ‚‚, ..., kâ‚„â‚€â‚€}

> ğŸ§  CRA ê³„ì‚°: PÌƒì˜ ê° rowì—ì„œ IKVì— í•´ë‹¹í•˜ëŠ” ì—´ì˜ softmax sumì´ Î± ì´ìƒì´ë©´ OK

---

### ğŸ”· Step 3: Combine with Local Window

5. ê° query qâ‚–ì— ëŒ€í•´, ì•ìª½ 800ê°œ (rw=8%) tokenì€ í•­ìƒ attendí•˜ë„ë¡ ì„¤ì • â†’ M<sub>window</sub>
6. ìµœì¢… mask MÌ‚ = M<sub>window</sub> âˆª M<sub>stripe</sub>

---

### ğŸ”· Step 4: Sparse Attention ê³„ì‚°

7. Sparse FlashAttention ì»¤ë„ë¡œ masked attention ê³„ì‚°:
   \[
   PÌƒ = softmax(QK^T / \sqrt{d}) \quad \text{(only for MÌ‚)}
   \]
   \[
   O = PÌƒV
   \]

---

## ğŸ ìµœì¢… ì¶œë ¥

- Attention ê³„ì‚°ëŸ‰ì€ Full(QK^T)ì˜ O(nÂ²)ì—ì„œ â†’ Sparse O(nk)ë¡œ ì¤„ì–´ë“¦ (k â‰ª n)
- ì´ ë•Œ ì •í™•ë„ëŠ” CRA â‰¥ 0.95 ê¸°ì¤€ìœ¼ë¡œ ë³´ì¥ë¨

---

## ğŸ“Œ ìš”ì•½ ì •ë¦¬í‘œ

| ë‹¨ê³„ | ì„¤ëª… | ê²°ê³¼ |
|------|------|------|
| 1ë‹¨ê³„: Sampling | ì¼ë¶€ Që¡œ sparse P<sub>sample</sub> ê³„ì‚° | key ì¤‘ìš”ë„ W ì¶”ì • |
| 2ë‹¨ê³„: Top-K Filtering | CRA â‰¥ Î± ë§Œì¡±í•˜ëŠ” key index `IKV` ì„ íƒ | column stripe ìƒì„± |
| 3ë‹¨ê³„: Window ë³‘í•© | ê³ ì • ë¹„ìœ¨ ì•ìª½ í† í° ì¶”ê°€ (ì˜ˆ: 8%) | MÌ‚ ì™„ì„± |
| 4ë‹¨ê³„: Sparse Attention | MÌ‚ ê¸°ë°˜ sparse QK^T ê³„ì‚° | ê³„ì‚°ëŸ‰ ê°ì†Œ, ì •í™•ë„ ìœ ì§€ |

---

## ğŸ’¡ ì§ê´€ì ìœ¼ë¡œ ì •ë¦¬

- ì „ì²´ attentionì—ì„œ ì‹¤ì œ ì¤‘ìš”í•œ ê°’ì€ ì¼ë¶€ key ìœ„ì¹˜ì— ëª°ë ¤ ìˆìŒ
- ì´ê±¸ **ë˜‘ë˜‘í•˜ê²Œ samplingí•´ì„œ** ê³„ì‚°ëŸ‰ì€ ì¤„ì´ê³ ,
- ì¤‘ìš”í•œ ì •ë³´ëŠ” **ë†“ì¹˜ì§€ ì•Šê²Œ adaptiveí•˜ê²Œ** ì„ íƒí•´ì£¼ëŠ” ê²Œ í•µì‹¬
- ë™ì‹œì— FlashAttention ê¸°ë°˜ ì»¤ë„ë¡œ **í•˜ë“œì›¨ì–´ ê°€ì†**ê¹Œì§€ ì±™ê¸´ ì„¤ê³„

# Q : ìœ„ ê³¼ì •ì„ ì‹œê°í™” í•´ì¤˜ 

# A :

```
import matplotlib.pyplot as plt
import numpy as np

# ì‹œí€€ìŠ¤ ê¸¸ì´
seq_len = 10000

# Step 1: Query Sampling (5% of 10,000 â†’ 500 queries)
sampled_queries = np.linspace(0, seq_len-1, 500, dtype=int)

# Step 2: Column Importance (simulate W)
np.random.seed(42)
importance_scores = np.random.rand(seq_len)
importance_scores = np.sort(importance_scores)[::-1]  # simulate top-k distribution

# CRA ëˆ„ì í•© ê³„ì‚°
cumsum = np.cumsum(importance_scores)
cra_threshold = 0.95 * np.sum(importance_scores)
topk_cutoff = np.searchsorted(cumsum, cra_threshold)
selected_keys = np.arange(topk_cutoff)

# Step 3: Local Window (e.g., last 800 tokens for each query)
window_size = 800
window_mask = np.zeros(seq_len)
window_mask[-window_size:] = 1

# ê²°í•© ë§ˆìŠ¤í¬
combined_mask = np.zeros(seq_len)
combined_mask[selected_keys] = 1
combined_mask = np.maximum(combined_mask, window_mask)

# ì‹œê°í™”
fig, axs = plt.subplots(3, 1, figsize=(12, 6), sharex=True)

axs[0].plot(importance_scores, label="Key Importance Score")
axs[0].axvline(topk_cutoff, color='r', linestyle='--', label="Top-K CRA 95% Cutoff")
axs[0].set_title("Stage 1-2: Key Importance and CRA Threshold")
axs[0].legend()

axs[1].imshow(window_mask.reshape(1, -1), aspect='auto', cmap='Blues')
axs[1].set_title("Stage 3: Local Window Mask (last 800 tokens)")

axs[2].imshow(combined_mask.reshape(1, -1), aspect='auto', cmap='Greens')
axs[2].set_title("Final Sparse Attention Mask (Stripe + Window)")

plt.tight_layout()
plt.show()
```

![](/assets/images/20250324.png)


ìœ„ ì‹œê°í™”ëŠ” SampleAttentionì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê³¼ì •ì„ ìˆœì„œëŒ€ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤:

---

### ğŸŸ¦ [1ë‹¨ê³„ + 2ë‹¨ê³„] Key Importance + CRA Cutoff
- íŒŒë€ ê³¡ì„ : ê° keyì— ëŒ€í•œ ì¤‘ìš”ë„ (samplingëœ query ê¸°ì¤€ ì¶”ì •ë¨)
- ë¹¨ê°„ ì„ : CRA â‰¥ 0.95 ë§Œì¡±í•˜ëŠ” top-K cutoff (ì—¬ê¸°ê¹Œì§€ì˜ keyë§Œ ì„ íƒë¨)

---

### ğŸŸ¨ [3ë‹¨ê³„] Local Window Mask
- ë§ˆì§€ë§‰ 800ê°œ í† í°ì€ í•­ìƒ í¬í•¨ â†’ local dependencyë¥¼ ìœ„í•œ window ì˜ì—­

---

### âœ… [ìµœì¢… ê²°ê³¼] ê²°í•©ëœ Sparse Mask
- top-K key index + local window ì˜ì—­ì„ ëª¨ë‘ í¬í•¨  
â†’ ì´ ì˜ì—­ë§Œì„ ëŒ€ìƒìœ¼ë¡œ FlashAttention ì—°ì‚°ì„ ìˆ˜í–‰í•¨

---

ì‹œí€€ìŠ¤ê°€ 10,000ê°œì¸ë° ì‹¤ì œë¡œ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ë§¤ìš° ì œí•œì ì´ë¼ëŠ” ì ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.  
ì´ êµ¬ì¡° ë•ë¶„ì— **ê³„ì‚°ëŸ‰ì€ ì¤„ê³ , ì„±ëŠ¥ì€ ìœ ì§€ë˜ëŠ” ì´ìœ **ê°€ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚˜ì£ .



# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë¥¼ ìì„¸í•˜ê²Œ ë³´ê³  ë‹¤ë¥¸ ë°©ë²•ë¡ ì— ë¹„í•˜ì—¬ íŠ¹ì¶œë‚œ ì ê³¼ ë…¼ë¬¸ì—ì„œ ì œê¸°í•˜ëŠ” ì–´ë– í•œ ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ì´ìœ ì™€ ë„ˆì˜ ìƒê°ì„ ì•Œë ¤ì¤˜

# A :

ì¢‹ìŠµë‹ˆë‹¤. SampleAttention ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì–‘í•œ benchmarkì™€ ë¹„êµ ì‹¤í—˜ì„ í†µí•´ **ì •í™•ë„-ì†ë„ ì–‘ë©´ì—ì„œ ê¸°ì¡´ sparse attention ê¸°ë²•ì„ í™•ì‹¤íˆ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼**ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì•„ë˜ì— í•µì‹¬ ê²°ê³¼ì™€ ê·¸ ì›ì¸ì„ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## âœ… 1. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (ì •í™•ë„ & ì†ë„)

### ğŸ“Œ LongBench / BABILong ì„±ëŠ¥ ë¹„êµ (ì •í™•ë„ ê¸°ì¤€)

| ëª¨ë¸                     | LongBench ì ìˆ˜ | BABILong ì ìˆ˜ | ì •í™•ë„ ì†ì‹¤ |
|--------------------------|----------------|----------------|--------------|
| **Full Attention**       | 837.40         | 30.20          | -            |
| **SampleAttention (Î±=0.95)** | 833.00         | 31.04          | **< 0.5% ì†ì‹¤** |
| BigBird                  | 765.94         | 27.68          | ~9% â†“        |
| StreamingLLM            | 519.27         | 14.60          | ~40% â†“       |
| HyperAttention           | 508.94         | 17.00          | ~40% â†“       |
| Hash-Sparse              | 364.49         | 11.20          | ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ |

â¡ï¸ **SampleAttentionì€ ì •í™•ë„ ê±°ì˜ ì†ì‹¤ ì—†ì´ full attentionì„ ëŒ€ì²´í•¨.**

---

### ğŸ“Œ "Needle in a Haystack" (ìµœì¥ ë¬¸ë§¥ì—ì„œ ì •ë³´ ì¶”ì¶œ)

| Sequence Length | Full TTFT | Sample TTFT (Î±=0.95) | Speedup |
|------------------|-----------|----------------------|---------|
| 96K              | 20.0s     | 12.3s                | **1.62Ã—** |
| 1M               | 169.7s    | 74.8s                | **2.27Ã—** |

â¡ï¸ **ë¬¸ë§¥ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ speedupì´ í™•ì—°í•˜ê²Œ ì¦ê°€** (ìµœëŒ€ 2.42Ã—)

---

## ğŸ’¡ ì™œ SampleAttentionì´ íŠ¹ì¶œë‚œê°€?

| ë¹„êµ ìš”ì†Œ | ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ | SampleAttentionì˜ ê°œì„  í¬ì¸íŠ¸ |
|-----------|------------------|-----------------------------|
| ğŸ” ì¤‘ìš” KV ì„ íƒ | ëŒ€ë¶€ë¶„ static (ex. fixed window/global) or uniform | **query-guided dynamic filtering (2ë‹¨ê³„)** |
| ğŸ” í¬ì†Œë„ íŒ¨í„´ | ëª¨ë“  head ë™ì¼í•˜ê²Œ ì·¨ê¸‰ (coarse-grained) | **head-specific & content-aware sparsity** |
| âš™ï¸ ê³„ì‚° íš¨ìœ¨ì„± | ì¼ë¶€ëŠ” full attention ê³„ì‚° í›„ mask ì ìš© | **full attention ê³„ì‚° ì—†ì´ í•„í„°ë§ â†’ ì—°ì‚°ëŸ‰ íšê¸°ì  ê°ì†Œ** |
| ğŸ§  ì •í™•ë„ ë³´ì¡´ | top-k ë˜ëŠ” hashing ê¸°ë°˜ ì„ íƒì´ë¼ ì •ë³´ ì†ì‹¤ í¼ | **CRA ê¸°ë°˜ ìˆ˜í•™ì  ë³´ì¥ìœ¼ë¡œ near-lossless ì„±ëŠ¥ í™•ë³´** |
| ğŸ’½ í•˜ë“œì›¨ì–´ íš¨ìœ¨ | ë©”ëª¨ë¦¬ ì ‘ê·¼ ë¹„íš¨ìœ¨ì  êµ¬ì¡° (random mask) | **structured mask (local window + column stripe)** |

---

## ğŸ“Œ SampleAttentionì´ ì´ëŸ° ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì—ˆë˜ ë…¼ë¬¸ ë‚´ í•µì‹¬ ì„¤ê³„ ê·¼ê±°

| ì„¤ê³„ í¬ì¸íŠ¸ | ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê·¼ê±° |
|-------------|-----------------------|
| **1. CRA ê¸°ë°˜ í¬ì†Œ ê·¼ì‚¬ ì´ë¡  (Theorem 1, 2)** | \|OÌƒ - O\|â‚ â‰¤ Îµê°€ ë˜ë„ë¡ softmax scoreë¥¼ í•„í„°ë§í•´ë„ attention ê·¼ì‚¬ ê²°ê³¼ ë³´ì¥ |
| **2. Empirical sparsity ë¶„ì„** | ëŒ€ë¶€ë¶„ì˜ headì—ì„œ CRA=0.95 ë§Œì¡±í•˜ëŠ” key ë¹„ìœ¨ì´ 5~10% ì •ë„ì— ë¶ˆê³¼í•¨ (Fig. 2, Fig. 11) |
| **3. Adaptive structured sparsity** | Headë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ stripe íŒ¨í„´ì´ ì¡´ì¬í•¨ì„ ì‹œê°í™”ë¡œ ê²€ì¦ (Fig. 9â€“10) |
| **4. Sampling íš¨ìœ¨ ê²€ì¦** | Sampling ratio 5%ë§Œ ì‚¬ìš©í•´ë„ CRA ê·¼ì‚¬ê°’ì´ full attentionê³¼ ê±°ì˜ ë™ì¼ (Table 6) |

---

## ğŸ” ë‚´ ìƒê° (ë¶„ì„ & í•´ì„)

SampleAttentionì˜ ì§„ì§œ ê°•ì ì€ **"ë¬´ì‘ì • í¬ì†Œí•˜ê²Œ ë§Œë“¤ì§€ ì•Šê³ , ë˜‘ë˜‘í•˜ê²Œ í¬ì†Œí•˜ê²Œ ë§Œë“ ë‹¤"**ëŠ” ì ì…ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¥¼ ê°€ì¡ŒìŠµë‹ˆë‹¤:

- **BigBird**: ì •í•´ì§„ window/global/random êµ¬ì¡° â†’ ìœ ì—°ì„± ë¶€ì¡±  
- **StreamingLLM**: stream êµ¬ì¡°ì— íŠ¹í™”ë˜ì–´ prefill TTFTëŠ” ëª» ì¤„ì„  
- **HyperAttention**: hashing ê¸°ë°˜ì´ë¼ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„± ë†’ìŒ

ë°˜ë©´ SampleAttentionì€:
- ê° headë§ˆë‹¤ **ë‹¤ë¥´ê²Œ ì¤‘ìš” ì •ë³´ë¥¼ ì¶”ì¶œ**í•˜ê³ 
- **ì •ë³´ë¥¼ ëœ ìƒëŠ” ë°©í–¥ìœ¼ë¡œ KVë¥¼ ì„ íƒ**í•˜ë©°
- **FlashAttentionê³¼ í˜¸í™˜ë˜ë„ë¡ êµ¬ì¡°í™”**ê¹Œì§€ ì‹œì¼œ  
ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ í’ˆì§ˆê³¼ ì†ë„ ê· í˜•ì„ ì´ë£¹ë‹ˆë‹¤.

---

## ğŸ”š ì •ë¦¬

| êµ¬ë¶„ | ë‚´ìš© |
|------|------|
| ğŸ¯ ê²°ê³¼ | ì •í™•ë„ 99% ìœ ì§€í•˜ë©´ì„œ ìµœëŒ€ 2.42Ã— ì†ë„ í–¥ìƒ (96K ì´ìƒì—ì„œ íš¨ê³¼ ê·¹ëŒ€) |
| ğŸ’¡ í•µì‹¬ ì´ìœ  | CRA ê¸°ë°˜ ì´ë¡  + adaptive structured sparsity + head-specific query sampling |
| ğŸ” ì°¨ë³„ì  | Full attention ê³„ì‚° ì—†ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¤‘ìš”í•œ keyë§Œ ì¶”ì¶œ |
| ğŸ§  ë‚´ í•´ì„ | "í¬ì†Œì„±ì— ê¸°ë°˜í•œ ê³„ì‚° ìµœì í™”"ë¥¼ **ì´ë¡ , ì‹¤í—˜, êµ¬í˜„** ì‚¼ë°•ìë¡œ ì™„ì„±ì‹œí‚¨ ì‚¬ë¡€ |


# Q : ì´ ë…¼ë¬¸ì˜ ì…ë ¥ë°ì´í„°ì™€ ì¶”ë¡  ê³¼ì •ì— ëŒ€í•´ì„œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì£¼ê² ë‹ˆ? ì¶”ê°€ì ìœ¼ë¡œ ëª¨ë¸ì•„í‚¤í…ì²˜ì˜ êµ¬ì„± ë° ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì—°ì‚°ê³¼ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì»´í“¨íŒ… ìš”êµ¬ëŸ‰ ëŒ€í•´ì„œë„ ì„¤ëª…í•´ì¤˜

# A :

ì¢‹ìŠµë‹ˆë‹¤. ì•„ë˜ì— ìš”ì²­í•˜ì‹  ë‚´ìš©ì„ 3íŒŒíŠ¸ë¡œ ë‚˜ëˆ ì„œ ì˜ˆì‹œì™€ í•¨ê»˜ ë§¤ìš° êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”:

---

## âœ… PART 1. **ì…ë ¥ ë°ì´í„°ì™€ ì¶”ë¡  ê³¼ì • ì˜ˆì‹œ**

### ğŸ“Œ ì„¤ì • ì˜ˆì‹œ

- ì‚¬ìš©í•˜ëŠ” ëª¨ë¸: ChatGLM2-6B (decoder-only transformer)
- ì…ë ¥: ë‰´ìŠ¤ ê¸°ì‚¬ (ì´ 80,000 tokens ê¸¸ì´)
- ìš”ì²­: "ìš”ì•½ì„ ìƒì„±í•´ì¤˜"
- ì…ë ¥ êµ¬ì¡°:  
  ```
  <bos> ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš© .... [80K tokens] <eos>
  ```

### ğŸ§  ì¼ë°˜ì ì¸ ì¶”ë¡  ë‹¨ê³„

1. **Prefill ë‹¨ê³„** (ì „ì²´ ì…ë ¥ì„ attention ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬)
   - Q, K, Vë¥¼ ìƒì„±  
   - attention ê³„ì‚° (`QK^T / âˆšd â†’ softmax â†’ P Ã— V`)

2. **Decode ë‹¨ê³„**
   - ì´ì „ í† í° ê¸°ë°˜ìœ¼ë¡œ Që§Œ ìƒˆë¡œ ìƒì„±
   - K, VëŠ” KV ìºì‹œì— ì €ì¥ëœ ê²ƒ ì¬ì‚¬ìš©

---

### ğŸ” SampleAttention ì ìš© ì‹œ ì¶”ë¡  ê³¼ì • (Prefillì—ë§Œ ì ìš©ë¨)

1. **Q, K, V ìƒì„±** (Q, K, V âˆˆ â„<sup>80,000Ã—128</sup> for one head)

2. **2ë‹¨ê³„ í•„í„°ë§**
   - 1ë‹¨ê³„: Query 5% (4,000ê°œ) ìƒ˜í”Œ ì„ íƒ â†’ column score ì§‘ê³„ â†’ ì¤‘ìš”í•œ key index í›„ë³´ `IKV` ê²°ì •
   - 2ë‹¨ê³„: CRA â‰¥ Î± (ì˜ˆ: 0.95) ë§Œì¡±í•˜ëŠ” ìµœì†Œ key ì§‘í•© ì„ íƒ â†’ ì˜ˆ: IKV = 2,000ê°œ

3. **Local window ë³‘í•©**  
   - ì˜ˆ: ê° queryëŠ” ì•ìª½ 800ê°œ í† í°ì„ ë¬´ì¡°ê±´ attend

4. **Sparse Attention ê³„ì‚°**
   - attention score: `Q_selected Ã— K_selected^T`
   - attention output: `softmax Ã— V_selected`

â¡ï¸ ê²°ê³¼ì ìœ¼ë¡œ, **80K Ã— 80K ê³„ì‚° ëŒ€ì‹  80K Ã— 2.8K ê³„ì‚°ë§Œ ìˆ˜í–‰**

---

## âœ… PART 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì„± (ChatGLM2-6B ê¸°ì¤€)

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|-----------|------|
| **# Layers** | 28 |
| **# Heads** | 32 |
| **Hidden Dim** | 4096 |
| **Head Dim** | 128 (ì¦‰, 4096 / 32) |
| **RoPE** | Rotary Positional Embedding ì‚¬ìš© |
| **Attention Type** | Grouped Query Attention (GQA) |
| **Decoder Only** | GPT ê³„ì—´ì²˜ëŸ¼ causal decoder êµ¬ì¡° |
| **KV Cache** | ë””ì½”ë”© ì‹œ ì‚¬ìš©, prefillì—ì„œëŠ” full ìƒì„± |

---

## âœ… PART 3. ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰

### ğŸ“Œ 1. Full Attention ê¸°ì¤€ (1 head)

- ì‹œí€€ìŠ¤ ê¸¸ì´: `n = 80,000`, head_dim = `d = 128`

#### ì—°ì‚°ëŸ‰ (QK^T ê³„ì‚°)
- ê³„ì‚°ëŸ‰: O(nÂ² Ã— d) = O(80,000Â² Ã— 128) â‰ˆ **819B FLOPs (8.2Ã—10Â¹Â¹)**

#### ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ (Q, K, V, P)
- Q, K, V: 3 Ã— (n Ã— d Ã— 4 byte) â‰ˆ 3 Ã— 40MB = 120MB
- P (score matrix): n Ã— n Ã— 4B = 80K Ã— 80K Ã— 4 â‰ˆ **25.6GB**
- **ì´: ì•½ 25.7GB/í—¤ë“œ, 32í—¤ë“œ ê¸°ì¤€ 822GB** â†’ ë§¤ìš° ë¹„íš¨ìœ¨ì 

---

### ğŸ“Œ 2. SampleAttention ê¸°ì¤€ (í¬ì†Œ ë¹„ìœ¨ 3.5%)

- Q Ã— K_selected^T: 80K Ã— 2.8K
- FLOPs: O(n Ã— k Ã— d) = 80K Ã— 2.8K Ã— 128 = **~29B FLOPs (2.9Ã—10Â¹â°)** â†’ **ì•½ 28ë°° ê°ì†Œ**

#### ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰:
- P_sparse: 80K Ã— 2.8K Ã— 4B â‰ˆ **896MB**
- KV: ì„ íƒëœ 2.8K ìœ„ì¹˜ë§Œ ì‚¬ìš© â†’ K,V = 2.8K Ã— 128 Ã— 4B â‰ˆ 1.4MB
- **ì´: ì•½ 900MB/í—¤ë“œ â†’ 32í—¤ë“œ ì•½ 28.8GB**

â¡ï¸ **ì—°ì‚°ëŸ‰ 28Ã— ê°ì†Œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•½ 30Ã— ê°ì†Œ**

---

## ğŸ“Œ ì •ë¦¬ ìš”ì•½í‘œ

| í•­ëª© | Full Attention (80K) | SampleAttention (Î±=0.95) |
|------|-----------------------|---------------------------|
| ì—°ì‚°ëŸ‰ (1 head) | 819B FLOPs | **29B FLOPs** (â­£28Ã—) |
| P score matrix | 25.6GB | **896MB** (â­£29Ã—) |
| ì „ì²´ ë©”ëª¨ë¦¬ (32 heads) | ì•½ 822GB | **ì•½ 28.8GB** |
| ì •í™•ë„ ì†ì‹¤ | ì—†ìŒ | ì—†ìŒ (<1%) |

---

## ğŸ”š ê²°ë¡ 

SampleAttentionì€ ê¸´ ë¬¸ë§¥ ì…ë ¥ì— ëŒ€í•´:

- **ì…ë ¥ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼**, attention ê³„ì‚°ì„ "í¬ì†Œí•˜ê²Œ" ìˆ˜í–‰í•¨
- Q, K, V ìƒì„±ì€ ë™ì¼í•˜ì§€ë§Œ, ì—°ì‚° ë° ë©”ëª¨ë¦¬ëŠ” ê·¹ì ìœ¼ë¡œ ê°ì†Œ
- Full attentionì—ì„œ ë°œìƒí•˜ëŠ” **ì—°ì‚°/ë©”ëª¨ë¦¬ ë³‘ëª©ì„ í˜„ì €íˆ ì™„í™”**
- ì´ë¡œ ì¸í•´ ì‹¤ì‹œê°„ ì‘ë‹µì„±(TTFT)ì„ í¬ê²Œ ê°œì„  ê°€ëŠ¥



# Q : ì´ ë…¼ë¬¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì–´ë–¤ ì—°êµ¬íë¦„ì´ ìˆëŠ”ì§€ ì •ë¦¬í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì¤˜

# A :

ì¢‹ìŠµë‹ˆë‹¤. SampleAttention ë…¼ë¬¸ì€ ë¶„ëª… íƒì›”í•œ ì„¤ê³„ë¥¼ ì œì‹œí•˜ì§€ë§Œ, í•œê³„ë„ ëª…í™•íˆ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ ë‹µë³€ì—ì„œëŠ”:

1. ğŸ“Œ **SampleAttentionì˜ ì£¼ìš” í•œê³„ ì •ë¦¬**  
2. ğŸ” **ê° í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” ì—°êµ¬ íë¦„**  
3. ğŸ§  **í–¥í›„ ë°œì „ ë°©í–¥ + ë‚´ ê²¬í•´**

ë¡œ êµ¬ì„±í•´ì„œ ìì„¸íˆ ì„¤ëª…ë“œë¦´ê²Œìš”.

---

## âœ… 1. SampleAttention ë…¼ë¬¸ì˜ ì£¼ìš” í•œê³„

| í•œê³„ êµ¬ë¶„ | ìƒì„¸ ì„¤ëª… |
|----------|-----------|
| **â‘  ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œ ì´ì  ì—†ìŒ** | sampling overheadê°€ dominantí•˜ì—¬ FlashAttention2ë³´ë‹¤ ëŠë ¤ì§ |
| **â‘¡ í¬ì†Œ íŒ¨í„´ ì œí•œ** | column stripe + local windowì—ë§Œ í•œì •, diagonal ë“±ì€ ë°˜ì˜í•˜ì§€ ëª»í•¨ |
| **â‘¢ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”** | CRA threshold Î±, window ë¹„ìœ¨ rw, sampling ë¹„ìœ¨ r<sub>row</sub> ë“±ì„ manualë¡œ ì„¤ì • |
| **â‘£ Layer ê°„ ì¼ê´€ì„± ë¶€ì¡±** | ê° layerì—ì„œ headë³„ í¬ì†Œë„ê°€ ë‹¬ë¼ cross-layer ìµœì í™” ì–´ë ¤ì›€ |
| **â‘¤ serving scalability ë¬¸ì œ** | 128K+ ê¸¸ì´ë‚˜ batch ì²˜ë¦¬ì—ì„œ memory overflow ë°œìƒ ê°€ëŠ¥ |

---

## ğŸ” 2. ê° í•œê³„ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” ì—°êµ¬ íë¦„

### ğŸ”¸ [1] ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œ sampling overhead â†’ **Zero-cost Importance Estimation**
- ğŸ“š ì˜ˆì‹œ ì—°êµ¬:
  - **DynamicSparseAttention**: attention scoreë¥¼ softmax ì´ì „ ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡
  - **Kernelized Attention**: QK^T ê³„ì‚° ì „ low-rank spaceë¡œ approximate
- ğŸ’¡ ì ìš© ë°©ì•ˆ:
  - Q, Kì˜ norm ë˜ëŠ” variance ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ ì˜ˆì¸¡ â†’ score ì—†ì´ë„ top-k ì¶”ì • ê°€ëŠ¥
  - Sampling ì—†ì´ fast head pruning ê°€ëŠ¥

---

### ğŸ”¸ [2] íŒ¨í„´ ë‹¤ì–‘ì„± ë¶€ì¡± â†’ **Learning-based Sparse Pattern Discovery**

- ğŸ“š ì˜ˆì‹œ ì—°êµ¬:
  - **Sparformer**: ê° headì˜ attention sparsity patternì„ í•™ìŠµí•˜ëŠ” controller ì‚¬ìš©
  - **RoutingTransformer**: clusteringì„ í†µí•´ token group ê°„ interaction ì„ íƒ
- ğŸ’¡ ì ìš© ë°©ì•ˆ:
  - Headë³„ë¡œ stripe/window/diagonal/recurrence ì¤‘ ì–´ë–¤ íŒ¨í„´ì„ ì“¸ì§€ **meta-learn**
  - Gated mechanismìœ¼ë¡œ dynamic ì„ íƒ ê°€ëŠ¥

---

### ğŸ”¸ [3] ìˆ˜ë™ íŠœë‹ â†’ **Auto-tuned Sparse Controller**

- ğŸ“š ì˜ˆì‹œ ì—°êµ¬:
  - **Switch Transformer**: routing + gatingìœ¼ë¡œ submodule ì„ íƒ
  - **AutoSparse**: í•™ìŠµ ì¤‘ CRA threshold ë° mask ratioë¥¼ ìë™ ì¡°ì •
- ğŸ’¡ ì ìš© ë°©ì•ˆ:
  - CRA threshold Î±ë¥¼ lossì— í¬í•¨ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ backpropagation ê°€ëŠ¥
  - ë˜ëŠ” í•™ìŠµ ê¸°ë°˜ RL controllerë¡œ ì ì ˆí•œ sparse config ì„ íƒ

---

### ğŸ”¸ [4] Layer ê°„ ì¼ê´€ì„± ë¶€ì¡± â†’ **Cross-layer Sparse Routing**

- ğŸ“š ì˜ˆì‹œ ì—°êµ¬:
  - **GShard / M6-Transformer**: token importanceë¥¼ ê¸°ë°˜ìœ¼ë¡œ layer ê°„ path ì„ íƒ
- ğŸ’¡ ì ìš© ë°©ì•ˆ:
  - íŠ¹ì • layerì—ì„œ ì„ íƒëœ ì¤‘ìš” í† í°ì€ ë‹¤ìŒ layerì—ë„ propagate (residual style)
  - attention sparsity maskë¥¼ ê³µìœ í•˜ê±°ë‚˜ shiftí•˜ëŠ” ë°©ì‹ ê³ ë ¤

---

### ğŸ”¸ [5] Serving ë©”ëª¨ë¦¬ ì´ìŠˆ â†’ **Chunked & Recurrent Attention with Cache Quantization**

- ğŸ“š ì˜ˆì‹œ ì—°êµ¬:
  - **Memorizing Transformers**: long contextë¥¼ chunkë³„ë¡œ ì²˜ë¦¬ + memory token ì €ì¥
  - **FastGen / H2O / SkVQ**: KV ìºì‹œë¥¼ ì¤‘ìš”ë„ ê¸°ë°˜ ì••ì¶• or quantization
- ğŸ’¡ ì ìš© ë°©ì•ˆ:
  - Prefill ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ì¤‘ìš” KVë§Œ sliding-window ë°©ì‹ìœ¼ë¡œ ìœ ì§€
  - cacheë¥¼ int8 ë˜ëŠ” bfloat16 ë“± ì €ì •ë°€ë¡œ ì €ì¥

---

## ğŸ§  3. ì •ë¦¬ ë° ë‚´ ê²¬í•´: í†µí•©ì  ë°œì „ ë°©í–¥

SampleAttentionì€ "í¬ì†Œ attentionì€ ì •í™•ë„ ì†ì‹¤ì„ ê°ìˆ˜í•´ì•¼ í•œë‹¤"ëŠ” ê³ ì •ê´€ë…ì„ ë’¤ì—ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ:

- **ì •ì  íŒ¨í„´ ì„ íƒ**ê³¼ **ìˆ˜ë™ íŠœë‹**ì€ í™•ì¥ì„± í•œê³„ê°€ ìˆìŒ  
- ì•ìœ¼ë¡œëŠ” **í•™ìŠµ ê°€ëŠ¥í•œ sparse controller + ë©”íƒ€ íŒ¨í„´ ì„ íƒ**ì´ í•µì‹¬ íë¦„ì´ ë  ê²ƒì´ë¼ ë´…ë‹ˆë‹¤.

### ğŸŒŸ í†µí•© ë°©í–¥ ì œì•ˆ
> "Adaptive Sparse Attention 2.0" =  
> `Learnable Pattern Selector + Zero-cost Importance Estimator + Auto-Hyper Controller`

ì´ëŸ¬í•œ ìš”ì†Œê°€ ê²°í•©ëœ ì—°êµ¬ê°€ ë‚˜ì˜¨ë‹¤ë©´,
- ëª¨ë“  ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ ë¹ ë¥´ê³ ,
- ëª¨ë“  ëª¨ë¸ì— plug-and-playë¡œ ì ìš©ë˜ë©°,
- tuning ì—†ì´ë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì´ìƒì ì¸ êµ¬ì¡°ê°€ ë  ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ“Œ ì •ë¦¬ í‘œ: í•œê³„ì™€ ëŒ€ì‘ ì—°êµ¬ íë¦„

| í•œê³„ | ëŒ€ì‘ ë°©í–¥ | ëŒ€í‘œ ì—°êµ¬ íë¦„ |
|------|-----------|----------------|
| ì§§ì€ ì‹œí€€ìŠ¤ ì†ë„â†“ | Zero-cost importance | DynamicSparseAttention, Kernelized Attention |
| í¬ì†Œ íŒ¨í„´ ì œí•œ | íŒ¨í„´ ì„ íƒ í•™ìŠµí™” | Sparformer, RoutingTransformer |
| í•˜ì´í¼íŒŒë¼ë¯¸í„° | Auto-tuning / RL | AutoSparse, Switch Transformer |
| layerë³„ ë¶ˆê· í˜• | Cross-layer token routing | GShard, M6-Transformer |
| ì„œë¹™ memory ë³‘ëª© | Chunk + Cache ì••ì¶• | FastGen, H2O, SkVQ |
