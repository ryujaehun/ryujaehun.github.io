---
title: "Humanity's Last Exam"
date: 2025-02-12
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>이 포스트를 읽는데 약 {{ time }} {% if time == '1' %}분{% else %}분{% endif %} 이 소요될 수 있습니다.</p>

[논문 링크](https://arxiv.org/abs/2501.14249v2)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문의 강점 및 독창적인 지점

1. **문제 인식 및 대응**: 이 논문은 기존 벤치마크(MMLU 등)가 LLM(대형 언어 모델)에 의해 거의 90% 이상의 정확도를 기록하며 포화 상태에 도달한 문제를 지적하고 있습니다. 이를 해결하기 위해 **Humanity's Last Exam (HLE)**이라는 새로운 벤치마크를 제안합니다.

2. **벤치마크의 범위와 난이도**:
   - **3,000개 이상의 질문**이 수록되어 있으며, 수학, 인문학, 자연 과학 등 **다양한 학문 분야**를 포함합니다.
   - **멀티모달 질문**(텍스트+이미지)과 **텍스트 기반 질문**을 포함하고 있으며, **정답이 명확하고 검증 가능한** 형식(객관식 및 단답형)으로 구성되어 있습니다.
   - **기존 LLM이 해결할 수 없는 난이도**의 질문만 채택되도록 다단계 검증(모델 테스트 → 전문가 리뷰 → 최종 승인)을 거쳤습니다.

3. **독창적인 데이터셋 제작 과정**:
   - **LLM 사전 검증**: 질문을 제출하기 전에 최신 LLM(GPT-4O, Gemini 등)으로 테스트하여 모델이 틀리는 질문만을 채택합니다.
   - **전문가 리뷰**: 석사 이상 학위를 보유한 전문가들이 질문을 리뷰하여 품질과 난이도를 보장합니다.
   - **글로벌 협업**: 50개국, 500개 이상의 기관에서 약 1,000명의 전문가가 기여했습니다.

4. **모델 성능 평가**:
   - **최신 LLM의 낮은 정확도**: GPT-4O, Claude, Gemini 등 최신 모델들도 HLE에서 **10% 미만의 정확도**를 기록하였습니다.
   - **캘리브레이션 오류**: 모델들이 틀린 답변에도 **높은 확신(>90%)**을 보이며, 이는 **모델의 환각(hallucination)** 문제를 강조합니다.

---

### 핵심 알고리즘 및 과정 설명

HLE의 알고리즘 자체는 복잡한 계산을 수행하는 것이 아니라, 데이터셋의 수집, 검증, 평가 과정에 중점을 둡니다. 이 과정을 간단한 예시로 설명하겠습니다.

1. **질문 제출 및 사전 검증**:
   - 예시 질문 (수학): *"5차 다항식 방정식의 근을 구하는 일반적인 해법은 존재하는가?"*
   - 최신 LLM(GPT-4O, Claude 등)으로 질문을 테스트합니다.
     - 만약 모델이 **틀린 답**을 제공하거나 **확신 없이** 답변을 하면, 이 질문은 다음 단계로 넘어갑니다.

2. **전문가 리뷰 및 수정**:
   - 전문가들이 해당 질문의 난이도와 품질을 평가합니다.
   - 예시로, 답변이 애매하거나 검색을 통해 쉽게 찾을 수 있으면 **수정하거나 폐기**합니다.

3. **최종 벤치마크 포함**:
   - 최종적으로 **정답이 명확하고 LLM이 답변하기 어려운** 질문만 HLE에 포함됩니다.

4. **모델 평가**:
   - 모델은 **체계적인 프롬프트**로 질문에 답변합니다:
     - *"Answer: {답변}, Confidence: {신뢰도}"*
   - GPT-4O와 같은 **심사 모델**이 정답과 모델의 답변을 비교하여 정확성을 평가합니다.

---

### 논문의 한계점

1. **벤치마크의 지속 가능성 문제**:
   - LLM의 발전 속도가 빠르기 때문에, 현재 어렵다고 평가된 HLE도 **빠른 시일 내에 포화 상태**에 이를 가능성이 있습니다. 논문에서도 **2025년 말까지 모델의 50% 이상 정확도** 달성을 예상하고 있습니다.

2. **실제 적용 가능성의 제한**:
   - HLE는 **폐쇄형 학문적 질문**에 집중하고 있습니다. 이는 **개방형 문제 해결**이나 **창의적 사고**를 요구하는 실제 응용 문제에는 한계가 있을 수 있습니다.

3. **캘리브레이션 및 모델 불확실성 문제**:
   - 모델들이 **틀린 답변에도 높은 확신**을 보이며, 이는 **모델의 신뢰성 문제**로 이어질 수 있습니다. 이는 단순히 벤치마크를 넘어 AI 시스템 전반의 **안전성 문제**로 직결됩니다.

4. **다양성 부족의 가능성**:
   - 다양한 분야의 전문가들이 참여했지만, 특정 분야나 지역 중심의 **편향된 질문**이 포함될 수 있습니다.

---

### 결론

이 논문은 **LLM의 한계를 극복하기 위한 새로운 벤치마크(HLE)**를 제시함으로써 AI 연구와 정책 결정에 **정확한 평가 기준**을 제공합니다. **철저한 검증 과정**과 **글로벌 전문가 협업**을 통한 데이터셋 제작은 독창적이며, 기존 벤치마크의 포화 문제를 효과적으로 해결합니다. 그러나 **빠른 모델 발전 속도**로 인해 HLE 역시 **지속 가능한 평가 기준으로 유지되기는 어렵다는 한계**가 존재합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### 논문의 핵심 알고리즘: **Humanity's Last Exam (HLE)** 데이터셋 생성 및 평가 프로세스

이 논문은 전통적인 의미의 **알고리즘**보다는 **데이터셋 생성 및 모델 평가 프로세스**를 중심으로 논의를 전개합니다. HLE는 기존 벤치마크(MMLU 등)에서 높은 성능을 보이는 최신 LLM들의 한계를 드러내기 위해 설계된 **고난도 멀티모달 벤치마크**입니다. 이 과정은 크게 **질문 수집**, **검증**, **모델 평가**의 3단계로 나눌 수 있습니다.

---

### 1. **질문 수집 및 필터링 프로세스**

HLE의 질문 수집은 **다단계 검증 알고리즘**을 통해 이루어집니다. 이 과정을 통해 LLM이 쉽게 해결할 수 없는 고난이도 문제만 남도록 설계되었습니다.

#### **프로세스 개요**:
1. **질문 제출**: 전 세계의 **전문가(석사/박사)**가 각 분야에서 고안한 질문을 제출.
2. **1차 필터링 (LLM 자동 검증)**: 제출된 질문을 최신 LLM(GPT-4O, Claude, Gemini 등)에 입력하여 모델이 정답을 맞추는지 검증.
3. **2차 필터링 (전문가 리뷰)**: 모델을 통과한 질문 중 **품질 검증** 및 **난이도 평가**를 위해 전문가들이 리뷰.
4. **최종 승인 및 포함**: 엄격한 기준을 통과한 질문만 최종 HLE 벤치마크에 포함.

#### **예시 1: 수학 문제**

**질문**:  
*"모든 5차 다항식 방정식은 해를 갖는 닫힌 형태의 해법을 가질 수 있는가?"*

1. **1차 필터링 (LLM 검증)**:  
   - GPT-4O에 입력 → *"모든 5차 방정식은 일반적인 해법이 존재하지 않습니다."*  
   - 정답이므로 **질문 폐기**.

2. **질문 수정 후 재제출**:  
   - 질문을 **더 구체적이고 복잡하게 수정**: *"다항식 \( f(x) = x^5 - 5x + 2 \)의 실근의 개수를 구하시오."*
   - GPT-4O에 입력 → **틀린 답변** 반환.

3. **2차 필터링 (전문가 리뷰)**:  
   - 수학 전문가가 문제를 검토하여 난이도와 정확성을 확인.  
   - 문제와 해법이 **고급 수학적 지식을 요구**한다고 판단되어 HLE 포함.

---

### 2. **모델 평가 및 정답 검증 알고리즘**

HLE는 **정확한 성능 측정**을 위해 모델의 답변을 자동으로 검증하는 프로세스를 사용합니다.

#### **프로세스 개요**:
1. **모델 입력 및 응답 수집**:
   - 모델에 질문을 입력하고 **답변 + 신뢰도(confidence)**를 수집.
   - **포맷 예시**:  
     ```
     Explanation: 문제를 풀기 위한 설명  
     Answer: 최종 답변  
     Confidence: 85%
     ```

2. **정답 검증 (자동 심사 알고리즘)**:
   - **GPT-4O 기반 자동 심사 모델**이 모델의 답변과 HLE의 정답을 비교.
   - **정확성 평가 기준**:
     - *extracted_final_answer*: 모델이 제공한 최종 답변.
     - *correct_answer*: HLE에 수록된 정확한 정답.
     - 두 답변이 일치하면 **정답(Yes)**, 다르면 **오답(No)**으로 기록.

3. **모델 성능 평가 지표**:
   - **정확도(Accuracy)**: 전체 질문 중 모델이 맞춘 비율.
   - **캘리브레이션 오류(Calibration Error)**: 모델의 신뢰도와 실제 정확도의 차이. (높을수록 **모델 환각(hallucination)** 심각)

---

### **모델 평가 예시**

**질문**:  
*"화학에서, 2단계 전기 순환 반응의 π-전자 수를 구하시오."*

1. **모델 응답 (GPT-4O)**:
   ```
   Explanation: 전기 순환 반응의 π-전자 수를 결정하기 위해 Huckel 규칙을 적용합니다.
   Answer: 6 π-electrons
   Confidence: 95%
   ```

2. **정답 검증 (자동 심사)**:
   - HLE 정답: *4 π-electrons*  
   - GPT-4O의 답변과 **불일치**, 따라서 **오답**으로 평가.

3. **성능 기록**:
   - **정확도**: 오답이므로 점수에 포함되지 않음.
   - **캘리브레이션 오류**: 모델은 **95% 확신**했으나 오답, 따라서 높은 환각 지수 기록.

---

### 3. **모델 평가 결과**

HLE를 통한 모델 성능 평가 결과는 다음과 같습니다.

| **모델**                  | **정확도(%)** | **캘리브레이션 오류(%)** |
| ------------------------- | ------------- | ------------------------ |
| GPT-4O                    | 3.3           | 92.5                     |
| Claude 3.5 Sonnet         | 4.3           | 88.9                     |
| Gemini 2.0 Flash          | 7.7           | 91.2                     |
| DeepSeek-R1 (텍스트 전용) | 9.4           | 81.8                     |

- **결론**: 모든 최신 LLM이 HLE에서 **10% 미만의 정확도**를 기록, 높은 수준의 **캘리브레이션 오류(>90%)**를 보였습니다. 이는 모델들이 **확신을 가지고 틀린 답변**을 내놓는 **환각(hallucination)** 문제를 심각하게 드러냅니다.

---

### **정리**

1. **핵심 알고리즘**:
   - LLM이 **해결하지 못하는 고난이도 문제**를 식별하여 데이터셋에 포함.
   - 모델 답변을 **자동 검증 알고리즘**을 통해 평가.
   - 모델의 **정확도와 신뢰도**를 동시에 분석하여 성능을 평가.

2. **HLE의 기여**:
   - 기존 벤치마크의 포화 문제를 해결하고, **최신 LLM의 한계를 정확하게 측정**할 수 있는 방법론을 제시.
   - AI 연구 및 개발에서 **신뢰성 있는 평가 도구**로 활용 가능.

3. **한계점**:
   - **빠른 모델 발전**으로 인해 HLE 역시 **조만간 포화 상태**에 이를 수 있음.
   - **폐쇄형 문제** 중심으로 설계되어 **창의적 문제 해결 능력** 평가에는 한계가 있음.

# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### 논문에서 제시한 결과 요약

**Humanity’s Last Exam (HLE)** 벤치마크를 통해 최신 LLM(대형 언어 모델)의 성능을 평가한 결과, **모든 모델이 낮은 정확도(10% 미만)**와 **높은 캘리브레이션 오류**를 기록했습니다. 이는 기존 벤치마크(MMLU 등)에서 90% 이상의 정확도를 달성한 것과 대조적입니다.

#### **모델 성능 비교 결과**

| **모델**                  | **정확도(%)** | **캘리브레이션 오류(%)** |
| ------------------------- | ------------- | ------------------------ |
| GPT-4O                    | 3.3           | 92.5                     |
| Claude 3.5 Sonnet         | 4.3           | 88.9                     |
| Gemini 2.0 Flash Thinking | 7.7           | 91.2                     |
| DeepSeek-R1 (텍스트 전용) | 9.4           | 81.8                     |

- **정확도(Accuracy)**: 모델이 문제를 맞춘 비율.
- **캘리브레이션 오류(Calibration Error)**: 모델이 틀린 답을 **높은 확신(>90%)**으로 제시하는 정도.

---

### 기존 방법론과의 차별점 및 특출난 점

1. **기존 벤치마크의 포화 현상 해결**:
   - **기존 벤치마크(MMLU)**: GPT-4, Claude 등 LLM이 **90% 이상의 정확도**를 기록하여 더 이상 모델의 발전을 측정하기 어려움.
   - **HLE**: 기존 모델이 **거의 풀 수 없는 문제**로 구성되어 모델의 진정한 한계를 드러냄. 모든 모델이 10% 미만의 정확도를 기록한 것은 **새로운 평가의 필요성**을 강하게 시사.

2. **문제 출제 방식의 혁신**:
   - **다단계 검증 시스템**:
     - **LLM 사전 필터링**: 문제 제출 전에 최신 LLM으로 문제를 테스트하여 **모델이 맞추는 문제는 자동 탈락**.
     - **전문가 리뷰**: 모델을 통과한 문제는 **석박사급 전문가들이 검토**하여 난이도와 품질을 보장.
   - **기존 벤치마크**는 대부분 기존 학술 문제나 교재 기반 문제를 포함하고 있어 LLM이 **인터넷 검색 기반으로 정답을 찾는 경우가 많았음**.

3. **멀티모달 문제 도입**:
   - HLE는 **텍스트 + 이미지** 문제를 포함하여 **LLM의 시각적 이해 능력**까지 평가합니다.
   - 기존 벤치마크(MMLU 등)는 **텍스트 기반** 문제로 한정되어 있었으며, 이는 모델이 단순 텍스트 분석에 최적화된 방식으로 높은 성능을 기록할 수 있는 여지를 남김.

4. **모델 환각(hallucination) 문제 드러냄**:
   - HLE에서는 **모델이 자신 없는 질문에 대해 높은 확신을 보이며 오답을 내놓는 경향**(캘리브레이션 오류)이 발견됨.
   - 이는 기존 벤치마크에서는 드러나지 않던 모델의 **신뢰성 문제**를 폭로하는 데 기여.

---

### 논문에서 제시하는 성공 요인

1. **문제 난이도 조절 및 검증 프로세스**:
   - 논문은 HLE의 성공을 **철저한 검증 절차**와 **모델이 풀 수 없는 문제만을 선택**한 것에 기인한다고 설명합니다.
   - **전문가 검토**를 통해 단순히 모델이 못 푸는 문제뿐만 아니라, **인간 전문가조차도 고급 지식과 깊은 이해가 필요한 문제**를 선택함으로써 벤치마크의 질을 보장.

2. **정확하고 검증 가능한 답안 구성**:
   - HLE는 **정답이 명확하고 검색으로 찾기 어려운 문제**로 구성되어 모델이 **문제 자체의 본질을 이해하고 풀어야만** 답을 찾을 수 있도록 설계.

3. **멀티모달 문제 도입**:
   - **텍스트와 이미지를 결합한 문제**는 단순한 언어 모델링을 넘어선 **복합적인 추론 능력**을 요구하여 모델의 약점을 더 잘 드러낼 수 있음.

---

### 나의 생각: HLE의 성공 이유와 한계

#### **성공 이유**

1. **모델 '환각' 문제를 효과적으로 드러냄**:
   - 기존 벤치마크에서는 모델이 오답을 제시할 때 **낮은 확신을 보이거나 불확실성을 표현**할 수 있었지만, HLE에서는 **모델이 확신을 가지고 오답**을 제시하는 사례가 많아졌습니다.
   - 이는 AI 시스템의 **안전성 평가**에 중요한 기여를 합니다.

2. **실제 학문적 난이도 반영**:
   - HLE는 **대학원 수준**의 고난이도 문제를 포함하고 있어, 기존 벤치마크보다 **더 현실적이고 실질적인 평가 기준**을 제공합니다.
   - 특히, **다양한 분야**의 문제(수학, 화학, 인문학 등)가 포함되어 있어 모델의 **범용성**을 테스트하는 데 적합합니다.

3. **빠른 벤치마크 포화 방지**:
   - 모델이 쉽게 답을 찾을 수 없는 문제로 구성되어 있어 **단기간 내에 벤치마크가 포화되는 것을 방지**할 수 있습니다.
   - 이는 연구자들에게 **지속 가능한 평가 도구**로서의 가능성을 제시합니다.

---

#### **한계 및 개선점**

1. **벤치마크의 지속 가능성 문제**:
   - 논문에서도 언급했듯이, **모델의 발전 속도가 빠르기 때문에** HLE 역시 **2025년 말까지 50% 이상의 정확도**를 기록할 가능성이 있습니다.
   - 이는 HLE가 궁극적으로 **또 다른 포화된 벤치마크**가 될 수 있음을 시사합니다.

2. **폐쇄형 문제의 한계**:
   - HLE는 **닫힌 형태의 문제(객관식, 단답형)**에 초점을 맞추고 있어, **개방형 문제 해결**이나 **창의적 사고력**을 평가하는 데는 한계가 있습니다.
   - 이는 모델의 **창의성**이나 **오픈엔디드 문제 해결 능력**을 테스트하는 새로운 벤치마크가 필요함을 시사합니다.

3. **편향 가능성**:
   - **글로벌 전문가**들이 문제를 출제했지만, 특정 분야나 지역 중심의 **편향된 질문**이 포함될 가능성도 존재합니다.
   - 다양한 문화적 배경과 분야를 아우르는 **더 폭넓은 문제 출제**가 필요합니다.

---

### **결론**

**HLE**는 기존 벤치마크의 한계를 극복하고 **최신 LLM의 진정한 한계**를 드러내는 **혁신적인 평가 도구**입니다. 특히 **다단계 검증 프로세스**와 **모델 환각 문제**를 드러내는 방식은 AI 시스템의 신뢰성을 평가하는 데 중요한 기여를 합니다. 그러나 **빠른 모델 발전 속도**로 인해 HLE 역시 **궁극적으로 포화될 위험**이 있으며, **개방형 문제 해결**을 평가할 새로운 벤치마크가 필요합니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **HLE 논문의 한계를 극복하기 위한 연구 흐름**

**Humanity's Last Exam (HLE)**는 LLM(대형 언어 모델)의 한계를 평가하기 위해 고안된 고난이도 벤치마크입니다. 그러나 이 벤치마크도 몇 가지 **구조적 한계**를 가지고 있으며, 이를 극복하기 위해 AI 연구 커뮤니티에서는 다양한 접근법을 탐색하고 있습니다. 아래는 HLE의 한계를 극복하기 위한 연구 흐름과 방법론을 정리한 내용입니다.

---

### **1. 벤치마크 포화 문제 극복: 동적/적응형 벤치마크 개발**

#### **한계점**:
- HLE는 현재 LLM이 풀기 어려운 문제로 구성되어 있지만, **빠른 AI 발전 속도**로 인해 모델이 곧 높은 정확도를 기록할 가능성이 있습니다. 이는 **벤치마크의 포화** 문제로 이어질 수 있습니다.

#### **연구 흐름**:

1. **동적 벤치마크(Dynamic Benchmarks)**:
   - **자동으로 문제 난이도를 조정**하거나 **새로운 문제를 생성**하는 벤치마크.
   - 모델의 성능에 따라 **문제의 난이도와 유형을 실시간으로 조절**하여 지속적으로 모델을 평가.
   
   **예시 연구**:
   - **DynaBench** ([Kiela et al., 2021](https://arxiv.org/abs/2104.14337)): 모델이 문제를 해결할 때마다 **더 어려운 문제를 생성**하여 지속적으로 난이도를 높이는 벤치마크.

2. **적응형 벤치마크(Adaptive Benchmarks)**:
   - 모델의 **약점을 실시간으로 파악**하고, 해당 부분을 집중적으로 평가하는 **피드백 루프 기반** 벤치마크.
   - 특정 모델이 자주 틀리는 유형의 문제를 **자동 탐지**하여 데이터셋에 반영.

   **예시 연구**:
   - **Adversarial NLI (ANLI)** ([Nie et al., 2020](https://arxiv.org/abs/1910.14599)): 모델이 틀린 문제를 분석하여 **적대적(Adversarial) 문제를 생성**하고, 이를 통해 모델의 취약점을 드러내는 벤치마크.

---

### **2. 폐쇄형 문제의 한계 극복: 개방형 문제(Open-Ended Tasks)로 확장**

#### **한계점**:
- HLE는 **객관식**과 **단답형** 문제로 구성되어 있어, **창의적 사고**나 **복잡한 문제 해결 능력**을 평가하기 어렵습니다.

#### **연구 흐름**:

1. **창의적 문제 해결(Creative Problem Solving) 평가**:
   - 모델이 **독창적인 아이디어**를 제시하거나, **새로운 지식을 생성**하는 능력을 평가.
   - **에세이 작성**, **소설 창작**, **과학적 가설 설정** 등 개방형 작업을 통해 모델의 **창의성**을 평가.

   **예시 연구**:
   - **ARC Challenge** ([Chollet et al., 2024](https://arxiv.org/abs/2412.04604)): 인간의 추론 능력을 테스트하기 위해 **개방형 문제와 창의적 문제 해결 능력**을 평가하는 벤치마크.
   
2. **자기평가(Self-Evaluation) 및 메타인지 평가(Metacognition)**:
   - 모델이 **자신의 답변에 대한 신뢰도**를 스스로 평가하고, **틀린 답변에 대한 불확실성을 인식**하는 능력을 테스트.
   - 단순히 답을 맞추는 것뿐만 아니라, **언제 자신이 모르는지를 아는 능력**을 평가.

   **예시 연구**:
   - **Beyond Imitation Game** ([Srivastava et al., 2023](https://arxiv.org/abs/2206.04615)): 모델의 **자기 인식**과 **자기 신뢰도 평가**를 통한 AI의 진정한 이해도를 측정하는 방법론 제안.

---

### **3. 멀티모달 및 상호작용형 벤치마크로의 확장**

#### **한계점**:
- HLE는 일부 **멀티모달 문제(텍스트 + 이미지)**를 포함하고 있지만, 여전히 **다양한 데이터 타입(비디오, 오디오 등)**을 포괄하는 데 한계가 있습니다.
- 모델의 **상호작용 능력**을 테스트하는 벤치마크가 부족합니다.

#### **연구 흐름**:

1. **멀티모달 추론(Multimodal Reasoning)**:
   - **텍스트, 이미지, 비디오, 오디오** 등 다양한 형태의 데이터를 종합적으로 이해하고 추론하는 능력 평가.
   - **시각적 상식**, **영상 속 사건 예측**, **음성 인식과 텍스트 결합** 등 다양한 입력을 결합한 문제를 통해 모델의 **다양한 데이터 처리 능력**을 평가.

   **예시 연구**:
   - **MathVista** ([Lu et al., 2024](https://arxiv.org/abs/2310.02255)): **수학적 추론과 시각적 데이터**를 결합한 문제로 멀티모달 LLM의 수학적 추론 능력을 평가.

2. **상호작용형 벤치마크(Interactive Benchmarks)**:
   - 모델이 **사용자와 상호작용**하며 문제를 해결하는 능력을 평가.
   - **실시간 피드백**을 반영하여 대화의 흐름을 유지하거나, 복잡한 작업을 수행하는 데 필요한 능력을 테스트.

   **예시 연구**:
   - **τ-Bench** ([Yao et al., 2024](https://arxiv.org/abs/2406.12045)): 모델이 **도구와 사용자**와 상호작용하여 문제를 해결하는 **실시간 상호작용 능력**을 평가하는 벤치마크.

---

### **4. 모델의 안전성과 신뢰성 평가: 환각(Hallucination) 및 편향(Bias) 해결**

#### **한계점**:
- HLE에서 드러난 모델의 **환각(hallucination)** 문제(틀린 답을 확신하며 제시)는 AI의 신뢰성을 저해하는 주요 요소입니다.
- 또한, 모델이 특정 분야나 문화적 배경에 **편향**될 가능성도 존재합니다.

#### **연구 흐름**:

1. **환각 및 신뢰성 평가(Hallucination and Reliability Evaluation)**:
   - 모델이 **틀린 정보를 자신 있게 제시하는 문제**를 줄이기 위한 평가 및 개선 방법론.
   - **사실성 검증(Factuality)**과 **출처 기반 응답**을 통해 모델의 신뢰도를 향상.

   **예시 연구**:
   - **Model-Written Evaluations** ([Perez et al., 2022](https://arxiv.org/abs/2212.09251)): 모델이 생성한 답변의 **사실성**을 평가하고, 환각 문제를 감지하는 새로운 평가 방법 제안.
   - **Measuring Short-Form Factuality** ([Wei et al., 2024](https://arxiv.org/abs/2411.04368)): 모델의 **짧은 답변에서의 사실성**을 평가하는 벤치마크.

2. **편향(Bias) 및 공정성(Fairness) 평가**:
   - 모델이 **특정 인종, 성별, 지역** 등에 대해 **편향된 답변**을 제공하지 않도록 검증.
   - 다양한 문화적, 지역적 배경을 고려한 **다양한 문제 구성**을 통해 모델의 **공정성**을 평가.

   **예시 연구**:
   - **AGIEval** ([Zhong et al., 2023](https://arxiv.org/abs/2304.06364)): **문화적 편향** 및 **공정성**을 평가하기 위해 **다양한 배경**의 문제로 모델을 평가.

---

### **5. 실제 응용 및 자율적 연구 능력 평가로의 확장**

#### **한계점**:
- HLE는 주로 **학문적 문제**를 기반으로 설계되어 있어, 모델이 **실제 환경에서 문제를 해결**하거나 **자율적으로 연구를 수행**하는 능력을 평가하기 어렵습니다.

#### **연구 흐름**:

1. **자율 연구 및 과학적 탐구(Autonomous Scientific Research)**:
   - 모델이 스스로 **연구 질문을 설정**하고, **실험 설계** 및 **결과 해석**까지 수행하는 능력을 평가.
   - AI가 **과학적 발견**에 기여할 수 있는지를 평가하는 벤치마크.

   **예시 연구**:
   - **Lab-Bench** ([Laurent et al., 2024](https://arxiv.org/abs/2407.10362)): LLM의 **생물학 연구 능력**을 평가하여 AI가 **과학적 연구**에 기여할 수 있는지를 측정.

2. **실시간 문제 해결 및 응용(Task-Oriented AI)**:
   - 모델이 실제 문제(예: **프로그래밍 버그 수정**, **법률 자문**, **의료 진단**)를 해결하는 능력을 평가.
   - **실제 환경**에서 모델이 얼마나 유용하게 작동하는지를 검증.

   **예시 연구**:
   - **Swe-Bench** ([Jimenez et al., 2024](https://arxiv.org/abs/2310.06770)): 모델이 **실제 GitHub 이슈**를 해결할 수 있는지를 평가하는 벤치마크.
   - **MLE-Bench** ([Chan et al., 2024](https://arxiv.org/abs/2410.07095)): 모델의 **머신러닝 엔지니어링 작업 수행 능력**을 평가.

---

### **결론**

**Humanity's Last Exam (HLE)**는 LLM의 한계를 드러내는 중요한 벤치마크이지만, **벤치마크 포화**, **폐쇄형 문제의 한계**, **환각 및 편향 문제** 등의 한계를 가지고 있습니다. 이를 극복하기 위해 AI 연구 커뮤니티는 **동적/적응형 벤치마크**, **개방형 문제 확장**, **멀티모달 및 상호작용형 평가**, **모델 신뢰성 강화**, 그리고 **실제 응용 및 자율적 연구 능력 평가**로 연구 흐름을 확장하고 있습니다.

이러한 흐름은 AI 시스템이 단순한 언어 처리 능력을 넘어, **신뢰성**, **창의성**, **자율성**을 갖춘 **범용 인공지능(AGI)**으로 발전하는 데 기여할 것입니다.