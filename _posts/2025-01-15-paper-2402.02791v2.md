---
title: "Rethinking Optimization and Architecture for Tiny Language Models"
date: 2025-01-15
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>이 포스트를 읽는데 약 {{ time }} {% if time == '1' %}분{% else %}분{% endif %} 이 소요될 수 있습니다.</p>

[논문 링크](https://arxiv.org/abs/2402.02791v2)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 요약 및 강점

논문 "Rethinking Optimization and Architecture for Tiny Language Models"은 경량 언어 모델(LM)을 모바일 디바이스에서도 실행 가능하도록 최적화하는 데 초점을 맞춥니다. 이를 위해 **신경망 아키텍처 설계, 파라미터 초기화, 최적화 전략**에 대해 면밀히 분석하며, 특히 PanGu-π Pro 모델 시리즈(1B 및 1.5B 파라미터)에 적용하여 기존 모델보다 뛰어난 성능을 달성했습니다.

#### 주요 강점
1. **효율적 토크나이저 압축**: 빈도가 낮은 어휘를 제거하여 모델 파라미터 효율성을 극대화함.
2. **구조적 최적화**: 모델의 깊이, 너비, FFN(expansion rate)을 조정하여 경량 모델의 성능과 속도를 균형 있게 향상.
3. **파라미터 상속 기법**: 대규모 모델의 핵심 레이어와 파라미터를 선정하여 초기화에 사용함으로써 성능과 학습 속도를 개선.
4. **다중 라운드 학습**: 데이터 망각 문제를 해결하고 어려운 샘플에 대한 학습을 강화.
5. **경쟁 모델 대비 성능 우위**: PanGu-π-1.5B Pro는 Qwen-1.8B(16.67% 더 적은 파라미터)와 Phi2-2.7B를 초과하는 SOTA 성능을 달성.

---

### 핵심 알고리즘 설명: 예시와 함께

PanGu-π-1B Pro 모델은 다음 주요 단계를 따릅니다.

#### 1. **토크나이저 압축**
- **기본 원리**: 코퍼스의 토큰 빈도를 분석하여 상위 48,000개의 어휘가 전체 데이터의 97.86%를 커버함을 확인.
- **결과**: 토크나이저 크기를 100k에서 48k로 축소하면 파라미터 비중이 18.07%로 감소하면서 성능도 향상.

**예시**:
- 입력 데이터: `"The quick brown fox jumps over the lazy dog."`
- 기존 토크나이저: 100k 어휘 → "The", "quick", "brown", "fox", 등 개별 토큰 처리.
- 압축된 토크나이저: 상위 빈도 어휘로 압축 후 동일 데이터 처리.

#### 2. **아키텍처 튜닝**
- **깊이와 너비**: 
    - 깊이 증가 시 정확도 향상(ARC-E: 34.39→37.01), 속도 저하.
    - 최적 깊이: 20층, 최적 너비: 1792.

**예시 입력**:
- 기존 모델(12층): 낮은 성능.
- 개선 모델(20층): 더 높은 정확도, 학습에서 더욱 깊이 있는 표현 학습.

#### 3. **파라미터 초기화**
- 대규모 모델(PanGu-π-7B)로부터 특정 중요한 레이어의 파라미터 상속.
- **결과**: 상속된 모델은 학습 초기 손실이 낮고, 최종 성능이 더 우수.

#### 4. **다중 라운드 학습**
- 데이터 순환 학습을 통해 망각 문제 해결.
- 어려운 데이터(높은 손실 값)에 대한 학습 샘플링 전략 적용.

---

### 논문의 한계점

1. **일반화된 최적화 공식 부족**: 제안된 전략들이 특정 데이터 세트와 하드웨어에 맞춰 설계되었으며, 다른 환경에서의 일반화에 대한 추가 연구 필요.
2. **실제 사용 시 제약**: 제한된 실시간 응용 시나리오에서 경량 모델의 성능 측면에서 더 많은 실험이 필요.
3. **데이터 다양성 부족**: 영어와 중국어 코퍼스 위주로 구성된 데이터로 인해 다국어 확장성 부족.

---

### 결론

PanGu-π Pro 시리즈는 경량화된 언어 모델의 성능을 극대화하기 위한 구조적, 최적화적 접근을 명확히 제시하며, 특히 토크나이저 압축, 아키텍처 튜닝, 파라미터 초기화, 다중 라운드 학습의 결합이 효과적임을 실험적으로 증명합니다. 하지만 더 다양한 시나리오와 데이터 세트에 대한 추가 검증이 필요합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### 논문의 핵심 알고리즘: 예시와 함께

논문에서 제안하는 핵심 알고리즘은 **경량 언어 모델의 최적화**를 위한 세 가지 주요 영역에 집중합니다:
1. **토크나이저 압축**
2. **신경망 아키텍처 최적화**
3. **파라미터 초기화 및 다중 라운드 학습**

아래에 각 알고리즘의 동작 원리와 전체 프로세스를 예시와 함께 설명합니다.

---

### 1. **토크나이저 압축**

#### **알고리즘 개요**
대규모 언어 모델이 사용하는 토크나이저는 방대한 어휘 크기로 인해 경량 모델에서 불필요한 파라미터를 차지합니다. 논문에서는 **빈도 기반의 어휘 제거**를 통해 토크나이저를 최적화하고 성능과 효율성을 동시에 높입니다.

#### **작동 방식**
1. **빈도 분석**: 전체 데이터(1.6T 토큰)에서 토큰 빈도를 계산하여 **상위 48k 토큰**이 97.86%의 데이터를 커버함을 확인.
2. **어휘 크기 축소**: 저빈도 토큰 제거로 토크나이저를 압축.
3. **성능 평가**: 다양한 어휘 크기에 대해 모델 성능과 효율성을 비교.

#### **예시**
- 원문: `"The quick brown fox jumps over the lazy dog."`
- 초기 토크나이저: 100k 어휘 크기, 각 단어를 개별 토큰으로 처리 (`"The", "quick", "brown" ...`).
- 압축 후 토크나이저(48k): 빈도가 낮은 단어 `"lazy"`를 제거, 이를 빈번한 단어 `"lazy"`의 유사 형태로 병합.

#### **결과**
- 어휘 크기: 100k → 48k로 축소.
- 평균 성능(ARC-E, HellaSwag, C3 기준): 40.40 → 41.19로 개선.

---

### 2. **신경망 아키텍처 최적화**

#### **알고리즘 개요**
경량 모델에서 **깊이(Depth)**와 **너비(Width)**, 그리고 FFN의 **확장 비율(Expansion Rate)**이 성능과 효율성에 미치는 영향을 분석하고 최적의 조합을 도출.

#### **작동 방식**
1. **깊이와 너비 조정**:
   - 깊이 증가: 성능 향상(ARC-E: 34.39 → 37.01), 속도 감소.
   - 너비 조정: 최적 너비 설정(1792)으로 성능/속도 균형 유지.
2. **FFN 확장 비율**: 2.77로 고정하여 효율적 데이터 학습 구현.

#### **예시**
- **기존 구조(12층, 너비 2048)**:
  - 입력 데이터: `"The quick brown fox jumps over the lazy dog."`
  - 결과: 적당한 성능(42.41), 속도는 보통.
- **최적화된 구조(20층, 너비 1792)**:
  - 입력 데이터: 동일.
  - 결과: 성능 향상(43.79), 속도 감소(30%).

#### **결과**
- 깊이 20층, 너비 1792로 설정한 PanGu-π-1B Pro는 이전 구조 대비 성능 8.87 향상.

---

### 3. **파라미터 초기화 및 다중 라운드 학습**

#### **알고리즘 개요**
1. **파라미터 초기화**:
   - 대규모 모델(PanGu-π-7B)의 핵심 레이어와 파라미터를 상속하여 성능과 학습 효율성을 향상.
2. **다중 라운드 학습**:
   - 데이터의 망각 문제를 해결하고 어려운 샘플에 대해 학습 집중.

#### **작동 방식**
1. **파라미터 초기화**:
   - 대규모 모델에서 중요한 레이어 선택: 처음 3개 레이어와 마지막 3개 레이어를 우선 선택.
   - 내부 뉴런 가중치 선택: L2-노름 및 테일러 확장을 사용하여 중요 가중치 식별.
2. **다중 라운드 학습**:
   - 샘플링 전략: 높은 손실 값을 가진 데이터에 높은 확률로 재학습.
   - 두 번째 라운드에서 성능 향상.

#### **예시**
- **기존 초기화**:
  - 입력 데이터: `"The quick brown fox."`
  - 초기 손실: 3.2, 최종 손실: 2.5.
- **초기화 상속**:
  - 동일 입력 데이터.
  - 초기 손실: 2.8, 최종 손실: 2.2.
- **다중 라운드 학습**:
  - 1라운드: 손실 값 높은 데이터(`"fox jumps"`)를 추가 학습.

#### **결과**
- 파라미터 상속 모델은 초기 손실이 낮아 학습 속도와 최종 성능 모두 우수.
- 다중 라운드 학습은 성능을 추가로 2% 이상 향상.

---

### **정리된 알고리즘 단계**

| 단계                   | 알고리즘 기법                       | 주요 결과                          |
| ---------------------- | ----------------------------------- | ---------------------------------- |
| **1. 토크나이저 압축** | 빈도 기반 저빈도 토큰 제거          | 토크나이저 크기 100k → 48k로 축소. |
| **2. 아키텍처 최적화** | 깊이 20층, 너비 1792, FFN 2.77 설정 | 성능 8.87, 효율성 향상.            |
| **3. 초기화 및 학습**  | 상속 초기화 + 다중 라운드 학습 적용 | 초기 손실 감소, 성능 추가 향상.    |

이 방식은 경량 모델의 최적화를 위해 **성능, 효율성, 확장 가능성**을 모두 고려한 종합적인 접근법입니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### 논문에서 제시한 결과 및 다른 방법론과의 비교

논문은 **PanGu-π Pro** 시리즈(특히 1B 및 1.5B 모델)의 성능이 기존의 경쟁 모델보다 우수하다는 결과를 제시합니다. **표 8**에서 성능 비교 결과를 보면, PanGu-π-1.5B Pro는 다음과 같은 점에서 두드러집니다:

1. **SOTA 달성**: PanGu-π-1.5B Pro는 16.67% 더 적은 파라미터로 Qwen-1.8B를 평균 성능(56.49 vs. 55.04)에서 능가합니다.
2. **경쟁 모델 대비 뛰어난 성능**: 
   - Phi2-2.7B(2.7B 파라미터)보다 평균적으로 11.4% 높은 성능을 보임.
   - 기존 PanGu-π-1B 대비 평균 성능이 8.87 증가.

#### 세부적인 성능 비교 (대표적인 벤치마크 예시)
| 모델               | CMMLU (지식) | MMLU (추론) | AGI-Eval (이해) | 평균 성능 |
| ------------------ | ------------ | ----------- | --------------- | --------- |
| **PanGu-π-1B Pro** | 46.56        | 50.38       | 41.58           | 51.28     |
| Qwen-1.8B          | 52.12        | 46.43       | 35.83           | 55.04     |
| Phi2-2.7B          | 32.18        | 58.49       | 28.51           | 45.09     |

---

### **특출난 점과 기여 요소**

PanGu-π Pro의 뛰어난 성능은 **효율성과 효과성**을 모두 고려한 세 가지 방법론 덕분입니다:

#### 1. **효율적인 토크나이저 설계**
   - 기존의 대형 모델은 다국어 데이터를 처리하기 위해 대규모 토크나이저(100k 이상의 어휘)를 사용하며, 이는 경량 모델에서는 불필요한 파라미터 오버헤드를 초래합니다.
   - **PanGu-π Pro**는 빈도 기반 분석을 통해 상위 48k 어휘만 남김으로써 효율성을 극대화합니다.
   - **논문 제시 이유**:
     - 어휘 크기 축소가 전체 파라미터에서 토크나이저가 차지하는 비율을 38.19% → 18.07%로 감소시켜, 모델의 주요 연산에 더 많은 자원을 활용할 수 있게 함.
   - **나의 생각**:
     - 이 접근법은 경량 모델에서 매우 효과적입니다. 하지만 다국어 지원이 필요한 환경에서는 제거된 어휘가 성능 저하를 유발할 가능성이 있으며, 언어 특화 데이터에서 추가 실험이 필요합니다.

#### 2. **구조 최적화**
   - 모델 깊이(Depth)를 12층에서 20층으로 늘리고 너비(Width)를 2048에서 1792로 조정해, 성능과 속도 사이의 균형을 최적화.
   - **논문 제시 이유**:
     - 깊이는 모델의 표현력을 높이지만 속도를 저하시킴. 너비와 확장 비율을 조정하여 적절한 균형점을 찾음.
     - **결과**: 깊이를 20층으로 설정했을 때 성능이 최대화(41.19 → 42.02)되고 속도는 유지됨.
   - **나의 생각**:
     - 깊이 증가로 성능이 증가하지만, 지나치게 깊은 모델은 속도 저하를 초래합니다. 논문의 실험은 이러한 트레이드오프를 잘 보여주며, 20층이라는 결정이 매우 실용적으로 보입니다.

#### 3. **파라미터 초기화와 상속**
   - **파라미터 초기화**: 대규모 모델(PanGu-π-7B)로부터 중요한 레이어를 상속하고, 각 레이어 내 뉴런 가중치를 선택적으로 유지.
   - **논문 제시 이유**:
     - 중요한 레이어(초기 및 최종 레이어)가 모델 성능에 큰 영향을 미침. 중간 레이어는 상대적으로 덜 중요하므로 제거 가능.
     - **결과**: 상속된 모델이 학습 초기 손실 감소와 빠른 수렴을 보여줌.
   - **나의 생각**:
     - 파라미터 상속은 큰 모델의 학습 결과를 효과적으로 재활용하는 방식으로, 경량 모델의 성능 향상에 유리합니다. 하지만 상속 과정에서 일부 레이어가 불필요하게 유지될 가능성도 있어 자동화된 상속 기준이 필요할 것으로 보입니다.

#### 4. **다중 라운드 학습**
   - 한 번의 학습으로는 일부 데이터가 잘 학습되지 않거나 망각(catatastrophic forgetting) 문제가 발생.
   - **논문 제시 이유**:
     - 높은 손실 값을 보이는 데이터에 대해 샘플링 확률을 높이고, 다중 라운드로 학습하여 잃어버린 지식을 보충.
     - **결과**: 두 번째 라운드 학습에서 평균 성능이 약 2% 향상.
   - **나의 생각**:
     - 이 기법은 학습 데이터가 제한적이거나 경량 모델에서 특히 유용합니다. 하지만 추가 라운드 학습이 비용을 증가시키므로 최적화된 샘플링 전략이 필요합니다.

---

### **결론과 통찰**

PanGu-π Pro 모델의 특출난 점은 **효율성, 효과성, 확장 가능성**의 균형을 이루는 설계입니다. 논문에서 제안한 방법론은 각 단계에서 경량 모델이 직면하는 문제를 효과적으로 해결합니다.  
**특히**:
1. 토크나이저 압축과 구조 최적화로 **파라미터 효율성을 극대화**.
2. 파라미터 상속과 다중 라운드 학습으로 **성능 개선**.
3. 이러한 전략들이 조화를 이루며 적은 리소스에서 경쟁 모델보다 우수한 성능을 달성.

하지만 **다국어 지원, 데이터 다양성, 추가 라운드 학습 비용**과 같은 측면에서 추가 연구가 필요합니다. 논문이 제안한 방법론은 경량 언어 모델의 효율적 설계에 중요한 참고자료로 활용될 수 있습니다.



# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### **논문의 입력 데이터와 추론 과정**

#### **입력 데이터**
논문은 1.6조 개의 토큰으로 구성된 방대한 다국어 코퍼스를 사용하며, 영어와 중국어 데이터를 약 1:1의 비율로 구성합니다. 입력 데이터의 전처리와 모델 입력 변환 과정은 다음과 같습니다:

1. **데이터 전처리**:
   - 텍스트 데이터를 토큰화하여 정수 시퀀스로 변환.
   - 논문은 Byte-Pair Encoding(BPE)을 활용하여 어휘를 48,000개로 압축한 **압축 토크나이저**를 사용.

**예시**:
- 원문: `"The quick brown fox jumps over the lazy dog."`
- 압축된 토크나이저(48k): `["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]` → `[125, 302, 1987, 873, 4532, 243, 125, 5823, 203]`

2. **입력 데이터 준비**:
   - 모델은 고정된 시퀀스 길이(`N`)만큼 데이터를 패딩하여 처리.
   - 예를 들어, 시퀀스 길이 10에서 입력이 `[125, 302, 1987, 873]`일 경우 `[125, 302, 1987, 873, 0, 0, 0, 0, 0, 0]`로 패딩.

---

#### **추론 과정**
모델은 Transformer 기반 아키텍처를 사용하여 입력 데이터를 처리하며, 아래 단계를 거칩니다:

1. **임베딩 층**:
   - 입력 토큰을 고차원 임베딩 공간으로 매핑.
   - 예: 토큰 `[125, 302]` → 벡터 `[v_125, v_302]`로 변환 (차원: 1x1792).

2. **Self-Attention**:
   - 입력 시퀀스의 각 토큰 간 관계를 학습.
   - 각 토큰은 Query, Key, Value 벡터로 변환되며, `QK^T`를 계산한 뒤 Softmax와 Value를 곱해 출력.

   **예시 연산**:
   - Query: \( Q = XW_Q \)
   - Key: \( K = XW_K \)
   - Value: \( V = XW_V \)
   - Attention: \( \text{Softmax}(QK^T / \sqrt{d_k})V \)

3. **Feed-Forward Network (FFN)**:
   - 각 Attention 출력에 비선형 변환을 적용하여 표현력을 증가.
   - 예: \( \text{ReLU}(XW_1 + b_1)W_2 + b_2 \).

4. **출력층**:
   - 최종 토큰 확률 분포를 계산해 다음 토큰을 예측.
   - 예: Softmax로 다음 토큰 `[873]` 확률 \( P(t|t_{<t}) \) 계산.

---

### **모델 아키텍처 구성**

#### **구성 요소**
논문에서 사용된 PanGu-π Pro 모델은 Transformer 기반이며, 주요 구성 요소는 다음과 같습니다:

| 구성 요소          | 설명                                                                       |
| ------------------ | -------------------------------------------------------------------------- |
| **토크나이저**     | 48k 크기의 압축 토크나이저(BPE 기반).                                      |
| **임베딩 층**      | 토큰을 고차원 벡터로 변환 (1792차원).                                      |
| **Transformer 층** | 20층(1B 모델) 또는 22층(1.5B 모델), 각 층은 Self-Attention과 FFN으로 구성. |
| **FFN 확장 비율**  | 기본 Hidden 차원(1792)의 2.77배 확장.                                      |
| **출력층**         | 다음 토큰의 확률 분포를 계산.                                              |

#### **추론 시 요구 연산**
- **Self-Attention 연산량**:
  - \( O(N^2d) \): \( N \)은 시퀀스 길이, \( d \)는 Hidden 크기(1792).
- **FFN 연산량**:
  - \( O(N \times d \times 2.77d) \): 확장 비율로 인해 FFN에서 연산량 증가.

#### **메모리 요구량**
1. **KV 캐시**:
   - Self-Attention에서 Key와 Value를 저장하는 데 필요.
   - 메모리 사용량: \( O(N \times d \times 2) \).
2. **모델 파라미터**:
   - 1B 모델: 약 4GB (FP16 기준).
   - 1.5B 모델: 약 6GB.

---

### **컴퓨팅 요구량**

#### **훈련**
1. **연산 비용**:
   - 1.6T 토큰을 학습하며, 총 FLOPs는 약 \( O(10^{19}) \).
2. **배치 크기와 학습 속도**:
   - 논문에서는 Batch Size 2M으로 학습, 총 1,000 Ascend 910 GPU를 사용.

#### **추론**
1. **시간 복잡도**:
   - 시퀀스 길이 \( N \), 모델 크기 \( d \)에서 Self-Attention의 복잡도는 \( O(N^2 \cdot d) \).
2. **실행 속도**:
   - 단일 NVIDIA V100 GPU 기준, 20층 모델에서 초당 약 29.49 토큰 생성.

---

### **결론**
이 논문의 입력 데이터와 추론 과정은 **효율적이고 최적화된 아키텍처**로 인해 경쟁 모델 대비 적은 메모리와 컴퓨팅 자원으로 높은 성능을 제공합니다. 특히, 토크나이저 압축과 파라미터 상속, 다중 라운드 학습이 요구량 감소에 크게 기여했습니다. 하지만 메모리 효율성을 더욱 높이기 위해 **Group Query Attention(GQA)**과 같은 최신 기술을 도입할 여지가 있습니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **이 논문의 한계와 이를 극복하기 위한 연구 흐름**

논문에서 제시된 PanGu-π Pro 모델은 경량 언어 모델의 효율성과 성능을 극대화하기 위한 여러 혁신적인 방법을 제시했지만, 여전히 몇 가지 한계점이 존재합니다. 이러한 한계를 극복하기 위한 관련 연구 흐름은 다음과 같이 요약할 수 있습니다.

---

### **1. 다국어 지원과 어휘 다양성 부족**

#### **한계**
- 압축된 토크나이저(48k 어휘)는 영어와 중국어 코퍼스의 빈도 기반으로 설계되었지만, 다른 언어에서는 성능 저하 가능성이 존재합니다.
- 드문 어휘의 제거가 다국어 모델의 표현 능력을 제한할 수 있음.

#### **해결 연구 흐름**
1. **언어 적응 토크나이저**:
   - 각 언어별로 토크나이저를 학습하거나 동적으로 조정 가능한 **언어 적응형 토크나이저** 개발.
   - 예: [mBERT](https://arxiv.org/abs/1810.04805)와 같은 멀티링구얼 언어 모델이 특정 언어에서 토큰화 성능을 극대화하기 위해 공통 어휘를 유지하면서도 언어 특화 서브워드 생성.

2. **메타러닝 기반 토크나이저**:
   - 언어 간 공통성을 학습하고 각 언어에 맞춘 어휘 생성을 자동화하는 메타러닝 접근법.
   - 예: [XLM-R](https://arxiv.org/abs/1911.02116)처럼 다국어 학습을 위한 샘플 효율적 학습 전략 사용.

3. **코드-스위칭 데이터 활용**:
   - 다국어 및 다도메인 데이터에서 **코드-스위칭** 데이터를 활용해 희귀 어휘의 학습 효율성 증가.

---

### **2. 모델 크기와 메모리 제한**

#### **한계**
- PanGu-π Pro는 메모리 효율성을 개선했지만, 긴 시퀀스를 처리하는 데 필요한 메모리 요구량(Key-Value 캐시)이 여전히 크며, 특히 모바일 디바이스와 같은 메모리 제한 환경에서는 제약이 큼.

#### **해결 연구 흐름**
1. **효율적인 Attention 메커니즘**:
   - **Group Query Attention (GQA)**: 기존 Multi-Head Attention에서 Key와 Value를 그룹화하여 메모리 사용량을 줄이는 방식.
     - 논문에서도 GQA를 실험적으로 적용했으며, MHA와 유사한 성능을 보여줌.
     - 참고: [GQA](https://arxiv.org/abs/2305.13245)
   - **Sparse Attention**:
     - 긴 시퀀스에서 전체 Attention을 계산하지 않고, 중요한 부분만 선택적으로 Attention 계산.
     - 예: [Longformer](https://arxiv.org/abs/2004.05150)

2. **Token Reduction Mechanisms**:
   - 중요하지 않은 토큰을 제거하거나 병합하여 처리할 토큰 수를 줄이는 방법.
   - 예: [Dynamic Token Pruning](https://arxiv.org/abs/2203.07259)

3. **모델 양자화 (Quantization)**:
   - 모델 파라미터를 FP32에서 INT8 또는 INT4로 양자화하여 메모리와 계산 비용을 줄임.
   - 참고: [GPTQ](https://arxiv.org/abs/2306.04031)

---

### **3. 데이터 망각 문제와 학습 비용**

#### **한계**
- 다중 라운드 학습은 데이터 망각(catatastrophic forgetting)을 해결하는 효과적인 방법이지만, 추가적인 학습 비용이 발생.
- 희귀하거나 중요한 샘플만 골라서 학습하는 데 필요한 최적 샘플링 전략의 부족.

#### **해결 연구 흐름**
1. **효율적인 샘플링 전략**:
   - 높은 손실 값을 가진 데이터만을 선택적으로 학습하는 대신, 데이터 다양성을 유지하며 효율적으로 학습하는 방법 연구.
   - 예: **Active Learning** 기반 샘플링 방법.
     - 모델이 불확실성을 느끼는 샘플만 선택적으로 학습.

2. **Memory Augmented Networks**:
   - 학습된 데이터를 외부 메모리에 저장하고, 이를 기반으로 망각 문제를 해결.
   - 예: [Episodic Memory](https://arxiv.org/abs/1706.03762)를 활용한 학습.

3. **Progressive Learning**:
   - 데이터를 난이도 순으로 배치해 초기 라운드에서 쉬운 데이터를 학습한 후, 점차 어려운 데이터로 확장.
   - 참고: Curriculum Learning.

---

### **4. 확장성 및 모델 일반화**

#### **한계**
- 모델은 주로 영어와 중국어 데이터에서 학습되었기 때문에 다른 언어와 도메인으로 확장하는 데 제약이 있을 수 있음.
- 작은 모델일수록 일반화 성능이 큰 모델에 비해 떨어질 가능성이 있음.

#### **해결 연구 흐름**
1. **Few-shot/Zero-shot 학습**:
   - 제한된 데이터에서 일반화 성능을 극대화하는 기술.
   - 예: [T5](https://arxiv.org/abs/1910.10683), GPT-4의 Few-shot 학습 전략.

2. **Multi-task Learning**:
   - 다중 작업을 동시에 학습하여 도메인 간 일반화 성능을 높임.
   - 예: [FLAN](https://arxiv.org/abs/2210.11416)과 같은 멀티태스킹 최적화 모델.

3. **Cross-lingual Transfer**:
   - 다국어 데이터를 활용해 특정 언어의 데이터가 부족할 때 다른 언어에서 학습된 지식을 활용.
   - 예: [Unsupervised Cross-lingual Pre-training](https://arxiv.org/abs/1901.07291)

---

### **정리된 연구 방향**

| **한계**                       | **해결 방안**                                                                | **참고 연구 흐름**                                |
| ------------------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------- |
| 다국어 지원과 어휘 다양성 부족 | 언어 적응형 토크나이저, 메타러닝 기반 서브워드 생성, 코드 스위칭 데이터 활용 | mBERT, XLM-R                                      |
| 모델 크기와 메모리 요구량      | GQA, Sparse Attention, 모델 양자화                                           | GQA, Longformer, GPTQ                             |
| 데이터 망각 문제와 학습 비용   | Active Learning, Memory Augmented Networks, Curriculum Learning              | Episodic Memory, Progressive Learning             |
| 확장성 및 모델 일반화          | Few-shot/Zero-shot 학습, Multi-task Learning, Cross-lingual Transfer         | T5, FLAN, Unsupervised Cross-lingual Pre-training |

---

### **결론**
논문의 한계는 경량 언어 모델 연구의 공통적인 도전 과제와 맞닿아 있으며, 이를 극복하기 위한 연구 흐름은 **효율성, 확장성, 데이터 활용성**의 세 가지 방향으로 발전하고 있습니다. 특히, Sparse Attention, Meta-learning 기반 토크나이저 설계, 그리고 Active Learning 전략은 이 논문의 기법을 보완하며 경량 모델의 성능과 범용성을 더욱 강화할 수 있는 유망한 접근법입니다.

# Q : PanGu-π Pro 모델에서 토크나이저를 48k로 압축했을 때 다국어와 드문 어휘 표현 능력에 대한 영향을 어떻게 평가했으며, 다른 언어 데이터에서도 동일한 성능을 보장할 수 있는가?

 

### PanGu-π Pro 모델의 토크나이저 압축(48k)과 다국어 및 드문 어휘 표현 능력에 대한 평가

논문에서 PanGu-π Pro 모델의 토크나이저는 기존 100k 어휘 크기에서 48k로 축소되어 모델의 효율성을 크게 높였습니다. 그러나 다국어와 드문 어휘 표현 능력에 대해 직접적으로 평가된 내용은 제한적입니다. 다음은 논문에서 다룬 사항과 이를 기반으로 한 분석입니다.

---

### 논문에서 다룬 내용
1. **어휘 빈도 기반의 압축**:
   - 약 1.6T 토큰의 데이터에서 어휘 빈도를 분석해, 상위 48k 어휘가 전체 데이터의 97.86%를 커버함을 확인.
   - 드문 어휘(빈도가 낮은 2.14%의 데이터)는 제거.

2. **성능 평가**:
   - 압축된 토크나이저로 모델을 학습한 후, 여러 벤치마크(ARC-E, HellaSwag, C3 등)에서 성능 평가.
   - 어휘 크기를 점진적으로 줄여 성능을 비교했으며, 48k 어휘 크기가 성능과 효율성 사이에서 최적의 균형을 달성함.

3. **다국어 데이터 구성**:
   - 영어와 중국어 데이터를 1:1로 포함하여 학습했지만, 다국어 표현 능력에 대한 구체적인 언어별 평가나 실험은 부족.

---

### 토크나이저 압축의 잠재적 한계
1. **다국어 지원의 한계**:
   - 영어와 중국어 코퍼스에서 압축된 토크나이저가 다른 언어(예: 스페인어, 아랍어, 힌디어)에서도 동일한 성능을 발휘할 수 있는지 평가되지 않음.
   - 드문 어휘나 언어 특화 어휘의 표현 능력이 저하될 가능성.

2. **드문 어휘 손실**:
   - 빈도가 낮은 어휘가 중요한 맥락 정보를 담고 있을 수 있음.
   - 드문 어휘 제거로 인해 일부 문맥 표현이 제한될 위험.

---

### 다른 언어 데이터에서도 동일한 성능을 보장할 수 있는가?
다른 언어에서 성능을 보장하기 위해 다음과 같은 추가 연구 및 개선이 필요합니다:

1. **언어별 맞춤형 토크나이저**:
   - BPE 방식의 토크나이저 대신, 각 언어의 통계적 특성을 반영한 다국어 적응형 토크나이저를 설계.
   - 예를 들어, SentencePiece를 활용한 언어별 서브워드 생성.

2. **다국어 성능 평가**:
   - 다양한 언어로 구성된 데이터셋(예: XNLI, TyDi QA 등)을 활용하여 다국어 성능을 평가.
   - 다국어 표현 능력을 측정하기 위해 각 언어의 드문 어휘를 포함한 데이터 테스트 필요.

3. **추가적인 데이터 샘플링**:
   - 영어와 중국어 외에 데이터의 언어적 다양성을 높여 학습.
   - 예를 들어, 낮은 리소스 언어 데이터와 고빈도 언어 데이터를 균형 있게 포함.

4. **드문 어휘 복원 전략**:
   - 드문 어휘를 토크나이저 압축 과정에서 완전히 제거하지 않고, 추가적인 클러스터링 또는 맵핑 기법으로 표현.

---

### 결론
논문에서 제안한 48k 토크나이저는 성능과 효율성 측면에서 최적의 결과를 보여주었지만, 다국어 및 드문 어휘 표현 능력에 대한 직접적인 실험은 부족합니다. 다른 언어 데이터에서 동일한 성능을 보장하려면 언어 특화 토크나이저 설계, 다양한 언어 데이터셋 테스트, 드문 어휘 복원 전략 등의 추가 연구가 필요합니다.

# Q : 파라미터 상속 과정에서 중요한 레이어와 뉴런을 선택할 때 사용된 데이터 기반 메트릭(L2-노름, 테일러 확장 등)이 중간 레이어의 잠재적 중요성을 과소평가할 가능성은 없는가? 이를 보완할 방법은 무엇인가?



### 중간 레이어의 잠재적 중요성 과소평가 가능성과 이를 보완할 방법

#### 논문에서 제안된 데이터 기반 메트릭과 한계

논문에서 파라미터 상속 과정은 **L2-노름**, **테일러 확장**, **학습 가능한 마스크** 등의 메트릭을 활용하여 중요한 뉴런과 레이어를 선택합니다. 이러한 접근은 초기와 마지막 레이어가 일반적으로 중요한 정보를 담고 있다는 가정에 기반합니다. 그러나 다음과 같은 한계가 존재할 수 있습니다:

1. **중간 레이어의 역할 과소평가**:
   - 중간 레이어는 종종 추상적이고 고차원적인 표현을 학습하며, 특정 태스크나 입력에 따라 중요도가 다를 수 있음.
   - 논문에서는 중간 레이어의 중요도를 상대적으로 낮게 평가하며, 필요 없는 레이어로 간주해 제거.

2. **메트릭의 제한**:
   - L2-노름: 가중치 크기만 고려하기 때문에, 실제 뉴런의 기여도나 상호작용을 반영하지 못함.
   - 테일러 확장: 1차 근사값을 사용해 계산 복잡도를 줄였지만, 다층 구조에서 비선형 상호작용을 충분히 반영하지 못할 가능성.

---

#### 중간 레이어 과소평가 보완을 위한 방법

1. **동적 중요도 평가**:
   - 고정된 메트릭 대신, 입력 데이터나 태스크에 따라 레이어와 뉴런의 중요도를 동적으로 평가.
   - 예: **Input-dependent Attention** 기법을 사용하여 특정 입력 데이터에 대해 각 레이어의 상대적 중요성을 분석.

2. **태스크-특화 중요도 평가**:
   - 중간 레이어의 기여도가 태스크에 따라 달라질 수 있으므로, 다양한 태스크(추론, 번역, 요약)에서 레이어 중요도를 평가.
   - 예: 특정 태스크에서 중간 레이어 활성화가 중요한 경우, 해당 레이어를 제거하지 않도록 설정.

3. **상호작용 기반 분석**:
   - 뉴런 간 상호작용과 비선형 관계를 고려한 중요도 분석.
   - **Integrated Gradients** 또는 **Layer-wise Relevance Propagation (LRP)**와 같은 기법을 통해 중간 레이어의 기여도를 정량적으로 측정.

4. **중간 레이어의 가중치 재활용**:
   - 중요도가 낮은 중간 레이어를 완전히 제거하지 않고, 경량화하여 유지하거나 다른 방식으로 활용.
   - 예: 중간 레이어를 병합하거나, 낮은 차원 표현으로 압축하여 상속.

5. **학습 과정에서의 중요도 변화 추적**:
   - 학습 도중 각 레이어의 중요도를 동적으로 측정해, 필요에 따라 구조를 조정.
   - 예: 학습 후반부에 중간 레이어의 중요도가 증가할 경우, 이를 반영하여 구조 최적화를 재설계.

---

#### 구체적 보완 전략 예시

1. **메트릭 보완**:
   - 단순한 L2-노름 대신, **Fisher Information**을 사용해 뉴런의 기여도를 정량화.
   - 테일러 확장에서는 2차 근사(Taylor-2)를 도입하여 비선형성을 더 잘 반영.

2. **다중 태스크 학습을 통한 검증**:
   - 중간 레이어를 제거하거나 유지한 모델을 다양한 태스크에 대해 검증해, 태스크별 중요성을 분석.

3. **학습 가능한 중요도 가중치**:
   - 각 레이어와 뉴런에 학습 가능한 가중치를 추가해 중요도를 학습 도중 동적으로 조정.

---

#### 결론

중간 레이어의 잠재적 중요성을 과소평가하는 문제는 모델의 표현력을 제한할 위험이 있습니다. 이를 보완하기 위해 동적 중요도 평가, 상호작용 분석, 학습 중 가중치 조정과 같은 기법을 활용하면 중간 레이어의 잠재적 기여도를 더욱 잘 반영할 수 있습니다. 논문에서 제안한 메트릭을 보완해 비선형성과 태스크-특화 중요도를 반영하는 연구는 경량화된 모델에서도 더 높은 표현력을 유지할 수 있도록 도울 것입니다.

# Q : 다중 라운드 학습에서 데이터 샘플링 비율(예: 50%)과 추가 라운드 횟수가 최적 성능에 기여한 구체적인 근거는 무엇이며, 이 과정에서 발생하는 추가 비용을 줄이는 효율적인 전략은 무엇인가?



### **다중 라운드 학습에서 데이터 샘플링 비율과 라운드 횟수의 최적 성능 기여 근거**

#### **논문에서 제시한 근거**
1. **망각 문제(catatastrophic forgetting) 완화**:
   - 논문에서는 모델이 학습 후반부에 이전 데이터를 "망각"하는 문제를 발견.
   - 이를 해결하기 위해 높은 손실 값을 보이는 데이터를 다시 학습하는 **다중 라운드 학습**을 도입.

2. **샘플링 비율의 최적화**:
   - 샘플링 비율을 변화시키며 성능 변화를 분석한 결과, **50% 샘플링 비율**이 최적의 성능을 달성.
   - **샘플링 전략**: 손실 값이 높은 데이터(학습이 어려운 데이터)에 더 높은 가중치를 부여하여 샘플링.

   | **샘플링 비율** | **평균 성능** (ARC-E, HellaSwag, C3) |
   | --------------- | ------------------------------------ |
   | 25%             | 53.17                                |
   | 50%             | **54.46**                            |
   | 75%             | 54.67                                |
   | 100%            | 54.87                                |

   - **결과**: 샘플링 비율을 50%에서 100%로 늘리면 성능이 미미하게 증가(0.41)하지만, 학습 비용은 비례하여 증가.

3. **라운드 횟수의 최적화**:
   - 두 번째 라운드에서 성능이 가장 크게 향상되며, 세 번째 라운드부터는 성능 향상이 포화 상태에 이름.
   - 추가적인 라운드는 성능 대비 비용 효율이 낮아짐.

   | **라운드 횟수** | **평균 성능** (ARC-E, HellaSwag, C3) |
   | --------------- | ------------------------------------ |
   | 1라운드         | 51.61                                |
   | 2라운드         | **54.46**                            |
   | 3라운드         | 54.44                                |

---

### **추가 비용을 줄이는 효율적인 전략**

#### **1. 데이터 샘플링 효율화**
1. **손실 기반 중요도 샘플링**:
   - 손실 값이 높은 데이터에 가중치를 주되, 반복적으로 학습된 데이터는 가중치를 줄이는 방식.
   - 예: 최근 학습 라운드에서 손실이 일정 이하로 감소한 데이터는 제외.
   
2. **클러스터링 기반 데이터 선택**:
   - 유사한 데이터는 하나의 대표 샘플로 대체하여 중복 학습을 줄임.
   - 예: K-means 클러스터링으로 데이터 그룹화 후 대표 샘플만 학습.

3. **적응형 샘플링**:
   - 학습 초기에는 모든 데이터를 균등하게 학습하고, 이후 라운드에서는 샘플링 확률을 점진적으로 높임.

---

#### **2. 라운드 학습 비용 최적화**
1. **지속적 학습 방식**:
   - 기존 학습된 가중치를 고정하고, 새로운 데이터 또는 어려운 데이터만 미세 조정(Fine-tuning).
   - 전체 모델 학습 대신, 마지막 몇 개 레이어만 업데이트하여 연산 비용 절감.

2. **진단 라운드 도입**:
   - 추가 라운드 실행 전에 진단 라운드로 학습이 필요한 데이터만 선별.
   - 예: 손실 값 변화가 작은 데이터는 학습에서 제외.

3. **효율적인 최적화 알고리즘 사용**:
   - AdamW 대신 더 경량화된 옵티마이저(SGD, LAMB)를 사용해 추가 라운드 비용 감소.

---

#### **3. 다중 라운드 대체 전략**
1. **Curriculum Learning**:
   - 처음에는 쉬운 데이터를 학습하고, 점차 어려운 데이터로 확장.
   - 어려운 데이터가 초기에 반복 학습되는 문제를 방지.

2. **학습 스케줄링**:
   - 학습률을 초기에는 높게 설정하고, 이후 라운드에서는 점진적으로 감소.

3. **지식 증류(Knowledge Distillation)**:
   - 첫 번째 라운드에서 대규모 모델이 생성한 출력값을 소규모 모델이 학습.
   - 추가 라운드 대신 증류 과정을 통해 성능 향상을 달성.

---

### **결론**
논문에서 제시된 다중 라운드 학습은 데이터 샘플링 비율(50%)과 라운드 횟수(2라운드)를 통해 효율성과 성능 간 균형을 이루었습니다. 그러나 추가 비용 문제를 줄이기 위해 **손실 기반 중요도 샘플링**, **지속적 학습 방식**, **지식 증류**와 같은 효율화 전략을 활용하면 더 낮은 비용으로 최적 성능을 달성할 수 있습니다. 이러한 접근은 특히 자원이 제한된 환경에서 유용할 것입니다.