---
title: "An Empirical Study of Qwen3 Quantization"
date: 2025-05-12
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>이 포스트를 읽는데 약 {{ time }} {% if time == '1' %}분{% else %}분{% endif %} 이 소요될 수 있습니다.</p>

[논문 링크](https://arxiv.org/abs/2505.02214v1)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### ✅ 결론 요약

Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다.

---

## 1. 논문의 핵심 강점 및 독창성

| 강점 항목                      | 상세 설명                                                                                                                                                       |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **종합적인 PTQ 비교**          | Qwen3-0.6B~~72B까지 다양한 크기 모델에 대해 RTN, GPTQ, AWQ, SmoothQuant, BiLLM 등 5가지 PTQ(Post-Training Quantization) 방법을 1~~8bit 구간에서 정량적으로 평가 |
| **다양한 평가 지표 적용**      | PPL(WikiText2, C4), 0-shot reasoning(PIQA, ARC, HellaSwag 등), 5-shot MMLU                                                                                      |
| **AWQ vs GPTQ 성능비교**       | 4bit에서는 GPTQ가 MMLU 등에서 가장 낮은 성능 감소율 보임 (예: Qwen3-14B에서 FP16 대비 1% ↓)                                                                     |
| **초저비트(1-3bit) 실험 포함** | 대부분의 논문이 다루지 않는 2bit 및 binarization 방법(BiLLM) 실험도 포함                                                                                        |
| **LLaMA3와 비교 분석**         | 동일한 quantization 세팅에서도 Qwen3은 LLaMA3 대비 더 큰 성능 하락 → redundancy 차이 때문으로 분석                                                              |

---

## 2. 핵심 알고리즘 설명 (예시 중심)

### 🧠 대상: **Post-Training Quantization (PTQ)**

> 정규화된 사전학습 weight와 activation을 사후적으로 quantize하여 float16 → low-bit(integer)로 압축

#### 예시 입력

Qwen3-4B 모델, 입력 시퀀스 길이 2048, task = PIQA (0-shot), Quantization = GPTQ, bit = 4

#### 전체 과정

1. **Calibration**

   * 128개의 샘플 (C4 데이터셋) 입력
   * Layer-wise로 weight의 범위 추정 (min/max), outlier 분석

2. **Weight Quantization (GPTQ)**

   * 각 Linear Layer weight W → `int4 + scale + zero-point`로 변환
   * GPTQ는 weight의 중요도 기반으로 error를 최소화하는 방향으로 rounding
   * 수식:

     $$
     W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
     $$

     단, error가 큰 축에 대해 extra correction 수행

3. **Inference**

   * Quantized Linear:

     $$
     y = (W_q \cdot x) + b_q
     $$
   * 계산은 INT4로, 결과는 float32로 복원됨

4. **평가**

   * PIQA 정확도 (GPTQ, 4bit, 4B 모델): 76.0 (FP16: 78.1 → 성능 감소 2.1%)

---

## 3. 성능 비교 요약 (4bit 기준)

| Model Size | GPTQ (4bit) MMLU | AWQ (4bit) MMLU | SmoothQuant (w4a8) | FP16 |
| ---------- | ---------------- | --------------- | ------------------ | ---- |
| 0.6B       | 40.4             | 47.3            | 32.8               | 52.3 |
| 1.7B       | 53.2             | 57.5            | 47.9               | 61.0 |
| 4B         | 68.9             | 69.2            | 63.2               | 73.0 |
| 8B         | 72.7             | 73.8            | 61.6               | 76.7 |
| 14B        | 78.5             | 78.7            | 72.7               | 80.7 |

➡ GPTQ와 AWQ는 대부분의 경우 성능을 잘 유지함. SmoothQuant는 activation quantization의 영향으로 하락 폭이 큼.

---

## 4. 한계점

| 문제점                                  | 설명                                                                                                 |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Activation Quantization에 매우 민감** | w4a8 설정에서도 GPTQ/AWQ 대비 성능 저하가 큼 (e.g. 14B에서 SmoothQuant는 72.7 vs GPTQ 78.5)          |
| **초저비트 환경에서 급격한 성능 하락**  | 2bit/1bit에서는 거의 usable하지 않은 수준의 PPL/accuracy (e.g. 4B-GPTQ: PPL 13.5 → 113x 증가)        |
| **모델 크기에 따라 robust 성능 차이**   | 큰 모델일수록 quantization에 강함 (Redundancy로 noise absorbing), 작은 모델에서는 10% 이상 성능 감소 |
| **Qwen3 특유의 민감성**                 | LLaMA3 대비 quantization noise에 훨씬 민감, 강한 pretraining이 오히려 overfitting된 표현으로 작용    |

---

## 5. 향후 연구 제안

* **Rotation/Channel Reorder 기반 Quantization**

  * 예: SpinQuant \[11], RPTQ \[18] → redundant 표현 재배열로 정보 손실 완화

* **Outlier-aware Activation Quantization**

  * Activation에서 발생하는 large magnitude token 대응 필요

* **Hybrid Bit-width Quantization**

  * 중요 레이어에만 6~~8bit, 기타 2~~3bit → latency, memory 절감과 성능 균형


# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



논문의 핵심 알고리즘은 **Qwen3 LLM을 다양한 Post-Training Quantization(PTQ) 기법으로 1\~8bit로 정밀도 축소하면서도, 최대한 성능 저하를 줄이는 것**입니다. 그중에서도 주요 알고리즘은 다음의 3가지입니다:

---

## ✅ 핵심 알고리즘 1: **GPTQ (Gradient Post-Training Quantization)**

### 🌟 요약

* GPTQ는 weight-only quantization 방식으로, 주어진 full-precision weight를 **error-aware하게** quantize함.
* 주요 특징은 **Hessian approximation**을 사용해 중요한 weight는 더 정밀하게 보존하고, 덜 중요한 weight만 과감히 양자화함.

---

### 🧪 예시: Qwen3-4B, GPTQ, 4bit, MLP Layer

#### 1. 원래의 weight matrix

```text
W (FP32) = [
  [1.23, -0.74,  0.08],
  [-2.15,  0.63,  0.99],
  ...
]
```

#### 2. Calibration 데이터 통과

* 입력 데이터 128개 (from C4) 통과하여 Hessian 행렬 근사 생성:

  $$
  H \approx \frac{1}{N} \sum_{i=1}^{N} \nabla^2 \mathcal{L}(W_i)
  $$
* 중요도가 높은 weight는 distortion이 덜 일어나게 조절.

#### 3. Quantization

* weight를 $W_q = \text{round}(W / \Delta) \cdot \Delta$ 로 변환
* 단, 중요한 weight는 rounding 전에 error compensation 적용:

  ```text
  W_q = Quantize(W - E) + E
  ```

  여기서 $E$는 이전 양자화로 생긴 오차 누적값 (dequantized residual)

#### 4. 결과

* Quantized weight (INT4)와 scale factor (float32)를 저장
* 추론 시:

  $$
  \hat{y} = \text{dequant}(W_q) \cdot x
  $$

---

## ✅ 핵심 알고리즘 2: **AWQ (Activation-aware Weight Quantization)**

### 🌟 요약

* GPTQ보다 빠르고 lightweight한 방식
* 활성화의 dynamic range (outlier)에 따라 weight를 스케일링하여 quantization loss를 최소화

---

### 🧪 예시: Qwen3-8B, AWQ, 4bit

#### 1. 입력 활성화 분석

* 특정 레이어의 activation을 관찰:

  ```text
  x = [1.2, -5.3, 0.7, 10.1, -0.2]  # outlier 포함
  ```

#### 2. Weight scaling factor 계산

* outlier를 줄이기 위해 weight column마다 scaling factor $s$를 부여

  $$
  W' = W \cdot s, \quad x' = x / s
  $$

#### 3. Weight 양자화

* $W'$를 int4로 양자화 → clipping 후 rounding

#### 4. 추론 시

* 원래대로 복원:

  $$
  y = (W_q \cdot x') \cdot s
  $$

  이 방식은 outlier를 보정하여 전체 dynamic range가 narrow해져 quantization이 용이함

---

## ✅ 핵심 알고리즘 3: **BiLLM (Binarized LLM)**

### 🌟 요약

* weight를 **1bit**로 양자화하여 극단적인 압축을 달성
* $\{-1, +1\}$ 만 사용
* 대신 residual error를 보완하는 구조가 필요

---

### 🧪 예시: Qwen3-4B, BiLLM, 1.06bit

#### 1. Weight binarization

* 각 weight matrix를 다음과 같이 binarize:

  $$
  W_b = \text{sign}(W)
  $$

#### 2. Scaling factor 학습

* 각 레이어마다 binarized weight와 원래의 weight 차이를 보정하기 위한 $\alpha$ 계산:

  $$
  W \approx \alpha \cdot W_b
  $$

#### 3. 추론

* 추론 시에는 INT1과 scalar 곱만 수행:

  $$
  y = (\alpha \cdot W_b) \cdot x
  $$

---

## 🔄 전체 알고리즘 흐름 요약

```mermaid
graph TD
A[FP16 Qwen3 Model] --> B[Calibration with 128 samples]
B --> C[Weight Importance Estimation (e.g., GPTQ, AWQ)]
C --> D[Quantization (1~8bit)]
D --> E[Store quantized weights + scale]
E --> F[LLM Inference]
F --> G[Eval on PIQA, MMLU, C4]
```

---

## 📌 비교 정리

| 방법  | 핵심 전략                      | 비트폭 | 특징                        |
| ----- | ------------------------------ | ------ | --------------------------- |
| GPTQ  | error-aware rounding (Hessian) | 2–8bit | 가장 정밀함, 느림           |
| AWQ   | activation-aware rescaling     | 2–8bit | 빠르고 효과적               |
| BiLLM | sign만 유지 + scale            | \~1bit | 극한 압축, 성능 유지 도전적 |




# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### ✅ 결론 요약

이 논문은 기존 PTQ(Post-Training Quantization) 기법들(GPTQ, AWQ, SmoothQuant, BiLLM 등)을 Qwen3에 광범위하게 적용하여 **Qwen3가 특히 ultra-low-bit (≤3bit)** 에서 성능 저하가 **타 모델 대비 심각**하다는 점을 정량적으로 보여줍니다. 특히:

* **4bit 이상에서는 AWQ와 GPTQ가 우수한 성능 보존**
* **2bit/3bit에서는 Qwen3가 LLaMA3보다 훨씬 더 취약**
* **BiLLM은 binarized (1bit 수준)임에도 일부 3bit보다 좋은 결과 제공**

---

## 📊 주요 실험 결과 요약 (Qwen3-14B 기준)

| 방법        | 비트폭 (W/A) | PPL (C4) | MMLU | 특징                             |
| ----------- | ------------ | -------- | ---- | -------------------------------- |
| FP16        | 16/16        | 9.68     | 80.7 | full precision baseline          |
| GPTQ        | 4/16         | 10.3     | 78.5 | **성능 저하 ≈ 2.2%**             |
| AWQ         | 4/16         | 10.3     | 78.7 | GPTQ보다 근소히 우수             |
| SmoothQuant | 4/8          | 46.0     | 72.7 | **활성화 양자화로 성능 하락 큼** |
| BiLLM       | \~1bit       | 20.4     | 39.9 | 3bit AWQ보다 우수, 이례적        |

---

## 🌟 특출난 점: 논문이 보여주는 Qwen3-specific한 현상

| 구분                          | 설명                                                                                         |
| ----------------------------- | -------------------------------------------------------------------------------------------- |
| **1. Quantization 민감도**    | Qwen3는 다른 LLM(LLaMA3 등)보다 quantization에 훨씬 민감함                                   |
| **2. 사전학습 품질의 역설**   | 더 정교하게 학습된 모델일수록 redundancy가 적고, quantization error를 흡수하지 못함          |
| **3. BiLLM의 의미 있는 성능** | 1bit 수준에서도 일부 3bit보다 좋은 성능은 기존 상식 깨뜨림                                   |
| **4. PTQ 방법별 강건성 차이** | GPTQ는 전반적으로 최상위 성능, AWQ는 속도/정확도 균형, SmoothQuant는 작은 모델에 특히 불리함 |

---

## 🧠 논문이 제시하는 원인: "정교한 사전학습 = 낮은 redundancy"

> Qwen3는 LLaMA3 등보다 훨씬 더 많은 품질 관리된 데이터로 학습되어 있어, 표현들이 매우 압축적이고 정제됨 →
> 이로 인해 양자화 시 약간의 precision 손실도 성능에 **치명적**.

논문에서는 아래와 같은 실증으로 이를 지지합니다:

* LLaMA3-8B (AWQ w3a16) → C4 PPL: 11.6
* Qwen3-8B (AWQ w3a16) → C4 PPL: 23.8 (**2배 이상 나쁨**)

> 이는 고성능 LLM일수록 PTQ에 더 정교한 전략이 필요하다는 점을 강조

---

## 🤔 개인적 해석과 의견

| 항목                       | 평가                                                                                                                                                                  |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **문제의식의 명확성**      | 단순히 quantization을 적용하는 것이 아니라, 왜 성능이 나빠지는지 **원인 분석까지 정량적으로 수행**한 점에서 높은 평가를 받을 만함                                     |
| **BiLLM 관련 결과**        | 일반적으로 1bit quantization은 usable하지 않다고 여겨지지만, BiLLM이 3bit보다 우수한 성능을 보이는 부분은 **더 깊은 분석이 필요**함. (구조 보정? ReLU clipping 전략?) |
| **데이터셋 구성의 통일성** | 모든 PTQ 기법을 동일한 calibration 데이터(128 samples from C4), 동일한 평가셋에 적용하여 **실험의 공정성 보장**                                                       |
| **제안의 부족**            | 다만 논문 자체는 **새로운 PTQ 방법을 제안하지는 않음**. 현존 기법의 평가 및 진단에 집중한 메타-연구로 보아야 함                                                       |

---

## 🔄 향후 제안 (논문+내 제언)

| 제안 방식                        | 설명                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| **Reordering PTQ**               | 채널 순서 변경하여 outlier 제거 (예: RPTQ)                   |
| **Rotation Quantization**        | SpinQuant처럼 rotation을 통한 표현 flattening                |
| **Layer-aware Bit-width Tuning** | 중요 레이어는 6bit, 덜 중요한 건 2bit 등 selective bit-width |




# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



이 논문 **"An Empirical Study of Qwen3 Quantization"** 의 한계는 크게 두 가지로 요약됩니다:

---

### ✅ 핵심 한계 요약

| 한계점                                 | 설명                                                                                                     |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **1. Ultra-low-bit (≤3bit) 성능 급락** | Qwen3는 redundancy가 적은 고성능 모델이라서 2\~3bit quantization에서 성능이 급격히 저하됨                |
| **2. Activation Quantization에 취약**  | SmoothQuant(w4a8, w4a4) 실험에서 MMLU 기준 10\~25pt 이상 성능 하락 발생 → 특히 outlier activation에 민감 |

---

## 🔍 이를 극복하기 위한 주요 연구 흐름 3가지

---

### 1️⃣ **Rotation-Based Quantization (e.g., SpinQuant)**

| 핵심 아이디어 | Quantization 전에 weight/activation 공간을 rotation하여 **양자화 손실을 균등하게 분산** |
| ------------- | --------------------------------------------------------------------------------------- |
| 대표 논문     | SpinQuant: LLM Quantization with Learned Rotations (arXiv:2405.16406, 2024)             |
| 기술 요약     |                                                                                         |

* Rotation matrix $R$ 학습 후:

  $$
  W' = W \cdot R,\quad x' = R^{-1} x
  $$

  * → Outlier 값들이 압축되어 범위가 좁아짐
  * → Quantization clipping이 적게 일어나고, reconstruction loss 감소 |

\| 장점 | GPTQ 대비 더 정교한 표현 압축 가능 (특히 activation-aware GPTQ에 효과적) |
\| 단점 | 학습 overhead와 rotation matrix 저장 부담 |

---

### 2️⃣ **Reordering-based PTQ (e.g., RPTQ)**

\| 핵심 아이디어 | activation 혹은 weight channel 순서를 **정렬(정렬 기반 permutation)** 하여 **정보 밀도 향상** |
\| 대표 논문 | RPTQ: Reorder-based Post-Training Quantization for LLMs (arXiv:2304.01089) |
\| 기술 요약 |

* 활성화에서 큰 값이 집중된 채널을 먼저 정렬:

  $$
  W \Rightarrow \text{sort}(W, key=\text{activation variance})
  $$
* → Quantizer가 표현해야 할 분포를 더 압축적으로 만들 수 있음 |
  \| 장점 | 특별한 추가 학습 없이 단순한 permutation으로 효과 |
  \| 적용 기대 | Qwen3의 outlier activation 문제에 특히 적합 |

---

### 3️⃣ **Mixed-Precision Quantization + Layer Sensitivity Profiling**

\| 핵심 아이디어 | Layer 별로 중요도를 분석해 **중요한 레이어에는 6\~8bit**, 중요도가 낮은 레이어는 **2\~4bit** 할당 |
\| 대표 접근 | GPTQ + Hessian 기반 sensitivity 분석 |
\| 구체 전략 |

* Qwen3-14B의 attention/output layer는 6bit 유지
* feed-forward 일부와 embedding은 3bit로 축소
* → 전체 모델의 평균 비트수는 4bit 이하로 낮출 수 있음 |
  \| 장점 | 비트당 효율을 최적화하면서 성능 보존 |
  \| 관련 연구 | "Quant-Noise" (Fairseq), "BitSplit" quantization, "DoReFa-Net" |

---

## 📊 전략별 비교 정리

| 전략명              | 대상              | 주 목적      | 성능 회복 기대 | 계산 비용 | 적용 난이도 |
| ------------------- | ----------------- | ------------ | -------------- | --------- | ----------- |
| **SpinQuant**       | Weight/Activation | 분포 평탄화  | ★★★★☆          | 높음      | 중          |
| **RPTQ**            | Activation        | outlier 제거 | ★★★★☆          | 낮음      | 낮음        |
| **Mixed-Precision** | Layer-wise        | 중요도 반영  | ★★★★★          | 중        | 중          |

---

## 💡 내 생각: Qwen3에는 어떤 방법이 가장 적합한가?

> **Rotation + Mixed Precision 조합**이 가장 유력

* 이유: Qwen3은 redundancy가 매우 낮고, activation 값 분포가 넓음 → rotation으로 분포를 평탄화하고, 중요 레이어만 high-bit로 보호하는 전략이 실용적임
* BiLLM처럼 binarization을 고려하는 것도 흥미롭지만, production-level에서는 mixed precision이 더 안정적임



# Q : Qwen3 모델은 기존 LLM들(LLaMA3 등)에 비해 ultra-low-bit quantization(3bit 이하)에서 왜 더 큰 성능 저하를 보이는가? 이 현상이 사전학습 방식이나 표현 압축과 어떤 관련이 있는가?

 

### ✅ 결론부터 요약

Qwen3는 **표현 압축이 매우 정교하게 이뤄진 고성능 모델**이기 때문에, ultra-low-bit (≤3bit) quantization에서 미세한 weight 오차에도 성능이 급격히 저하된다. 이는 **redundancy(중복 표현)의 부족** 때문이며, 이는 LLaMA3 같은 모델과의 가장 큰 차이점이다.

---

## 🔍 1. 성능 저하의 실측 증거

| 모델      | 방법 (w3a16) | PPL(C4) | 성능 비교         |
| --------- | ------------ | ------- | ----------------- |
| LLaMA3-8B | AWQ          | 11.6    | 준수한 성능 유지  |
| Qwen3-8B  | AWQ          | 23.8    | **2배 이상 악화** |

> Qwen3는 3bit quantization에 **더 민감하며**, LLaMA3보다 PPL이 훨씬 나빠짐

---

## 🧠 2. 사전학습 방식의 차이: 표현 압축 vs 표현 중복

### Qwen3:

* 대규모 고품질 데이터로 **fine-tuning된 pretraining**
* 적은 수의 파라미터로도 높은 정확도를 내기 위해 **불필요한 표현을 제거함**
* 즉, 하나의 neuron/weight에 **더 많은 정보**가 집중됨

### LLaMA3:

* 상대적으로 **다소 일반적인 pretraining 데이터**
* 표현 공간의 redundancy가 많아, 일부 손실에 대한 **robustness 확보**

➡ 결과적으로, Qwen3는 3bit 이하로 양자화될 때 **한 weight의 손실이 곧 정보 손실**로 이어짐

---

## 📐 3. 표현 압축과 quantization noise 간 상호작용

* 양자화는 weight를 아래와 같이 근사함:

  $$
  W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
  $$
* 정밀도가 낮을수록 (e.g., 3bit → 8개의 표현값만 존재) → **quantization error**가 증가
* redundancy가 있다면 이 error는 다른 neuron에서 보완 가능
* 하지만 Qwen3는 이러한 보완 메커니즘이 부족

➡ 결국, **표현 압축이 뛰어난 모델ほど quantization에 취약**

---

## 🔄 4. 논문에서의 해석

> “Advanced pre-training techniques… tend to produce models with less parameter redundancy, consequently making them more sensitive to quantization-induced information loss.”

* 이는 단순한 실험 결과가 아니라, **학습 방식의 진화에 따라 quantization 전략도 진화해야 한다**는 시사점을 던짐

---

## 🔧 보완 가능성

* activation/channel rotation (SpinQuant)
* attention block selective precision
* pre-quantization distillation (loss-aware finetune)



# Q : GPTQ, AWQ, SmoothQuant, BiLLM 등의 quantization 방법은 각각 어떤 방식으로 weight 또는 activation의 정보를 압축하고, Qwen3의 구조적 특성에 대해 어떤 방식으로 대응하고 있는가?

 

### ✅ 결론 요약

GPTQ, AWQ, SmoothQuant, BiLLM은 서로 다른 방식으로 weight 또는 activation의 정보 손실을 줄이기 위한 기법이다.
이들은 Qwen3의 **low-redundancy, high-precision** 구조에 대해 다음과 같은 방식으로 대응하며, **error 보상, outlier 처리, 이진화 등** 각기 다른 전략을 취한다.

---

## 📦 방법별 정보 압축 전략 및 Qwen3 대응 요약

| 방법            | 압축 대상           | 핵심 전략                                           | Qwen3 대응 방식                                  | 성능 요약 (Qwen3-14B, 4bit 기준) |
| --------------- | ------------------- | --------------------------------------------------- | ------------------------------------------------ | -------------------------------- |
| **GPTQ**        | Weight              | Gradient-aware rounding (Hessian 기반 error 최소화) | 중요 weight의 정보 손실 억제                     | MMLU: **78.5**                   |
| **AWQ**         | Weight              | Activation-aware weight scaling                     | outlier activation에 선제적 대응                 | MMLU: **78.7**                   |
| **SmoothQuant** | Activation + Weight | Weight rescaling via activation statistics          | Activation 범위 정규화로 quantization noise 저감 | MMLU(w4a8): **72.7**             |
| **BiLLM**       | Weight (1bit)       | Binarization + residual scaling                     | 극한 압축에서의 구조적 정규화                    | MMLU(\~1bit): **39.9**           |

---

## 🔍 1. GPTQ (Gradient Post-Training Quantization)

### ⛏️ 핵심 메커니즘

* 각 weight의 영향력을 Hessian matrix로 근사하여, **가장 손실이 적은 방향으로 rounding**
* 수식 기반:

  $$
  \min \|W - W_q\|_H = (W - W_q)^T H (W - W_q)
  $$

### 🎯 Qwen3 대응

* Qwen3의 weight는 매우 민감하므로, 중요 weight를 정밀하게 보존하는 GPTQ는 **고정밀 모델에 적합**
* 특히 4bit 환경에서 가장 안정적인 성능 유지

---

## 🔍 2. AWQ (Activation-aware Weight Quantization)

### ⛏️ 핵심 메커니즘

* **Activation 값의 분포(특히 outlier)** 를 보고, 각 채널의 weight를 scaling하여 압축
* 수식:

  $$
  W' = \frac{W}{s}, \quad x' = x \cdot s \quad \text{(scale factor s는 activation 기반)}
  $$

### 🎯 Qwen3 대응

* Qwen3는 **activation outlier가 많음** → weight만 정규화하면 quantization error가 커짐
* 이를 막기 위해 activation의 영향까지 고려 → **activation noise 제어에 효과적**

---

## 🔍 3. SmoothQuant

### ⛏️ 핵심 메커니즘

* Activation의 variance를 기준으로 LayerNorm 이전에 weight rescaling 수행
* 목적: **activation의 dynamic range를 줄여서 activation quantization 시 clipping 방지**

### 🎯 Qwen3 대응

* Qwen3에서 SmoothQuant는 w4a8 또는 w4a4 등 activation quantization 시 **성능 급감**

  * 이유: Qwen3의 **LayerNorm 및 GeLU 이후 activation 분포가 매우 wide** → clipping에 매우 취약

---

## 🔍 4. BiLLM (Binarized LLM)

### ⛏️ 핵심 메커니즘

* weight를 {+1, -1}로 양자화
* 단순 binarization이 아닌, residual 정보를 scaling factor로 유지하여 복원 가능성 향상:

  $$
  W \approx \alpha \cdot \text{sign}(W)
  $$

### 🎯 Qwen3 대응

* 일반적으로 고성능 모델에서 binarization은 불리하지만,
* BiLLM은 일부 레이어만 binarize하거나, residual 정규화를 잘해서 3bit보다 좋은 성능을 낼 수 있음 (특히 Qwen3-32B에서)

---

## 📈 실험 예시 (Qwen3-14B, 4bit weight 기준 MMLU 성능)

| 방법           | MMLU (↑) | PPL (C4, ↓) | 특징                           |
| -------------- | -------- | ----------- | ------------------------------ |
| FP16           | 80.7     | 9.68        | baseline                       |
| GPTQ           | 78.5     | 10.3        | 가장 안정적                    |
| AWQ            | 78.7     | 10.3        | outlier 대응 탁월              |
| SmoothQuant    | 72.7     | 46.0        | activation 양자화에 민감       |
| BiLLM (\~1bit) | 39.9     | 20.4        | 극한 조건에서도 의미 있는 성능 |

---

## 🔚 요약 정리

| 방법        | Qwen3 구조 대응 전략                                | 장점                    | 단점                       |
| ----------- | --------------------------------------------------- | ----------------------- | -------------------------- |
| GPTQ        | 중요한 weight를 정밀하게 보존 (low redundancy 대응) | 정확도 높음             | 느림, 메모리 사용 ↑        |
| AWQ         | activation outlier를 사전 scale로 정규화            | 빠르고 정확도도 좋음    | 극단적인 outlier에는 한계  |
| SmoothQuant | activation range 조절                               | 낮은 latency, 좋은 w8a8 | activation clipping에 취약 |
| BiLLM       | 극단적 quantization에 residual 보정                 | 압축률 최고             | 성능 유지 어려움           |


# Q : 논문에서 사용한 calibration 방식(128개 C4 샘플 기반)이 quantization 성능에 어떤 영향을 미치는가? calibration 데이터의 양이나 다양성을 조절할 경우 성능 향상 여지가 있는가?

 

### ✅ 결론 요약

Qwen3 quantization에서 사용된 **128개 C4 샘플 기반의 calibration**은 **상대적으로 적은 양**이지만, 일반적인 PTQ 방식(GPTQ, AWQ 등)에서는 **충분히 유효한 수준**으로 판단된다. 다만, **bit 수가 작아질수록** 또는 **activation quantization을 포함할수록**, calibration 데이터의 **양과 다양성**이 성능에 **큰 영향을 미칠 수 있다**.

---

## 🔍 1. 논문에서 사용한 Calibration 방식 요약

| 항목      | 세부 내용                                                     |
| --------- | ------------------------------------------------------------- |
| 데이터셋  | **C4 (Colossal Cleaned Common Crawl)**                        |
| 샘플 수   | **128개 샘플**                                                |
| 입력 길이 | 2048 토큰                                                     |
| 목적      | weight 분포, activation range 추정 (e.g., min/max, Hessian)   |
| 대상 방법 | GPTQ, AWQ, SmoothQuant, BiLLM 등 모두 동일한 calibration 사용 |

➡ 실험의 **공정성을 보장**하기 위해 모든 방법에 동일한 calibration 조건 적용

---

## 🧠 2. Calibration 데이터 양이 적을 때 영향

### 양이 적은 경우:

* **분포 추정 정확도 ↓** → 특히 activation의 outlier 범위를 과소평가
* **Hessian 근사 오차 ↑** → GPTQ의 rounding precision 낮아짐
* **채널별 scaling factor 오류 발생 가능** (AWQ, SmoothQuant)

### 결과적으로:

* **bit 수가 낮을수록 더 민감**함
  → 8bit에서는 거의 영향 없음
  → 4bit 이하에서는 **정확도 최대 5\~10%까지 차이 가능**

---

## 📈 실험적 근거 (논문 외 GPTQ 공식 repo 기준)

* 32개 샘플: 성능 불안정
* 128개 샘플: baseline 수준
* 512~~1024개 샘플: 1~~2pt 향상 가능 (특히 MMLU 등 reasoning task)

---

## 🔁 데이터 다양성의 중요성

| 조건                        | 설명                                 | 영향                                                                              |
| --------------------------- | ------------------------------------ | --------------------------------------------------------------------------------- |
| **단일 도메인 텍스트**      | 뉴스/위키 위주 → narrow distribution | 일부 layer 편향 발생 가능                                                         |
| **다양한 도메인/길이 포함** | code, dialogue, QA 혼합              | LayerNorm, MLP 등 다양한 분포 추정 가능 → **activation quantization에 특히 유리** |

> 특히 SmoothQuant처럼 activation statistics에 기반한 방법은 **다양성 확보가 핵심**

---

## 💡 내 제안: 실용적 개선 방법

| 전략                               | 설명                                                                          |
| ---------------------------------- | ----------------------------------------------------------------------------- |
| **샘플 수 증가 (e.g., 512\~1024)** | 대부분의 weight/activation 분포가 안정화됨                                    |
| **stratified sampling**            | 다양한 토픽/domain/text style을 고르게 반영                                   |
| **token-level calibration**        | LayerNorm 및 RMS-based quantizer가 있는 경우, 입력 diversity를 높이는 게 중요 |

---

## 🔬 실험 설계 예시

```python
# 다양한 calibration 비교 실험
for n_samples in [64, 128, 512, 1024]:
    data = sample_c4(n=n_samples)
    quant_model = gptq_calibrate(model, data)
    eval_mmlu(quant_model)  # accuracy 측정
```

➡ 대부분의 경우 **128개 정도로 기본적인 안정성은 확보되나**,
**ultra-low-bit (<4bit) 또는 activation quantization 적용 시**에는 더 많은/다양한 데이터가 필요함

