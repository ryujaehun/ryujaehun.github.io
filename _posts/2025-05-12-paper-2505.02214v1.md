---
title: "An Empirical Study of Qwen3 Quantization"
date: 2025-05-12
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>ì´ í¬ìŠ¤íŠ¸ë¥¼ ì½ëŠ”ë° ì•½ {{ time }} {% if time == '1' %}ë¶„{% else %}ë¶„{% endif %} ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2505.02214v1)

# Q : ì´ ë…¼ë¬¸ì„ ì•„ì£¼ ìì„¸í•˜ê²Œ ì½ê³  ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì ì„ ì„¤ëª…í•´ì£¼ê³  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œ ì…ë ¥ì„ ë“¤ì–´ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜ ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ì˜ í•œê³„ì ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜



### âœ… ê²°ë¡  ìš”ì•½

Qwen3ì˜ ì •ë°€í•œ ì‚¬ì „í•™ìŠµ ë•ë¶„ì— ê³ ì„±ëŠ¥ì„ ë³´ì´ë‚˜, 3bit ì´í•˜ì˜ ultra-low-bit quantizationì— ë§¤ìš° ë¯¼ê°í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ ì €í•˜ê°€ í¬ë‹¤. 4bit ì´ìƒì—ì„œëŠ” GPTQì™€ AWQê°€ ìµœìƒì˜ trade-offë¥¼ ì œê³µí•˜ë©°, BiLLMì€ 1bitì—ì„œë„ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤. ì´ëŠ” Qwen3ì´ redundancyê°€ ì ì€ ëª¨ë¸ êµ¬ì¡°ë¼ëŠ” ì ì—ì„œ ë¹„ë¡¯ëœ ê²ƒì´ë©°, í–¥í›„ LLM quantization ì—°êµ¬ê°€ ì´ë¥¼ ê³ ë ¤í•´ ë”ìš± ì •êµí•œ ì „ëµì„ ê°œë°œí•´ì•¼ í•¨ì„ ì‹œì‚¬í•œë‹¤.

---

## 1. ë…¼ë¬¸ì˜ í•µì‹¬ ê°•ì  ë° ë…ì°½ì„±

| ê°•ì  í•­ëª©                      | ìƒì„¸ ì„¤ëª…                                                                                                                                                       |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ì¢…í•©ì ì¸ PTQ ë¹„êµ**          | Qwen3-0.6B~~72Bê¹Œì§€ ë‹¤ì–‘í•œ í¬ê¸° ëª¨ë¸ì— ëŒ€í•´ RTN, GPTQ, AWQ, SmoothQuant, BiLLM ë“± 5ê°€ì§€ PTQ(Post-Training Quantization) ë°©ë²•ì„ 1~~8bit êµ¬ê°„ì—ì„œ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€ |
| **ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ ì ìš©**      | PPL(WikiText2, C4), 0-shot reasoning(PIQA, ARC, HellaSwag ë“±), 5-shot MMLU                                                                                      |
| **AWQ vs GPTQ ì„±ëŠ¥ë¹„êµ**       | 4bitì—ì„œëŠ” GPTQê°€ MMLU ë“±ì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ ê°ì†Œìœ¨ ë³´ì„ (ì˜ˆ: Qwen3-14Bì—ì„œ FP16 ëŒ€ë¹„ 1% â†“)                                                                     |
| **ì´ˆì €ë¹„íŠ¸(1-3bit) ì‹¤í—˜ í¬í•¨** | ëŒ€ë¶€ë¶„ì˜ ë…¼ë¬¸ì´ ë‹¤ë£¨ì§€ ì•ŠëŠ” 2bit ë° binarization ë°©ë²•(BiLLM) ì‹¤í—˜ë„ í¬í•¨                                                                                        |
| **LLaMA3ì™€ ë¹„êµ ë¶„ì„**         | ë™ì¼í•œ quantization ì„¸íŒ…ì—ì„œë„ Qwen3ì€ LLaMA3 ëŒ€ë¹„ ë” í° ì„±ëŠ¥ í•˜ë½ â†’ redundancy ì°¨ì´ ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„                                                              |

---

## 2. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (ì˜ˆì‹œ ì¤‘ì‹¬)

### ğŸ§  ëŒ€ìƒ: **Post-Training Quantization (PTQ)**

> ì •ê·œí™”ëœ ì‚¬ì „í•™ìŠµ weightì™€ activationì„ ì‚¬í›„ì ìœ¼ë¡œ quantizeí•˜ì—¬ float16 â†’ low-bit(integer)ë¡œ ì••ì¶•

#### ì˜ˆì‹œ ì…ë ¥

Qwen3-4B ëª¨ë¸, ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ 2048, task = PIQA (0-shot), Quantization = GPTQ, bit = 4

#### ì „ì²´ ê³¼ì •

1. **Calibration**

   * 128ê°œì˜ ìƒ˜í”Œ (C4 ë°ì´í„°ì…‹) ì…ë ¥
   * Layer-wiseë¡œ weightì˜ ë²”ìœ„ ì¶”ì • (min/max), outlier ë¶„ì„

2. **Weight Quantization (GPTQ)**

   * ê° Linear Layer weight W â†’ `int4 + scale + zero-point`ë¡œ ë³€í™˜
   * GPTQëŠ” weightì˜ ì¤‘ìš”ë„ ê¸°ë°˜ìœ¼ë¡œ errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ rounding
   * ìˆ˜ì‹:

     $$
     W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
     $$

     ë‹¨, errorê°€ í° ì¶•ì— ëŒ€í•´ extra correction ìˆ˜í–‰

3. **Inference**

   * Quantized Linear:

     $$
     y = (W_q \cdot x) + b_q
     $$
   * ê³„ì‚°ì€ INT4ë¡œ, ê²°ê³¼ëŠ” float32ë¡œ ë³µì›ë¨

4. **í‰ê°€**

   * PIQA ì •í™•ë„ (GPTQ, 4bit, 4B ëª¨ë¸): 76.0 (FP16: 78.1 â†’ ì„±ëŠ¥ ê°ì†Œ 2.1%)

---

## 3. ì„±ëŠ¥ ë¹„êµ ìš”ì•½ (4bit ê¸°ì¤€)

| Model Size | GPTQ (4bit) MMLU | AWQ (4bit) MMLU | SmoothQuant (w4a8) | FP16 |
| ---------- | ---------------- | --------------- | ------------------ | ---- |
| 0.6B       | 40.4             | 47.3            | 32.8               | 52.3 |
| 1.7B       | 53.2             | 57.5            | 47.9               | 61.0 |
| 4B         | 68.9             | 69.2            | 63.2               | 73.0 |
| 8B         | 72.7             | 73.8            | 61.6               | 76.7 |
| 14B        | 78.5             | 78.7            | 72.7               | 80.7 |

â¡ GPTQì™€ AWQëŠ” ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì„±ëŠ¥ì„ ì˜ ìœ ì§€í•¨. SmoothQuantëŠ” activation quantizationì˜ ì˜í–¥ìœ¼ë¡œ í•˜ë½ í­ì´ í¼.

---

## 4. í•œê³„ì 

| ë¬¸ì œì                                   | ì„¤ëª…                                                                                                 |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------- |
| **Activation Quantizationì— ë§¤ìš° ë¯¼ê°** | w4a8 ì„¤ì •ì—ì„œë„ GPTQ/AWQ ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜ê°€ í¼ (e.g. 14Bì—ì„œ SmoothQuantëŠ” 72.7 vs GPTQ 78.5)          |
| **ì´ˆì €ë¹„íŠ¸ í™˜ê²½ì—ì„œ ê¸‰ê²©í•œ ì„±ëŠ¥ í•˜ë½**  | 2bit/1bitì—ì„œëŠ” ê±°ì˜ usableí•˜ì§€ ì•Šì€ ìˆ˜ì¤€ì˜ PPL/accuracy (e.g. 4B-GPTQ: PPL 13.5 â†’ 113x ì¦ê°€)        |
| **ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ robust ì„±ëŠ¥ ì°¨ì´**   | í° ëª¨ë¸ì¼ìˆ˜ë¡ quantizationì— ê°•í•¨ (Redundancyë¡œ noise absorbing), ì‘ì€ ëª¨ë¸ì—ì„œëŠ” 10% ì´ìƒ ì„±ëŠ¥ ê°ì†Œ |
| **Qwen3 íŠ¹ìœ ì˜ ë¯¼ê°ì„±**                 | LLaMA3 ëŒ€ë¹„ quantization noiseì— í›¨ì”¬ ë¯¼ê°, ê°•í•œ pretrainingì´ ì˜¤íˆë ¤ overfittingëœ í‘œí˜„ìœ¼ë¡œ ì‘ìš©    |

---

## 5. í–¥í›„ ì—°êµ¬ ì œì•ˆ

* **Rotation/Channel Reorder ê¸°ë°˜ Quantization**

  * ì˜ˆ: SpinQuant \[11], RPTQ \[18] â†’ redundant í‘œí˜„ ì¬ë°°ì—´ë¡œ ì •ë³´ ì†ì‹¤ ì™„í™”

* **Outlier-aware Activation Quantization**

  * Activationì—ì„œ ë°œìƒí•˜ëŠ” large magnitude token ëŒ€ì‘ í•„ìš”

* **Hybrid Bit-width Quantization**

  * ì¤‘ìš” ë ˆì´ì–´ì—ë§Œ 6~~8bit, ê¸°íƒ€ 2~~3bit â†’ latency, memory ì ˆê°ê³¼ ì„±ëŠ¥ ê· í˜•


# Q : ë…¼ë¬¸ì˜ í•µì‹¬ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ì •ë¦¬í•´ ì¤˜



ë…¼ë¬¸ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì€ **Qwen3 LLMì„ ë‹¤ì–‘í•œ Post-Training Quantization(PTQ) ê¸°ë²•ìœ¼ë¡œ 1\~8bitë¡œ ì •ë°€ë„ ì¶•ì†Œí•˜ë©´ì„œë„, ìµœëŒ€í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¤„ì´ëŠ” ê²ƒ**ì…ë‹ˆë‹¤. ê·¸ì¤‘ì—ì„œë„ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒì˜ 3ê°€ì§€ì…ë‹ˆë‹¤:

---

## âœ… í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ 1: **GPTQ (Gradient Post-Training Quantization)**

### ğŸŒŸ ìš”ì•½

* GPTQëŠ” weight-only quantization ë°©ì‹ìœ¼ë¡œ, ì£¼ì–´ì§„ full-precision weightë¥¼ **error-awareí•˜ê²Œ** quantizeí•¨.
* ì£¼ìš” íŠ¹ì§•ì€ **Hessian approximation**ì„ ì‚¬ìš©í•´ ì¤‘ìš”í•œ weightëŠ” ë” ì •ë°€í•˜ê²Œ ë³´ì¡´í•˜ê³ , ëœ ì¤‘ìš”í•œ weightë§Œ ê³¼ê°íˆ ì–‘ìí™”í•¨.

---

### ğŸ§ª ì˜ˆì‹œ: Qwen3-4B, GPTQ, 4bit, MLP Layer

#### 1. ì›ë˜ì˜ weight matrix

```text
W (FP32) = [
  [1.23, -0.74,  0.08],
  [-2.15,  0.63,  0.99],
  ...
]
```

#### 2. Calibration ë°ì´í„° í†µê³¼

* ì…ë ¥ ë°ì´í„° 128ê°œ (from C4) í†µê³¼í•˜ì—¬ Hessian í–‰ë ¬ ê·¼ì‚¬ ìƒì„±:

  $$
  H \approx \frac{1}{N} \sum_{i=1}^{N} \nabla^2 \mathcal{L}(W_i)
  $$
* ì¤‘ìš”ë„ê°€ ë†’ì€ weightëŠ” distortionì´ ëœ ì¼ì–´ë‚˜ê²Œ ì¡°ì ˆ.

#### 3. Quantization

* weightë¥¼ $W_q = \text{round}(W / \Delta) \cdot \Delta$ ë¡œ ë³€í™˜
* ë‹¨, ì¤‘ìš”í•œ weightëŠ” rounding ì „ì— error compensation ì ìš©:

  ```text
  W_q = Quantize(W - E) + E
  ```

  ì—¬ê¸°ì„œ $E$ëŠ” ì´ì „ ì–‘ìí™”ë¡œ ìƒê¸´ ì˜¤ì°¨ ëˆ„ì ê°’ (dequantized residual)

#### 4. ê²°ê³¼

* Quantized weight (INT4)ì™€ scale factor (float32)ë¥¼ ì €ì¥
* ì¶”ë¡  ì‹œ:

  $$
  \hat{y} = \text{dequant}(W_q) \cdot x
  $$

---

## âœ… í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ 2: **AWQ (Activation-aware Weight Quantization)**

### ğŸŒŸ ìš”ì•½

* GPTQë³´ë‹¤ ë¹ ë¥´ê³  lightweightí•œ ë°©ì‹
* í™œì„±í™”ì˜ dynamic range (outlier)ì— ë”°ë¼ weightë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ quantization lossë¥¼ ìµœì†Œí™”

---

### ğŸ§ª ì˜ˆì‹œ: Qwen3-8B, AWQ, 4bit

#### 1. ì…ë ¥ í™œì„±í™” ë¶„ì„

* íŠ¹ì • ë ˆì´ì–´ì˜ activationì„ ê´€ì°°:

  ```text
  x = [1.2, -5.3, 0.7, 10.1, -0.2]  # outlier í¬í•¨
  ```

#### 2. Weight scaling factor ê³„ì‚°

* outlierë¥¼ ì¤„ì´ê¸° ìœ„í•´ weight columnë§ˆë‹¤ scaling factor $s$ë¥¼ ë¶€ì—¬

  $$
  W' = W \cdot s, \quad x' = x / s
  $$

#### 3. Weight ì–‘ìí™”

* $W'$ë¥¼ int4ë¡œ ì–‘ìí™” â†’ clipping í›„ rounding

#### 4. ì¶”ë¡  ì‹œ

* ì›ë˜ëŒ€ë¡œ ë³µì›:

  $$
  y = (W_q \cdot x') \cdot s
  $$

  ì´ ë°©ì‹ì€ outlierë¥¼ ë³´ì •í•˜ì—¬ ì „ì²´ dynamic rangeê°€ narrowí•´ì ¸ quantizationì´ ìš©ì´í•¨

---

## âœ… í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ 3: **BiLLM (Binarized LLM)**

### ğŸŒŸ ìš”ì•½

* weightë¥¼ **1bit**ë¡œ ì–‘ìí™”í•˜ì—¬ ê·¹ë‹¨ì ì¸ ì••ì¶•ì„ ë‹¬ì„±
* $\{-1, +1\}$ ë§Œ ì‚¬ìš©
* ëŒ€ì‹  residual errorë¥¼ ë³´ì™„í•˜ëŠ” êµ¬ì¡°ê°€ í•„ìš”

---

### ğŸ§ª ì˜ˆì‹œ: Qwen3-4B, BiLLM, 1.06bit

#### 1. Weight binarization

* ê° weight matrixë¥¼ ë‹¤ìŒê³¼ ê°™ì´ binarize:

  $$
  W_b = \text{sign}(W)
  $$

#### 2. Scaling factor í•™ìŠµ

* ê° ë ˆì´ì–´ë§ˆë‹¤ binarized weightì™€ ì›ë˜ì˜ weight ì°¨ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•œ $\alpha$ ê³„ì‚°:

  $$
  W \approx \alpha \cdot W_b
  $$

#### 3. ì¶”ë¡ 

* ì¶”ë¡  ì‹œì—ëŠ” INT1ê³¼ scalar ê³±ë§Œ ìˆ˜í–‰:

  $$
  y = (\alpha \cdot W_b) \cdot x
  $$

---

## ğŸ”„ ì „ì²´ ì•Œê³ ë¦¬ì¦˜ íë¦„ ìš”ì•½

```mermaid
graph TD
A[FP16 Qwen3 Model] --> B[Calibration with 128 samples]
B --> C[Weight Importance Estimation (e.g., GPTQ, AWQ)]
C --> D[Quantization (1~8bit)]
D --> E[Store quantized weights + scale]
E --> F[LLM Inference]
F --> G[Eval on PIQA, MMLU, C4]
```

---

## ğŸ“Œ ë¹„êµ ì •ë¦¬

| ë°©ë²•  | í•µì‹¬ ì „ëµ                      | ë¹„íŠ¸í­ | íŠ¹ì§•                        |
| ----- | ------------------------------ | ------ | --------------------------- |
| GPTQ  | error-aware rounding (Hessian) | 2â€“8bit | ê°€ì¥ ì •ë°€í•¨, ëŠë¦¼           |
| AWQ   | activation-aware rescaling     | 2â€“8bit | ë¹ ë¥´ê³  íš¨ê³¼ì                |
| BiLLM | signë§Œ ìœ ì§€ + scale            | \~1bit | ê·¹í•œ ì••ì¶•, ì„±ëŠ¥ ìœ ì§€ ë„ì „ì  |




# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë¥¼ ìì„¸í•˜ê²Œ ë³´ê³  ë‹¤ë¥¸ ë°©ë²•ë¡ ì— ë¹„í•˜ì—¬ íŠ¹ì¶œë‚œ ì ê³¼ ë…¼ë¬¸ì—ì„œ ì œê¸°í•˜ëŠ” ì–´ë– í•œ ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ì´ìœ ì™€ ë„ˆì˜ ìƒê°ì„ ì•Œë ¤ì¤˜



### âœ… ê²°ë¡  ìš”ì•½

ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ PTQ(Post-Training Quantization) ê¸°ë²•ë“¤(GPTQ, AWQ, SmoothQuant, BiLLM ë“±)ì„ Qwen3ì— ê´‘ë²”ìœ„í•˜ê²Œ ì ìš©í•˜ì—¬ **Qwen3ê°€ íŠ¹íˆ ultra-low-bit (â‰¤3bit)** ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ **íƒ€ ëª¨ë¸ ëŒ€ë¹„ ì‹¬ê°**í•˜ë‹¤ëŠ” ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ:

* **4bit ì´ìƒì—ì„œëŠ” AWQì™€ GPTQê°€ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë³´ì¡´**
* **2bit/3bitì—ì„œëŠ” Qwen3ê°€ LLaMA3ë³´ë‹¤ í›¨ì”¬ ë” ì·¨ì•½**
* **BiLLMì€ binarized (1bit ìˆ˜ì¤€)ì„ì—ë„ ì¼ë¶€ 3bitë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ ì œê³µ**

---

## ğŸ“Š ì£¼ìš” ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (Qwen3-14B ê¸°ì¤€)

| ë°©ë²•        | ë¹„íŠ¸í­ (W/A) | PPL (C4) | MMLU | íŠ¹ì§•                             |
| ----------- | ------------ | -------- | ---- | -------------------------------- |
| FP16        | 16/16        | 9.68     | 80.7 | full precision baseline          |
| GPTQ        | 4/16         | 10.3     | 78.5 | **ì„±ëŠ¥ ì €í•˜ â‰ˆ 2.2%**             |
| AWQ         | 4/16         | 10.3     | 78.7 | GPTQë³´ë‹¤ ê·¼ì†Œíˆ ìš°ìˆ˜             |
| SmoothQuant | 4/8          | 46.0     | 72.7 | **í™œì„±í™” ì–‘ìí™”ë¡œ ì„±ëŠ¥ í•˜ë½ í¼** |
| BiLLM       | \~1bit       | 20.4     | 39.9 | 3bit AWQë³´ë‹¤ ìš°ìˆ˜, ì´ë¡€ì         |

---

## ğŸŒŸ íŠ¹ì¶œë‚œ ì : ë…¼ë¬¸ì´ ë³´ì—¬ì£¼ëŠ” Qwen3-specificí•œ í˜„ìƒ

| êµ¬ë¶„                          | ì„¤ëª…                                                                                         |
| ----------------------------- | -------------------------------------------------------------------------------------------- |
| **1. Quantization ë¯¼ê°ë„**    | Qwen3ëŠ” ë‹¤ë¥¸ LLM(LLaMA3 ë“±)ë³´ë‹¤ quantizationì— í›¨ì”¬ ë¯¼ê°í•¨                                   |
| **2. ì‚¬ì „í•™ìŠµ í’ˆì§ˆì˜ ì—­ì„¤**   | ë” ì •êµí•˜ê²Œ í•™ìŠµëœ ëª¨ë¸ì¼ìˆ˜ë¡ redundancyê°€ ì ê³ , quantization errorë¥¼ í¡ìˆ˜í•˜ì§€ ëª»í•¨          |
| **3. BiLLMì˜ ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥** | 1bit ìˆ˜ì¤€ì—ì„œë„ ì¼ë¶€ 3bitë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì€ ê¸°ì¡´ ìƒì‹ ê¹¨ëœ¨ë¦¼                                   |
| **4. PTQ ë°©ë²•ë³„ ê°•ê±´ì„± ì°¨ì´** | GPTQëŠ” ì „ë°˜ì ìœ¼ë¡œ ìµœìƒìœ„ ì„±ëŠ¥, AWQëŠ” ì†ë„/ì •í™•ë„ ê· í˜•, SmoothQuantëŠ” ì‘ì€ ëª¨ë¸ì— íŠ¹íˆ ë¶ˆë¦¬í•¨ |

---

## ğŸ§  ë…¼ë¬¸ì´ ì œì‹œí•˜ëŠ” ì›ì¸: "ì •êµí•œ ì‚¬ì „í•™ìŠµ = ë‚®ì€ redundancy"

> Qwen3ëŠ” LLaMA3 ë“±ë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ í’ˆì§ˆ ê´€ë¦¬ëœ ë°ì´í„°ë¡œ í•™ìŠµë˜ì–´ ìˆì–´, í‘œí˜„ë“¤ì´ ë§¤ìš° ì••ì¶•ì ì´ê³  ì •ì œë¨ â†’
> ì´ë¡œ ì¸í•´ ì–‘ìí™” ì‹œ ì•½ê°„ì˜ precision ì†ì‹¤ë„ ì„±ëŠ¥ì— **ì¹˜ëª…ì **.

ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ì‹¤ì¦ìœ¼ë¡œ ì´ë¥¼ ì§€ì§€í•©ë‹ˆë‹¤:

* LLaMA3-8B (AWQ w3a16) â†’ C4 PPL: 11.6
* Qwen3-8B (AWQ w3a16) â†’ C4 PPL: 23.8 (**2ë°° ì´ìƒ ë‚˜ì¨**)

> ì´ëŠ” ê³ ì„±ëŠ¥ LLMì¼ìˆ˜ë¡ PTQì— ë” ì •êµí•œ ì „ëµì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°

---

## ğŸ¤” ê°œì¸ì  í•´ì„ê³¼ ì˜ê²¬

| í•­ëª©                       | í‰ê°€                                                                                                                                                                  |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ë¬¸ì œì˜ì‹ì˜ ëª…í™•ì„±**      | ë‹¨ìˆœíˆ quantizationì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì™œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§€ëŠ”ì§€ **ì›ì¸ ë¶„ì„ê¹Œì§€ ì •ëŸ‰ì ìœ¼ë¡œ ìˆ˜í–‰**í•œ ì ì—ì„œ ë†’ì€ í‰ê°€ë¥¼ ë°›ì„ ë§Œí•¨                                     |
| **BiLLM ê´€ë ¨ ê²°ê³¼**        | ì¼ë°˜ì ìœ¼ë¡œ 1bit quantizationì€ usableí•˜ì§€ ì•Šë‹¤ê³  ì—¬ê²¨ì§€ì§€ë§Œ, BiLLMì´ 3bitë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë¶€ë¶„ì€ **ë” ê¹Šì€ ë¶„ì„ì´ í•„ìš”**í•¨. (êµ¬ì¡° ë³´ì •? ReLU clipping ì „ëµ?) |
| **ë°ì´í„°ì…‹ êµ¬ì„±ì˜ í†µì¼ì„±** | ëª¨ë“  PTQ ê¸°ë²•ì„ ë™ì¼í•œ calibration ë°ì´í„°(128 samples from C4), ë™ì¼í•œ í‰ê°€ì…‹ì— ì ìš©í•˜ì—¬ **ì‹¤í—˜ì˜ ê³µì •ì„± ë³´ì¥**                                                       |
| **ì œì•ˆì˜ ë¶€ì¡±**            | ë‹¤ë§Œ ë…¼ë¬¸ ìì²´ëŠ” **ìƒˆë¡œìš´ PTQ ë°©ë²•ì„ ì œì•ˆí•˜ì§€ëŠ” ì•ŠìŒ**. í˜„ì¡´ ê¸°ë²•ì˜ í‰ê°€ ë° ì§„ë‹¨ì— ì§‘ì¤‘í•œ ë©”íƒ€-ì—°êµ¬ë¡œ ë³´ì•„ì•¼ í•¨                                                       |

---

## ğŸ”„ í–¥í›„ ì œì•ˆ (ë…¼ë¬¸+ë‚´ ì œì–¸)

| ì œì•ˆ ë°©ì‹                        | ì„¤ëª…                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| **Reordering PTQ**               | ì±„ë„ ìˆœì„œ ë³€ê²½í•˜ì—¬ outlier ì œê±° (ì˜ˆ: RPTQ)                   |
| **Rotation Quantization**        | SpinQuantì²˜ëŸ¼ rotationì„ í†µí•œ í‘œí˜„ flattening                |
| **Layer-aware Bit-width Tuning** | ì¤‘ìš” ë ˆì´ì–´ëŠ” 6bit, ëœ ì¤‘ìš”í•œ ê±´ 2bit ë“± selective bit-width |




# Q : ì´ ë…¼ë¬¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì–´ë–¤ ì—°êµ¬íë¦„ì´ ìˆëŠ”ì§€ ì •ë¦¬í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì¤˜



ì´ ë…¼ë¬¸ **"An Empirical Study of Qwen3 Quantization"** ì˜ í•œê³„ëŠ” í¬ê²Œ ë‘ ê°€ì§€ë¡œ ìš”ì•½ë©ë‹ˆë‹¤:

---

### âœ… í•µì‹¬ í•œê³„ ìš”ì•½

| í•œê³„ì                                  | ì„¤ëª…                                                                                                     |
| -------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **1. Ultra-low-bit (â‰¤3bit) ì„±ëŠ¥ ê¸‰ë½** | Qwen3ëŠ” redundancyê°€ ì ì€ ê³ ì„±ëŠ¥ ëª¨ë¸ì´ë¼ì„œ 2\~3bit quantizationì—ì„œ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë¨                |
| **2. Activation Quantizationì— ì·¨ì•½**  | SmoothQuant(w4a8, w4a4) ì‹¤í—˜ì—ì„œ MMLU ê¸°ì¤€ 10\~25pt ì´ìƒ ì„±ëŠ¥ í•˜ë½ ë°œìƒ â†’ íŠ¹íˆ outlier activationì— ë¯¼ê° |

---

## ğŸ” ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ì£¼ìš” ì—°êµ¬ íë¦„ 3ê°€ì§€

---

### 1ï¸âƒ£ **Rotation-Based Quantization (e.g., SpinQuant)**

| í•µì‹¬ ì•„ì´ë””ì–´ | Quantization ì „ì— weight/activation ê³µê°„ì„ rotationí•˜ì—¬ **ì–‘ìí™” ì†ì‹¤ì„ ê· ë“±í•˜ê²Œ ë¶„ì‚°** |
| ------------- | --------------------------------------------------------------------------------------- |
| ëŒ€í‘œ ë…¼ë¬¸     | SpinQuant: LLM Quantization with Learned Rotations (arXiv:2405.16406, 2024)             |
| ê¸°ìˆ  ìš”ì•½     |                                                                                         |

* Rotation matrix $R$ í•™ìŠµ í›„:

  $$
  W' = W \cdot R,\quad x' = R^{-1} x
  $$

  * â†’ Outlier ê°’ë“¤ì´ ì••ì¶•ë˜ì–´ ë²”ìœ„ê°€ ì¢ì•„ì§
  * â†’ Quantization clippingì´ ì ê²Œ ì¼ì–´ë‚˜ê³ , reconstruction loss ê°ì†Œ |

\| ì¥ì  | GPTQ ëŒ€ë¹„ ë” ì •êµí•œ í‘œí˜„ ì••ì¶• ê°€ëŠ¥ (íŠ¹íˆ activation-aware GPTQì— íš¨ê³¼ì ) |
\| ë‹¨ì  | í•™ìŠµ overheadì™€ rotation matrix ì €ì¥ ë¶€ë‹´ |

---

### 2ï¸âƒ£ **Reordering-based PTQ (e.g., RPTQ)**

\| í•µì‹¬ ì•„ì´ë””ì–´ | activation í˜¹ì€ weight channel ìˆœì„œë¥¼ **ì •ë ¬(ì •ë ¬ ê¸°ë°˜ permutation)** í•˜ì—¬ **ì •ë³´ ë°€ë„ í–¥ìƒ** |
\| ëŒ€í‘œ ë…¼ë¬¸ | RPTQ: Reorder-based Post-Training Quantization for LLMs (arXiv:2304.01089) |
\| ê¸°ìˆ  ìš”ì•½ |

* í™œì„±í™”ì—ì„œ í° ê°’ì´ ì§‘ì¤‘ëœ ì±„ë„ì„ ë¨¼ì € ì •ë ¬:

  $$
  W \Rightarrow \text{sort}(W, key=\text{activation variance})
  $$
* â†’ Quantizerê°€ í‘œí˜„í•´ì•¼ í•  ë¶„í¬ë¥¼ ë” ì••ì¶•ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ |
  \| ì¥ì  | íŠ¹ë³„í•œ ì¶”ê°€ í•™ìŠµ ì—†ì´ ë‹¨ìˆœí•œ permutationìœ¼ë¡œ íš¨ê³¼ |
  \| ì ìš© ê¸°ëŒ€ | Qwen3ì˜ outlier activation ë¬¸ì œì— íŠ¹íˆ ì í•© |

---

### 3ï¸âƒ£ **Mixed-Precision Quantization + Layer Sensitivity Profiling**

\| í•µì‹¬ ì•„ì´ë””ì–´ | Layer ë³„ë¡œ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•´ **ì¤‘ìš”í•œ ë ˆì´ì–´ì—ëŠ” 6\~8bit**, ì¤‘ìš”ë„ê°€ ë‚®ì€ ë ˆì´ì–´ëŠ” **2\~4bit** í• ë‹¹ |
\| ëŒ€í‘œ ì ‘ê·¼ | GPTQ + Hessian ê¸°ë°˜ sensitivity ë¶„ì„ |
\| êµ¬ì²´ ì „ëµ |

* Qwen3-14Bì˜ attention/output layerëŠ” 6bit ìœ ì§€
* feed-forward ì¼ë¶€ì™€ embeddingì€ 3bitë¡œ ì¶•ì†Œ
* â†’ ì „ì²´ ëª¨ë¸ì˜ í‰ê·  ë¹„íŠ¸ìˆ˜ëŠ” 4bit ì´í•˜ë¡œ ë‚®ì¶œ ìˆ˜ ìˆìŒ |
  \| ì¥ì  | ë¹„íŠ¸ë‹¹ íš¨ìœ¨ì„ ìµœì í™”í•˜ë©´ì„œ ì„±ëŠ¥ ë³´ì¡´ |
  \| ê´€ë ¨ ì—°êµ¬ | "Quant-Noise" (Fairseq), "BitSplit" quantization, "DoReFa-Net" |

---

## ğŸ“Š ì „ëµë³„ ë¹„êµ ì •ë¦¬

| ì „ëµëª…              | ëŒ€ìƒ              | ì£¼ ëª©ì       | ì„±ëŠ¥ íšŒë³µ ê¸°ëŒ€ | ê³„ì‚° ë¹„ìš© | ì ìš© ë‚œì´ë„ |
| ------------------- | ----------------- | ------------ | -------------- | --------- | ----------- |
| **SpinQuant**       | Weight/Activation | ë¶„í¬ í‰íƒ„í™”  | â˜…â˜…â˜…â˜…â˜†          | ë†’ìŒ      | ì¤‘          |
| **RPTQ**            | Activation        | outlier ì œê±° | â˜…â˜…â˜…â˜…â˜†          | ë‚®ìŒ      | ë‚®ìŒ        |
| **Mixed-Precision** | Layer-wise        | ì¤‘ìš”ë„ ë°˜ì˜  | â˜…â˜…â˜…â˜…â˜…          | ì¤‘        | ì¤‘          |

---

## ğŸ’¡ ë‚´ ìƒê°: Qwen3ì—ëŠ” ì–´ë–¤ ë°©ë²•ì´ ê°€ì¥ ì í•©í•œê°€?

> **Rotation + Mixed Precision ì¡°í•©**ì´ ê°€ì¥ ìœ ë ¥

* ì´ìœ : Qwen3ì€ redundancyê°€ ë§¤ìš° ë‚®ê³ , activation ê°’ ë¶„í¬ê°€ ë„“ìŒ â†’ rotationìœ¼ë¡œ ë¶„í¬ë¥¼ í‰íƒ„í™”í•˜ê³ , ì¤‘ìš” ë ˆì´ì–´ë§Œ high-bitë¡œ ë³´í˜¸í•˜ëŠ” ì „ëµì´ ì‹¤ìš©ì ì„
* BiLLMì²˜ëŸ¼ binarizationì„ ê³ ë ¤í•˜ëŠ” ê²ƒë„ í¥ë¯¸ë¡­ì§€ë§Œ, production-levelì—ì„œëŠ” mixed precisionì´ ë” ì•ˆì •ì ì„



# Q : Qwen3 ëª¨ë¸ì€ ê¸°ì¡´ LLMë“¤(LLaMA3 ë“±)ì— ë¹„í•´ ultra-low-bit quantization(3bit ì´í•˜)ì—ì„œ ì™œ ë” í° ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì´ëŠ”ê°€? ì´ í˜„ìƒì´ ì‚¬ì „í•™ìŠµ ë°©ì‹ì´ë‚˜ í‘œí˜„ ì••ì¶•ê³¼ ì–´ë–¤ ê´€ë ¨ì´ ìˆëŠ”ê°€?

 

### âœ… ê²°ë¡ ë¶€í„° ìš”ì•½

Qwen3ëŠ” **í‘œí˜„ ì••ì¶•ì´ ë§¤ìš° ì •êµí•˜ê²Œ ì´ë¤„ì§„ ê³ ì„±ëŠ¥ ëª¨ë¸**ì´ê¸° ë•Œë¬¸ì—, ultra-low-bit (â‰¤3bit) quantizationì—ì„œ ë¯¸ì„¸í•œ weight ì˜¤ì°¨ì—ë„ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ëœë‹¤. ì´ëŠ” **redundancy(ì¤‘ë³µ í‘œí˜„)ì˜ ë¶€ì¡±** ë•Œë¬¸ì´ë©°, ì´ëŠ” LLaMA3 ê°™ì€ ëª¨ë¸ê³¼ì˜ ê°€ì¥ í° ì°¨ì´ì ì´ë‹¤.

---

## ğŸ” 1. ì„±ëŠ¥ ì €í•˜ì˜ ì‹¤ì¸¡ ì¦ê±°

| ëª¨ë¸      | ë°©ë²• (w3a16) | PPL(C4) | ì„±ëŠ¥ ë¹„êµ         |
| --------- | ------------ | ------- | ----------------- |
| LLaMA3-8B | AWQ          | 11.6    | ì¤€ìˆ˜í•œ ì„±ëŠ¥ ìœ ì§€  |
| Qwen3-8B  | AWQ          | 23.8    | **2ë°° ì´ìƒ ì•…í™”** |

> Qwen3ëŠ” 3bit quantizationì— **ë” ë¯¼ê°í•˜ë©°**, LLaMA3ë³´ë‹¤ PPLì´ í›¨ì”¬ ë‚˜ë¹ ì§

---

## ğŸ§  2. ì‚¬ì „í•™ìŠµ ë°©ì‹ì˜ ì°¨ì´: í‘œí˜„ ì••ì¶• vs í‘œí˜„ ì¤‘ë³µ

### Qwen3:

* ëŒ€ê·œëª¨ ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ **fine-tuningëœ pretraining**
* ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‚´ê¸° ìœ„í•´ **ë¶ˆí•„ìš”í•œ í‘œí˜„ì„ ì œê±°í•¨**
* ì¦‰, í•˜ë‚˜ì˜ neuron/weightì— **ë” ë§ì€ ì •ë³´**ê°€ ì§‘ì¤‘ë¨

### LLaMA3:

* ìƒëŒ€ì ìœ¼ë¡œ **ë‹¤ì†Œ ì¼ë°˜ì ì¸ pretraining ë°ì´í„°**
* í‘œí˜„ ê³µê°„ì˜ redundancyê°€ ë§ì•„, ì¼ë¶€ ì†ì‹¤ì— ëŒ€í•œ **robustness í™•ë³´**

â¡ ê²°ê³¼ì ìœ¼ë¡œ, Qwen3ëŠ” 3bit ì´í•˜ë¡œ ì–‘ìí™”ë  ë•Œ **í•œ weightì˜ ì†ì‹¤ì´ ê³§ ì •ë³´ ì†ì‹¤**ë¡œ ì´ì–´ì§

---

## ğŸ“ 3. í‘œí˜„ ì••ì¶•ê³¼ quantization noise ê°„ ìƒí˜¸ì‘ìš©

* ì–‘ìí™”ëŠ” weightë¥¼ ì•„ë˜ì™€ ê°™ì´ ê·¼ì‚¬í•¨:

  $$
  W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
  $$
* ì •ë°€ë„ê°€ ë‚®ì„ìˆ˜ë¡ (e.g., 3bit â†’ 8ê°œì˜ í‘œí˜„ê°’ë§Œ ì¡´ì¬) â†’ **quantization error**ê°€ ì¦ê°€
* redundancyê°€ ìˆë‹¤ë©´ ì´ errorëŠ” ë‹¤ë¥¸ neuronì—ì„œ ë³´ì™„ ê°€ëŠ¥
* í•˜ì§€ë§Œ Qwen3ëŠ” ì´ëŸ¬í•œ ë³´ì™„ ë©”ì»¤ë‹ˆì¦˜ì´ ë¶€ì¡±

â¡ ê²°êµ­, **í‘œí˜„ ì••ì¶•ì´ ë›°ì–´ë‚œ ëª¨ë¸ã»ã© quantizationì— ì·¨ì•½**

---

## ğŸ”„ 4. ë…¼ë¬¸ì—ì„œì˜ í•´ì„

> â€œAdvanced pre-training techniquesâ€¦ tend to produce models with less parameter redundancy, consequently making them more sensitive to quantization-induced information loss.â€

* ì´ëŠ” ë‹¨ìˆœí•œ ì‹¤í—˜ ê²°ê³¼ê°€ ì•„ë‹ˆë¼, **í•™ìŠµ ë°©ì‹ì˜ ì§„í™”ì— ë”°ë¼ quantization ì „ëµë„ ì§„í™”í•´ì•¼ í•œë‹¤**ëŠ” ì‹œì‚¬ì ì„ ë˜ì§

---

## ğŸ”§ ë³´ì™„ ê°€ëŠ¥ì„±

* activation/channel rotation (SpinQuant)
* attention block selective precision
* pre-quantization distillation (loss-aware finetune)



# Q : GPTQ, AWQ, SmoothQuant, BiLLM ë“±ì˜ quantization ë°©ë²•ì€ ê°ê° ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ weight ë˜ëŠ” activationì˜ ì •ë³´ë¥¼ ì••ì¶•í•˜ê³ , Qwen3ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì— ëŒ€í•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ëŒ€ì‘í•˜ê³  ìˆëŠ”ê°€?

 

### âœ… ê²°ë¡  ìš”ì•½

GPTQ, AWQ, SmoothQuant, BiLLMì€ ì„œë¡œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ weight ë˜ëŠ” activationì˜ ì •ë³´ ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•œ ê¸°ë²•ì´ë‹¤.
ì´ë“¤ì€ Qwen3ì˜ **low-redundancy, high-precision** êµ¬ì¡°ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ëŒ€ì‘í•˜ë©°, **error ë³´ìƒ, outlier ì²˜ë¦¬, ì´ì§„í™” ë“±** ê°ê¸° ë‹¤ë¥¸ ì „ëµì„ ì·¨í•œë‹¤.

---

## ğŸ“¦ ë°©ë²•ë³„ ì •ë³´ ì••ì¶• ì „ëµ ë° Qwen3 ëŒ€ì‘ ìš”ì•½

| ë°©ë²•            | ì••ì¶• ëŒ€ìƒ           | í•µì‹¬ ì „ëµ                                           | Qwen3 ëŒ€ì‘ ë°©ì‹                                  | ì„±ëŠ¥ ìš”ì•½ (Qwen3-14B, 4bit ê¸°ì¤€) |
| --------------- | ------------------- | --------------------------------------------------- | ------------------------------------------------ | -------------------------------- |
| **GPTQ**        | Weight              | Gradient-aware rounding (Hessian ê¸°ë°˜ error ìµœì†Œí™”) | ì¤‘ìš” weightì˜ ì •ë³´ ì†ì‹¤ ì–µì œ                     | MMLU: **78.5**                   |
| **AWQ**         | Weight              | Activation-aware weight scaling                     | outlier activationì— ì„ ì œì  ëŒ€ì‘                 | MMLU: **78.7**                   |
| **SmoothQuant** | Activation + Weight | Weight rescaling via activation statistics          | Activation ë²”ìœ„ ì •ê·œí™”ë¡œ quantization noise ì €ê° | MMLU(w4a8): **72.7**             |
| **BiLLM**       | Weight (1bit)       | Binarization + residual scaling                     | ê·¹í•œ ì••ì¶•ì—ì„œì˜ êµ¬ì¡°ì  ì •ê·œí™”                    | MMLU(\~1bit): **39.9**           |

---

## ğŸ” 1. GPTQ (Gradient Post-Training Quantization)

### â›ï¸ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

* ê° weightì˜ ì˜í–¥ë ¥ì„ Hessian matrixë¡œ ê·¼ì‚¬í•˜ì—¬, **ê°€ì¥ ì†ì‹¤ì´ ì ì€ ë°©í–¥ìœ¼ë¡œ rounding**
* ìˆ˜ì‹ ê¸°ë°˜:

  $$
  \min \|W - W_q\|_H = (W - W_q)^T H (W - W_q)
  $$

### ğŸ¯ Qwen3 ëŒ€ì‘

* Qwen3ì˜ weightëŠ” ë§¤ìš° ë¯¼ê°í•˜ë¯€ë¡œ, ì¤‘ìš” weightë¥¼ ì •ë°€í•˜ê²Œ ë³´ì¡´í•˜ëŠ” GPTQëŠ” **ê³ ì •ë°€ ëª¨ë¸ì— ì í•©**
* íŠ¹íˆ 4bit í™˜ê²½ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ ì„±ëŠ¥ ìœ ì§€

---

## ğŸ” 2. AWQ (Activation-aware Weight Quantization)

### â›ï¸ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

* **Activation ê°’ì˜ ë¶„í¬(íŠ¹íˆ outlier)** ë¥¼ ë³´ê³ , ê° ì±„ë„ì˜ weightë¥¼ scalingí•˜ì—¬ ì••ì¶•
* ìˆ˜ì‹:

  $$
  W' = \frac{W}{s}, \quad x' = x \cdot s \quad \text{(scale factor sëŠ” activation ê¸°ë°˜)}
  $$

### ğŸ¯ Qwen3 ëŒ€ì‘

* Qwen3ëŠ” **activation outlierê°€ ë§ìŒ** â†’ weightë§Œ ì •ê·œí™”í•˜ë©´ quantization errorê°€ ì»¤ì§
* ì´ë¥¼ ë§‰ê¸° ìœ„í•´ activationì˜ ì˜í–¥ê¹Œì§€ ê³ ë ¤ â†’ **activation noise ì œì–´ì— íš¨ê³¼ì **

---

## ğŸ” 3. SmoothQuant

### â›ï¸ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

* Activationì˜ varianceë¥¼ ê¸°ì¤€ìœ¼ë¡œ LayerNorm ì´ì „ì— weight rescaling ìˆ˜í–‰
* ëª©ì : **activationì˜ dynamic rangeë¥¼ ì¤„ì—¬ì„œ activation quantization ì‹œ clipping ë°©ì§€**

### ğŸ¯ Qwen3 ëŒ€ì‘

* Qwen3ì—ì„œ SmoothQuantëŠ” w4a8 ë˜ëŠ” w4a4 ë“± activation quantization ì‹œ **ì„±ëŠ¥ ê¸‰ê°**

  * ì´ìœ : Qwen3ì˜ **LayerNorm ë° GeLU ì´í›„ activation ë¶„í¬ê°€ ë§¤ìš° wide** â†’ clippingì— ë§¤ìš° ì·¨ì•½

---

## ğŸ” 4. BiLLM (Binarized LLM)

### â›ï¸ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

* weightë¥¼ {+1, -1}ë¡œ ì–‘ìí™”
* ë‹¨ìˆœ binarizationì´ ì•„ë‹Œ, residual ì •ë³´ë¥¼ scaling factorë¡œ ìœ ì§€í•˜ì—¬ ë³µì› ê°€ëŠ¥ì„± í–¥ìƒ:

  $$
  W \approx \alpha \cdot \text{sign}(W)
  $$

### ğŸ¯ Qwen3 ëŒ€ì‘

* ì¼ë°˜ì ìœ¼ë¡œ ê³ ì„±ëŠ¥ ëª¨ë¸ì—ì„œ binarizationì€ ë¶ˆë¦¬í•˜ì§€ë§Œ,
* BiLLMì€ ì¼ë¶€ ë ˆì´ì–´ë§Œ binarizeí•˜ê±°ë‚˜, residual ì •ê·œí™”ë¥¼ ì˜í•´ì„œ 3bitë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒ (íŠ¹íˆ Qwen3-32Bì—ì„œ)

---

## ğŸ“ˆ ì‹¤í—˜ ì˜ˆì‹œ (Qwen3-14B, 4bit weight ê¸°ì¤€ MMLU ì„±ëŠ¥)

| ë°©ë²•           | MMLU (â†‘) | PPL (C4, â†“) | íŠ¹ì§•                           |
| -------------- | -------- | ----------- | ------------------------------ |
| FP16           | 80.7     | 9.68        | baseline                       |
| GPTQ           | 78.5     | 10.3        | ê°€ì¥ ì•ˆì •ì                     |
| AWQ            | 78.7     | 10.3        | outlier ëŒ€ì‘ íƒì›”              |
| SmoothQuant    | 72.7     | 46.0        | activation ì–‘ìí™”ì— ë¯¼ê°       |
| BiLLM (\~1bit) | 39.9     | 20.4        | ê·¹í•œ ì¡°ê±´ì—ì„œë„ ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥ |

---

## ğŸ”š ìš”ì•½ ì •ë¦¬

| ë°©ë²•        | Qwen3 êµ¬ì¡° ëŒ€ì‘ ì „ëµ                                | ì¥ì                     | ë‹¨ì                        |
| ----------- | --------------------------------------------------- | ----------------------- | -------------------------- |
| GPTQ        | ì¤‘ìš”í•œ weightë¥¼ ì •ë°€í•˜ê²Œ ë³´ì¡´ (low redundancy ëŒ€ì‘) | ì •í™•ë„ ë†’ìŒ             | ëŠë¦¼, ë©”ëª¨ë¦¬ ì‚¬ìš© â†‘        |
| AWQ         | activation outlierë¥¼ ì‚¬ì „ scaleë¡œ ì •ê·œí™”            | ë¹ ë¥´ê³  ì •í™•ë„ë„ ì¢‹ìŒ    | ê·¹ë‹¨ì ì¸ outlierì—ëŠ” í•œê³„  |
| SmoothQuant | activation range ì¡°ì ˆ                               | ë‚®ì€ latency, ì¢‹ì€ w8a8 | activation clippingì— ì·¨ì•½ |
| BiLLM       | ê·¹ë‹¨ì  quantizationì— residual ë³´ì •                 | ì••ì¶•ë¥  ìµœê³              | ì„±ëŠ¥ ìœ ì§€ ì–´ë ¤ì›€           |


# Q : ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ calibration ë°©ì‹(128ê°œ C4 ìƒ˜í”Œ ê¸°ë°˜)ì´ quantization ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€? calibration ë°ì´í„°ì˜ ì–‘ì´ë‚˜ ë‹¤ì–‘ì„±ì„ ì¡°ì ˆí•  ê²½ìš° ì„±ëŠ¥ í–¥ìƒ ì—¬ì§€ê°€ ìˆëŠ”ê°€?

 

### âœ… ê²°ë¡  ìš”ì•½

Qwen3 quantizationì—ì„œ ì‚¬ìš©ëœ **128ê°œ C4 ìƒ˜í”Œ ê¸°ë°˜ì˜ calibration**ì€ **ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì–‘**ì´ì§€ë§Œ, ì¼ë°˜ì ì¸ PTQ ë°©ì‹(GPTQ, AWQ ë“±)ì—ì„œëŠ” **ì¶©ë¶„íˆ ìœ íš¨í•œ ìˆ˜ì¤€**ìœ¼ë¡œ íŒë‹¨ëœë‹¤. ë‹¤ë§Œ, **bit ìˆ˜ê°€ ì‘ì•„ì§ˆìˆ˜ë¡** ë˜ëŠ” **activation quantizationì„ í¬í•¨í• ìˆ˜ë¡**, calibration ë°ì´í„°ì˜ **ì–‘ê³¼ ë‹¤ì–‘ì„±**ì´ ì„±ëŠ¥ì— **í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤**.

---

## ğŸ” 1. ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ Calibration ë°©ì‹ ìš”ì•½

| í•­ëª©      | ì„¸ë¶€ ë‚´ìš©                                                     |
| --------- | ------------------------------------------------------------- |
| ë°ì´í„°ì…‹  | **C4 (Colossal Cleaned Common Crawl)**                        |
| ìƒ˜í”Œ ìˆ˜   | **128ê°œ ìƒ˜í”Œ**                                                |
| ì…ë ¥ ê¸¸ì´ | 2048 í† í°                                                     |
| ëª©ì       | weight ë¶„í¬, activation range ì¶”ì • (e.g., min/max, Hessian)   |
| ëŒ€ìƒ ë°©ë²• | GPTQ, AWQ, SmoothQuant, BiLLM ë“± ëª¨ë‘ ë™ì¼í•œ calibration ì‚¬ìš© |

â¡ ì‹¤í—˜ì˜ **ê³µì •ì„±ì„ ë³´ì¥**í•˜ê¸° ìœ„í•´ ëª¨ë“  ë°©ë²•ì— ë™ì¼í•œ calibration ì¡°ê±´ ì ìš©

---

## ğŸ§  2. Calibration ë°ì´í„° ì–‘ì´ ì ì„ ë•Œ ì˜í–¥

### ì–‘ì´ ì ì€ ê²½ìš°:

* **ë¶„í¬ ì¶”ì • ì •í™•ë„ â†“** â†’ íŠ¹íˆ activationì˜ outlier ë²”ìœ„ë¥¼ ê³¼ì†Œí‰ê°€
* **Hessian ê·¼ì‚¬ ì˜¤ì°¨ â†‘** â†’ GPTQì˜ rounding precision ë‚®ì•„ì§
* **ì±„ë„ë³„ scaling factor ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥** (AWQ, SmoothQuant)

### ê²°ê³¼ì ìœ¼ë¡œ:

* **bit ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë” ë¯¼ê°**í•¨
  â†’ 8bitì—ì„œëŠ” ê±°ì˜ ì˜í–¥ ì—†ìŒ
  â†’ 4bit ì´í•˜ì—ì„œëŠ” **ì •í™•ë„ ìµœëŒ€ 5\~10%ê¹Œì§€ ì°¨ì´ ê°€ëŠ¥**

---

## ğŸ“ˆ ì‹¤í—˜ì  ê·¼ê±° (ë…¼ë¬¸ ì™¸ GPTQ ê³µì‹ repo ê¸°ì¤€)

* 32ê°œ ìƒ˜í”Œ: ì„±ëŠ¥ ë¶ˆì•ˆì •
* 128ê°œ ìƒ˜í”Œ: baseline ìˆ˜ì¤€
* 512~~1024ê°œ ìƒ˜í”Œ: 1~~2pt í–¥ìƒ ê°€ëŠ¥ (íŠ¹íˆ MMLU ë“± reasoning task)

---

## ğŸ” ë°ì´í„° ë‹¤ì–‘ì„±ì˜ ì¤‘ìš”ì„±

| ì¡°ê±´                        | ì„¤ëª…                                 | ì˜í–¥                                                                              |
| --------------------------- | ------------------------------------ | --------------------------------------------------------------------------------- |
| **ë‹¨ì¼ ë„ë©”ì¸ í…ìŠ¤íŠ¸**      | ë‰´ìŠ¤/ìœ„í‚¤ ìœ„ì£¼ â†’ narrow distribution | ì¼ë¶€ layer í¸í–¥ ë°œìƒ ê°€ëŠ¥                                                         |
| **ë‹¤ì–‘í•œ ë„ë©”ì¸/ê¸¸ì´ í¬í•¨** | code, dialogue, QA í˜¼í•©              | LayerNorm, MLP ë“± ë‹¤ì–‘í•œ ë¶„í¬ ì¶”ì • ê°€ëŠ¥ â†’ **activation quantizationì— íŠ¹íˆ ìœ ë¦¬** |

> íŠ¹íˆ SmoothQuantì²˜ëŸ¼ activation statisticsì— ê¸°ë°˜í•œ ë°©ë²•ì€ **ë‹¤ì–‘ì„± í™•ë³´ê°€ í•µì‹¬**

---

## ğŸ’¡ ë‚´ ì œì•ˆ: ì‹¤ìš©ì  ê°œì„  ë°©ë²•

| ì „ëµ                               | ì„¤ëª…                                                                          |
| ---------------------------------- | ----------------------------------------------------------------------------- |
| **ìƒ˜í”Œ ìˆ˜ ì¦ê°€ (e.g., 512\~1024)** | ëŒ€ë¶€ë¶„ì˜ weight/activation ë¶„í¬ê°€ ì•ˆì •í™”ë¨                                    |
| **stratified sampling**            | ë‹¤ì–‘í•œ í† í”½/domain/text styleì„ ê³ ë¥´ê²Œ ë°˜ì˜                                   |
| **token-level calibration**        | LayerNorm ë° RMS-based quantizerê°€ ìˆëŠ” ê²½ìš°, ì…ë ¥ diversityë¥¼ ë†’ì´ëŠ” ê²Œ ì¤‘ìš” |

---

## ğŸ”¬ ì‹¤í—˜ ì„¤ê³„ ì˜ˆì‹œ

```python
# ë‹¤ì–‘í•œ calibration ë¹„êµ ì‹¤í—˜
for n_samples in [64, 128, 512, 1024]:
    data = sample_c4(n=n_samples)
    quant_model = gptq_calibrate(model, data)
    eval_mmlu(quant_model)  # accuracy ì¸¡ì •
```

â¡ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° **128ê°œ ì •ë„ë¡œ ê¸°ë³¸ì ì¸ ì•ˆì •ì„±ì€ í™•ë³´ë˜ë‚˜**,
**ultra-low-bit (<4bit) ë˜ëŠ” activation quantization ì ìš© ì‹œ**ì—ëŠ” ë” ë§ì€/ë‹¤ì–‘í•œ ë°ì´í„°ê°€ í•„ìš”í•¨

