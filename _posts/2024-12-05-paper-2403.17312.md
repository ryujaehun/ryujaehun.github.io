---
title: "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"
date: 2024-12-05
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2403.17312)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

### 논문의 강점과 독창성
1. **독창적인 알고리즘 및 시스템 설계**:
   - **Sparse Window Attention (SWA)**: 이 알고리즘은 중요 토큰을 선택하여 메모리 사용량을 줄이는 동시에 정확도를 유지하는 독특한 접근 방식을 제안합니다.
   - **세 단계 동적 스케줄링**: 메모리 캐싱과 재계산을 동적으로 조정하여 리소스 제약 하드웨어에서 최적의 성능을 달성합니다.

2. **효율성 개선**:
   - ALISA는 FlexGen 대비 최대 3배, vLLM 대비 1.9배의 처리량을 달성했습니다.
   - 메모리 사용량을 줄이고 처리 속도를 높이기 위해 KV 텐서를 INT8로 압축하는 최적화를 제안했습니다.

3. **적용 범위**:
   - LLaMA, OPT, Pythia 등 다양한 LLM 모델에 적용 가능하며, 여러 NLP 작업에서 일관되게 높은 성능을 보여줍니다.

---

### 핵심 알고리즘: Sparse Window Attention (SWA)
**예시 입력**:
- 입력 시퀀스: `["The", "capital", "of", "France", "is"]`
- SWA 알고리즘은 과거 토큰 중 중요한 토큰만 선택하여 계산에 포함합니다.

**단계별 과정**:
1. **중요 토큰 식별**:
   - 각 토큰의 attention weight를 기반으로 중요한 토큰을 선택합니다. 예: "capital", "France"가 높은 weight를 가지므로 선택.

2. **Sparse 패턴 생성**:
   - 선택된 토큰을 기반으로 locally static 패턴(최근 토큰)과 globally dynamic 패턴(중요 토큰)을 생성합니다.
   - 예: `locally static: "is"`, `globally dynamic: "France", "capital"`.

3. **계산 최적화**:
   - 선택된 KV 텐서만 사용하여 attention 계산 수행.
   - 메모리 사용량 감소 및 처리 속도 증가.

---

### 논문의 한계점
1. **KV Sparsity와 정확도 간 트레이드오프**:
   - SWA에서 80% 이상의 sparsity를 적용하면 일부 작업에서 정확도가 감소할 수 있습니다.

2. **동적 스케줄링의 복잡성**:
   - 동적 스케줄링의 최적화는 모델, 하드웨어, 작업에 따라 다르며, 사전 설정이 필요합니다.

3. **특정 하드웨어 의존성**:
   - 제안된 최적화는 GPU-CPU 간 메모리 대역폭에 크게 의존하며, 하드웨어 제약이 있는 환경에서는 효과가 제한될 수 있습니다.

4. **추가 계산 비용**:
   - Sparse Window Attention의 sparse 패턴 계산과 동적 스케줄링은 추가 계산 오버헤드를 초래할 수 있습니다.

---

이 논문은 메모리 제약 환경에서 대규모 언어 모델 추론을 최적화하는 데 있어 획기적인 아이디어와 실질적인 성능 개선을 제안하지만, 알고리즘의 효율성은 사용 사례와 하드웨어 환경에 따라 달라질 수 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

### 핵심 알고리즘: Sparse Window Attention (SWA)

Sparse Window Attention(SWA)는 **토큰의 중요도를 기반으로 메모리 사용을 줄이고 효율성을 높이는 알고리즘**입니다. 중요한 토큰만 선택적으로 처리하여 **KV 캐싱의 메모리 사용량을 줄이는 동시에 성능을 유지**합니다. 이 과정은 다음과 같은 단계로 나뉩니다.

---

### **예시 입력**
1. 입력 문장: `"The capital of France is Paris."`
2. 현재까지 처리된 토큰: `["The", "capital", "of", "France", "is"]`
3. 새로 예측해야 할 토큰: `"Paris"`

---

### **1단계: 중요 토큰 식별**
SWA는 각 토큰의 **attention weight**를 분석하여 중요도를 판단합니다.

- **입력 데이터**:
  - Query(Q): 현재 예측 토큰 `"Paris"`의 query 벡터.
  - Key(K): 이전 모든 토큰의 key 벡터.
  - Value(V): 이전 모든 토큰의 value 벡터.

- **Attention 계산**:
  \[
  AW(Q, K) = \text{Softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}}\right)
  \]
  여기서 \(AW(Q, K)\)는 각 토큰에 대한 attention weight입니다. 예를 들어:
  - `"capital"`: 0.4
  - `"France"`: 0.35
  - 나머지 토큰: 0.1 이하 (중요도가 낮음).

- **결론**:
  - 중요도가 높은 `"capital"`과 `"France"`는 선택.
  - 중요도가 낮은 `"The"`, `"of"`, `"is"`는 계산에서 제외(압축).

---

### **2단계: Sparse 패턴 생성**
SWA는 **local static**과 **global dynamic** 패턴을 결합하여 메모리 사용을 줄입니다.

1. **Locally Static 토큰**:
   - 최근 몇 개의 토큰을 항상 유지.
   - 예를 들어, `"is"`는 최신 토큰이므로 선택.

2. **Globally Dynamic 토큰**:
   - 전체 토큰 중 중요도가 높은 토큰을 선택.
   - 예: `"capital"`과 `"France"`는 높은 attention weight로 인해 선택.

3. **최종 Sparse 패턴**:
   - 선택된 토큰: `["capital", "France", "is"]`
   - 삭제된 토큰: `["The", "of"]`

---

### **3단계: Sparse 계산**
선택된 토큰으로 attention 계산을 수행하여 **메모리와 연산 비용을 줄임**.

1. **Sparse KV 텐서 생성**:
   - 선택된 토큰만 포함한 작은 \(K, V\) 텐서를 생성.
   - 메모리 사용량 감소.

2. **Attention 결과 계산**:
   - 선택된 Sparse \(K, V\)에 대해:
     \[
     \text{Output} = \text{Softmax}(Q \cdot K_{\text{sparse}}^T) \cdot V_{\text{sparse}}
     \]
   - 위 결과로 새 토큰 `"Paris"`를 생성.

---

### **4단계: 캐싱 및 업데이트**
1. 선택된 중요 토큰의 \(K, V\)를 GPU 또는 CPU 메모리에 캐싱.
2. 새로 생성된 토큰 `"Paris"`의 \(K, V\)를 업데이트.
3. 이전의 중요하지 않은 토큰은 삭제하여 메모리를 효율적으로 사용.

---

### **알고리즘 정리**
1. **Sparse 토큰 선택**:
   - 중요 토큰과 최신 토큰만 선택.
2. **KV 텐서 크기 축소**:
   - Sparse \(K, V\) 텐서로 연산을 줄임.
3. **동적 메모리 관리**:
   - 중요도가 낮은 토큰의 데이터는 CPU로 이동하거나 삭제.
4. **효율적인 연산**:
   - Sparse한 구조로도 원래 Attention 성능을 유지.

---

### **SWA의 주요 장점**
1. **효율성**:
   - 메모리 사용량과 계산량을 크게 줄임.
   - 예를 들어, 80% sparsity를 적용하면 정확도 손실 없이 연산 성능이 향상.
2. **유연성**:
   - 다양한 LLM에 적용 가능.
3. **정확도 유지**:
   - Sparse Attention에서도 Dense Attention과 유사한 정확도 유지.

---

SWA는 메모리와 연산 효율성을 극대화하면서도 정확도 저하를 최소화한 획기적인 알고리즘입니다. **중요 토큰 선택**과 **Sparse 패턴 생성**의 핵심 아이디어는 LLM 추론 최적화에 큰 기여를 합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### 논문에서 제시한 결과와 다른 방법론 대비 특출난 점

#### **논문의 주요 결과**
1. **성능 향상**:
   - ALISA는 **FlexGen** 대비 **최대 3배**, **vLLM** 대비 **최대 1.9배의 처리량 증가**를 달성.
   - 특히, **메모리 제약 시스템**에서 장시간 추론이 가능하며, GPU와 CPU 간 메모리 전송을 효율적으로 관리.

2. **정확도 유지**:
   - SWA를 통해 **최대 80% KV Sparsity**에서도 Dense Attention과 유사한 정확도를 유지.
   - 기존 Sparse Attention 방법(예: Local Attention, Strided Attention) 대비 **3~4배 높은 스파스성**에서도 성능 유지.

3. **메모리 사용 감소**:
   - Sparse Window Attention과 KV Tensor 압축(INT8)을 결합하여 메모리 사용량을 획기적으로 감소.
   - GPU 메모리 용량이 적은 환경에서도 더 큰 모델이나 긴 시퀀스를 처리 가능.

---

### **ALISA가 특출난 이유**

#### **1. Sparse Window Attention (SWA)**
- 기존 Sparse Attention 기법(Local, Strided)은 중요 토큰을 선택하지 않고, 고정된 슬라이딩 윈도우나 일정한 간격의 토큰만 처리.
- **SWA는 토큰별 중요도를 동적으로 계산**하여, 중요 토큰만 포함하는 Sparse 패턴을 생성:
  - **Locally Static 패턴**: 가장 최근의 토큰을 항상 포함하여 순차적인 정보를 유지.
  - **Globally Dynamic 패턴**: Attention Weight가 높은 중요한 토큰을 동적으로 선택.
- 이러한 설계는 메모리 사용량을 줄이면서도 Dense Attention에 가까운 정확도를 유지.

#### **2. 동적 스케줄링**
- 기존 방법(예: FlexGen)은 KV Tensor의 GPU와 CPU 간 전송을 **정적**으로 관리:
  - 특정 규칙에 따라 미리 정의된 토큰 수를 전송, 모든 단계에서 동일한 정책 적용.
- ALISA는 **세 가지 단계의 동적 스케줄링**을 통해 메모리와 계산을 효율적으로 관리:
  - **Phase I**: GPU 메모리 내에서 모든 KV Tensor를 캐싱.
  - **Phase II**: GPU 메모리가 부족하면 중요 토큰만 CPU로 오프로드.
  - **Phase III**: CPU로 오프로드된 토큰 중 일부를 삭제하고, 필요할 경우 GPU에서 재계산.
- 이 접근 방식은 메모리 전송 오버헤드를 줄이고, GPU 메모리를 더 효율적으로 활용.

#### **3. KV Tensor 압축 (INT8 Quantization)**
- 기존 LLM 최적화 기법은 주로 **모델 가중치**를 압축.
- ALISA는 **KV Tensor를 INT8로 압축**하여 메모리 사용량과 전송 비용을 줄임:
  - INT8 형식에서도 Sparse Attention의 정확도를 유지.
  - GPU와 CPU 간 데이터 전송 시 필요한 메모리 대역폭 감소.

#### **4. 전반적인 통합 설계**
- ALISA는 **알고리즘-시스템 간 협업** 설계를 도입:
  - Sparse Attention과 동적 스케줄링을 결합하여, 메모리 효율성과 계산 성능을 극대화.
  - 이전 연구들이 알고리즘 또는 시스템 중 하나에만 초점을 맞췄던 것과 차별화됨.

---

### **ALISA의 결과를 뒷받침하는 논문의 이유**
1. **Attention Weight의 고스파스성**:
   - 논문은 LLM 추론 중 Attention Weight가 **80~95%가량 sparse**하다는 것을 실험적으로 보여줌.
   - 이를 활용하여 메모리와 계산량을 줄이면서 성능을 유지할 수 있음을 입증.

2. **KV Tensor의 불균등한 중요성**:
   - 실험 결과, 특정 토큰(Key/Value Tensor)의 중요도가 높으며, 나머지는 무시 가능.
   - Sparse Window Attention이 이러한 중요성을 효과적으로 활용.

3. **CPU와 GPU 간 전송 오버헤드 문제**:
   - 기존 방법론(FlexGen, vLLM)은 CPU와 GPU 간 전송 오버헤드로 인해 처리량이 제한.
   - ALISA는 동적 스케줄링과 재계산을 결합하여 이 문제를 해결.

4. **정확도 유지**:
   - 기존 Sparse Attention(Local, Strided)은 Dense Attention과 유사한 결과를 내지 못했으나, SWA는 Dense Attention의 power-law 분포를 유지.

---

### **나의 분석 및 의견**

#### **긍정적인 측면**
- ALISA는 Sparse Attention과 동적 메모리 관리를 결합하여, 리소스 제약 환경에서 LLM 추론의 한계를 극복했습니다.
- Sparse Window Attention은 단순히 메모리를 줄이는 것을 넘어, Dense Attention의 정확도 특성을 유지하도록 설계되었습니다.
- 동적 스케줄링은 이론적으로도 최적화 문제로 모델링되었으며, 실제로 효율성을 입증.

#### **개선 가능성**
1. **Sparse Attention 오버헤드**:
   - Sparse Attention 생성 및 관리에는 추가 계산 비용이 들어감. 이를 줄이는 방법이 연구될 필요가 있음.
2. **유니버설 적용성**:
   - 특정 하드웨어 환경(GPU-CPU 대역폭)에 의존하기 때문에, 더 다양한 시스템에서의 적용 가능성을 분석해야 함.
3. **추론 속도와 정확도 간 균형**:
   - 80% Sparsity를 넘어가면 정확도 손실이 발생. 이를 보완할 수 있는 정교한 중요 토큰 선택 방법이 필요.

---

결론적으로, ALISA는 **Sparse Attention을 효율적으로 설계하고, 이를 동적 메모리 관리로 통합**함으로써, 리소스 제약이 큰 시스템에서도 고성능 LLM 추론을 가능하게 한 획기적인 방법론입니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘

# A :

### **논문의 입력 데이터와 추론 과정**
ALISA 논문은 **대규모 언어 모델(LLM)**의 효율적인 추론을 목표로 하며, 입력 데이터와 추론 과정은 다음과 같이 진행됩니다.

---

#### **1. 입력 데이터**
- **입력 데이터 형식**: 텍스트 토큰 시퀀스
  - 예: "The capital of France is Paris."
- **처리 단계**:
  1. 입력 텍스트를 **토큰화**하여 벡터화.
     - 예: `["The", "capital", "of", "France", "is"] → [101, 245, 34, 456, 89]`
     - 각 토큰은 고차원 임베딩 벡터로 변환.
  2. 임베딩된 벡터는 모델의 **Transformer 레이어**로 입력.

---

#### **2. 추론 과정**
추론은 **Autoregressive(자기회귀적)** 방식으로 수행됩니다.

##### **(1) Prefilling 단계**
- 입력 시퀀스를 한 번에 처리하여 초기 **Key (K)**와 **Value (V)** 텐서를 생성.
- 예: `["The", "capital", "of", "France", "is"]`가 입력되면, 모든 토큰에 대해 \( K, V \)가 계산되어 캐싱됨.

##### **(2) Decoding 단계**
- 새로운 토큰을 한 번에 하나씩 생성.
- 이전 토큰과 현재 토큰을 기반으로 \( Q, K, V \)를 조합하여 다음 토큰을 예측:
  - 입력: `"Paris"`의 Query (\( Q \)).
  - 캐싱된 \( K, V \): `"The", "capital", "of", "France", "is"`에 대한 정보.
  - 계산: Attention Score와 Softmax를 통해 `"Paris"` 예측.

##### **(3) Sparse Window Attention (SWA)**
- 캐싱된 \( K, V \) 중 **중요한 토큰만 선택**하여 Sparse Attention 계산:
  - **Locally Static**: 최근 토큰은 항상 포함.
  - **Globally Dynamic**: Attention Weight가 높은 토큰만 추가로 포함.
- 선택된 \( K, V \)만으로 새로운 토큰 생성.
- 예: `"capital"`과 `"France"`가 중요한 토큰으로 선택, 나머지는 제외.

---

### **모델 아키텍처의 구성**

#### **1. Transformer 아키텍처**
ALISA는 표준 Transformer 기반 LLM을 활용하며, 주요 구성 요소는 다음과 같습니다:
1. **입력 임베딩**:
   - 텍스트 토큰을 고차원 벡터로 변환.
   - 입력 크기: `[Batch Size, Sequence Length, Embedding Dim]`.

2. **Self-Attention 레이어**:
   - 주요 계산: \( Q, K, V \) 생성 및 Attention 계산.
   - Attention 계산:
     \[
     \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
     \]
     여기서 \( Q \), \( K \), \( V \)의 크기:
     - \( Q, K, V \): `[Batch Size, Sequence Length, Head Dim]`

3. **Feed-Forward Network (FFN)**:
   - 각 Attention 출력을 고차원으로 변환 후 다시 축소.
   - 주요 계산:
     - \( \text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2 \)
     - 입력 및 출력 크기: `[Batch Size, Sequence Length, Model Dim]`.

4. **Layer Normalization 및 Skip Connection**:
   - 각 Attention 및 FFN 뒤에 적용.

5. **최종 출력**:
   - Linear Layer와 Softmax를 통해 다음 토큰 예측 확률 생성.

---

### **연산 및 메모리 요구량**

#### **1. 메모리 요구량**
1. **KV 텐서 저장**:
   - \( K \)와 \( V \)는 시퀀스 길이에 따라 선형적으로 증가:
     - 크기: `[Batch Size, Sequence Length, Hidden Dim]`
   - 메모리 사용량:
     \[
     \text{메모리} = 4 \cdot (\text{Batch Size} \times \text{Sequence Length} \times \text{Hidden Dim})
     \]
     - FP16 기준, 1개의 토큰당 약 4바이트 사용.

2. **모델 가중치**:
   - 모델 크기(예: 13B 파라미터)와 GPU 메모리에 따라 고정.
   - FP16 기준, 약 52GB 사용.

3. **Intermediate Tensors**:
   - Attention 계산 중 생성되는 중간 행렬:
     - Softmax 결과, MatMul, Output 행렬.

---

#### **2. 연산 요구량**
1. **Self-Attention의 계산 복잡도**:
   - \(\mathcal{O}(\text{Sequence Length}^2 \cdot \text{Hidden Dim})\):
     - 일반적인 Dense Attention.
   - SWA 사용 시 \(\mathcal{O}(\text{Sparse Tokens} \cdot \text{Sequence Length} \cdot \text{Hidden Dim})\)로 감소.
     - Sparse Tokens 수에 따라 계산량 선형 감소.

2. **Feed-Forward Layer**:
   - \(\mathcal{O}(\text{Sequence Length} \cdot \text{Hidden Dim}^2)\).

3. **전체 연산량**:
   - Dense Attention에서 SWA로 변환 시, 최대 80% 연산량 감소 가능.

---

### **ALISA의 계산 및 메모리 최적화**
1. **SWA 도입**:
   - 불필요한 \( K, V \)를 제거하여 메모리와 연산량을 동시에 줄임.

2. **동적 스케줄링**:
   - CPU와 GPU 간 메모리 전송을 최소화.
   - GPU에서 재계산으로 일부 오프로드 오버헤드 제거.

3. **KV 텐서 압축**:
   - FP16을 INT8로 압축하여 메모리 사용량을 추가로 감소.

---

결론적으로, ALISA는 입력 데이터를 효율적으로 처리하고 Sparse Attention 및 동적 메모리 관리 기법을 통해 LLM 추론에서의 메모리 및 연산 요구량을 크게 줄였습니다. 이러한 최적화는 대규모 모델에서도 메모리 제약 환경에서 고성능을 유지할 수 있도록 설계되었습니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :

### **ALISA 논문의 한계와 이를 극복하기 위한 연구 흐름**

ALISA 논문은 Sparse Attention 및 동적 메모리 관리로 LLM 추론을 최적화했지만, 몇 가지 한계점이 존재합니다. 이를 극복하기 위한 연구 흐름과 방법론을 아래에 정리했습니다.

---

### **1. KV Sparsity와 정확도 간의 트레이드오프**

#### **한계점**:
- SWA는 80% 이상의 sparsity에서 일부 작업의 정확도가 감소합니다.
  - 특히, 복잡한 맥락이나 긴 텍스트의 경우, 중요 토큰을 정확히 선택하지 못하면 정확도가 크게 떨어질 수 있음.

#### **연구 흐름**:
1. **학습 기반 중요 토큰 선택**:
   - 중요 토큰 선택을 사전에 학습된 모델 또는 추가적인 fine-tuning을 통해 자동화.
   - 예: 중요도를 예측하는 lightweight sub-model 설계.

2. **다중 sparsity 패턴 결합**:
   - SWA의 고정된 sparsity 패턴 대신 여러 패턴을 동적으로 결합.
   - 예: Sliding Window, Strided Attention, Global Sparse 패턴을 조합하여 정확도 손실을 줄임.

3. **Gradient-Aware Sparsity**:
   - Gradient를 활용하여 Attention Score 변화에 민감한 토큰만 선택.
   - 중요 토큰 선택 과정에서 모델의 loss gradient를 참고하여 선택 정확도를 높임.

---

### **2. 동적 스케줄링의 복잡성**

#### **한계점**:
- ALISA의 동적 스케줄링은 CPU와 GPU 간의 메모리 전송 및 재계산을 효율적으로 관리하지만, 하드웨어 특성에 따라 최적화가 어렵고 설정이 복잡함.
- 특정 하드웨어(GPU-CPU 대역폭)에 의존하여 보편적이지 않음.

#### **연구 흐름**:
1. **스케줄링 자동화**:
   - Reinforcement Learning(RL)을 활용하여 최적의 메모리 스케줄링 정책을 자동으로 학습.
   - 예: RL 알고리즘이 각 단계에서 GPU와 CPU의 메모리 상태를 분석하고 최적의 스케줄링 결정.

2. **온디맨드 재계산 정책**:
   - 재계산을 사전 설정하는 대신, 실시간으로 I/O 대역폭과 계산 비용을 비교하여 결정.
   - GPU의 부하 상태를 기반으로 캐싱 또는 재계산을 동적으로 전환.

3. **다중 디바이스 확장**:
   - GPU-CPU 간 전송만 최적화하는 대신, 다중 GPU 또는 GPU-TPU 혼합 환경에 최적화된 스케줄링 연구.
   - 분산 메모리 관리 기법 도입.

---

### **3. Sparse Attention 생성의 추가 연산 오버헤드**

#### **한계점**:
- SWA의 중요 토큰 선택 및 Sparse KV 텐서 생성 과정에서 추가 연산 오버헤드 발생.
- Sparse Attention이 작은 모델에서 비효율적일 수 있음.

#### **연구 흐름**:
1. **Sparse Matrix Hardware 가속기**:
   - Sparse Matrix 연산에 특화된 하드웨어를 활용하여 SWA의 추가 연산 비용을 줄임.
   - 예: NVIDIA의 Ampere 아키텍처에서 Sparse Tensor Core 사용.

2. **Sparse 구조 최적화**:
   - Sparse 구조를 보다 규칙적으로 설계하여 GPU의 병렬 처리 효율성을 극대화.
   - 예: Block Sparse Attention으로 변환.

3. **Sparse Attention 사전 학습**:
   - 모델 학습 단계에서 Sparse Attention을 미리 학습하여, 추론 시 추가 연산을 최소화.

---

### **4. 특정 데이터와 작업에서 성능 저하**

#### **한계점**:
- ALISA는 일반적으로 성능이 우수하지만, 특정 데이터셋(예: 복잡한 질의 응답)에서는 Dense Attention 대비 성능이 낮아질 수 있음.

#### **연구 흐름**:
1. **Task-Aware Sparsity**:
   - 작업별로 최적의 Sparsity 전략을 선택.
   - 예: 문장 생성 작업에서는 Local Sparsity 강화, 질문 응답 작업에서는 Global Sparsity 강화.

2. **Hierarchical Sparse Attention**:
   - Attention을 계층적으로 나눠, 중요한 정보가 높은 레벨에서 집중되도록 설계.
   - 예: 주요 토큰은 더 많은 Attention Score를 할당하고, 나머지는 가중치 감소.

3. **Multimodal 데이터 적용**:
   - 텍스트뿐만 아니라 이미지, 오디오 등 다중 모달 데이터에 Sparse Attention 기법 확장.
   - 모달 간 상호작용 정보를 기반으로 중요 토큰을 선택.

---

### **5. KV Compression(압축) 기술의 한계**

#### **한계점**:
- KV 텐서를 INT8로 압축하면 메모리 사용량은 감소하지만, 일부 작업에서 정확도 손실 발생 가능.
- 압축률이 더 높은 기술(INT4 등) 적용 시 정확도 손실이 증가.

#### **연구 흐름**:
1. **Adaptive Precision Quantization**:
   - 토큰별로 가중치 민감도를 분석하여, 중요한 텐서는 고정밀도(FP16), 덜 중요한 텐서는 저정밀도(INT8)로 압축.
   - 예: 중요 토큰의 KV는 FP16, 덜 중요한 토큰은 INT8로 관리.

2. **Lossless Compression Techniques**:
   - Entropy 기반 압축 기법을 활용하여 정보 손실 없이 메모리 사용량 감소.
   - 예: KV 텐서의 정보를 Huffman Coding으로 압축.

3. **Gradient-Aware Quantization**:
   - 모델 학습 중 Gradient를 기준으로 양자화를 최적화하여 압축 시 정확도 손실 최소화.

---

### **6. 정적 Sparse Attention의 한계**

#### **한계점**:
- SWA는 동적으로 중요 토큰을 선택하지만, 선택된 Sparse 패턴이 모든 작업에 동일하게 적용될 수는 없음.

#### **연구 흐름**:
1. **Dynamic Sparse Patterns**:
   - 추론 중 Attention Weight의 변화에 따라 Sparse 패턴을 실시간으로 수정.
   - 예: 긴 시퀀스에서는 Local Sparsity 비율을 증가, 짧은 시퀀스에서는 Global Sparsity 비율 증가.

2. **Feedback-Driven Sparsity**:
   - 이전 단계의 추론 결과를 기반으로 Sparse Attention 패턴을 업데이트.
   - 예: 특정 작업에서 성능이 저하되면 Sparse 비율을 동적으로 조정.

3. **Reinforcement Learning 기반 Sparse Pattern 최적화**:
   - RL을 통해 Sparse Attention 패턴을 실시간으로 학습하고 업데이트.

---

### **종합**
ALISA의 한계를 극복하려면 Sparse Attention 기법의 유연성과 정확도를 개선하고, 하드웨어와 시스템 수준에서 효율성을 극대화하는 방향으로 연구가 진행되어야 합니다. 주요 연구 흐름은 **학습 기반 중요 토큰 선택**, **동적 스케줄링 자동화**, **압축 기술의 정교화**, **Sparse Attention의 실시간 최적화**로 요약됩니다. 이들은 LLM 추론 효율성을 더욱 높이는 데 기여할 수 있습니다.
