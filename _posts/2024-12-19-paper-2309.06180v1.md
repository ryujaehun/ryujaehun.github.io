---
title: "Efficient Memory Management for Large Language Model Serving with PagedAttention"
date: 2024-12-19
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2309.06180v1)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



결론 및 핵심 요약

이 논문은 LLM(대규모 언어 모델)의 효율적인 메모리 관리와 고성능 서빙을 위해 PagedAttention이라는 새로운 주의 알고리즘을 제안하며, 이를 기반으로 한 vLLM 시스템을 설계했습니다. 기존 시스템 대비 2~4배의 처리량 향상을 제공하며, 특히 메모리 낭비를 줄이고, 다양한 디코딩 알고리즘에 적합한 유연한 메모리 공유 기능을 제공합니다.

강점 및 독창성
	1.	독창적 메모리 관리:
	•	운영체제의 가상 메모리(paging) 개념을 LLM 서빙에 적용.
	•	KV 캐시를 작은 블록 단위로 분리해 메모리 파편화를 줄이고 블록 단위 메모리 공유를 가능하게 함.
	2.	효율적인 서빙 시스템:
	•	기존 시스템(예: Orca, FasterTransformer) 대비 최대 4배 높은 처리량을 제공.
	•	긴 입력 시퀀스와 복잡한 디코딩 알고리즘에서 더욱 성능 우위.
	3.	유연한 메모리 사용:
	•	Copy-on-Write 방식으로 KV 캐시의 중복을 줄이며, 공유 가능한 메모리를 효과적으로 활용.
	•	디코딩 알고리즘(병렬 샘플링, 빔 검색 등)에서도 높은 메모리 절약률(최대 55%)을 기록.
	4.	확장성:
	•	단일 GPU를 넘어 분산 환경에서도 동작 가능, Megatron-LM 스타일 병렬화를 지원.

핵심 알고리즘 설명: PagedAttention

예시 입력:
“Four score and seven years ago”

과정:
	1.	블록 분할:
	•	입력 토큰의 KV 캐시는 블록(예: 16개 토큰)으로 나뉨.
	•	블록은 물리적으로 연속되지 않아도 됨.
	2.	Query-Block 매칭:
	•	쿼리 토큰(예: “forth”)의 벡터는 각 블록의 키와 값 벡터에 곱해져 점수를 계산.
	•	각 블록별 결과를 합산해 최종 출력 생성.
	3.	블록 테이블 관리:
	•	논리 블록을 물리 블록에 매핑. 동적으로 메모리를 할당해, 내부 파편화 감소.
	4.	Copy-on-Write 메커니즘:
	•	병렬 샘플링 시, 공유 블록에 새로운 값이 쓰일 경우 해당 블록만 복사해 업데이트.

논문의 한계점
	1.	GPU 커널 오버헤드:
	•	PagedAttention이 기존의 고도로 최적화된 커널(예: FasterTransformer) 대비 최대 26% 느림.
	•	이는 시스템 전체 성능에는 큰 영향을 주지 않으나, 개선 여지가 있음.
	2.	일반화 제한:
	•	PagedAttention은 LLM 서빙에 특화되어 있어, 일반적인 DNN(딥러닝 네트워크) 작업에서는 적용이 제한적.
	3.	재계산 vs 스왑:
	•	메모리 부족 시 재계산 또는 스왑 전략을 사용하지만, 각 전략의 효율성이 데이터셋과 블록 크기에 따라 달라져 일관성이 부족.

주요 비교 (성능)

시스템	처리량 개선	메모리 효율(%)	파편화 해결
vLLM	2~4배	96.3%	내부/외부 해결
Orca (Oracle)	-	~40%	부분적 해결
FasterTransformer	~22배 낮음	~30%	해결 불가

결론

이 논문은 LLM 서빙 시스템에서 메모리 문제를 근본적으로 해결하며, 운영체제의 메모리 관리 기술을 딥러닝 서빙 시스템에 성공적으로 통합했습니다. 이를 통해 긴 시퀀스와 복잡한 디코딩 작업에서도 뛰어난 성능을 보여줍니다. 추가적인 GPU 커널 최적화와 다른 DNN 워크로드로의 확장이 향후 연구 방향이 될 수 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



핵심 알고리즘: PagedAttention

PagedAttention은 LLM(대규모 언어 모델)에서 사용하는 **주의 메커니즘(Self-Attention)**을 개선한 알고리즘으로, 기존의 연속 메모리 사용 방식 대신 페이지 블록 단위의 비연속 메모리 관리를 채택합니다. 이 알고리즘은 운영체제의 가상 메모리 기법(페이지 테이블, Copy-on-Write 등)을 차용하여 메모리 낭비를 줄이고, 유연한 메모리 관리와 공유를 가능하게 합니다.

예시를 통한 과정 설명

입력 데이터:
	•	입력 문장: "Four score and seven years ago"
	•	토큰화된 형태: [Four, score, and, seven, years, ago]

1단계: 블록으로 나누기
	•	각 토큰(Key-Value 캐시 포함)은 블록으로 그룹화됩니다.
	•	블록 크기: 4 토큰(block size = 4)으로 설정.
	•	블록 0: [Four, score, and, seven]
	•	블록 1: [years, ago]
	•	물리적 메모리 위치는 연속적일 필요 없음.

2단계: 쿼리와 블록 간 연산
	1.	입력 쿼리 토큰: "years"
	•	쿼리 벡터 계산: ￼
	•	￼: Key 벡터와 점곱을 통해 주의 점수 계산.
	2.	블록 0에 대해:
	•	블록 0의 키 벡터: ￼
	•	주의 점수 계산:
￼
	•	벡터 값과 곱해 최종 출력 계산.
	3.	블록 1에 대해:
	•	블록 1의 키 벡터: ￼
	•	동일한 방식으로 주의 점수 계산 및 출력 생성.

3단계: 메모리 관리
	1.	블록 테이블 구성:
	•	논리 블록: 입력 순서에 따른 블록 번호.
	•	물리 블록: 실제 GPU 메모리에서의 위치.
	•	예:
	•	논리 블록 0 → 물리 블록 7
	•	논리 블록 1 → 물리 블록 3
	2.	Copy-on-Write (CoW):
	•	병렬 샘플링이 발생할 경우, 블록을 공유하다가 수정이 필요하면 새로운 메모리를 할당.
	•	예:
	•	블록 0이 “Four score and seven”을 포함하고 다른 요청에서 “Four score and eight”으로 수정될 경우, 블록 0 복사 후 수정.
	3.	블록 동적 할당:
	•	새로운 토큰 생성 시 논리 블록에 물리 블록 동적으로 매핑.
	•	예:
	•	“ago” 다음 “fathers” 생성 → 새 물리 블록 할당.

알고리즘의 주요 단계 정리

단계	설명
블록 분할	KV 캐시를 고정된 크기의 블록으로 나눔.
주의 계산	각 쿼리 토큰이 블록 내 키와 값 벡터와 계산 수행.
블록 매핑	논리 블록을 물리 블록으로 동적 매핑.
Copy-on-Write	메모리 공유 중 수정 시 복사본 생성.
유연한 할당	필요 시 새로운 물리 블록 동적 할당, 메모리 낭비 최소화.

예시 요약

PagedAttention은 KV 캐시를 비연속 메모리에서 관리하며, 기존의 연속적 메모리 할당과 대비하여 효율적인 메모리 활용을 보장합니다. 각 블록 단위의 연산과 메모리 매핑으로 메모리 파편화를 방지하며, Copy-on-Write를 통해 메모리 공유의 유연성을 극대화합니다.

결과적으로, LLM 서빙 시스템에서 메모리 제한을 극복하고 처리량(Throughput)을 크게 향상시키는 데 기여합니다.



# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



논문에서 제시한 결과
	1.	성능 비교 결과 (처리량 증가):
	•	vLLM은 기존 시스템(Orca, FasterTransformer) 대비 2~4배의 처리량 향상을 기록.
	•	특히, 긴 시퀀스나 복잡한 디코딩 알고리즘(병렬 샘플링, 빔 검색)에서 더 높은 성능.
	•	예: OPT-13B 모델 기준으로, vLLM은 Orca(Oracle) 대비 2.7배 더 많은 요청을 동시 처리.
	2.	메모리 효율:
	•	96.3%의 메모리 활용률을 달성.
	•	기존 Orca(Max) 시스템의 메모리 낭비율(~60%) 대비 크게 개선.
	3.	복잡한 디코딩 작업에서의 성능:
	•	병렬 샘플링: 최대 9.8% 메모리 절약 및 Orca(Oracle) 대비 1.3배의 처리량 증가.
	•	빔 검색: 최대 55% 메모리 절약 및 Orca(Oracle) 대비 2.3배 처리량 증가.
	4.	공유 프리픽스 처리:
	•	공유된 프리픽스가 포함된 요청의 경우(1-shot 및 5-shot):
	•	Orca(Oracle) 대비 최대 3.58배의 처리량 증가.
	5.	효율적 스케줄링:
	•	긴 시퀀스 및 요청 간 메모리 파편화 문제를 해결하여 최대 22배 더 많은 요청 처리.

vLLM의 특출난 점

1. PagedAttention 기반 메모리 관리
	•	기존의 한계:
	•	Orca와 FasterTransformer는 KV 캐시를 연속된 메모리 공간에 저장하며, 내부 및 외부 파편화가 심각.
	•	과도한 메모리 예약으로 메모리 낭비가 큼.
	•	vLLM의 차별점:
	•	KV 캐시를 작은 고정 크기의 블록으로 나누어 동적으로 관리.
	•	비연속적 메모리 할당으로 파편화 방지 및 Copy-on-Write로 메모리 공유 가능.

2. 병렬 디코딩에서의 강점
	•	병렬 샘플링 및 빔 검색에서 다른 시퀀스 간 KV 캐시를 공유하며, 메모리 사용량을 대폭 감소.
	•	기존 Orca에서는 각 요청의 KV 캐시를 별도 관리해 메모리 낭비가 심했음.

3. 스케줄링 및 프리엠션
	•	블록 기반 관리 덕분에 요청이 GPU 메모리를 초과할 경우, 필요한 블록만 교체하거나 재계산(recompute) 가능.
	•	재계산과 스왑 전략을 상황에 따라 병행하여 효율성 극대화.

vLLM의 성과를 도출한 주요 기법

(1) PagedAttention
	•	원리:
	•	메모리 블록을 비연속적으로 배치하고 논리-물리 블록 매핑을 통해 유연한 메모리 접근 가능.
	•	Copy-on-Write를 통해 동일한 KV 캐시를 공유하되, 수정 시 복사를 통해 충돌 방지.
	•	결과:
	•	메모리 파편화를 제거하며, 높은 처리량과 메모리 효율성 제공.
	•	긴 시퀀스에서도 메모리 낭비 없이 대규모 배치 처리 가능.

(2) KV 캐시의 블록 단위 동적 관리
	•	기존의 문제:
	•	Orca는 모든 요청에 대해 최대 시퀀스 길이(2048 토큰)를 예약해 메모리 낭비 초래.
	•	vLLM의 해결:
	•	필요 시 동적으로 블록 할당.
	•	병렬 디코딩의 경우 초기 프리픽스 블록을 공유하여 메모리 절약.

(3) 분산 환경 지원
	•	기존 한계:
	•	Megatron-LM 스타일 병렬화 환경에서 KV 캐시의 공유 어려움.
	•	vLLM의 개선:
	•	중앙화된 스케줄러를 사용하여 각 GPU 간 블록 테이블을 동기화.
	•	KV 캐시를 동적으로 관리하면서도 성능 저하 없이 확장 가능.

논문에서 제시한 이유와 내 생각

논문에서 제시하는 이유:
	1.	운영체제 기술의 적용:
	•	운영체제의 가상 메모리(paging) 개념을 LLM 서빙의 메모리 관리에 적용해 메모리 효율성을 대폭 개선.
	•	Copy-on-Write와 같은 기법으로 메모리 공유 및 충돌 해결.
	2.	GPU 메모리 제한 극복:
	•	GPU 메모리 성능이 계산 성능에 비해 덜 발전하고 있어, 메모리 사용의 효율화가 필수적.
	•	메모리를 최대한 활용할 수 있는 동적 관리 기법이 중요.
	3.	멀티 샘플링 및 디코딩 패턴 지원:
	•	병렬 샘플링, 빔 검색 등 복잡한 디코딩 알고리즘에서 메모리 공유를 통해 효율적 서빙 가능.

내 생각:
	1.	PagedAttention의 설계 우수성:
	•	기존 메모리 파편화 문제를 시스템 설계 단계에서 근본적으로 해결.
	•	메모리 공유와 동적 확장을 동시에 지원해 현대 LLM 서빙의 주요 병목을 해결.
	2.	Copy-on-Write와 OS 개념의 적절한 응용:
	•	메모리의 물리적 비연속성을 논리적으로 매핑해 GPU의 한계를 극복한 점이 독창적.
	•	다만, GPU 커널 최적화 오버헤드(26% 증가)와 메모리 재계산의 부하 문제는 보완 필요.
	3.	높은 실용성:
	•	vLLM은 단순히 연구 수준의 제안이 아니라, 실제 대규모 모델 서빙에서 바로 사용 가능한 시스템으로 설계됨.
	•	GitHub 오픈 소스(https://github.com/vllm-project/vllm)로 활용 가능성을 높임.

결론적으로, vLLM은 LLM 서빙 시스템의 메모리 관리 문제를 근본적으로 해결하며, 실제 클라우드 및 대규모 서비스에 직접 활용 가능성이 높은 연구라고 판단됩니다.


# Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니? 



vLLM 구현을 위한 추가 요구 사항

PagedAttention 및 vLLM 시스템을 기존 LLM 서빙 환경에 통합하기 위해 추가적으로 필요한 구현 사항은 다음과 같습니다.

1. PagedAttention 알고리즘 구현

추가 구현 내용:
	•	비연속 블록 관리:
	•	KV 캐시를 고정 크기의 블록으로 나누고, 논리 블록과 물리 블록 간의 매핑 테이블 관리 필요.
	•	Copy-on-Write(COW) 구현으로, 공유된 블록 수정 시 복사본 생성.
	•	GPU 커널 최적화:
	•	주의 계산 시 블록별 메모리 접근 및 연산 수행을 위한 커널 설계.
	•	블록 간 읽기/쓰기 및 병렬 연산 지원.

예상 공수:
	•	개발 시간:
	•	GPU 기반 커널 개발 및 최적화: 약 6~8주.
	•	블록 매핑 및 COW 메커니즘 구현: 약 3~4주.
	•	테스트 및 검증:
	•	다양한 디코딩 알고리즘(병렬 샘플링, 빔 검색 등) 적용 및 성능 튜닝: 약 4~6주.

컴퓨팅 자원:
	•	GPU 자원:
	•	GPU 커널 디버깅 및 테스트를 위해 고성능 GPU(A100 이상)가 필요.
	•	각 커널 최적화에 약 300~500 GPU 시간이 소모될 것으로 예상.

2. KV 캐시 메모리 관리 시스템 구축

추가 구현 내용:
	•	블록 테이블 관리:
	•	논리 블록에서 물리 블록으로의 매핑 및 동적 할당 알고리즘.
	•	요청의 입출력 길이에 따라 블록 생성, 공유, 삭제 등을 처리하는 블록 관리자.
	•	메모리 스케줄링 및 프리엠션:
	•	메모리 초과 상황에서 재계산 또는 스왑 전략 구현.
	•	CPU-메모리 간 블록 스왑 처리(PCIe 대역폭 고려).

예상 공수:
	•	개발 시간:
	•	블록 테이블 및 동적 관리 로직 구현: 약 3~4주.
	•	스케줄링 및 스왑 메커니즘 구현: 약 2~3주.
	•	테스트 및 검증:
	•	다양한 시나리오에서 메모리 할당 최적화 검증: 약 3~4주.

컴퓨팅 자원:
	•	CPU와 GPU 간 대역폭 테스트:
	•	GPU-CPU 스왑 속도 검증 및 최적화에 약 100~200 GPU 시간.
	•	메모리 효율 검증:
	•	각기 다른 디코딩 요청을 처리하는 대규모 시뮬레이션에 약 500~1000 GPU 시간.

3. 분산 서빙 환경 통합

추가 구현 내용:
	•	중앙화된 스케줄러:
	•	분산된 GPU 노드에서 KV 캐시 매핑 테이블을 관리하고 동기화.
	•	Megatron-LM 병렬화 지원:
	•	각 GPU 워커에서 KV 캐시를 병렬로 처리할 수 있도록 SPMD(단일 프로그램 다중 데이터) 통합.
	•	요청 간 동적 배치:
	•	각 요청의 입력 및 출력 길이를 기준으로 효율적으로 배치 및 처리.

예상 공수:
	•	개발 시간:
	•	스케줄러 통합 및 Megatron-LM 병렬화 적용: 약 4~6주.
	•	테스트 및 검증:
	•	대규모 분산 워크로드에서 동작 검증 및 최적화: 약 6~8주.

컴퓨팅 자원:
	•	분산 GPU 클러스터:
	•	최소 4~8개의 A100 GPU로 클러스터 테스트 필요.
	•	분산 통신 검증을 위해 1000~2000 GPU 시간 필요.

4. 추가 연산 오버헤드 분석

(1) PagedAttention 오버헤드:
	•	기존 FasterTransformer 커널 대비 20~26% 높은 연산 비용 발생.
	•	주요 원인:
	•	블록 테이블 접근 및 블록별 비연속 메모리 연산.
	•	GPU 계산량 증가:
	•	기존 커널이 한 요청 당 약 ￼의 연산이라면, PagedAttention은 블록 크기 ￼로 나눠 ￼ 정도의 추가 오버헤드 발생.

(2) Copy-on-Write 오버헤드:
	•	공유된 블록 수정 시, 블록 단위 복사가 필요하므로 약 ￼ 추가 연산.

(3) 스왑/재계산 오버헤드:
	•	GPU와 CPU 간 스왑:
	•	￼ PCIe 대역폭 기준으로, 16KB 블록 단위 스왑에 약 250~500ms 소요.
	•	재계산:
	•	긴 시퀀스(2048 토큰 기준) 재계산에 약 30~50ms 추가 연산 발생.

종합 계산 및 결론

구현 요소	개발 시간	필요 자원	추가 오버헤드
PagedAttention GPU 커널	6~8주	300~500 GPU 시간	기존 커널 대비 20~26% 연산 증가
KV 캐시 메모리 관리 시스템	3~4주	500~1000 GPU 시간	블록 매핑 및 스케줄링에 약간의 메모리 비용
분산 환경 및 병렬화 통합	6~8주	1000~2000 GPU 시간	SPMD 병렬화에 추가 통신 비용 발생

vLLM 구현은 기존 LLM 서빙 시스템에 비해 GPU 연산 오버헤드와 개발 복잡도가 증가하지만, 높은 처리량과 메모리 효율성 향상으로 이를 상쇄합니다. 긴 시퀀스와 복잡한 디코딩 알고리즘을 자주 사용하는 환경에서는 충분히 투자할 가치가 있다고 판단됩니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



입력 데이터 및 추론 과정의 예시

1. 입력 데이터
	1.	문장:
	•	“Translate English to French: ‘I love programming’”
	2.	토큰화 과정:
	•	입력 문장은 Tokenizer를 통해 다음과 같이 토큰화:
￼
	•	각 토큰은 모델의 임베딩 레이어를 통해 고정된 차원(예: 512차원)의 벡터로 변환.
	•	예: ￼.

2. 추론 과정

추론은 입력 데이터를 기반으로 출력 텍스트를 생성하는 단계입니다.
	1.	Prompt Phase (입력 처리):
	•	모델은 전체 입력 문장을 한 번에 처리하고, 각 토큰에 대한 Query, Key, Value 벡터를 생성.
	•	Self-Attention 계산:
	•	각 입력 토큰의 Query ￼, Key ￼, Value ￼ 벡터를 생성:
￼
	•	Attention Score 계산:
￼
	•	최종 출력 계산:
￼
	•	KV 캐시 생성:
	•	각 토큰의 Key, Value 벡터를 KV 캐시에 저장.
	2.	Decoding Phase (토큰 생성):
	•	첫 번째 출력 토큰(“J’aime”)을 생성한 후, 이를 다음 입력으로 사용.
	•	Autoregressive 방식으로 한 번에 한 토큰씩 처리:
	•	이전 토큰의 Key-Value 캐시를 활용하여 새로운 Query 벡터와 Attention 계산.
	•	디코딩 종료 조건:
	•	특정 길이 초과 또는 (End of Sequence) 토큰 생성 시 종료.

모델 아키텍처 구성

1. Transformer 구조

LLM은 Transformer 아키텍처를 기반으로 하며, 주요 구성 요소는 다음과 같습니다.
	1.	입력 임베딩:
	•	토큰을 고정된 차원으로 변환.
	•	입력 길이 ￼이 길면, 모델의 입력 메모리 사용량은 ￼.
	2.	Self-Attention 레이어:
	•	입력 길이 ￼에 대해, Attention 계산은 ￼의 계산량을 요구.
	•	KV 캐시는 각 토큰에 대해 ￼ 크기의 메모리 사용:
￼
	3.	피드포워드 네트워크 (FFN):
	•	Attention 출력에 ￼ 연산.
	•	주요 연산: ￼.
	4.	출력 레이어:
	•	디코딩 단계에서 각 출력 토큰에 대해 ￼의 계산량 발생.
	•	￼: 전체 어휘 크기.

2. 모델 규모와 연산

OPT-13B 모델
	•	모델 크기: 13B (130억) 파라미터.
	•	임베딩 레이어: 약 4GB.
	•	Self-Attention: 약 9GB.
	•	메모리 요구량:
	•	모델 가중치: 26GB (FP16 기준).
	•	KV 캐시: ￼:
	•	예: ￼ → 약 12GB.
	•	전체 메모리: 약 38GB.

OPT-175B 모델
	•	모델 크기: 175B (1750억) 파라미터.
	•	임베딩 레이어: 약 35GB.
	•	Self-Attention: 약 70GB.
	•	메모리 요구량:
	•	모델 가중치: 346GB.
	•	KV 캐시: ￼:
	•	예: ￼ → 약 264GB.
	•	전체 메모리: 약 610GB.

계산량
	•	Self-Attention의 연산량:
￼
	•	OPT-13B: 약 ￼ FLOPs (2048 토큰 입력 기준).
	•	OPT-175B: 약 ￼ FLOPs.

결론 및 요약
	1.	추론 과정 요약:
	•	입력 문장을 토큰화해 임베딩.
	•	KV 캐시를 기반으로 Self-Attention과 디코딩을 수행.
	•	결과적으로 새로운 토큰을 생성하여 출력.
	2.	모델 아키텍처 요구:
	•	Transformer 기반 아키텍처로, Attention과 FFN의 계산이 주요 병목.
	•	메모리 요구량은 모델 크기, 입력 길이, KV 캐시 크기에 따라 크게 증가.
	3.	컴퓨팅 및 메모리 요구량:
	•	OPT-13B: 약 38GB 메모리 및 ￼ FLOPs.
	•	OPT-175B: 약 610GB 메모리 및 ￼ FLOPs.
	•	긴 입력 시퀀스와 복잡한 디코딩 작업에서는 메모리 관리와 계산 효율화가 필수적.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘


논문의 한계와 이를 극복하기 위한 연구 흐름

논문에서 제시된 vLLM 시스템은 LLM 서빙에서 메모리 효율성과 처리량을 크게 향상시키는 중요한 기여를 했지만, 여전히 몇 가지 한계가 존재합니다. 이러한 한계를 극복하기 위한 연구 흐름과 접근 방식을 다음과 같이 정리할 수 있습니다.

1. GPU 커널 오버헤드 감소

한계점
	•	PagedAttention 커널의 오버헤드:
	•	기존 고도로 최적화된 FasterTransformer 커널 대비 약 20~26% 더 높은 연산 비용이 발생.
	•	이는 블록 단위 메모리 접근과 논리-물리 매핑의 비연속적 연산에서 기인.

해결 방안 연구 흐름
	1.	커널 최적화:
	•	텐서 연산 합성(Fused Kernel):
	•	메모리 접근과 계산을 더욱 통합하여 커널 호출 수 감소.
	•	Warp-Level 병렬화 개선:
	•	블록 단위 메모리 접근이 효율적으로 이루어지도록 Warp 간 메모리 동기화를 최적화.
	2.	패턴 기반 최적화:
	•	반복적으로 접근하는 블록 패턴(예: 긴 시퀀스의 프리픽스)을 분석하여 캐싱 전략 도입.
	•	프리패칭(Pre-fetching) 기법으로 자주 사용하는 블록을 GPU 메모리에 미리 로드.
	3.	하드웨어 가속기 활용:
	•	토큰-기반 가속기(Tensor Cores):
	•	Self-Attention 연산에 특화된 Tensor Cores 활용.
	•	새로운 하드웨어 구조(예: TPUs)에서 PagedAttention을 재설계하여 연산 효율성을 향상.

2. 메모리 스왑 및 재계산 비용 최적화

한계점
	•	스왑과 재계산의 성능 병목:
	•	블록 크기가 작을수록 GPU-CPU 간 스왑이 느려지고, 재계산은 추가 연산 비용을 초래.
	•	PCIe 대역폭의 한계로 인해 GPU-CPU 메모리 교환 속도가 병목.

해결 방안 연구 흐름
	1.	압축 및 인코딩:
	•	스왑되는 KV 캐시를 블록 단위로 압축하여 전송 속도와 메모리 사용량 감소.
	•	예: 비손실 압축 알고리즘이나 **양자화(Quantization)**를 활용.
	2.	고속 메모리 인터페이스:
	•	HBM(High Bandwidth Memory):
	•	GPU 내 고속 메모리를 활용해 메모리 교환 속도를 증가.
	•	CPU와 GPU 간의 대역폭 병목을 해결하기 위한 NVLink와 같은 고속 인터페이스 사용.
	3.	KV 캐시 삭제 및 복구 정책:
	•	스왑 대신 Selective KV 캐시 삭제:
	•	특정 요청의 KV 캐시를 삭제하고, 필요 시 재계산.
	•	재계산 최적화를 위해 단일 병렬 연산으로 KV 캐시를 복구.

3. 긴 입력 시퀀스 처리 효율화

한계점
	•	긴 입력 시퀀스(예: 2048 토큰 이상)에서 KV 캐시 크기가 기하급수적으로 증가하며, 메모리 사용량이 급격히 증가.
	•	긴 시퀀스가 짧은 시퀀스보다 디코딩 과정에서 더 많은 메모리 낭비를 유발.

해결 방안 연구 흐름
	1.	효율적 KV 캐시 관리:
	•	히스토리 트리밍(History Trimming):
	•	긴 입력 시퀀스에서 가장 오래된 토큰의 KV 캐시를 삭제하거나 요약.
	•	압축 KV 캐시:
	•	중요도가 낮은 KV 캐시는 낮은 정밀도로 저장.
	2.	로컬 Attention 도입:
	•	전 시퀀스가 아닌 특정 윈도우(예: 마지막 512 토큰)만 Attention 연산에 포함.
	•	예: Longformer와 같은 모델에서 활용되는 슬라이딩 윈도우 Attention 기법.
	3.	모델 구조 변경:
	•	기존 Transformer 구조를 효율적 Attention 기법(예: Reformer, Performer)으로 대체.
	•	O(n^2) 연산을 O(n \log n) 또는 O(n)으로 줄이는 모델 설계.

4. 다양한 디코딩 알고리즘 지원 한계

한계점
	•	병렬 샘플링이나 빔 검색과 같은 복잡한 디코딩 알고리즘에서 메모리와 연산 병목이 발생.
	•	여러 요청이 동시에 진행되면, 메모리 충돌이 빈번히 발생.

해결 방안 연구 흐름
	1.	다중 디코딩 최적화:
	•	샘플 간 공유 극대화:
	•	병렬 샘플링에서 가능한 KV 캐시를 모두 공유하고, 새로운 값만 Copy-on-Write로 저장.
	•	빔 간 블록 동적 관리:
	•	빔 검색에서 공유 가능한 블록을 실시간으로 재분배.
	2.	모델 디코딩 속도 향상:
	•	디코딩 속도를 높이기 위해 Softmax 최적화와 어휘 필터링 적용.
	•	GPU 연산 병렬화를 활용해 한 번에 여러 샘플을 처리.
	3.	다양한 디코딩 환경 통합:
	•	기존에 지원되지 않는 온디맨드 디코딩 및 스트리밍 디코딩 추가.

5. 분산 서빙의 확장성

한계점
	•	분산 GPU 클러스터에서 KV 캐시의 동기화 및 분배로 인해 네트워크 병목이 발생.
	•	중앙화된 스케줄러의 부하가 증가하며, 클러스터 규모 확장에 제한이 있음.

해결 방안 연구 흐름
	1.	분산 스케줄링 최적화:
	•	중앙화된 스케줄러 대신 분산 스케줄러 사용:
	•	각 GPU 노드가 독립적으로 블록을 관리하며, 필요 시 동기화.
	•	스케줄링 부하를 줄이기 위해 지능형 스케줄링 알고리즘(예: 강화 학습 기반) 적용.
	2.	모델 병렬화 개선:
	•	ZeRO-Inference 방식으로 메모리 사용을 각 GPU에 분산.
	•	모델의 일부를 CPU에서 처리하고, GPU는 핵심 계산만 담당.
	3.	효율적 통신 프로토콜:
	•	분산 환경에서 KV 캐시 동기화를 최적화하기 위해 gRPC, NCCL을 활용.
	•	메시지 압축 및 전달 최적화를 통해 네트워크 병목 해소.

종합 요약

한계점	해결 방안
GPU 커널 오버헤드	Fused Kernel, Warp-Level 병렬화, Tensor Core 활용
스왑/재계산 병목	데이터 압축, 고속 메모리 인터페이스, 선택적 KV 캐시 삭제
긴 시퀀스 처리	히스토리 트리밍, 로컬 Attention, 효율적 Attention 기법
복잡한 디코딩 알고리즘 지원 부족	샘플/빔 공유 극대화, Softmax 최적화, 스트리밍 디코딩 도입
분산 환경 확장성 제한	분산 스케줄러, ZeRO-Inference, 고속 통신 프로토콜

이러한 흐름은 vLLM의 한계를 극복하면서도, LLM 서빙 시스템의 확장성과 효율성을 더욱 높이는 방향으로 연구를 이끌어 갈 수 있습니다.

# Q : PagedAttention에서 메모리 블록 크기(Block Size)를 설정하는 것이 성능과 메모리 효율성에 어떤 영향을 미치며, 이를 동적으로 조정하기 위한 최적의 알고리즘이나 방법론은 무엇인가?
 
PagedAttention에서 메모리 블록 크기(Block Size)의 영향과 동적 조정 방안

1. 블록 크기(Block Size)의 성능과 메모리 효율성에 미치는 영향
	1.	블록 크기가 작을 때:
	•	장점:
	•	메모리 낭비가 감소:
	•	블록 단위로 메모리를 할당하므로 내부 메모리 낭비(fragmentation)가 최소화됨.
	•	필요하지 않은 공간을 예약하지 않아 작은 요청 처리에 적합.
	•	Copy-on-Write(COW) 시 복사 비용이 작음.
	•	단점:
	•	블록 수 증가로 인해 메모리 관리 오버헤드가 커짐.
	•	블록 테이블 크기가 증가하고, 더 많은 매핑 연산이 필요.
	•	GPU 메모리 접근에서 비연속적 메모리 접근이 빈번해져 성능 저하 가능.
	2.	블록 크기가 클 때:
	•	장점:
	•	GPU 연산의 병렬성을 최대화:
	•	한 번에 더 많은 데이터를 처리하므로 GPU의 계산 효율이 향상.
	•	블록 테이블의 크기가 작아지고 관리 오버헤드 감소.
	•	단점:
	•	내부 메모리 낭비 증가:
	•	요청이 블록 크기보다 작을 경우, 남는 공간이 비효율적으로 사용됨.
	•	COW 복사 비용이 증가:
	•	큰 블록을 복사해야 하므로 추가 메모리 및 시간 오버헤드 발생.

결론:
	•	작은 블록: 메모리 효율성 우위.
	•	큰 블록: 연산 성능 우위.

2. 블록 크기를 동적으로 조정하기 위한 최적 알고리즘

PagedAttention에서 블록 크기를 고정적으로 설정하는 대신, 요청의 특성(예: 입력 길이, 출력 길이, 디코딩 방식)에 따라 동적으로 조정할 수 있습니다.
	1.	요청 기반 동적 블록 크기 조정:
	•	요청의 길이에 따라 블록 크기를 동적으로 설정:
	•	긴 요청 → 큰 블록 크기.
	•	짧은 요청 → 작은 블록 크기.
	•	구현:
	•	요청이 들어올 때 예상 출력 길이를 기반으로 적절한 블록 크기를 할당.
	2.	히스토리 기반 학습:
	•	이전 요청 처리 데이터를 사용해 블록 크기를 학습.
	•	예: 각 요청에 대해 실제로 사용된 메모리와 처리 시간을 기록.
	•	머신러닝 기반 최적화:
	•	Reinforcement Learning(RL) 알고리즘을 사용해 최적의 블록 크기 정책을 학습.
	3.	실시간 조정 알고리즘:
	•	온디맨드 블록 병합 및 분할:
	•	작은 블록을 병합해 큰 블록 생성(긴 요청 처리 시).
	•	큰 블록을 분할해 작은 블록 생성(짧은 요청 처리 시).

3. 동적 조정의 기술적 도전과 해결 방안
	1.	도전 과제:
	•	블록 매핑 테이블 복잡성 증가:
	•	블록 크기가 가변적일 경우, 테이블 관리가 더 복잡해짐.
	•	실시간 처리 지연:
	•	블록 크기 변경 시 추가 연산 발생.
	•	GPU 커널 지원 제한:
	•	가변 블록 크기를 지원하는 커널은 더 높은 오버헤드를 가질 수 있음.
	2.	해결 방안:
	•	테이블 압축:
	•	동일한 크기의 연속 블록을 하나의 엔트리로 묶어 테이블 크기 감소.
	•	지연 스케줄링:
	•	블록 크기 변경은 요청 간 전환 시점에만 수행.
	•	하드웨어 특화 최적화:
	•	Tensor Core 또는 특정 GPU 하드웨어를 활용해 가변 블록 관리 최적화.

최적 알고리즘 제안: 요청 기반 동적 블록 크기 조정
	1.	요청이 들어오면 예상 입력/출력 길이를 분석.
	2.	길이에 따라 블록 크기를 동적으로 설정:
	•	￼: 블록 크기 64.
	•	￼: 블록 크기 16.
	3.	이전 요청 데이터를 기반으로 블록 크기 조정을 지속 학습.

이 접근법은 성능과 메모리 효율성을 동시에 극대화하며, PagedAttention의 적용 가능성을 확장할 수 있습니다.

# Q : vLLM이 Copy-on-Write(COW)와 같은 메모리 관리 기법을 통해 병렬 샘플링 및 빔 검색에서 메모리 절약을 이루었지만, 이러한 기법이 시스템의 스케일 업 및 복잡한 분산 환경에서 어떤 한계를 가질 수 있는가?

 

vLLM의 Copy-on-Write(COW) 기반 메모리 관리 기법은 병렬 샘플링과 빔 검색에서 효율적으로 메모리를 절약할 수 있도록 설계되었습니다. 그러나 시스템의 스케일 업 및 복잡한 분산 환경에서 몇 가지 한계를 드러낼 수 있습니다. 아래에서는 이러한 한계와 그 원인을 정리합니다.

1. 분산 환경에서 Copy-on-Write의 한계

1.1 데이터 동기화 비용 증가
	•	원인:
	•	분산 환경에서는 Copy-on-Write로 인해 블록이 복사되거나 업데이트될 때 각 GPU 간 동기화가 필요.
	•	각 GPU가 KV 캐시의 물리 블록을 복사할 때, 네트워크를 통해 변경된 데이터를 동기화해야 함.
	•	문제점:
	•	네트워크 대역폭과 지연 시간이 병목으로 작용할 가능성.
	•	특히, 수백 개의 GPU가 연동된 대규모 클러스터에서는 데이터 전송 비용이 성능 저하를 유발.

1.2 스케줄링 복잡성 증가
	•	원인:
	•	Copy-on-Write의 물리 블록은 공유되기 때문에 스케줄러가 요청 간 블록 매핑 상태를 지속적으로 관리해야 함.
	•	대규모 시스템에서는 요청의 수가 증가하면서 매핑 테이블 관리 복잡도가 증가.
	•	문제점:
	•	스케줄러의 중앙 집중화가 과부하를 유발.
	•	복잡한 디코딩 패턴(병렬 샘플링, 빔 검색)에서 동기화 지연 증가.

2. 스케일 업에서 Copy-on-Write의 한계

2.1 메모리 복사 비용 증가
	•	원인:
	•	Copy-on-Write는 블록을 수정해야 할 때 물리 블록 전체를 복사.
	•	스케일 업 상황에서는 더 많은 요청이 동시 실행되므로 복사 횟수와 데이터 크기가 비례 증가.
	•	문제점:
	•	복사 비용이 기하급수적으로 증가하여 메모리 효율성 저하.
	•	특히, 큰 블록 크기를 사용할 경우 복사 비용이 메모리 대역폭을 초과.

2.2 복잡한 디코딩 알고리즘에서 충돌 가능성
	•	원인:
	•	병렬 샘플링 및 빔 검색에서 각 요청이 블록을 공유하지만, 디코딩 패턴에 따라 충돌 빈도 증가.
	•	예: 빔 검색의 경우 각 빔이 서로 다른 방향으로 디코딩되며, 공통 블록을 자주 업데이트.
	•	문제점:
	•	Copy-on-Write의 빈번한 복사가 병렬 처리 효율성을 감소.
	•	블록 공유와 독립성 간의 균형이 깨질 가능성.

3. 확장성 관련 Copy-on-Write의 구조적 한계

3.1 블록 크기와 메모리 파편화
	•	원인:
	•	Copy-on-Write는 블록 단위로 동작하기 때문에 블록 크기가 클수록 메모리 낭비가 커질 가능성.
	•	반대로, 블록 크기가 작아지면 Copy-on-Write의 복사 비용이 증가.
	•	문제점:
	•	시스템 확장에서 최적의 블록 크기를 동적으로 결정하기 어려움.
	•	긴 시퀀스와 짧은 시퀀스가 혼재된 환경에서 메모리 사용 효율이 떨어질 수 있음.

3.2 중앙 집중화된 메모리 매핑
	•	원인:
	•	vLLM은 중앙화된 블록 매핑 테이블을 사용하여 논리 블록과 물리 블록을 관리.
	•	스케일 업 시 테이블 크기와 조회 빈도가 증가하며, 관리 오버헤드가 커짐.
	•	문제점:
	•	중앙화된 관리로 인해 스케일 업이 진행되면서 시스템 병목이 발생할 가능성.
	•	분산된 요청이 블록 매핑을 동기화하지 못하면 데이터 불일치가 발생.

4. 개선 가능성 및 제안

4.1 네트워크 대역폭 최적화
	•	문제: Copy-on-Write는 분산 환경에서 네트워크 병목을 유발.
	•	해결 방안:
	•	데이터 압축 및 인코딩:
	•	물리 블록을 복사하거나 전송하기 전에 압축.
	•	이벤트 기반 동기화:
	•	변경된 블록만 동기화하도록 이벤트 기반 업데이트 설계.

4.2 분산 매핑 테이블 도입
	•	문제: 중앙 집중식 테이블 관리의 확장성 한계.
	•	해결 방안:
	•	로컬 블록 관리:
	•	각 GPU가 독립적으로 블록을 관리하고, 필요 시 블록 상태를 교환.
	•	분산 해시 테이블:
	•	블록 매핑 테이블을 GPU 클러스터에 분산하여 관리.

4.3 병렬 디코딩 알고리즘 최적화
	•	문제: Copy-on-Write의 빈번한 복사로 인해 병렬 처리 효율 저하.
	•	해결 방안:
	•	지연 복사(Deferred Copy):
	•	복사 작업을 요청이 완료될 때까지 지연.
	•	스냅샷 기반 디코딩:
	•	복사를 최소화하기 위해 스냅샷을 생성하고, 필요 시 복구.

4.4 하이브리드 메모리 관리
	•	문제: Copy-on-Write는 모든 경우에 최적이 아님.
	•	해결 방안:
	•	Copy-on-Write와 재계산을 혼합:
	•	수정 빈도가 낮은 블록은 복사(COW), 빈도가 높은 블록은 재계산.

요약

한계점	원인 및 문제점	해결 방안
네트워크 동기화 비용 증가	분산 환경에서 복사된 블록의 동기화	데이터 압축, 이벤트 기반 동기화
중앙 집중식 테이블 관리	스케일 업 시 테이블 크기 및 오버헤드 증가	분산 매핑 테이블, 로컬 블록 관리
빈번한 Copy-on-Write로 인한 성능 저하	병렬 샘플링 및 빔 검색에서 블록 충돌 발생	지연 복사, 스냅샷 기반 디코딩
블록 크기 설정 문제	블록 크기에 따른 메모리 낭비 또는 복사 비용 증가	하이브리드 메모리 관리, 동적 블록 크기 조정

Copy-on-Write는 메모리 효율성을 높이는 데 효과적이지만, 대규모 분산 환경 및 스케일 업에서 추가적인 최적화가 필요합니다. 이를 해결하기 위한 네트워크, 테이블 분산, 하이브리드 관리 전략이 연구되어야 합니다.

# Q : 긴 입력 시퀀스와 복잡한 디코딩 알고리즘을 처리할 때, vLLM에서 제안된 블록 단위 메모리 관리가 기존 시스템(Orca, FasterTransformer) 대비 실제로 얼마나 효율적인지, 이를 더 최적화할 수 있는 추가적인 개선점은 무엇인가?

 


vLLM의 블록 단위 메모리 관리 효율성 평가 및 개선 가능성

vLLM의 **블록 단위 메모리 관리(PagedAttention 기반)**는 긴 입력 시퀀스와 복잡한 디코딩 알고리즘에서 기존 시스템(Orca, FasterTransformer)에 비해 크게 향상된 메모리 효율성을 제공합니다. 그러나 여전히 개선 여지가 있습니다. 아래에서는 비교된 효율성을 정리하고, 이를 최적화할 수 있는 추가적인 방안을 제안합니다.

1. vLLM의 효율성 분석

1.1 긴 입력 시퀀스 처리에서의 효율성
	•	기존 시스템의 한계:
	•	Orca 및 FasterTransformer는 KV 캐시를 연속적인 메모리 공간에 저장.
	•	긴 시퀀스(2048 토큰 이상)에서는 최대 시퀀스 길이를 기준으로 메모리를 예약.
	•	이로 인해 **내부 파편화(Internal Fragmentation)**와 **외부 파편화(External Fragmentation)**가 발생.
	•	예: Orca(Max)는 약 40~60%의 메모리만 실제로 사용.
	•	vLLM의 장점:
	•	KV 캐시를 고정 크기 블록으로 분리하고, 필요한 만큼만 동적 할당.
	•	Copy-on-Write(COW)와 블록 재사용을 통해 메모리 낭비를 최소화.
	•	메모리 활용도: 96.3% (Orca 대비 2.5배 효율적).
	•	긴 시퀀스에서도 동적 블록 확장으로 메모리 부족 문제 방지.

1.2 복잡한 디코딩 알고리즘에서의 효율성
	•	병렬 샘플링:
	•	동일한 입력 프리픽스를 공유하여 메모리 사용량을 절약.
	•	메모리 절약율: 6~9.8% (Alpaca 데이터셋 기준).
	•	빔 검색:
	•	각 빔의 KV 캐시를 공유하며, 새로운 토큰 생성 시 블록 단위로 Copy-on-Write.
	•	메모리 절약율: 37~55% (Alpaca 데이터셋 기준).
	•	비교:
	•	Orca는 요청별 KV 캐시를 독립적으로 관리하므로 메모리 공유가 불가능.
	•	FasterTransformer는 요청 간의 병렬 처리를 최적화하지만, 메모리 관리는 효율적이지 않음.

1.3 성능 결과
	•	처리량:
	•	긴 시퀀스와 복잡한 디코딩 알고리즘에서 vLLM은 Orca(Oracle) 대비 2~4배 높은 처리량 기록.
	•	FasterTransformer 대비 최대 22배 처리량 향상.
	•	메모리 효율:
	•	vLLM의 메모리 활용도는 96.3%로, Orca(Max)의 40%를 크게 초과.

2. vLLM의 블록 단위 메모리 관리의 한계
	1.	블록 크기의 비효율성:
	•	고정 크기 블록은 짧은 시퀀스에서는 불필요한 메모리 낭비를 초래.
	•	긴 시퀀스에서는 블록 크기가 작아지면 Copy-on-Write와 매핑 테이블 접근 빈도가 증가.
	2.	Copy-on-Write의 복사 오버헤드:
	•	복잡한 디코딩 알고리즘(빔 검색)에서는 각 요청이 다른 방향으로 디코딩되어, 블록 복사가 빈번히 발생.
	3.	동적 스케줄링의 부재:
	•	블록 재사용 및 할당 우선순위를 동적으로 최적화하는 정책이 부족.

3. 추가적인 개선 방안

3.1 동적 블록 크기 조정
	•	문제:
	•	고정된 블록 크기는 다양한 시퀀스 길이와 디코딩 알고리즘에 최적화되지 않음.
	•	해결 방안:
	•	블록 크기 조정 알고리즘:
	•	요청의 입력 길이를 기반으로 블록 크기를 동적으로 설정.
	•	긴 시퀀스 → 큰 블록 크기, 짧은 시퀀스 → 작은 블록 크기.
	•	히스토리 기반 학습:
	•	이전 요청 데이터를 활용해 블록 크기 최적화.
	•	강화 학습(RL)을 사용해 블록 크기와 성능의 상관 관계 학습.

3.2 효율적 Copy-on-Write 정책
	•	문제:
	•	복사 빈도와 데이터 크기 증가가 성능 저하를 유발.
	•	해결 방안:
	•	지연 복사(Deferred Copy):
	•	즉시 블록 복사를 수행하지 않고, 일정 시점에서 한꺼번에 처리.
	•	차등 복사 전략:
	•	중요도가 낮은 토큰(KV 캐시 크기가 작은 블록)은 낮은 우선순위로 복사.
	•	압축 복사:
	•	복사 전 KV 캐시를 압축하여 데이터 전송량 감소.

3.3 히스토리 트리밍 및 메모리 요약
	•	문제:
	•	긴 입력 시퀀스에서 모든 KV 캐시를 저장하는 것은 메모리 낭비.
	•	해결 방안:
	•	히스토리 트리밍(History Trimming):
	•	가장 오래된 토큰의 KV 캐시를 삭제하거나 요약하여 메모리 절약.
	•	예: 마지막 ￼개의 토큰만 유지.
	•	요약 압축:
	•	이전 토큰의 키-값 벡터를 평균화하여 요약 벡터 생성.

3.4 분산 메모리 관리 최적화
	•	문제:
	•	대규모 GPU 클러스터에서 블록 매핑 동기화 비용 증가.
	•	해결 방안:
	•	분산 매핑 테이블:
	•	각 GPU가 블록 매핑 테이블의 일부를 관리하며 동기화 빈도 감소.
	•	캐시 일관성 프로토콜:
	•	블록 업데이트가 있을 때만 동기화 수행.

4. 종합 요약

기존 문제	vLLM의 현재 개선	추가 개선 방안
긴 시퀀스에서 메모리 낭비	블록 단위 동적 할당으로 해결	히스토리 트리밍, 압축 KV 캐시 사용
복잡한 디코딩에서 블록 복사 오버헤드	Copy-on-Write로 일부 해결	지연 복사, 압축 복사, 차등 복사 정책 적용
블록 크기의 비효율성	고정 블록 크기로 제한적 해결	동적 블록 크기 조정, RL 기반 최적화
분산 환경에서 동기화 비용	중앙 스케줄러로 일부 해결	분산 매핑 테이블, 캐시 일관성 프로토콜 도입

결론

vLLM의 블록 단위 메모리 관리는 긴 입력 시퀀스와 복잡한 디코딩 알고리즘에서 기존 Orca 및 FasterTransformer 대비 크게 개선된 성능과 메모리 효율성을 보여줍니다. 그러나 동적 블록 크기 조정, 효율적 Copy-on-Write 정책, 히스토리 트리밍과 같은 추가적인 최적화가 적용된다면, 더 높은 확장성과 처리량을 제공할 수 있습니다.