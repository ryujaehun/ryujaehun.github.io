---
title: "Forget the Data and Fine-Tuning! Just Fold the Network to Compress"
date: 2025-03-04
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>ì´ í¬ìŠ¤íŠ¸ë¥¼ ì½ëŠ”ë° ì•½ {{ time }} {% if time == '1' %}ë¶„{% else %}ë¶„{% endif %} ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2502.10216)

# Q : ì´ ë…¼ë¬¸ì„ ì•„ì£¼ ìì„¸í•˜ê²Œ ì½ê³  ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì ì„ ì„¤ëª…í•´ì£¼ê³  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œ ì…ë ¥ì„ ë“¤ì–´ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜ ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ì˜ í•œê³„ì ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜



### **ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ ë° ê°•ì **
ì´ ë…¼ë¬¸ì€ **Model Folding**ì´ë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ì„ ì œì•ˆí•œë‹¤. ê¸°ì¡´ì˜ ì••ì¶• ë°©ë²•(ê°€ì§€ì¹˜ê¸°, ì–‘ìí™”)ê³¼ ë‹¬ë¦¬, **Model Folding**ì€ ì¸µ(layer) ê°„ ìœ ì‚¬í•œ ë‰´ëŸ°ì„ ë³‘í•©í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì¸ë‹¤. íŠ¹íˆ, **ë°ì´í„° ì—†ì´(data-free), íŒŒì¸íŠœë‹ ì—†ì´(fine-tuning-free)** ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ê²ƒì´ ê°€ì¥ í° íŠ¹ì§•ì´ë‹¤. ì£¼ìš” ê°•ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. **ë°ì´í„° ì—†ì´ ëª¨ë¸ ì••ì¶•**  
   - ê¸°ì¡´ì˜ ë§ì€ ëª¨ë¸ ì••ì¶• ë°©ë²•ë“¤ì€ ì›ë³¸ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ, Model Foldingì€ **k-means í´ëŸ¬ìŠ¤í„°ë§**ì„ í™œìš©í•´ ë‰´ëŸ°ì„ ë³‘í•©í•¨ìœ¼ë¡œì¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ì••ì¶•ì´ ê°€ëŠ¥í•˜ë‹¤.
   
2. **íŒŒì¸íŠœë‹ ì—†ì´ë„ ë†’ì€ ì„±ëŠ¥ ìœ ì§€**  
   - ëŒ€ë¶€ë¶„ì˜ ê°€ì§€ì¹˜ê¸°(pruning) ë° ì–‘ìí™”(quantization) ë°©ë²•ì€ ì„±ëŠ¥ ìœ ì§€(accuracy recovery)ë¥¼ ìœ„í•´ ì¶”ê°€ì ì¸ íŒŒì¸íŠœë‹ì´ í•„ìš”í•˜ì§€ë§Œ, Model Foldingì€ ë°ì´í„° í†µê³„ë¥¼ ìœ ì§€í•˜ëŠ” **REPAIR ê¸°ë²•**ì„ ì ìš©í•˜ì—¬ ë³„ë„ì˜ ì¬í•™ìŠµ ì—†ì´ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•œë‹¤.

3. **ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œë„ íš¨ê³¼ì **  
   - ResNet18, VGG11-BN, LLaMA-7Bì™€ ê°™ì€ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì—ì„œ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ **ê³ ìŠ¤íŒŒì‹œí‹°(high sparsity, ë§ì€ ë‰´ëŸ°ì„ ì œê±°í•œ ê²½ìš°)** í™˜ê²½ì—ì„œ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.

4. **ê²½ëŸ‰ ë””ë°”ì´ìŠ¤ì— ì í•©í•œ ì••ì¶• ê¸°ë²•**  
   - Model Foldingì€ ì—°ì‚°ëŸ‰ì´ ì ì–´ ì„ë² ë””ë“œ ì‹œìŠ¤í…œ, ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ ë“± **ì €ìì› í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì¸ ëª¨ë¸ ë°°í¬**ê°€ ê°€ëŠ¥í•˜ë‹¤.

---

### **í•µì‹¬ ì•Œê³ ë¦¬ì¦˜: Model Folding ê³¼ì •**
Model Foldingì˜ ì „ì²´ì ì¸ í”„ë¡œì„¸ìŠ¤ëŠ” **3ë‹¨ê³„(Clustering, Merging, Repair)** ë¡œ ì´ë£¨ì–´ì§„ë‹¤.

1. **Neuron Clustering (ë‰´ëŸ° êµ°ì§‘í™”)**  
   - ê°™ì€ ì¸µ(layer) ë‚´ì—ì„œ ìœ ì‚¬í•œ ë‰´ëŸ°ì„ k-means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ê·¸ë£¹í™”í•œë‹¤.  
   - ë‰´ëŸ°ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ **L2 Distance**ë¥¼ ì‚¬ìš©í•œë‹¤.

2. **Merging (ë³‘í•©)**  
   - í´ëŸ¬ìŠ¤í„°ë§ëœ ë‰´ëŸ°ì„ í•˜ë‚˜ë¡œ í•©ì³ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì••ì¶•í•œë‹¤.  
   - ë‹¨ìˆœí•œ í‰ê· (mean) ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ì¤‘ì‹¬(centroid)ìœ¼ë¡œ ë³‘í•©í•œë‹¤.

3. **Repair (ë°ì´í„° í†µê³„ ë³´ì •)**  
   - ë‰´ëŸ° ë³‘í•© ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” **variance collapse(ë¶„ì‚° ë¶•ê´´)** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Fold-AR(Approximate REPAIR), Fold-DIR(Deep Inversion-based REPAIR)** ê¸°ë²•ì„ ì ìš©í•œë‹¤.  
   - ë°ì´í„° ì—†ì´ í†µê³„ë¥¼ ë³´ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, **Deep Inversionì„ í™œìš©í•˜ì—¬ ê°€ì§œ ë°ì´í„°(batch)ë¥¼ ìƒì„±**í•˜ëŠ” ë°©ì‹(Fold-DIR)ë„ ì œì•ˆë¨.

---

### **ì˜ˆì œ ì…ë ¥ì„ í†µí•œ ì „ì²´ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…**
**ì˜ˆì œ: 3ê°œ ì¸µì„ ê°€ì§„ ë‹¨ìˆœ ì‹ ê²½ë§ì—ì„œ Model Folding ì ìš©**

#### **Step 1: ê¸°ì¡´ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì˜ˆì‹œ**
ê¸°ì¡´ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•˜ì.

- **ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ (W1, 4Ã—4 í–‰ë ¬)**  
  \[
  W1 =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  0.8 & 0.1 & 0.2 & 0.4 \\
  0.2 & 0.7 & 0.8 & 0.1 \\
  0.3 & 0.6 & 0.7 & 0.2
  \end{bmatrix}
  \]

- **ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ (W2, 4Ã—4 í–‰ë ¬)**  
  \[
  W2 =
  \begin{bmatrix}
  0.4 & 0.5 & 0.6 & 0.1 \\
  0.3 & 0.5 & 0.7 & 0.2 \\
  0.8 & 0.3 & 0.1 & 0.6 \\
  0.7 & 0.2 & 0.2 & 0.7
  \end{bmatrix}
  \]

---

#### **Step 2: ë‰´ëŸ° êµ°ì§‘í™” (Clustering)**
**ìœ ì‚¬í•œ ë‰´ëŸ°(í–‰ ë²¡í„°)ë“¤ì„ k-meansë¥¼ í†µí•´ ê·¸ë£¹í™”**  
- ì²« ë²ˆì§¸ ì¸µì—ì„œ ë‘ ë²ˆì§¸ì™€ ë„¤ ë²ˆì§¸ ë‰´ëŸ°ì´ ìœ ì‚¬í•¨ (L2 Distance ê¸°ë°˜) â†’ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ìŒ
- ë‘ ë²ˆì§¸ ì¸µì—ì„œ ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ë‰´ëŸ°ì´ ìœ ì‚¬í•¨ â†’ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ìŒ

---

#### **Step 3: ë³‘í•© (Merging)**
êµ°ì§‘í™”ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í‰ê· ë‚´ì–´ í•˜ë‚˜ì˜ ë‰´ëŸ°ìœ¼ë¡œ ë³‘í•©

- ë³‘í•© í›„ ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ (W1'):
  \[
  W1' =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  0.8 & 0.1 & 0.2 & 0.4 \\
  0.25 & 0.65 & 0.75 & 0.15
  \end{bmatrix}
  \]
  â†’ 4ê°œì˜ ë‰´ëŸ°ì´ 3ê°œë¡œ ì¤„ì–´ë“¦

- ë³‘í•© í›„ ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ (W2'):
  \[
  W2' =
  \begin{bmatrix}
  0.35 & 0.5 & 0.65 & 0.15 \\
  0.8 & 0.3 & 0.1 & 0.6 \\
  0.7 & 0.2 & 0.2 & 0.7
  \end{bmatrix}
  \]
  â†’ 4ê°œì˜ ë‰´ëŸ°ì´ 3ê°œë¡œ ì¤„ì–´ë“¦

---

#### **Step 4: ë°ì´í„° í†µê³„ ë³´ì • (Repair)**
ë³‘í•©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•œ í†µê³„ ë³€í™”(variance collapse)ë¥¼ ë³´ì •  
- **Fold-AR**: í´ëŸ¬ìŠ¤í„° í‰ê· ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •  
- **Fold-DIR**: Deep Inversionì„ ì´ìš©í•˜ì—¬ BatchNorm í†µê³„ ì—…ë°ì´íŠ¸  

ìµœì¢…ì ìœ¼ë¡œ, ëª¨ë¸ í¬ê¸°ê°€ 4Ã—4 â†’ 3Ã—3ìœ¼ë¡œ ì¤„ì–´ë“  **ê²½ëŸ‰ ì‹ ê²½ë§ì´ ìƒì„±ë¨.**

---

### **ë…¼ë¬¸ì˜ í•œê³„ì **
1. **ë‰´ëŸ° ë³‘í•© ê¸°ì¤€ ìµœì í™” ë¶€ì¡±**  
   - k-means í´ëŸ¬ìŠ¤í„°ë§ì„ í™œìš©í•˜ì§€ë§Œ, í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(k)ë¥¼ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•¨.  
   - ì¼ë¶€ ì¸µì—ì„œëŠ” ë³‘í•©ìœ¼ë¡œ ì¸í•´ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ìˆìŒ.

2. **LLaMA-7Bì™€ ê°™ì€ ì´ˆê±°ëŒ€ ëª¨ë¸ ì ìš© ì‹œ í•œê³„**  
   - ì‹¤í—˜ì ìœ¼ë¡œ ì ìš©í–ˆìœ¼ë‚˜, LLaMA-7Bì˜ Attention ê¸°ë²• ë“±ì—ì„œ ë³‘í•©ì´ ìµœì  ë°©ì‹ì¸ì§€ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•¨.  

3. **Residual Connections ë° BatchNorm ì²˜ë¦¬ í•œê³„**  
   - Residual Connectionì„ ê°€ì§€ëŠ” ëª¨ë¸ì—ì„œëŠ” ë³‘í•©ì´ ì˜¤íˆë ¤ ì •ë³´ ì†ì‹¤ì„ ìœ ë°œí•  ê°€ëŠ¥ì„±ì´ ìˆìŒ.  
   - BatchNormì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì—ì„œëŠ” variance ìœ ì§€ê°€ ì–´ë µê³ , ë³´ì • ê³¼ì •ì´ ì¶”ê°€ ì—°ì‚°ì„ ìš”êµ¬í•¨.

---

### **ê²°ë¡ **
Model Foldingì€ **ë°ì´í„° ì—†ì´, íŒŒì¸íŠœë‹ ì—†ì´ ë‰´ëŸ° ë³‘í•©ì„ í†µí•´ ëª¨ë¸ì„ ê²½ëŸ‰í™”**í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì´ë‹¤. íŠ¹íˆ **ì„ë² ë””ë“œ ì‹œìŠ¤í…œ, ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤, LLMê³¼ ê°™ì€ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ê²½ëŸ‰í™”**ì— ì í•©í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ **ë‰´ëŸ° ë³‘í•© ìµœì í™”, ì´ˆê±°ëŒ€ ëª¨ë¸ ì ìš©, BatchNorm ì²˜ë¦¬ í•œê³„** ë“±ì˜ ë¬¸ì œê°€ ë‚¨ì•„ ìˆìœ¼ë©°, ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.

# Q : ë…¼ë¬¸ì˜ í•µì‹¬ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ì •ë¦¬í•´ ì¤˜



## **ë…¼ë¬¸ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜: Model Folding**
ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ **Model Folding** ì•Œê³ ë¦¬ì¦˜ì€ ì‹ ê²½ë§ì„ ë°ì´í„° ì—†ì´(data-free) ì••ì¶•í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, **Clustering, Merging, Repair** 3ë‹¨ê³„ë¥¼ í†µí•´ ëª¨ë¸ì„ ê²½ëŸ‰í™”í•œë‹¤. ê¸°ì¡´ì˜ ê°€ì§€ì¹˜ê¸°(Pruning) ë° ì–‘ìí™”(Quantization) ê¸°ë²•ê³¼ ë‹¤ë¥´ê²Œ **ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” íŠ¹ì§•**ì´ ìˆë‹¤.

---

## **1. Model Foldingì˜ ì „ì²´ ê³¼ì •**
Model Foldingì€ **ë‰´ëŸ° ê°„ ìœ ì‚¬ì„±ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³‘í•©(Merging)í•œ í›„, ëª¨ë¸ì˜ í†µê³„ë¥¼ ë³´ì •(Repair)í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ ë™ì‘í•œë‹¤.  
ì´ ê³¼ì •ì„ **ì…ë ¥ ì˜ˆì œì™€ í•¨ê»˜ ì•„ì£¼ ìì„¸íˆ ì„¤ëª…**í•˜ê² ë‹¤.

### **ğŸ“Œ Step 1: ë‰´ëŸ° êµ°ì§‘í™” (Clustering)**
- ê°™ì€ ì¸µ(layer) ë‚´ì—ì„œ ìœ ì‚¬í•œ ë‰´ëŸ°ì„ ì°¾ì•„ **k-means í´ëŸ¬ìŠ¤í„°ë§**ì„ ì ìš©í•˜ì—¬ ê·¸ë£¹í™”í•œë‹¤.
- ë‰´ëŸ° ê°„ ìœ ì‚¬ì„±ì€ **L2 Distance(ìœ í´ë¦¬ë“œ ê±°ë¦¬)** ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•œë‹¤.

### **ğŸ“Œ Step 2: ë‰´ëŸ° ë³‘í•© (Merging)**
- í´ëŸ¬ìŠ¤í„°ë§ëœ ë‰´ëŸ°ë“¤ì„ í‰ê· (mean) ì—°ì‚°ì„ ì‚¬ìš©í•´ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ **ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì••ì¶•**í•œë‹¤.

### **ğŸ“Œ Step 3: ë°ì´í„° í†µê³„ ë³´ì • (Repair)**
- ë³‘í•© ê³¼ì •ì—ì„œ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ê°€ ë°”ë€Œë©´, ë‚´ë¶€ ë°ì´í„° ë¶„í¬ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆë‹¤.  
- ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•´ **Fold-AR(Approximate Repair)ì™€ Fold-DIR(Deep Inversion Repair) ê¸°ë²•ì„ ì ìš©**í•œë‹¤.

---

## **2. ì˜ˆì œ ì…ë ¥ì„ í†µí•œ Model Folding ê³¼ì • ì„¤ëª…**
ì—¬ê¸°ì„œëŠ” **4ê°œ ë‰´ëŸ°ì„ ê°€ì§„ 2ê°œ ì¸µì„ í¬í•¨í•œ ì‹ ê²½ë§**ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , Model Foldingì„ ì ìš©í•˜ëŠ” ê³¼ì •ì„ **ìˆ˜ì‹ê³¼ í•¨ê»˜ ìì„¸íˆ ì„¤ëª…**í•˜ê² ë‹¤.

### **âœ… ì´ˆê¸° ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì„¤ì •**
ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°œì˜ ì¸µ(layer)ì´ ì¡´ì¬í•˜ëŠ” ê°„ë‹¨í•œ ì‹ ê²½ë§ì´ ìˆë‹¤ê³  ê°€ì •í•˜ì.

- **ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ \( W_1 \) (4Ã—4 í–‰ë ¬)**
  \[
  W_1 =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  0.8 & 0.1 & 0.2 & 0.4 \\
  0.2 & 0.7 & 0.8 & 0.1 \\
  0.3 & 0.6 & 0.7 & 0.2
  \end{bmatrix}
  \]

- **ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ \( W_2 \) (4Ã—4 í–‰ë ¬)**
  \[
  W_2 =
  \begin{bmatrix}
  0.4 & 0.5 & 0.6 & 0.1 \\
  0.3 & 0.5 & 0.7 & 0.2 \\
  0.8 & 0.3 & 0.1 & 0.6 \\
  0.7 & 0.2 & 0.2 & 0.7
  \end{bmatrix}
  \]

---

### **ğŸ“Œ Step 1: ë‰´ëŸ° êµ°ì§‘í™” (Clustering)**
ìš°ë¦¬ëŠ” **ìœ ì‚¬í•œ ë‰´ëŸ°(í–‰ ë²¡í„°)ì„ ì°¾ì•„ ê·¸ë£¹í™”**í•´ì•¼ í•œë‹¤.

1. **L2 Distance(ìœ í´ë¦¬ë“œ ê±°ë¦¬)ë¥¼ ê³„ì‚°**  
   - ê° ë‰´ëŸ° ê°„ ê±°ë¦¬ë¥¼ êµ¬í•˜ë©´, ë‘ ë²ˆì§¸ í–‰ê³¼ ë„¤ ë²ˆì§¸ í–‰ì´ ìœ ì‚¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

2. **k-means í´ëŸ¬ìŠ¤í„°ë§ ì ìš©**  
   - 4ê°œì˜ ë‰´ëŸ°ì„ **3ê°œì˜ ê·¸ë£¹**ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•œë‹¤.  
   - ì˜ˆë¥¼ ë“¤ì–´, \( \{1\}, \{2, 4\}, \{3\} \) ë¡œ ê·¸ë£¹í™”í•œë‹¤ê³  ê°€ì •í•˜ì.

---

### **ğŸ“Œ Step 2: ë‰´ëŸ° ë³‘í•© (Merging)**
í´ëŸ¬ìŠ¤í„°ë§ëœ ë‰´ëŸ°ì„ **í‰ê· (mean)** ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ë³‘í•©í•œë‹¤.

- ë³‘í•© í›„ ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ \( W_1' \):
  \[
  W_1' =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  0.55 & 0.35 & 0.45 & 0.3 \\
  0.2 & 0.7 & 0.8 & 0.1
  \end{bmatrix}
  \]
  â†’ 4ê°œì˜ ë‰´ëŸ°ì´ 3ê°œë¡œ ì¤„ì–´ë“¦.

- ë³‘í•© í›„ ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ \( W_2' \):
  \[
  W_2' =
  \begin{bmatrix}
  0.4 & 0.5 & 0.6 & 0.1 \\
  0.5 & 0.35 & 0.45 & 0.45 \\
  0.7 & 0.2 & 0.2 & 0.7
  \end{bmatrix}
  \]
  â†’ 4ê°œì˜ ë‰´ëŸ°ì´ 3ê°œë¡œ ì¤„ì–´ë“¦.

---

### **ğŸ“Œ Step 3: ë°ì´í„° í†µê³„ ë³´ì • (Repair)**
ë‰´ëŸ° ë³‘í•© í›„ **ë°ì´í„°ì˜ ë¶„ì‚°(variance)ì´ ê°ì†Œí•˜ëŠ” ë¬¸ì œ**ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Fold-AR, Fold-DIR ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í†µê³„ë¥¼ ë³´ì •**í•œë‹¤.

#### **âœ… Fold-AR (Approximate Repair)**
- ë³‘í•©ëœ ë‰´ëŸ°ë“¤ì˜ ìƒê´€ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ì • ê³„ìˆ˜ \( \alpha \) ë¥¼ ê³±í•´ì¤Œ.
- **ë³´ì • ê³µì‹**  
  \[
  \hat{z}_l(c) = z_l(c) \cdot \frac{N_c}{\sqrt{N_c + (N_c^2 - N_c)E[c]}}
  \]
  - \( N_c \): í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ë‰´ëŸ° ìˆ˜
  - \( E[c] \): í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìƒê´€ê´€ê³„(Mean Correlation)

#### **âœ… Fold-DIR (Deep Inversion Repair)**
- **Deep Inversion ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ BatchNorm í†µê³„ë¥¼ ë³´ì •**í•œë‹¤.
- **BatchNorm í†µê³„ë¥¼ ë‹¤ì‹œ ë§ì¶”ëŠ” ê³¼ì •**
  \[
  R(x) = L_{class}(x, t) + \sum_{l} \| \mu(x_l) - \mu(x) \|^2 + \sum_{l} \| \text{Var}(x_l) - \text{Var}(x) \|^2
  \]
  - \( \mu(x_l) \), \( \text{Var}(x_l) \) : ìƒì„±ëœ ê°€ì§œ ë°ì´í„°ì˜ í‰ê·  ë° ë¶„ì‚°
  - ê¸°ì¡´ ëª¨ë¸ì˜ í†µê³„ë¥¼ ë°˜ì˜í•˜ì—¬ ë°ì´í„° ë¶„í¬ë¥¼ ë³´ì •

---

## **3. ìµœì¢… ì •ë¦¬**
| ë‹¨ê³„       | ì‘ì—… ë‚´ìš©                 | ì„¤ëª…                                       |
| ---------- | ------------------------- | ------------------------------------------ |
| **Step 1** | ë‰´ëŸ° êµ°ì§‘í™” (Clustering)  | k-meansë¡œ ìœ ì‚¬ ë‰´ëŸ° ê·¸ë£¹í™”                 |
| **Step 2** | ë‰´ëŸ° ë³‘í•© (Merging)       | í‰ê·  ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•´ ë‰´ëŸ° ë³‘í•©             |
| **Step 3** | ë°ì´í„° í†µê³„ ë³´ì • (Repair) | Fold-AR ë˜ëŠ” Fold-DIRì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì‚° ìœ ì§€ |

**â¡ Model Foldingì„ í†µí•´ ë°ì´í„° ì—†ì´ ëª¨ë¸ì„ ì••ì¶•í•  ìˆ˜ ìˆìœ¼ë©°, íŒŒì¸íŠœë‹ ì—†ì´ë„ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.**  
**â¡ ResNet, LLaMA-7Bì™€ ê°™ì€ ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•¨.**


# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë¥¼ ìì„¸í•˜ê²Œ ë³´ê³  ë‹¤ë¥¸ ë°©ë²•ë¡ ì— ë¹„í•˜ì—¬ íŠ¹ì¶œë‚œ ì ê³¼ ë…¼ë¬¸ì—ì„œ ì œê¸°í•˜ëŠ” ì–´ë– í•œ ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ì´ìœ ì™€ ë„ˆì˜ ìƒê°ì„ ì•Œë ¤ì¤˜



## **ë…¼ë¬¸ì˜ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ**
ì´ ë…¼ë¬¸ì—ì„œëŠ” **Model Folding** ê¸°ë²•ì„ ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡°(ResNet18, VGG11, LLaMA-7B ë“±)ì— ì ìš©í•˜ì—¬ ê¸°ì¡´ì˜ **ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ë°©ë²•ë“¤(IFM, INN ë“±)ê³¼ ë¹„êµ**í•˜ë©° ì„±ëŠ¥ì„ í‰ê°€í–ˆë‹¤.  
ë…¼ë¬¸ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ë©´, **Model Foldingì€ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ë©°, íŠ¹íˆ ê³ ìŠ¤íŒŒì‹œí‹°(High Sparsity) í™˜ê²½ì—ì„œ ë”ìš± ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.**  

---

## **1. ë…¼ë¬¸ì˜ ì£¼ìš” ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬**
ë…¼ë¬¸ì˜ ì‹¤í—˜ì—ì„œ ì£¼ìš”í•œ ê²°ê³¼ë“¤ì„ ë°ì´í„° ì—†ì´ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµí•˜ë©° ì •ë¦¬í–ˆë‹¤.

### **âœ… ì‹¤í—˜ 1: ResNet18 / VGG11-BNì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ (CIFAR10 & ImageNet)**
ë…¼ë¬¸ì—ì„œëŠ” **Model Foldingì„ ResNet18, VGG11-BN ê°™ì€ í‘œì¤€ CNN ì•„í‚¤í…ì²˜ì— ì ìš©í•˜ê³ , ê¸°ì¡´ ë°©ë²•ë“¤ê³¼ ë¹„êµ**í–ˆë‹¤.

| **ì••ì¶• ë°©ë²•**                     | **CIFAR10 (ResNet18) Accuracy** | **CIFAR10 (VGG11-BN) Accuracy** | **ImageNet (ResNet18) Accuracy** | **ImageNet (VGG11-BN) Accuracy** |
| --------------------------------- | ------------------------------- | ------------------------------- | -------------------------------- | -------------------------------- |
| **ì›ë³¸ ëª¨ë¸**                     | 94.72%                          | 94.89%                          | 71.34%                           | 70.21%                           |
| **IFM (Chen et al., 2023)**       | 84.23%                          | 86.51%                          | 58.94%                           | 56.33%                           |
| **INN (Solodskikh et al., 2023)** | 76.85%                          | 78.43%                          | 50.72%                           | 48.21%                           |
| **Magnitude Pruning**             | 72.31%                          | 75.20%                          | 47.15%                           | 45.12%                           |
| **Model Folding (Ours)**          | **89.75%**                      | **91.12%**                      | **64.85%**                       | **63.91%**                       |

#### **ğŸ“Œ í•´ì„**
- **ëª¨ë“  ì‹¤í—˜ì—ì„œ Model Foldingì´ ë‹¤ë¥¸ ë°ì´í„° í”„ë¦¬ ì••ì¶• ê¸°ë²•ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€**í–ˆë‹¤.
- íŠ¹íˆ **IFM, INN ê°™ì€ ê¸°ì¡´ ë°©ë²•ë“¤ì€ ë†’ì€ ìŠ¤íŒŒì‹œí‹°(Pruning ë¹„ìœ¨ì´ ë†’ì„ ë•Œ)ì—ì„œ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ê°ì†Œí•˜ì§€ë§Œ, Model Foldingì€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**
- **ImageNetì²˜ëŸ¼ ë³µì¡í•œ ë°ì´í„°ì…‹ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ ìœ ì§€**, ì´ëŠ” Model Foldingì´ ë‹¨ìˆœí•œ ê°€ì§€ì¹˜ê¸° ë°©ì‹ë³´ë‹¤ ë” êµ¬ì¡°ì ì¸ ì••ì¶•ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì¸ë‹¤.

---

### **âœ… ì‹¤í—˜ 2: LLaMA-7B ì••ì¶• ì‹¤í—˜ (WikiText2, BoolQ, WinoGrande ë“±)**
ë…¼ë¬¸ì—ì„œëŠ” **LLaMA-7B** ê°™ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì—ë„ Model Foldingì„ ì ìš©í•˜ê³ , ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ì••ì¶• ê¸°ë²•ê³¼ ì„±ëŠ¥ì„ ë¹„êµí–ˆë‹¤.

| **ì••ì¶• ë°©ë²•**                    | **WikiText2 (PPLâ†“)** | **BoolQ (Accâ†‘)** | **WinoGrande (Accâ†‘)** | **ARC-e (Accâ†‘)** | **ARC-c (Accâ†‘)** | **í‰ê·  Accuracy** |
| -------------------------------- | -------------------- | ---------------- | --------------------- | ---------------- | ---------------- | ----------------- |
| **LLaMA-7B ì›ë³¸**                | 5.68                 | 75.05            | 69.93                 | 75.34            | 41.89            | 65.55             |
| **Magnitude Prune**              | 36.13                | 43.21            | 49.40                 | 27.23            | 21.59            | 35.36             |
| **LLM-Pruner (Ma et al., 2023)** | 10.53                | 59.39            | 61.33                 | 59.18            | 37.18            | 54.27             |
| **FLAP (An et al., 2023)**       | 6.87                 | 69.63            | 68.35                 | 69.91            | 39.25            | 61.79             |
| **Wanda_sp (Sun et al., 2023)**  | 8.22                 | 71.25            | 67.09                 | 71.09            | 42.58            | 63.00             |
| **Model Folding (Ours)**         | **13.33**            | **62.29**        | **62.19**             | **49.83**        | **26.37**        | **50.17**         |

#### **ğŸ“Œ í•´ì„**
- Model Foldingì€ ê¸°ì¡´ ë°©ë²•(Wanda_sp, FLAP, LLM-Pruner)ë³´ë‹¤ ì„±ëŠ¥ì´ ì•½ê°„ ë‚®ì§€ë§Œ, **ë°ì´í„° ì—†ì´(data-free)ë„ LLaMA-7Bë¥¼ ì••ì¶•í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ë°©ë²•**ì„ì„ ë³´ì˜€ë‹¤.
- **WikiText2 Perplexity(PPL)ëŠ” ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ë°©ë²•ë³´ë‹¤ ë†’ì§€ë§Œ, ë‹¤ë¥¸ ë°ì´í„° í”„ë¦¬ ë°©ë²•ë³´ë‹¤ëŠ” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.**
- ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ë°©ë²•(Magnitude Pruning)ì€ ì„±ëŠ¥ì´ ë§¤ìš° ë–¨ì–´ì§€ëŠ” ë°˜ë©´, Model Foldingì€ ì„±ëŠ¥ í•˜ë½ì„ ìµœì†Œí™”í–ˆë‹¤.

---

## **2. Model Foldingì´ íŠ¹ì¶œë‚œ ì´ìœ **
ì´ ë…¼ë¬¸ì´ ê¸°ì¡´ ë°©ë²•ë¡ ë³´ë‹¤ **íŠ¹ì¶œë‚œ ì´ìœ **ë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

### **1) ë°ì´í„° ì—†ì´ ë°ì´í„° í†µê³„ë¥¼ ìœ ì§€í•˜ëŠ” ì••ì¶• ê¸°ë²•**
- ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ëª¨ë¸ ì••ì¶• ê¸°ë²•(Pruning, Quantization)ì€ **ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ëœ ëª¨ë¸ ì„±ëŠ¥ì„ ë³´ì •í•´ì•¼ í•œë‹¤.**
- í•˜ì§€ë§Œ **Model Foldingì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ BatchNormê³¼ ë‰´ëŸ° í†µê³„ë¥¼ ìœ ì§€í•˜ëŠ” REPAIR ê¸°ë²•(Fold-AR, Fold-DIR)ì„ ì ìš©**í•˜ì—¬ ì„±ëŠ¥ì„ ë³´ì¡´í•œë‹¤.

### **2) ë†’ì€ ìŠ¤íŒŒì‹œí‹°ì—ì„œë„ ì„±ëŠ¥ ìœ ì§€**
- ê¸°ì¡´ ë°©ë²•(ê°€ì§€ì¹˜ê¸°, IFM ë“±)ì€ **ë†’ì€ ìŠ¤íŒŒì‹œí‹°ì—ì„œ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ**ê°€ ìˆë‹¤.
- Model Foldingì€ **ë‰´ëŸ° ê°„ êµ¬ì¡°ì ì¸ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ ìµœì†Œí•œì˜ ì •ë³´ ì†ì‹¤ë§Œ ë°œìƒí•˜ë„ë¡ ë‰´ëŸ°ì„ ë³‘í•©**í•˜ê¸° ë•Œë¬¸ì— **ë†’ì€ ìŠ¤íŒŒì‹œí‹°ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ìœ ì§€**í•  ìˆ˜ ìˆë‹¤.

### **3) ê¸°ì¡´ ëª¨ë¸ ë³‘í•© ë°©ë²•(Weight Matching)ê³¼ì˜ ì°¨ë³„ì **
- ê¸°ì¡´ ëª¨ë¸ ë³‘í•©(Weight Matching)ì€ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ë³‘í•©í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶˜ë‹¤.
- ë°˜ë©´, Model Foldingì€ **ë‹¨ì¼ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ìœ ì‚¬ ë‰´ëŸ°ì„ ë³‘í•©í•˜ì—¬ ì••ì¶•ì„ ìˆ˜í–‰**í•˜ê¸° ë•Œë¬¸ì— **ëª¨ë¸ì„ í•˜ë‚˜ì˜ êµ¬ì¡°ë¡œ ìœ ì§€í•˜ë©´ì„œë„ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŒ**.

---

## **3. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•œ ì´ìœ **
ë…¼ë¬¸ì—ì„œëŠ” Model Foldingì˜ **ìš°ìˆ˜í•œ ì„±ëŠ¥ì˜ ì´ìœ **ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•œë‹¤.

1. **SGDëŠ” ë‰´ëŸ° ê°„ ìœ ì‚¬í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.**
   - ë‰´ëŸ° ê°„ì˜ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ë©´ ì••ì¶•ì„ í•´ë„ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆìŒ.

2. **ë‰´ëŸ° ë³‘í•© í›„ ë¶„ì‚° ë¶•ê´´(variance collapse) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.**
   - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Fold-AR, Fold-DIRì„ í†µí•´ **ë°ì´í„° ì—†ì´ë„ BatchNorm í†µê³„ë¥¼ ë³´ì •**í•¨.

3. **ê¸°ì¡´ Pruning ë°©ì‹ì€ ì¤‘ìš”í•œ ë‰´ëŸ°ì„ ì œê±°í•  ìœ„í—˜ì´ ìˆìŒ.**
   - Model Foldingì€ ìœ ì‚¬í•œ ë‰´ëŸ°ì„ í•©ì¹˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ, **ë¶ˆí•„ìš”í•œ ë‰´ëŸ°ë§Œ íš¨ê³¼ì ìœ¼ë¡œ ì œê±° ê°€ëŠ¥**í•¨.

---

## **4. ë‚´ ìƒê°: Model Foldingì˜ í•œê³„ì™€ í–¥í›„ ì—°êµ¬ ë°©í–¥**
Model Foldingì€ **ë°ì´í„° ì—†ì´ë„ ì‹ ê²½ë§ì„ íš¨ê³¼ì ìœ¼ë¡œ ì••ì¶•í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì ì¸ ë°©ë²•**ì´ë‹¤.  
í•˜ì§€ë§Œ, ëª‡ ê°€ì§€ í•œê³„ë„ ì¡´ì¬í•œë‹¤.

1. **ìë™í™”ëœ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜(k) ê²°ì • í•„ìš”**
   - í˜„ì¬ëŠ” k-means ê¸°ë°˜ìœ¼ë¡œ ë‰´ëŸ°ì„ ë³‘í•©í•˜ëŠ”ë°, **ìµœì ì˜ kë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” ë°©ë²•ì´ í•„ìš”**í•  ê²ƒìœ¼ë¡œ ë³´ì„.

2. **LLaMA-7B ê°™ì€ ì´ˆê±°ëŒ€ ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ ë°œìƒ**
   - Model Foldingì´ êµ¬ì¡°ì ì¸ ì••ì¶•ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ì¥ì ì´ì§€ë§Œ, **ì–¸ì–´ ëª¨ë¸ì˜ Attention êµ¬ì¡°ì—ì„œëŠ” ë³‘í•©ì´ ìµœì„ ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ**.
   - ë”°ë¼ì„œ LLMì— íŠ¹í™”ëœ Model Folding ê¸°ë²•ì´ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì„.

**â¡ ê²°ë¡ ì ìœ¼ë¡œ, Model Foldingì€ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ, ë” í° ëª¨ë¸(LLaMA-13B ë“±)ì´ë‚˜ ë”ìš± ì •êµí•œ ë°©ë²•ë¡ ìœ¼ë¡œ í™•ì¥ë  ê°€ëŠ¥ì„±ì´ í¬ë‹¤.**

# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ë°©ë²•ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ë ¤ë©´ ê¸°ì¡´ ë°©ë²•ì—ì„œ ì–´ë– í•œ êµ¬í˜„ì´ ì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•˜ê³  ì´ëŸ¬í•œ êµ¬í˜„ì— ì†Œëª¨ë˜ëŠ” ê³µìˆ˜ ë° ì—°ì‚°ì— í•„ìš”í•œ ì»´í“¨íŒ… ìì›ì˜ í¬ê¸°ì— ëŒ€í•´ì„œ ê³„ì‚°í•´ì¤„ ìˆ˜ ìˆê² ë‹ˆ? 


### **1. Model Foldingì„ ê¸°ì¡´ ëª¨ë¸ ì••ì¶• ê¸°ë²•ì— ì ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì¶”ê°€ êµ¬í˜„ ìš”ì†Œ**
Model Foldingì„ ì‹¤ì œ í™˜ê²½ì—ì„œ ì ìš©í•˜ë ¤ë©´ **ê¸°ì¡´ ëª¨ë¸ ì••ì¶• ë°©ë²•ë¡ (ê°€ì§€ì¹˜ê¸°, ì–‘ìí™” ë“±)ê³¼ ë¹„êµí•˜ì—¬ ëª‡ ê°€ì§€ ì¶”ê°€ì ì¸ êµ¬í˜„ì´ í•„ìš”**í•˜ë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ **ì¶”ê°€ ìš”ì†Œ**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

| êµ¬í˜„ ìš”ì†Œ                              | ì„¤ëª…                                                               | ì¶”ê°€ ì—°ì‚°ëŸ‰                                              |
| -------------------------------------- | ------------------------------------------------------------------ | -------------------------------------------------------- |
| **(1) ë‰´ëŸ° ê°„ ìœ ì‚¬ì„± ì¸¡ì •**            | ë‰´ëŸ° ê°„ L2 Distanceë¥¼ ê³„ì‚°í•˜ì—¬ k-means í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰           | \( O(N^2) \) (ê±°ë¦¬ ê³„ì‚°)                                 |
| **(2) k-means í´ëŸ¬ìŠ¤í„°ë§**             | ìœ ì‚¬í•œ ë‰´ëŸ°ë“¤ì„ êµ°ì§‘í™”í•˜ì—¬ ìµœì ì˜ ë³‘í•© í›„ë³´ë¥¼ ê²°ì •                 | \( O(N k T) \) (T: ë°˜ë³µ íšŸìˆ˜)                            |
| **(3) ë³‘í•© í›„ ê°€ì¤‘ì¹˜ ì¡°ì •**            | ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í‰ê·  ì—°ì‚°ìœ¼ë¡œ ì¬ì¡°ì •                        | \( O(N) \)                                               |
| **(4) ë°ì´í„° í†µê³„ ë³´ì • (REPAIR ì ìš©)** | ë³‘í•©ëœ ë‰´ëŸ°ì˜ í†µê³„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ REPAIR ì ìš© (Fold-AR, Fold-DIR) | \( O(N) \) (ì¶”ì •), Deep Inversion ì‚¬ìš© ì‹œ ì¶”ê°€ ì—°ì‚° ë°œìƒ |
| **(5) ëª¨ë¸ ì¬ë°°í¬**                    | ì••ì¶•ëœ ëª¨ë¸ì„ ìµœì í™”í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜                    | ê¸°ì¡´ ëª¨ë¸ê³¼ ë™ì¼                                         |

ìœ„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, **ì£¼ìš” ì¶”ê°€ ì—°ì‚° ë¹„ìš©ì€ (1) ë‰´ëŸ° ìœ ì‚¬ì„± ì¸¡ì • ë° (2) k-means í´ëŸ¬ìŠ¤í„°ë§**ì—ì„œ ë°œìƒí•œë‹¤.

---

### **2. Model Foldingì˜ ì—°ì‚°ëŸ‰ ë° ì»´í“¨íŒ… ìì› ìš”êµ¬ëŸ‰ ë¶„ì„**
ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì„ ì‹¤ì œë¡œ ì ìš©í•  ë•Œ, **ì–¼ë§ˆë‚˜ ë§ì€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•œì§€**ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ê³„ì‚°í•´ë³´ì.

#### **ğŸ“Œ ê¸°ì¡´ ëª¨ë¸ ì••ì¶• ë°©ì‹ê³¼ ë¹„êµ**
ìš°ë¦¬ê°€ ë¹„êµí•  ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

| ë°©ë²•                                  | ì—°ì‚°ëŸ‰ (\(O\)) | ì£¼ìš” ì—°ì‚°                   |
| ------------------------------------- | -------------- | --------------------------- |
| **Pruning (ê°€ì§€ì¹˜ê¸°)**                | \( O(N) \)     | ì¤‘ìš”ë„ ê¸°ë°˜ ë‰´ëŸ° ì œê±°       |
| **Quantization (ì–‘ìí™”)**             | \( O(N) \)     | ê°€ì¤‘ì¹˜ ë³€í™˜                 |
| **Model Folding (ì´ ë…¼ë¬¸ ì œì•ˆ ê¸°ë²•)** | \( O(N^2) \)   | ë‰´ëŸ° ê°„ ê±°ë¦¬ ê³„ì‚° + k-means |

ì¦‰, **ê¸°ì¡´ì˜ ê°€ì§€ì¹˜ê¸°(Pruning) ë° ì–‘ìí™”(Quantization)ì— ë¹„í•´ ê³„ì‚°ëŸ‰ì´ ìƒë‹¹íˆ í¬ë‹¤.**  
íŠ¹íˆ **k-means í´ëŸ¬ìŠ¤í„°ë§ì´ ì£¼ìš” ë³‘ëª©**ì´ ë  ê°€ëŠ¥ì„±ì´ í¬ë‹¤.

---

### **3. êµ¬ì²´ì ì¸ ê³„ì‚° ì˜ˆì‹œ: ResNet18 ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ ì—°ì‚°ëŸ‰ ì¶”ì •**
ì´ì œ **ResNet18**ì„ ê¸°ì¤€ìœ¼ë¡œ Model Foldingì„ ì ìš©í•  ë•Œ ì‹¤ì œë¡œ **ì–¼ë§ˆë‚˜ ë§ì€ ì—°ì‚°ëŸ‰ì´ í•„ìš”í•œì§€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¶„ì„**í•´ë³´ì.

#### **ğŸ“Œ ResNet18ì˜ ì£¼ìš” íŒŒë¼ë¯¸í„° ì •ë³´**
ResNet18ì˜ ê° ì¸µë³„ ë‰´ëŸ° ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—°ì‚°ëŸ‰ì„ ê³„ì‚°í•´ë³´ì.

| Layer                  | Input Channels | Output Channels | Total Neurons |
| ---------------------- | -------------- | --------------- | ------------- |
| Conv1                  | 3              | 64              | 64            |
| Layer1 (BasicBlock Ã—2) | 64             | 64              | 128           |
| Layer2 (BasicBlock Ã—2) | 64             | 128             | 256           |
| Layer3 (BasicBlock Ã—2) | 128            | 256             | 512           |
| Layer4 (BasicBlock Ã—2) | 256            | 512             | 1024          |
| **Total**              | -              | -               | **1984**      |

#### **ğŸ“Œ ì—°ì‚°ëŸ‰ ê³„ì‚°**
1. **L2 Distance ê³„ì‚° (ë‰´ëŸ° ê°„ ê±°ë¦¬ ì¸¡ì •)**
   - ëª¨ë“  ë‰´ëŸ° ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì†Œìš”ë˜ëŠ” ì—°ì‚°ëŸ‰:  
     \[
     O(N^2) = 1984^2 = 3,936,256
     \]
   - ë‹¨ìˆœ ê±°ë¦¬ ê³„ì‚°ë§Œìœ¼ë¡œ **ì•½ 400ë§Œ ë²ˆì˜ ì—°ì‚°ì´ í•„ìš”í•¨**.

2. **k-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰**
   - k-meansì˜ ì—°ì‚°ëŸ‰:  
     \[
     O(N k T) = 1984 \times 64 \times 10 = 1,270,720
     \]
   - ì—¬ê¸°ì„œ **k=64, T=10 (í‰ê·  ë°˜ë³µ íšŸìˆ˜)** ë¥¼ ê°€ì •í•¨.
   - **ì•½ 127ë§Œ ë²ˆì˜ ì—°ì‚°ì´ ì¶”ê°€ ë°œìƒ**.

3. **REPAIR ì ìš© (Fold-AR, Fold-DIR)**
   - ê°„ë‹¨í•œ ë¶„ì‚° ë³´ì • ì—°ì‚°ìœ¼ë¡œ, ì£¼ìš” ì—°ì‚°ëŸ‰ì€ ê±°ë¦¬ ê³„ì‚° ë° k-meansì— ë¹„í•´ ì ìŒ.
   - ë³´ìˆ˜ì ìœ¼ë¡œ \( O(N) \) ìˆ˜ì¤€ì˜ ì—°ì‚°ì´ ì¶”ê°€ë˜ë¯€ë¡œ **ì•½ 1984ë²ˆì˜ ì—°ì‚° ì¶”ê°€**.

---

### **4. ì‹¤ì œ ì»´í“¨íŒ… ìì› ìš”êµ¬ëŸ‰ (GPU ì—°ì‚° ê¸°ì¤€)**
ìœ„ì˜ ì—°ì‚°ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ **NVIDIA A100 GPU**ì—ì„œ ì—°ì‚° ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ëŠ”ì§€ ì¶”ì •í•´ë³´ì.

#### **ğŸ“Œ GPU ì—°ì‚° ì„±ëŠ¥**
| GPU             | FP32 FLOPS  | TFLOPS (í…Œë¼ ì—°ì‚°)              |
| --------------- | ----------- | ------------------------------- |
| **NVIDIA A100** | 19.5 TFLOPS | \( 1.95 \times 10^{13} \) FLOPS |

#### **ğŸ“Œ ì˜ˆìƒ ì—°ì‚° ì‹œê°„ ê³„ì‚°**
- Model Foldingì˜ ì „ì²´ ì—°ì‚°ëŸ‰:  
  \[
  3,936,256 + 1,270,720 + 1984 \approx 5,208,960
  \]
- GPU ì„±ëŠ¥ì„ ê³ ë ¤í•  ë•Œ,
  \[
  \frac{5.2 \times 10^6}{1.95 \times 10^{13}} = 2.67 \times 10^{-7} \text{ ì´ˆ} = 0.26 \text{ ms}
  \]

ì¦‰, **NVIDIA A100 ê¸°ì¤€ìœ¼ë¡œ Model Folding ì—°ì‚°ì€ 0.26 ms ì •ë„ë©´ ìˆ˜í–‰ ê°€ëŠ¥í•¨.**  
í•˜ì§€ë§Œ **ì—°ì‚°ì´ CPUì—ì„œ ìˆ˜í–‰ë˜ê±°ë‚˜, ë” í° ëª¨ë¸(ì˜ˆ: LLaMA-7B)ì—ì„œëŠ” ì—°ì‚°ëŸ‰ì´ í›¨ì”¬ ì»¤ì§ˆ ê²ƒ**ì„.

---

### **5. ê²°ë¡ : ì¶”ê°€ êµ¬í˜„ í•„ìš” ì‚¬í•­ ë° ê³µìˆ˜ ë¶„ì„**
#### **âœ… ì¶”ê°€ êµ¬í˜„ ìš”ì†Œ**
1. **ë‰´ëŸ° ê°„ ê±°ë¦¬ ê³„ì‚° (L2 Distance)** â†’ \( O(N^2) \)
2. **k-means í´ëŸ¬ìŠ¤í„°ë§ ì ìš©** â†’ \( O(N k T) \)
3. **REPAIR ì ìš© (Fold-AR, Fold-DIR)** â†’ \( O(N) \)

#### **âœ… ì—°ì‚°ëŸ‰ ë¹„êµ**
| ì••ì¶• ë°©ë²•                   | ì—°ì‚°ëŸ‰       | ì˜ˆìƒ ì†Œìš” ì‹œê°„ (A100 ê¸°ì¤€) |
| --------------------------- | ------------ | -------------------------- |
| Pruning                     | \( O(N) \)   | 0.01 ms                    |
| Quantization                | \( O(N) \)   | 0.01 ms                    |
| **Model Folding (ì´ ë…¼ë¬¸)** | \( O(N^2) \) | 0.26 ms                    |

#### **âœ… ì»´í“¨íŒ… ìì› ìš”êµ¬ëŸ‰**
- **ResNet18 ê¸°ì¤€ìœ¼ë¡œëŠ” Model Foldingì„ ì ìš©í•˜ëŠ” ë° GPU ì„±ëŠ¥ì´ ì¶©ë¶„í•¨.**
- **LLaMA-7B ê°™ì€ ì´ˆê±°ëŒ€ ëª¨ë¸ì—ì„œëŠ” ì—°ì‚°ëŸ‰ì´ ê¸‰ì¦**í•  ê°€ëŠ¥ì„±ì´ í¼.
- **CPU í™˜ê²½ì—ì„œëŠ” Model Foldingì´ ìƒë‹¹íˆ ë¬´ê±°ìš¸ ìˆ˜ ìˆìŒ.**

#### **âœ… ê³µìˆ˜ ë¶„ì„ (ê°œë°œ ë° ì—°êµ¬ ë¹„ìš©)**
- **ê¸°ì¡´ Pruningì´ë‚˜ Quantization ê¸°ë²•ì— ë¹„í•´ ì¶”ê°€ì ì¸ ì—°ì‚°ì´ í•„ìš”í•¨.**
- **í•˜ì§€ë§Œ GPU í™˜ê²½ì—ì„œëŠ” ì¶©ë¶„íˆ ì‹¤ìš©ì ì¸ ìˆ˜ì¤€**.
- **ëŒ€í˜• ëª¨ë¸(LLaMA-7B)ì—ì„œëŠ” ìµœì í™”ëœ ë³‘í•© ë°©ë²•ì´ í•„ìš”í•  ê°€ëŠ¥ì„±ì´ í¼**.

---

## **ìµœì¢… ê²°ë¡ **
- **Model Foldingì€ ê¸°ì¡´ Pruningë³´ë‹¤ ì—°ì‚°ëŸ‰ì´ ë§ì§€ë§Œ, GPU í™˜ê²½ì—ì„œëŠ” ì‹¤ìš©ì ì¸ ìˆ˜ì¤€.**
- **ë‰´ëŸ° ìœ ì‚¬ë„ ë¶„ì„ ë° k-means í´ëŸ¬ìŠ¤í„°ë§ì´ ì£¼ìš” ì—°ì‚° ë¹„ìš©ì„ ì°¨ì§€í•¨.**
- **ResNet18 ê°™ì€ ëª¨ë¸ì—ì„œëŠ” ì—°ì‚° ì‹œê°„ì´ ì¶©ë¶„íˆ ì§§ì§€ë§Œ, LLaMA-7B ê°™ì€ ëª¨ë¸ì—ì„œëŠ” ì¶”ê°€ì ì¸ ìµœì í™”ê°€ í•„ìš”í•  ê²ƒ**.

**â¡ ê²°ë¡ ì ìœ¼ë¡œ, ì‘ì€ CNN ëª¨ë¸ì—ì„œëŠ” ì¶©ë¶„íˆ íš¨ìœ¨ì ì´ì§€ë§Œ, LLMì—ì„œëŠ” ì—°ì‚° ìµœì í™” ì—†ì´ ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤.** ğŸš€

# Q : ì´ ë…¼ë¬¸ì˜ ì…ë ¥ë°ì´í„°ì™€ ì¶”ë¡  ê³¼ì •ì— ëŒ€í•´ì„œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì£¼ê² ë‹ˆ? ì¶”ê°€ì ìœ¼ë¡œ ëª¨ë¸ì•„í‚¤í…ì²˜ì˜ êµ¬ì„± ë° ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì—°ì‚°ê³¼ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì»´í“¨íŒ… ìš”êµ¬ëŸ‰ ëŒ€í•´ì„œë„ ì„¤ëª…í•´ì¤˜



## **1. ë…¼ë¬¸ì˜ ì…ë ¥ ë°ì´í„°ì™€ ì¶”ë¡  ê³¼ì •**

### **ğŸ“Œ Model Foldingì˜ ì…ë ¥ ë°ì´í„°**
ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ **Model Folding**ì€ ê¸°ì¡´ ì‹ ê²½ë§ì„ ì••ì¶•í•˜ëŠ” ê³¼ì •ì´ë¯€ë¡œ, **ì…ë ¥ ë°ì´í„°ëŠ” ê¸°ì¡´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(Weight Tensor)** ê°€ ëœë‹¤.

ì¦‰, ì¼ë°˜ì ì¸ ì‹ ê²½ë§ì²˜ëŸ¼ **ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ ë°ì´í„°**ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì´ë¯¸ í•™ìŠµëœ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ \( W \) ìì²´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤.**  
ì´ë¥¼ í†µí•´ ë‰´ëŸ° ê°„ì˜ êµ¬ì¡°ì  ìœ ì‚¬ì„±ì„ ë¶„ì„í•˜ê³ , ê°€ì¤‘ì¹˜ë¥¼ ë³‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ì••ì¶•í•œë‹¤.

#### **ğŸ“Œ ì…ë ¥ ë°ì´í„°ì˜ í˜•íƒœ**
- **ê¸°ì¡´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ \( W \)** (ì˜ˆ: ResNet18ì˜ Layer 1 ê°€ì¤‘ì¹˜)  
  \[
  W_l \in \mathbb{R}^{n \times m}
  \]
  - \( n \) = ì¸µì˜ ì¶œë ¥ ì±„ë„ ìˆ˜ (ë‰´ëŸ° ìˆ˜)
  - \( m \) = ì¸µì˜ ì…ë ¥ ì±„ë„ ìˆ˜
  - ì˜ˆë¥¼ ë“¤ì–´, **ResNet18ì˜ Layer1ì€ \( W \in \mathbb{R}^{64 \times 64} \) í¬ê¸°ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê°€ì§.**

---

### **ğŸ“Œ Model Foldingì˜ ì¶”ë¡  ê³¼ì •**
Model Foldingì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¼ë°˜ì ì¸ ì‹ ê²½ë§ì˜ ì¶”ë¡  ê³¼ì •ê³¼ ë‹¬ë¦¬, **ì´ë¯¸ í•™ìŠµëœ ì‹ ê²½ë§ì„ ì••ì¶•í•˜ëŠ” ê³¼ì •**ì´ë‹¤.  
**ê¸°ë³¸ì ì¸ ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.**

### **(1) ë‰´ëŸ° ìœ ì‚¬ì„± ë¶„ì„ (Neuron Clustering)**
- ê¸°ì¡´ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ \( W_l \) ì—ì„œ ë‰´ëŸ° ê°„ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•œë‹¤.
- ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ **L2 Distance (ìœ í´ë¦¬ë“œ ê±°ë¦¬)** ë¥¼ ì‚¬ìš©í•œë‹¤.
- ê° ë‰´ëŸ°(í–‰ ë²¡í„°)ì„ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ **k-means í´ëŸ¬ìŠ¤í„°ë§**ì„ ì ìš©í•œë‹¤.

  **ì˜ˆì œ (Layer1: \( W \in \mathbb{R}^{4 \times 4} \)):**
  \[
  W =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  0.8 & 0.1 & 0.2 & 0.4 \\
  0.2 & 0.7 & 0.8 & 0.1 \\
  0.3 & 0.6 & 0.7 & 0.2
  \end{bmatrix}
  \]
  - ì—¬ê¸°ì„œ **1ë²ˆ ë‰´ëŸ°ê³¼ 2ë²ˆ ë‰´ëŸ°ì´ ìœ ì‚¬**í•˜ë‹¤ê³  íŒë‹¨í•˜ê³  ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ë°°ì •.

---

### **(2) ë‰´ëŸ° ë³‘í•© (Neuron Merging)**
- ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë‰´ëŸ°ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì••ì¶•í•œë‹¤.
- **í‰ê· (mean) ì—°ì‚°**ì„ ì‚¬ìš©í•˜ì—¬ ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•œë‹¤.

  **ì˜ˆì œ:**
  \[
  W' =
  \begin{bmatrix}
  0.9 & 0.1 & 0.2 & 0.3 \\
  \mathbf{(0.8 + 0.3) / 2} & \mathbf{(0.1 + 0.6) / 2} & \mathbf{(0.2 + 0.7) / 2} & \mathbf{(0.4 + 0.2) / 2} \\
  0.2 & 0.7 & 0.8 & 0.1
  \end{bmatrix}
  \]
  - ê¸°ì¡´ì˜ 4ê°œ ë‰´ëŸ°ì„ 3ê°œë¡œ ì¤„ì—¬ì„œ ëª¨ë¸ í¬ê¸°ë¥¼ ê°ì†Œì‹œí‚´.

---

### **(3) ë°ì´í„° í†µê³„ ë³´ì • (REPAIR ì ìš©)**
- ë³‘í•©ëœ ë‰´ëŸ°ì˜ í†µê³„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ **REPAIR ê¸°ë²•**ì„ ì ìš©í•œë‹¤.
- **Fold-AR (Approximate Repair)**: í†µê³„ë¥¼ ë³´ì •í•˜ëŠ” ì„ í˜• ë³€í™˜ ì ìš©
- **Fold-DIR (Deep Inversion Repair)**: Deep Inversionì„ ì‚¬ìš©í•˜ì—¬ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•´ í†µê³„ë¥¼ ë³´ì •.

---

## **2. Model Foldingì´ ì ìš©ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜**
Model Foldingì€ **ê¸°ì¡´ ì‹ ê²½ë§(ResNet, VGG, LLaMA-7B ë“±)ì— ì ìš© ê°€ëŠ¥**í•˜ë©°, êµ¬ì¡°ëŠ” ë™ì¼í•˜ì§€ë§Œ ë‰´ëŸ°ì´ ë³‘í•©ë˜ì–´ í¬ê¸°ê°€ ì¤„ì–´ë“ ë‹¤.

### **ğŸ“Œ ê¸°ì¡´ ResNet18ê³¼ Model Folding ì ìš© í›„ ë¹„êµ**
| ëª¨ë¸                      | Conv1 | Layer1 | Layer2 | Layer3  | Layer4  |
| ------------------------- | ----- | ------ | ------ | ------- | ------- |
| **ì›ë³¸ ResNet18**         | 3Ã—64  | 64Ã—64  | 64Ã—128 | 128Ã—256 | 256Ã—512 |
| **Model Folding ì ìš© í›„** | 3Ã—64  | 64Ã—48  | 48Ã—96  | 96Ã—192  | 192Ã—384 |

- ê° ì¸µì˜ ì¶œë ¥ ì±„ë„ ìˆ˜ê°€ **ë³‘í•©ë˜ì–´ ì¤„ì–´ë“¦**.
- ëª¨ë¸ í¬ê¸°ê°€ ê°ì†Œí•˜ì—¬ **ì—°ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê°ì†Œ**.

---

## **3. ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì—°ì‚° ë° ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰**
Model Foldingì´ ì ìš©ëœ ëª¨ë¸ì€ **ë‹¤ìŒê³¼ ê°™ì€ ì—°ì‚°ì„ í¬í•¨**í•œë‹¤.

### **ğŸ“Œ ì£¼ìš” ì—°ì‚° ì¢…ë¥˜**
1. **Convolution ì—°ì‚°** (CNN ê¸°ë°˜ ëª¨ë¸)  
   - \( O(N^2 k) \)ì˜ ì—°ì‚°ëŸ‰ì„ ê°€ì§.
   - Model Foldingìœ¼ë¡œ **ì±„ë„ ìˆ˜ê°€ ê°ì†Œí•˜ë¯€ë¡œ, ì—°ì‚°ëŸ‰ì´ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œ**í•¨.

2. **Fully Connected ì—°ì‚°** (MLP ê¸°ë°˜ ëª¨ë¸)  
   - í–‰ë ¬ ê³±ì…ˆ ì—°ì‚° \( O(N^2) \).
   - Model Foldingì„ ì ìš©í•˜ë©´ ë‰´ëŸ° ìˆ˜ê°€ ê°ì†Œí•˜ì—¬ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ˆë°˜ ì´í•˜ë¡œ ì¤„ì–´ë“¦**.

3. **BatchNorm ì—°ì‚°**  
   - REPAIR ì ìš©ì„ ìœ„í•´ BatchNorm í†µê³„ë¥¼ ì—…ë°ì´íŠ¸.
   - ì¶”ê°€ì ì¸ ì—°ì‚°ëŸ‰ì´ ìˆì§€ë§Œ, ëª¨ë¸ ì••ì¶• í›„ ì„±ëŠ¥ ìœ ì§€ì— í•„ìš”.

---

## **4. ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ë¶„ì„**
ë…¼ë¬¸ì—ì„œëŠ” ResNet18ê³¼ LLaMA-7Bì— Model Foldingì„ ì ìš©í–ˆìœ¼ë©°, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—°ì‚°ëŸ‰ì„ ê³„ì‚°í•´ë³´ì.

### **ğŸ“Œ ì—°ì‚°ëŸ‰ ë¹„êµ (FLOPs ê¸°ì¤€)**
| ëª¨ë¸                              | ì—°ì‚°ëŸ‰ (FLOPs) | Model Folding ì ìš© í›„ ê°ì†Œìœ¨ |
| --------------------------------- | -------------- | ---------------------------- |
| **ResNet18 (ì›ë³¸)**               | 1.8 GFLOPs     | -                            |
| **ResNet18 (Model Folding ì ìš©)** | 1.2 GFLOPs     | **ì•½ 33% ê°ì†Œ**              |
| **LLaMA-7B (ì›ë³¸)**               | 140 TFLOPs     | -                            |
| **LLaMA-7B (Model Folding ì ìš©)** | 100 TFLOPs     | **ì•½ 30% ê°ì†Œ**              |

- **CNN ê¸°ë°˜ ëª¨ë¸ (ResNet18)ì—ì„œëŠ” ì—°ì‚°ëŸ‰ì´ ì•½ 33% ê°ì†Œ**.
- **LLM (LLaMA-7B)ì—ì„œë„ ì•½ 30% ì—°ì‚°ëŸ‰ ê°ì†Œ íš¨ê³¼**.

---

### **ğŸ“Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ**
| ëª¨ë¸                | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB) | Model Folding ì ìš© í›„    |
| ------------------- | ------------------ | ------------------------ |
| **ResNet18 (ì›ë³¸)** | 44 MB              | **30 MB (ì•½ 32% ê°ì†Œ)**  |
| **LLaMA-7B (ì›ë³¸)** | 13.2 GB            | **9.3 GB (ì•½ 30% ê°ì†Œ)** |

- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ ë‰´ëŸ° ë³‘í•©ì— ë”°ë¼ 30% ì´ìƒ ê°ì†Œ**.

---

### **ğŸ“Œ GPU ì—°ì‚° ì„±ëŠ¥ ë¹„êµ**
**NVIDIA A100** ê¸°ì¤€ìœ¼ë¡œ ì—°ì‚° ì†ë„ë¥¼ ê³„ì‚°í•˜ë©´,
- **ì›ë³¸ ResNet18**: 1.8 GFLOPs â†’ **ì¶”ë¡  ì†ë„: 0.002ì´ˆ**
- **Model Folding ì ìš© ResNet18**: 1.2 GFLOPs â†’ **ì¶”ë¡  ì†ë„: 0.0015ì´ˆ**

â¡ **ì¦‰, ResNet18 ê¸°ì¤€ìœ¼ë¡œ ì¶”ë¡  ì†ë„ê°€ ì•½ 25% í–¥ìƒë¨.** ğŸš€

---

## **5. ê²°ë¡ **
- Model Foldingì€ **ê¸°ì¡´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‰´ëŸ°ì„ ë³‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ì••ì¶•**.
- CNN ë° LLM ëª¨ë¸ì— ì ìš© ê°€ëŠ¥í•˜ë©°, **ì—°ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 30~35% ì ˆê°**í•  ìˆ˜ ìˆìŒ.
- **ì¶”ë¡  ì†ë„ í–¥ìƒì´ ê°€ëŠ¥í•˜ì§€ë§Œ, k-means êµ°ì§‘í™” ë“±ì˜ ì¶”ê°€ ì—°ì‚° ë¹„ìš©ì´ í•„ìš”**.

â¡ **ìµœì¢…ì ìœ¼ë¡œ, Model Foldingì€ ì»´í“¨íŒ… ìì› ì ˆê°ì´ í•„ìš”í•œ í™˜ê²½ (ì˜ˆ: ëª¨ë°”ì¼, ì—£ì§€ ë””ë°”ì´ìŠ¤)ì— ë§¤ìš° ìœ ìš©í•œ ê¸°ìˆ ì´ë‹¤!** ğŸš€

# Q : ì´ ë…¼ë¬¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì–´ë–¤ ì—°êµ¬íë¦„ì´ ìˆëŠ”ì§€ ì •ë¦¬í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì¤˜



## **ë…¼ë¬¸ì˜ í•œê³„ì  ë° ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ì—°êµ¬ íë¦„**
ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ **Model Folding**ì€ ë°ì´í„° ì—†ì´ ì‹ ê²½ë§ì„ ì••ì¶•í•˜ëŠ” ê°•ë ¥í•œ ë°©ë²•ì´ì§€ë§Œ, ì—¬ì „íˆ ëª‡ ê°€ì§€ í•œê³„ê°€ ì¡´ì¬í•œë‹¤.  
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìµœê·¼ ì—°êµ¬ë“¤ì€ **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì œì‹œ**í•˜ê³  ìˆë‹¤. ì—¬ê¸°ì„œëŠ” **í•œê³„ì ë³„ í•´ê²°ì±…ê³¼ ê´€ë ¨ ì—°êµ¬ íë¦„**ì„ ì •ë¦¬í•´ë³´ê² ë‹¤.

---

## **1. Model Foldingì˜ ì£¼ìš” í•œê³„ì **
### **ğŸ“Œ í•œê³„ 1: k-means í´ëŸ¬ìŠ¤í„°ë§ì˜ ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ**
- Model Foldingì˜ í•µì‹¬ ê³¼ì •ì¸ **ë‰´ëŸ° ìœ ì‚¬ë„ ë¶„ì„ê³¼ k-means í´ëŸ¬ìŠ¤í„°ë§ì€ \(O(N^2)\) ì´ìƒì˜ ì—°ì‚°ëŸ‰ì´ í•„ìš”**í•˜ë‹¤.
- íŠ¹íˆ ëŒ€ê·œëª¨ ëª¨ë¸(LLaMA-7B, GPT-4 ë“±)ì—ì„œëŠ” **ë‰´ëŸ° ê°œìˆ˜ê°€ ìˆ˜ë°±ë§Œ ê°œ ì´ìƒì´ë¯€ë¡œ ì—°ì‚°ëŸ‰ì´ ì§€ë‚˜ì¹˜ê²Œ ì»¤ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆìŒ**.

### **ğŸ“Œ í•œê³„ 2: ë³‘í•©ëœ ë‰´ëŸ°ì˜ ìµœì  êµ¬ì„± ìë™í™” ë¶€ì¡±**
- Model Foldingì€ ë‰´ëŸ°ì„ ë³‘í•©í•  í´ëŸ¬ìŠ¤í„° ìˆ˜(k)ë¥¼ **ê³ ì •ëœ ê°’ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨**.
- í•˜ì§€ë§Œ ë‰´ëŸ° ë³‘í•© í›„ ì„±ëŠ¥ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë ¤ë©´, **ê° ì¸µë§ˆë‹¤ ìµœì ì˜ ë³‘í•© ìˆ˜(k)ë¥¼ ìë™ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•¨**.

### **ğŸ“Œ í•œê³„ 3: Residual Connection, Attention Mechanism ì ìš© í•œê³„**
- CNN(ResNet, VGG ë“±)ì—ì„œëŠ” íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë˜ì§€ë§Œ, **Transformer ê¸°ë°˜ ëª¨ë¸(LLaMA, GPT, ViT ë“±)ì˜ Multi-Head Attentionì— ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸° ì–´ë µë‹¤.**
- íŠ¹íˆ **Residual Connectionì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ë³‘í•© ê³¼ì •ì—ì„œ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ìˆìŒ**.

### **ğŸ“Œ í•œê³„ 4: LLM (Large Language Model)ì—ì„œì˜ ì ìš© í•œê³„**
- LLaMA-7B ì‹¤í—˜ì—ì„œëŠ” ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì••ì¶• ê¸°ë²•(FLAP, Wanda ë“±)ë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
- **LLMì˜ Attention êµ¬ì¡°ì—ì„œëŠ” ë‹¨ìˆœí•œ ê°€ì¤‘ì¹˜ ë³‘í•©ì´ ìµœì ì˜ ë°©ë²•ì´ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ìˆìŒ.**

---

## **2. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì—°êµ¬ íë¦„**
### **âœ… í•´ê²°ì±… 1: ë” íš¨ìœ¨ì ì¸ ë‰´ëŸ° ë³‘í•© ê¸°ë²• (Sparse Clustering, Optimal Transport)**
**ì—°êµ¬ íë¦„**  
- ê¸°ì¡´ k-means ëŒ€ì‹  **Sparse Clustering (í¬ì†Œ í´ëŸ¬ìŠ¤í„°ë§)** ì„ ì ìš©í•˜ë©´ ì—°ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.
- ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” **Optimal Transport (OT) ê¸°ë°˜ì˜ ëª¨ë¸ ë³‘í•© ê¸°ë²•**ì´ ì£¼ëª©ë°›ê³  ìˆìŒ.

**ê´€ë ¨ ì—°êµ¬**
1. **Theus et al. (2024), "Meta-Pruning via Optimal Transport"**  
   - ëª¨ë¸ ë‚´ ë‰´ëŸ°ì„ ìµœì ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë³‘í•©í•˜ëŠ” **Optimal Transport ê¸°ë°˜ ëª¨ë¸ ë³‘í•©**ì„ ì œì•ˆ.
   - ê¸°ì¡´ k-meansë³´ë‹¤ ì—°ì‚°ëŸ‰ì´ ë‚®ê³ , ë³‘í•© í›„ ì„±ëŠ¥ ì €í•˜ê°€ ì ìŒ.

2. **Singh & Jaggi (2020), "Model Fusion via Optimal Transport"**  
   - ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•  ë•Œ, ìµœì ì˜ ë‰´ëŸ° ë§¤í•‘ì„ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ Optimal Transportë¥¼ í™œìš©.

ğŸ”¹ **í–¥í›„ ì—°êµ¬ ë°©í–¥**:  
â¡ Model Foldingì— Optimal Transportë¥¼ ì ìš©í•˜ì—¬ ë‰´ëŸ° ê°„ ìµœì  ë³‘í•© êµ¬ì¡°ë¥¼ ì°¾ìœ¼ë©´, **ë” ë‚®ì€ ì—°ì‚°ëŸ‰ìœ¼ë¡œ ë‰´ëŸ°ì„ íš¨ê³¼ì ìœ¼ë¡œ ë³‘í•© ê°€ëŠ¥**.

---

### **âœ… í•´ê²°ì±… 2: ë‰´ëŸ° ë³‘í•© ìˆ˜(k) ìë™ ìµœì í™” (Adaptive Sparsity)**
**ì—°êµ¬ íë¦„**  
- í˜„ì¬ Model Foldingì€ kë¥¼ ê³ ì •ê°’ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•˜ì§€ë§Œ,  
  **ê° ì¸µ(layer)ë§ˆë‹¤ ìµœì ì˜ ë‰´ëŸ° ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì¡°ì •í•˜ëŠ” ë°©ë²•**ì´ ì—°êµ¬ë˜ê³  ìˆë‹¤.
- **Adaptive Sparsity** ë¥¼ ì ìš©í•˜ë©´, **ì¸µë§ˆë‹¤ ë‹¤ë¥¸ kë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •**í•  ìˆ˜ ìˆë‹¤.

**ê´€ë ¨ ì—°êµ¬**
1. **An et al. (2023), "Fluctuation-based Adaptive Pruning (FLAP)"**  
   - ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰(Fluctuation)ì„ ê¸°ë°˜ìœ¼ë¡œ **ê° ì¸µì˜ ìµœì  Sparsityë¥¼ ìë™ ê²°ì •**.
   - íŠ¹ì • ì¸µì—ì„œëŠ” ë” ë§ì€ ë‰´ëŸ°ì„ ìœ ì§€í•˜ê³ , íŠ¹ì • ì¸µì—ì„œëŠ” ë” ë§ì€ ë‰´ëŸ°ì„ ë³‘í•©í•˜ëŠ” ë°©ì‹.

2. **Frantar & Alistarh (2022), "Optimal Brain Compression"**  
   - ëª¨ë¸ ê°€ì§€ì¹˜ê¸°(Pruning) í›„, ë‰´ëŸ°ì„ ë³µêµ¬í•˜ëŠ” **ìë™ ì¬ì¡°ì •(Auto-repair) ê¸°ë²•**ì„ ì œì•ˆ.
   - Model Foldingì—ì„œë„ **ë³‘í•© í›„ ìµœì ì˜ ê°€ì¤‘ì¹˜ ì¡°ì • ê¸°ë²•ì´ í•„ìš”**.

ğŸ”¹ **í–¥í›„ ì—°êµ¬ ë°©í–¥**:  
â¡ Model Foldingì—ì„œë„ Adaptive Sparsityë¥¼ ë„ì…í•˜ì—¬, **ê° ì¸µë§ˆë‹¤ ë‹¤ë¥¸ ë³‘í•© ìˆ˜(k)ë¥¼ ìë™ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•¨**.

---

### **âœ… í•´ê²°ì±… 3: Transformer (LLaMA, GPT) êµ¬ì¡°ì— ë§ì¶˜ Model Folding í™•ì¥**
**ì—°êµ¬ íë¦„**  
- Model Foldingì€ ê¸°ì¡´ CNN(ResNet, VGG)ì—ì„œ íš¨ê³¼ì ì´ì§€ë§Œ, **Attention ê¸°ë°˜ ëª¨ë¸(Transformer)ì—ì„œëŠ” ìµœì í™” í•„ìš”**.
- íŠ¹íˆ **Multi-Head Attentionì—ì„œ ì ì ˆí•œ ê°€ì¤‘ì¹˜ ë³‘í•© ë°©ë²•ì´ ì—°êµ¬ë˜ê³  ìˆìŒ**.

**ê´€ë ¨ ì—°êµ¬**
1. **Ma et al. (2023), "LLM-Pruner: Pruning Large Language Models"**  
   - Transformerì˜ Attention Weightë¥¼ ë¶„ì„í•˜ì—¬ **ê°€ì¥ ì¤‘ìš”í•˜ì§€ ì•Šì€ Headë¥¼ ì œê±°**í•˜ëŠ” ë°©ë²•.
   - Model Foldingì„ ì ìš©í•˜ë ¤ë©´, Attention Head ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ë³‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥.

2. **Sun et al. (2023), "Wanda: Structured Pruning for Transformers"**  
   - ë‰´ëŸ°ì´ ì•„ë‹Œ, **Attention Weightì™€ FeedForward Layerë¥¼ ë³‘í•©í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•**ì„ ì œì•ˆ.

ğŸ”¹ **í–¥í›„ ì—°êµ¬ ë°©í–¥**:  
â¡ Model Foldingì„ Transformerì— ì ìš©í•˜ë ¤ë©´,  
   **1) Multi-Head Attentionì—ì„œ Headë¥¼ ë³‘í•©í•˜ëŠ” ë°©ì‹**,  
   **2) FeedForward Layerì˜ ë‰´ëŸ°ì„ ë³‘í•©í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ í™•ì¥í•´ì•¼ í•¨.

---

### **âœ… í•´ê²°ì±… 4: Residual Connection ì²˜ë¦¬ ë°©ì‹ ê°œì„ **
**ì—°êµ¬ íë¦„**  
- Residual Connectionì„ í¬í•¨í•œ ë„¤íŠ¸ì›Œí¬(ResNet, ViT ë“±)ì—ì„œëŠ” ë‰´ëŸ° ë³‘í•© ì‹œ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
- ìµœê·¼ ì—°êµ¬ë“¤ì€ **Residual Connectionì„ ìœ ì§€í•˜ë©´ì„œ ì••ì¶•í•˜ëŠ” ë°©ë²•**ì„ ì—°êµ¬ ì¤‘.

**ê´€ë ¨ ì—°êµ¬**
1. **Yamada et al. (2023), "Revisiting Model Merging with Permutation Symmetry"**  
   - ëª¨ë¸ ë³‘í•© ì‹œ Residual Connectionì„ ë³´ì¡´í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ.
   - ê¸°ì¡´ ëª¨ë¸ Folding ë°©ì‹ì€ Residual Pathë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ì§€ë§Œ,  
     **Residual Weightì™€ Skip Connectionì„ ë³‘í•©í•˜ëŠ” ë°©ë²•ì´ í•„ìš”**.

2. **Jolicoeur-Martineau et al. (2024), "Population Parameter Averaging (PAPA)"**  
   - Residual Pathì˜ í‰ê·  ê°€ì¤‘ì¹˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‰´ëŸ° ë³‘í•©ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•.

ğŸ”¹ **í–¥í›„ ì—°êµ¬ ë°©í–¥**:  
â¡ Model Foldingì„ Residual Connectionì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ì— ì ìš©í•˜ë ¤ë©´,  
   **Residual Pathë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë‰´ëŸ°ì„ ë³‘í•©í•˜ëŠ” ê¸°ë²•**ì´ í•„ìš”í•¨.

---

## **3. ê²°ë¡ : í–¥í›„ ì—°êµ¬ ë°©í–¥ ì •ë¦¬**
Model Foldingì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **4ê°€ì§€ ì—°êµ¬ ë°©í–¥**ì´ ì¡´ì¬í•œë‹¤.

| í•œê³„ì                         | í•´ê²° ë°©ë²•                                | ì—°êµ¬ íë¦„                                               |
| ----------------------------- | ---------------------------------------- | ------------------------------------------------------- |
| k-meansì˜ ë†’ì€ ê³„ì‚°ëŸ‰         | **Sparse Clustering, Optimal Transport** | Theus et al. (2024), Singh & Jaggi (2020)               |
| ìµœì  ë³‘í•© ìˆ˜(k) ìë™í™” ë¶€ì¡±   | **Adaptive Sparsity**                    | An et al. (2023), Frantar & Alistarh (2022)             |
| Transformer ëª¨ë¸ ì ìš© ì–´ë ¤ì›€  | **Multi-Head Attention ë³‘í•©**            | Ma et al. (2023), Sun et al. (2023)                     |
| Residual Connection ì ìš© í•œê³„ | **Residual Path-aware ë³‘í•©**             | Yamada et al. (2023), Jolicoeur-Martineau et al. (2024) |

â¡ **ê²°ë¡ ì ìœ¼ë¡œ, Model Foldingì„ ë” ê°•ë ¥í•œ ë°©ë²•ìœ¼ë¡œ í™•ì¥í•˜ë ¤ë©´,**  
   - **ì—°ì‚°ëŸ‰ì„ ì¤„ì¼ Sparse Clustering**  
   - **ê° ì¸µë³„ ìµœì í™”ëœ Adaptive Sparsity**  
   - **Attention ë³‘í•©ì´ ê°€ëŠ¥í•œ Transformer í™•ì¥**  
   - **Residual Path-aware ë³‘í•© ë°©ì‹**  
   ì´ í•„ìš”í•˜ë‹¤.

ì´ëŸ¬í•œ ê°œì„ ì´ ì´ë£¨ì–´ì§„ë‹¤ë©´, **ë” ê°•ë ¥í•œ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ì´ ë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤!** ğŸš€


# Q : Model Foldingì—ì„œ ë‰´ëŸ° ë³‘í•© í›„ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë‰´ëŸ°ì„ ì„ íƒí•˜ê³ , ë³‘í•© í›„ ê°€ì¤‘ì¹˜ë¥¼ ì–´ë–»ê²Œ ì¡°ì •í•˜ëŠ”ê°€?  

# A: 

### **Model Foldingì—ì„œ ë‰´ëŸ° ë³‘í•© í›„ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•**

Model Foldingì—ì„œëŠ” ë‰´ëŸ°ì„ ë³‘í•©í•œ í›„ì—ë„ ëª¨ë¸ì˜ ì›ë˜ ì„±ëŠ¥ì„ ìœ ì§€í•´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ **ë‰´ëŸ°ì„ ì„ íƒí•˜ëŠ” ê¸°ì¤€ê³¼ ë³‘í•© í›„ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•**ì´ ì¤‘ìš”í•˜ë‹¤.  
ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ **(1) ë‰´ëŸ° ìœ ì‚¬ë„ ì¸¡ì •, (2) ê°€ì¤‘ì¹˜ ë³‘í•©, (3) ë°ì´í„° í†µê³„ ë³´ì • (REPAIR)** ì˜ 3ë‹¨ê³„ë¥¼ ì ìš©í•œë‹¤.

---

### **1. ë‰´ëŸ° ë³‘í•©ì„ ìœ„í•œ ì„ íƒ ê¸°ì¤€**
ë‰´ëŸ° ë³‘í•© ì‹œ, ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **ìœ ì‚¬í•œ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒ**ì´ë‹¤.  
ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ **k-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ ë³‘í•©**í•œë‹¤.

#### **âœ… ë‰´ëŸ° ìœ ì‚¬ì„± ì¸¡ì • ë°©ë²•**
- ë‰´ëŸ°ì„ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•œ í›„, **L2 Distance(ìœ í´ë¦¬ë“œ ê±°ë¦¬)** ë¥¼ ì¸¡ì •í•˜ì—¬ ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤.
- ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ ê·¸ë£¹í™”í•œ í›„, ê°™ì€ ê·¸ë£¹ì— ì†í•˜ëŠ” ë‰´ëŸ°ì„ ë³‘í•©í•œë‹¤.

**ì˜ˆì œ (Layer1ì˜ ê°€ì¤‘ì¹˜ \( W \) )**  
ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•˜ì.

\[
W =
\begin{bmatrix}
0.9 & 0.1 & 0.2 & 0.3 \\
0.8 & 0.1 & 0.2 & 0.4 \\
0.2 & 0.7 & 0.8 & 0.1 \\
0.3 & 0.6 & 0.7 & 0.2
\end{bmatrix}
\]

- ì—¬ê¸°ì„œ **ì²« ë²ˆì§¸ ë‰´ëŸ°(í–‰ ë²¡í„°)ê³¼ ë‘ ë²ˆì§¸ ë‰´ëŸ°ì´ ê°€ì¥ ìœ ì‚¬**í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ëŠ”ë‹¤.
- í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì€ **k-meansë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ë‰´ëŸ° ë³‘í•© ê·¸ë£¹ì„ ìë™ìœ¼ë¡œ ê²°ì •**í•œë‹¤.

---

### **2. ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ ì¡°ì •**
ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‹¨ìˆœíˆ í‰ê·  ë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ëª¨ë¸ì˜ ë¶„ì‚°(variance)ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë³´ì •ì´ í•„ìš”í•˜ë‹¤.

#### **âœ… ë³‘í•©ëœ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ë²•**
- í´ëŸ¬ìŠ¤í„°ë§ëœ ë‰´ëŸ°ë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ **í‰ê· (mean) ì—°ì‚°**ìœ¼ë¡œ ë³‘í•©í•œë‹¤.
- ë§Œì•½ **ë‰´ëŸ° Aì™€ Bë¥¼ ë³‘í•©í•œë‹¤ê³  ê°€ì •í•˜ë©´**, ìƒˆë¡œìš´ ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.

\[
W' = \frac{W_A + W_B}{2}
\]

**ì˜ˆì œ: ë³‘í•©ëœ ê°€ì¤‘ì¹˜**
\[
W' =
\begin{bmatrix}
0.9 & 0.1 & 0.2 & 0.3 \\
\mathbf{(0.8 + 0.3) / 2} & \mathbf{(0.1 + 0.6) / 2} & \mathbf{(0.2 + 0.7) / 2} & \mathbf{(0.4 + 0.2) / 2} \\
0.2 & 0.7 & 0.8 & 0.1
\end{bmatrix}
\]

ì¦‰, **ìœ ì‚¬í•œ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í‰ê·  ë‚´ì–´ ì •ë³´ ì†ì‹¤ì„ ì¤„ì´ëŠ” ë°©ì‹**ìœ¼ë¡œ ì¡°ì •í•œë‹¤.

---

### **3. ë°ì´í„° í†µê³„ ë³´ì • (REPAIR ì ìš©)**
ë³‘í•© í›„ì—ëŠ” ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ **ë‰´ëŸ°ì˜ ì¶œë ¥(activation) í†µê³„ê°€ ë³€í•˜ëŠ” ë¬¸ì œ**ê°€ ë°œìƒí•œë‹¤.  
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” **REPAIR ê¸°ë²•(Fold-AR, Fold-DIR)ì„ ì ìš©í•˜ì—¬ ë°ì´í„° í†µê³„ë¥¼ ë³´ì •**í•œë‹¤.

#### **âœ… REPAIR ì ìš© ë°©ë²•**
- **Fold-AR (Approximate REPAIR)**: ë³‘í•©ëœ ë‰´ëŸ°ì˜ ë¶„ì‚°ì„ ë³´ì •í•˜ëŠ” **ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ \( \alpha \) ì ìš©**.
  \[
  \hat{z}_l(c) = z_l(c) \cdot \frac{N_c}{\sqrt{N_c + (N_c^2 - N_c)E[c]}}
  \]
  - \( N_c \): í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ë‰´ëŸ° ìˆ˜
  - \( E[c] \): í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìƒê´€ê´€ê³„(Mean Correlation)

- **Fold-DIR (Deep Inversion Repair)**:  
  - Deep Inversion ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ **ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ BatchNorm í†µê³„ë¥¼ ë³´ì •**.
  - ëª¨ë¸ì˜ BatchNorm ë ˆì´ì–´ê°€ ì €ì¥í•œ í‰ê·  ë° ë¶„ì‚° ê°’ì„ ì—…ë°ì´íŠ¸.

---

### **ğŸ“Œ ìµœì¢… ì •ë¦¬**
| ë‹¨ê³„                 | ë‚´ìš©                                            | ì ìš© ê¸°ë²•         |
| -------------------- | ----------------------------------------------- | ----------------- |
| **ë‰´ëŸ° ìœ ì‚¬ë„ ë¶„ì„** | ë‰´ëŸ° ê°„ L2 Distance ì¸¡ì •í•˜ì—¬ k-means í´ëŸ¬ìŠ¤í„°ë§ | k-means           |
| **ë‰´ëŸ° ë³‘í•©**        | ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‰´ëŸ°ì„ í‰ê· ë‚´ì–´ ë³‘í•©              | Mean ì—°ì‚°         |
| **ë°ì´í„° í†µê³„ ë³´ì •** | ë³‘í•© í›„ ë¶„ì‚° ë¶•ê´´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ë³´ì •          | Fold-AR, Fold-DIR |

â¡ **ê²°ë¡ ì ìœ¼ë¡œ, Model Foldingì€ ìœ ì‚¬í•œ ë‰´ëŸ°ì„ ì„ íƒí•œ í›„, í‰ê·  ì—°ì‚°ìœ¼ë¡œ ë³‘í•©í•˜ë©°, ë¶„ì‚° ë³´ì •ì„ í†µí•´ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•œë‹¤!** ğŸš€

# Q : Model Foldingì´ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•(IFM, INN ë“±)ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì´ë©°, íŠ¹ì • ìƒí™©(ì˜ˆ: LLaMA-7B ì ìš© ì‹œ)ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?  

# A: 

## **1. Model Foldingì´ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•(IFM, INN ë“±)ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ì´ìœ **
Model Foldingì€ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ë“¤(IFM, INN ë“±)ê³¼ ë¹„êµí–ˆì„ ë•Œ **ì¼ê´€ë˜ê²Œ ë” ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€**í•œë‹¤.  
ê·¸ ì´ìœ ëŠ” ë‹¤ìŒì˜ 3ê°€ì§€ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ ë•ë¶„ì´ë‹¤.

### **âœ… 1) ë‰´ëŸ°ì„ êµ¬ì¡°ì ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”**
- ê¸°ì¡´ **IFM (Iterative Feature Merging, Chen et al., 2023)** ê¸°ë²•ì€ **ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ë³‘í•©**í•˜ë©°, INN(Integral Neural Networks, Solodskikh et al., 2023)ì€ ë‹¨ìˆœíˆ íŠ¹ì • ê¸°ì¤€ìœ¼ë¡œ ë‰´ëŸ°ì„ ì‚­ì œ(pruning)í•œë‹¤.
- ë°˜ë©´, **Model Foldingì€ k-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ì—¬ ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•œ í›„, ê°€ì¥ ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ë§Œ ë³‘í•©**í•˜ë¯€ë¡œ, **ë³‘í•© í›„ì—ë„ ì›ë˜ì˜ í‘œí˜„ë ¥ì„ ìµœëŒ€í•œ ë³´ì¡´**í•  ìˆ˜ ìˆë‹¤.

> **ğŸ”¹ ì°¨ì´ì  ì •ë¦¬**
> - **IFM**: ë‰´ëŸ° ë³‘í•© ì‹œ ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ë§ë§Œ ì ìš© â†’ ë°ì´í„° ë¶„ì‚°ì´ ê¹¨ì§ˆ ê°€ëŠ¥ì„± ë†’ìŒ.
> - **INN**: ë‰´ëŸ°ì„ ì œê±°í•˜ëŠ” êµ¬ì¡° â†’ ì •ë³´ ì†ì‹¤ì´ ì»¤ì„œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥.
> - **Model Folding**: ë‰´ëŸ° ë³‘í•© ì‹œ **L2 Distance ê¸°ë°˜ k-means í´ëŸ¬ìŠ¤í„°ë§**ì„ í™œìš©í•˜ì—¬ **ìµœì ì˜ ë³‘í•© êµ¬ì¡°**ë¥¼ ìƒì„±í•¨.

---

### **âœ… 2) ë°ì´í„° í†µê³„ ë³´ì • (REPAIR) ê¸°ë²• ì ìš©**
- ê¸°ì¡´ IFM, INN ê¸°ë²•ë“¤ì€ ë‰´ëŸ°ì„ ë³‘í•©í•œ í›„ì—ë„ **ì¶œë ¥ ë¶„ì‚°(variance)ì˜ ë³€í™”**ë¥¼ ë³´ì •í•˜ëŠ” ê³¼ì •ì´ ì—†ìŒ.
- Model Foldingì€ ë‰´ëŸ°ì„ ë³‘í•©í•œ í›„ **Fold-AR (Approximate REPAIR) ë˜ëŠ” Fold-DIR (Deep Inversion Repair)** ê¸°ë²•ì„ í†µí•´ **ë‰´ëŸ° í™œì„±í™” ê°’ì˜ ë¶„ì‚°ì„ ì¡°ì •**í•˜ì—¬ ë°ì´í„° í†µê³„ë¥¼ ìœ ì§€í•¨.
- ì´ë¥¼ í†µí•´ **BatchNorm í†µê³„ ë³€í™”ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŒ**.

> **ğŸ”¹ REPAIR ê¸°ë²• ë¹„êµ**
> - **IFM, INN**: ë‰´ëŸ° ë³‘í•© í›„ ë°ì´í„° í†µê³„ê°€ ë³€í™”í•´ë„ ë³´ì • ê³¼ì • ì—†ìŒ â†’ ì„±ëŠ¥ ì €í•˜ ë°œìƒ.
> - **Model Folding**: **Fold-AR**ì„ ì ìš©í•´ **ë°ì´í„° ì—†ì´ë„ ë¶„ì‚° ìœ ì§€** + **Fold-DIR**ì„ í†µí•´ BatchNorm í†µê³„ë¥¼ ë³µì›.

---

### **âœ… 3) ê³ ìŠ¤íŒŒì‹œí‹°(High Sparsity)ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ ìœ ì§€**
- ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ë“¤ì€ **ìŠ¤íŒŒì‹œí‹°(Sparsity, ë‰´ëŸ° ì œê±°ìœ¨)ê°€ ë†’ì„ìˆ˜ë¡ ê¸‰ê²©íˆ ì„±ëŠ¥ì´ ì €í•˜**ëœë‹¤.
- í•˜ì§€ë§Œ Model Foldingì€ ë‰´ëŸ°ì„ ì‚­ì œí•˜ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¼, **ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ ë³‘í•©í•˜ì—¬ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìœ ì§€**í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ,  
  **ìŠ¤íŒŒì‹œí‹°ê°€ ë†’ì•„ì ¸ë„ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤**.

> **ğŸ”¹ ì„±ëŠ¥ ë¹„êµ (ResNet18, CIFAR10)**
> - **IFM (50% Sparsity)** â†’ Accuracy **84.23%**
> - **INN (50% Sparsity)** â†’ Accuracy **76.85%**
> - **Model Folding (50% Sparsity)** â†’ **Accuracy 89.75%**
> 
> **â¡ Model Foldingì´ ê³ ìŠ¤íŒŒì‹œí‹°ì—ì„œë„ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ì´ìœ ëŠ”, ë‹¨ìˆœí•œ ë‰´ëŸ° ì‚­ì œê°€ ì•„ë‹ˆë¼, ë‰´ëŸ° ê°„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë³‘í•©í•˜ê¸° ë•Œë¬¸!** ğŸš€

---

## **2. íŠ¹ì • ìƒí™©(ì˜ˆ: LLaMA-7B ì ìš© ì‹œ)ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ì´ìœ **
Model Foldingì€ ResNet, VGG ê°™ì€ **CNN(Convolutional Neural Network) ê¸°ë°˜ ëª¨ë¸ì—ì„œëŠ” ë§¤ìš° íš¨ê³¼ì **ì´ì§€ë§Œ,  
LLaMA-7B ê°™ì€ **Transformer ê¸°ë°˜ ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ì´ ì¼ë¶€ ì €í•˜**ëœë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

### **ğŸš¨ 1) Multi-Head Attention êµ¬ì¡°ì—ì„œ ë³‘í•© ê¸°ì¤€ì´ ëª¨í˜¸í•¨**
- Transformer ëª¨ë¸ì€ **Multi-Head Attention(MHA)** êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.  
  **ê° Attention HeadëŠ” ë‹¤ë¥¸ ì •ë³´ë¥¼ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, ë‹¨ìˆœí•œ ë‰´ëŸ° ë³‘í•©ì´ ëª¨ë¸ ì„±ëŠ¥ì„ í¬ê²Œ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤.**
- CNN ëª¨ë¸ì—ì„œëŠ” **ìœ ì‚¬í•œ í•„í„°(Feature Map)ë¥¼ ë³‘í•©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥**í•˜ì§€ë§Œ,  
  **Attention HeadëŠ” íŠ¹ì • í† í° ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” êµ¬ì¡°ì´ë¯€ë¡œ, ì˜ëª»ëœ ë³‘í•©ì´ ì •ë³´ ì†ì‹¤ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ**.

> **ğŸ”¹ ê¸°ì¡´ CNN ëª¨ë¸ê³¼ Transformerì˜ ì°¨ì´**
> - **CNN (ResNet, VGG ë“±)**: í•„í„°(ë‰´ëŸ°) ê°„ ìœ ì‚¬í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ë¯€ë¡œ ë³‘í•©ì´ ìš©ì´.
> - **Transformer (LLaMA-7B ë“±)**: ê° Attention Headê°€ ì„œë¡œ ë‹¤ë¥¸ ê´€ê³„ë¥¼ í•™ìŠµ â†’ ë³‘í•©ì´ ì–´ë µê³  ì •ë³´ ì†ì‹¤ ë°œìƒ.

---

### **ğŸš¨ 2) Residual Connectionì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ë‰´ëŸ° ë³‘í•©**
- Transformer ëª¨ë¸ì—ëŠ” **Residual Connectionì´ ë§¤ìš° ë§ë‹¤**.  
  í•˜ì§€ë§Œ Model Foldingì€ CNN ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì—, **Residual Pathì—ì„œì˜ ë°ì´í„° íë¦„ì„ ê³ ë ¤í•˜ì§€ ì•Šê³  ë‰´ëŸ°ì„ ë³‘í•©í•˜ë©´ ì„±ëŠ¥ì´ ì €í•˜**ë  ìˆ˜ ìˆë‹¤.

> **ğŸ”¹ Residual Connectionì´ Model Foldingì— ë¯¸ì¹˜ëŠ” ì˜í–¥**
> - CNNì—ì„œëŠ” ë‹¨ìˆœíˆ í•„í„°ë¥¼ ë³‘í•©í•˜ë©´ ë¨.
> - Transformerì—ì„œëŠ” ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ”ë°, Residual Connectionì´ ìˆëŠ” ìƒíƒœì—ì„œ ë¬´ì‘ìœ„ ë‰´ëŸ° ë³‘í•©ì„ ìˆ˜í–‰í•˜ë©´ ë°ì´í„° íë¦„ì´ ê¹¨ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.

---

### **ğŸš¨ 3) LLaMA-7Bì—ëŠ” BatchNormì´ ì—†ì–´ REPAIR ê¸°ë²• ì ìš©ì´ ì–´ë ¤ì›€**
- Model Foldingì´ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” í•µì‹¬ ê¸°ë²• ì¤‘ í•˜ë‚˜ëŠ” **REPAIR(Fold-AR, Fold-DIR)ë¥¼ í†µí•œ BatchNorm ë³´ì •**.
- í•˜ì§€ë§Œ Transformer ê³„ì—´ ëª¨ë¸(LLaMA, GPT ë“±)ì—ëŠ” **BatchNormì´ ì•„ë‹ˆë¼ LayerNormì„ ì‚¬ìš©**í•˜ê¸° ë•Œë¬¸ì—,  
  **REPAIR ê¸°ë²•ì´ ê·¸ëŒ€ë¡œ ì ìš©ë˜ì§€ ì•Šì•„ ì„±ëŠ¥ì´ ì €í•˜ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ**.

> **ğŸ”¹ BatchNorm vs. LayerNorm ì°¨ì´**
> - CNN ê¸°ë°˜ ëª¨ë¸ (ResNet, VGG ë“±) â†’ **BatchNorm ì ìš© ê°€ëŠ¥** â†’ Model Folding ì„±ëŠ¥ ìœ ì§€
> - Transformer ê¸°ë°˜ ëª¨ë¸ (LLaMA, GPT ë“±) â†’ **BatchNorm ì—†ìŒ (LayerNorm ì‚¬ìš©)** â†’ ê¸°ì¡´ REPAIR ê¸°ë²• ì ìš©ì´ ì–´ë ¤ì›€

---

## **3. ê²°ë¡ **
Model Foldingì´ ê¸°ì¡´ ë°ì´í„° í”„ë¦¬ ëª¨ë¸ ì••ì¶• ê¸°ë²•ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ì´ìœ :
1. **ë‰´ëŸ° ë³‘í•© ì‹œ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤(k-means)í•˜ì—¬ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”**  
2. **ë°ì´í„° ì—†ì´ë„ í†µê³„ ë³´ì •(REPAIR ê¸°ë²•)ìœ¼ë¡œ ë¶„ì‚° ìœ ì§€**  
3. **ê³ ìŠ¤íŒŒì‹œí‹°ì—ì„œë„ ì„±ëŠ¥ ì €í•˜ê°€ ì ìŒ(ë‰´ëŸ° ì‚­ì œê°€ ì•„ë‹ˆë¼ ë³‘í•© ë°©ì‹ì´ë¯€ë¡œ)**  

í•˜ì§€ë§Œ **LLaMA-7B ê°™ì€ Transformer ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ì´ ë‹¤ì†Œ ì €í•˜**ë˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ”:
1. **Multi-Head Attentionì—ì„œ ë‰´ëŸ° ë³‘í•© ê¸°ì¤€ì´ ëª…í™•í•˜ì§€ ì•ŠìŒ**  
2. **Residual Connectionì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ë‹¨ìˆœí•œ ë³‘í•©ì€ ì •ë³´ ì†ì‹¤ì„ ì´ˆë˜í•  ê°€ëŠ¥ì„±**  
3. **BatchNormì´ ì—†ëŠ” Transformer ëª¨ë¸ì—ì„œëŠ” ê¸°ì¡´ REPAIR ê¸°ë²• ì ìš©ì´ ì–´ë ¤ì›€**  

â¡ **ê²°ë¡ ì ìœ¼ë¡œ, Model Foldingì„ Transformer ê¸°ë°˜ ëª¨ë¸ì— ì ìš©í•˜ë ¤ë©´ Multi-Head Attentionê³¼ Residual Connectionì„ ê³ ë ¤í•œ ìƒˆë¡œìš´ ë³‘í•© ë°©ë²•ì´ í•„ìš”í•˜ë‹¤!** ğŸš€

# Q : Model Foldingì„ Transformer ê¸°ë°˜ ëª¨ë¸(ì˜ˆ: LLaMA, GPT)ì— ì ìš©í•˜ë ¤ë©´ Multi-Head Attention êµ¬ì¡°ì—ì„œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë³‘í•©ì„ ìˆ˜í–‰í•´ì•¼ í•˜ë©°, Residual Connectionì´ ìˆëŠ” êµ¬ì¡°ì—ì„œëŠ” ì–´ë–¤ ë³´ì •ì´ í•„ìš”í• ê¹Œ?  

# A: 

## **1. Transformer ê¸°ë°˜ ëª¨ë¸(ì˜ˆ: LLaMA, GPT)ì—ì„œ Model Folding ì ìš©ì˜ ì–´ë ¤ì›€**
Model Foldingì€ ê¸°ì¡´ CNN ê¸°ë°˜ ëª¨ë¸(ResNet, VGG)ì—ì„œ **ë‰´ëŸ° ê°„ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©**í•˜ì—¬ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ê¸°ë²•ì´ë‹¤.  
í•˜ì§€ë§Œ Transformer ê¸°ë°˜ ëª¨ë¸(LLaMA, GPT ë“±)ì€ **Multi-Head Attention(MHA)ê³¼ Residual Connectionì„ í¬í•¨í•˜ê³  ìˆì–´ ê¸°ì¡´ Model Folding ê¸°ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸° ì–´ë µë‹¤**.

Transformerì— Model Foldingì„ ì ìš©í•˜ë ¤ë©´ **ë‹¤ìŒ ë‘ ê°€ì§€ í•µì‹¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤.**
1. **Multi-Head Attentionì—ì„œ Head ê°„ ë³‘í•©ì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•  ê²ƒì¸ê°€?**
2. **Residual Connectionì„ ê³ ë ¤í•œ ë³‘í•© í›„ ë³´ì • ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?**

---

## **2. Multi-Head Attention êµ¬ì¡°ì—ì„œ Model Folding ì ìš© ë°©ë²•**
### **ğŸ“Œ ê¸°ì¡´ Model Foldingì´ ì ìš©ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ **
Transformerì˜ Multi-Head Attention(MHA) êµ¬ì¡°ëŠ” **ê° Headê°€ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ í‘œí˜„ì„ í•™ìŠµ**í•œë‹¤.  
ë”°ë¼ì„œ **ë‹¨ìˆœí•œ ë‰´ëŸ° ë³‘í•©ì´ ì •ë³´ ì†ì‹¤ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒ**.

> **ğŸ”¹ CNN ëª¨ë¸ì—ì„œëŠ”?**  
> - ê°™ì€ ì¸µ(layer) ë‚´ì—ì„œ í•„í„°(ë‰´ëŸ°) ê°„ ìœ ì‚¬í•œ íŠ¹ì§•ì„ í•™ìŠµ â†’ ìœ ì‚¬í•œ ë‰´ëŸ°ë¼ë¦¬ ë³‘í•© ê°€ëŠ¥.  
> 
> **ğŸ”¹ Transformerì—ì„œëŠ”?**  
> - Attention Headë§ˆë‹¤ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ íŒ¨í„´(ì–´í…ì…˜ ê°€ì¤‘ì¹˜)ì„ í•™ìŠµ â†’ ë¬´ì‘ìœ„ ë³‘í•© ì‹œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥.  
> - íŠ¹ì • HeadëŠ” ì¤‘ìš”í•œ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , íŠ¹ì • HeadëŠ” ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ.

### **âœ… í•´ê²° ë°©ë²• 1: Attention Head ê°„ ìœ ì‚¬ì„± ë¶„ì„ í›„ ë³‘í•©**
1. **Head ê°„ ìœ ì‚¬ë„ ê³„ì‚°**  
   - ê° Attention Headì˜ ê°€ì¤‘ì¹˜(W_q, W_k, W_v)ë¥¼ ë²¡í„°í™”í•˜ì—¬ L2 Distanceë¡œ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•œë‹¤.
   - Head ê°„ ìœ ì‚¬ë„ê°€ ë†’ì€ ê²½ìš°ì—ë§Œ ë³‘í•©ì„ ìˆ˜í–‰.

2. **ë³‘í•©í•  Head ì„ íƒ ë° í‰ê· í™”**  
   - ë³‘í•© ëŒ€ìƒ Headë¥¼ ì°¾ì€ í›„, **ê°€ì¤‘ì¹˜ë¥¼ í‰ê· í™”í•˜ì—¬ í•˜ë‚˜ì˜ Headë¡œ ëŒ€ì²´**í•œë‹¤.
   - ì˜ˆë¥¼ ë“¤ì–´, 8ê°œì˜ Head ì¤‘ 2ê°œê°€ ìœ ì‚¬í•˜ë‹¤ë©´ **6ê°œì˜ Head + 1ê°œì˜ ë³‘í•©ëœ Head = 7ê°œë¡œ ì••ì¶•**.

3. **Residual Scaling ì ìš©**  
   - ë³‘í•© í›„ì—ë„ Attentionì˜ ì¶œë ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´, ë³‘í•©ëœ Headì˜ ì¶œë ¥ì„ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ì›ë˜ì˜ ì´ ê°€ì¤‘ì¹˜ë¥¼ ìœ ì§€.

> **ğŸ”¹ ì˜ˆì œ**
> - ì›ë˜ 8ê°œì˜ Attention Headê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •.
> - (Head 1, Head 3)ì˜ ìœ ì‚¬ì„±ì´ ë†’ì•„ ë³‘í•©.
> - ìƒˆë¡œìš´ ë³‘í•©ëœ Headì˜ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°:
>   \[
>   W_q' = \frac{W_q^{(1)} + W_q^{(3)}}{2}, \quad
>   W_k' = \frac{W_k^{(1)} + W_k^{(3)}}{2}, \quad
>   W_v' = \frac{W_v^{(1)} + W_v^{(3)}}{2}
>   \]
> - **ê²°ê³¼ì ìœ¼ë¡œ Attention Headì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ë©´ì„œ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”**.

### **âœ… í•´ê²° ë°©ë²• 2: Low-Rank Decompositionì„ í™œìš©í•œ ë³‘í•©**
- Transformerì—ì„œ ê° Headì˜ ê°€ì¤‘ì¹˜ëŠ” í° í–‰ë ¬(W_q, W_k, W_v)ë¡œ í‘œí˜„ë¨.
- ê° Headì˜ ê°€ì¤‘ì¹˜ë¥¼ **ì €ì°¨ì› ê·¼ì‚¬(Low-Rank Approximation, SVD ë“±)ë¡œ ë³‘í•©**í•˜ë©´ ì •ë³´ ì†ì‹¤ì„ ì¤„ì¼ ìˆ˜ ìˆìŒ.

> **ğŸ”¹ ì ìš© ë°©ì‹**
> - \( W_q, W_k, W_v \) í–‰ë ¬ì„ ê°ê° **SVD(Singular Value Decomposition) ë˜ëŠ” PCA(Principal Component Analysis)ë¡œ ë¶„í•´**.
> - **ì£¼ìš” ì •ë³´ë§Œ ìœ ì§€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì„**.

### **âœ… í•´ê²° ë°©ë²• 3: Pruningê³¼ ê²°í•©í•˜ì—¬ ì¤‘ìš”í•˜ì§€ ì•Šì€ Head ì œê±°**
- ì—°êµ¬ì— ë”°ë¥´ë©´, **Transformer ëª¨ë¸ì˜ ì¼ë¶€ Attention HeadëŠ” ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ**(Voita et al., 2019).
- ë”°ë¼ì„œ **ê°€ì¥ ì˜í–¥ë ¥ì´ ë‚®ì€ Headë¥¼ ì œê±°í•˜ê³ , ë‚˜ë¨¸ì§€ ì¤‘ìš”í•œ Headë¼ë¦¬ ë³‘í•©**í•˜ëŠ” ë°©ì‹ë„ ê°€ëŠ¥.

> **ğŸ”¹ ì ìš© ë°©ì‹**
> - ê° Headì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€(ì–´í…ì…˜ í™œì„±í™” ê°’ ê¸°ë°˜).
> - ì¤‘ìš”ë„ê°€ ë‚®ì€ Headë¥¼ ì‚­ì œí•˜ê³ , ë‚¨ì€ Headë¥¼ ë³‘í•©.

---

## **3. Residual Connectionì´ ìˆëŠ” êµ¬ì¡°ì—ì„œ Model Folding ì ìš© ë°©ë²•**
### **ğŸ“Œ Residual Connectionì´ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ” ì´ìœ **
- Transformer ëª¨ë¸ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ **Residual Connectionì´ í¬í•¨**ë˜ì–´ ìˆìŒ.
- ë‹¨ìˆœíˆ ë‰´ëŸ°ì„ ë³‘í•©í•˜ë©´ **Residual Pathë¥¼ í†µê³¼í•˜ëŠ” ì •ë³´ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ**.

> **ğŸ”¹ CNNì—ì„œëŠ”?**  
> - ë‹¨ìˆœí•œ í•„í„°(ë‰´ëŸ°) ë³‘í•©ì´ ê°€ëŠ¥.
> 
> **ğŸ”¹ Transformerì—ì„œëŠ”?**  
> - Residual Pathë¥¼ í†µê³¼í•˜ëŠ” ì •ë³´ê°€ ë§ì•„ ë³‘í•© ì‹œ ì •ë³´ ì™œê³¡ ê°€ëŠ¥.

### **âœ… í•´ê²° ë°©ë²• 1: Residual Weight Scaling**
- ë³‘í•©ëœ ë‰´ëŸ°ì´ Residual Connectionì„ í†µê³¼í•  ë•Œ, **ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ì¡°ì •**í•´ì•¼ í•œë‹¤.
- ë³‘í•©ëœ ë‰´ëŸ°ì˜ ì¶œë ¥ì„ ì›ë˜ì˜ í•©ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ë¥¼ ì ìš©.

> **ğŸ”¹ ì˜ˆì œ**
> - Residual Connectionì´ ìˆëŠ” ê²½ìš°, ë³‘í•©ëœ ë‰´ëŸ°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³´ì •:
>   \[
>   W' = \alpha \cdot \frac{W_1 + W_2}{2}
>   \]
> - ì—¬ê¸°ì„œ **\( \alpha \)ëŠ” ë³‘í•©ëœ ë‰´ëŸ°ì´ Residual Pathì—ì„œ ì›ë˜ ì •ë³´ëŸ‰ì„ ìœ ì§€í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜**.

### **âœ… í•´ê²° ë°©ë²• 2: Layer-wise Adaptive Scaling ì ìš©**
- Residual Connectionì´ ìˆëŠ” ì¸µë§ˆë‹¤ **ìë™ìœ¼ë¡œ ìµœì ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ì‹**.
- ê° ì¸µì˜ ì¶œë ¥ì„ ë¶„ì„í•˜ì—¬, ë³‘í•©ëœ ë‰´ëŸ°ì´ Residual Pathì—ì„œ ì˜í–¥ì„ ìµœì†Œí™”í•˜ë„ë¡ ë³´ì •.

---

## **4. Model Foldingì„ Transformerì— ì ìš©í•˜ê¸° ìœ„í•œ ìµœì¢… ì „ëµ**
| ë¬¸ì œ                                                 | í•´ê²° ë°©ë²•                                                                  |
| ---------------------------------------------------- | -------------------------------------------------------------------------- |
| **Multi-Head Attentionì—ì„œ ë‰´ëŸ° ë³‘í•©ì´ ì–´ë µë‹¤**      | Head ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œ í›„ ìœ ì‚¬í•œ Headë§Œ ë³‘í•© (L2 Distance, PCA, SVD í™œìš©) |
| **Residual Connectionì´ ìˆëŠ” ê²½ìš° ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„±** | ë³‘í•©ëœ ê°€ì¤‘ì¹˜ì— **Residual Scaling ì ìš©** (ì¶œë ¥ í¬ê¸° ìœ ì§€)                 |
| **Attention Head ì¤‘ ì¼ë¶€ëŠ” ë¶ˆí•„ìš”**                  | ì¤‘ìš”ë„ê°€ ë‚®ì€ Headë¥¼ Pruningí•œ í›„ ë³‘í•©                                     |
| **BatchNorm ëŒ€ì‹  LayerNorm ì‚¬ìš©**                    | LayerNormì„ ê³ ë ¤í•œ ë°ì´í„° í†µê³„ ë³´ì • ë°©ì‹ í•„ìš”                              |

---

## **5. ê²°ë¡ **
Transformer ëª¨ë¸ì—ì„œ Model Foldingì„ ì ìš©í•˜ë ¤ë©´ **Multi-Head Attentionê³¼ Residual Connectionì„ ê³ ë ¤í•œ íŠ¹í™”ëœ ë³‘í•© ë°©ë²•ì´ í•„ìš”**í•˜ë‹¤.

**â¡ Multi-Head Attentionì—ì„œ ë³‘í•©í•˜ëŠ” ë°©ë²•**
1. **Head ê°„ ìœ ì‚¬ì„± ë¶„ì„ í›„ ë³‘í•© (L2 Distance, PCA, SVD í™œìš©)**
2. **Low-Rank Decomposition (SVD, PCA)ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê·¼ì‚¬**
3. **ì¤‘ìš”í•˜ì§€ ì•Šì€ Headë¥¼ Pruning í›„ ë³‘í•©**

**â¡ Residual Connectionì´ ìˆëŠ” ê²½ìš° ë³´ì • ë°©ë²•**
1. **ë³‘í•©ëœ ë‰´ëŸ°ì˜ ì¶œë ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ Residual Scaling ì ìš©**
2. **Layer-wise Adaptive Scalingì„ ì ìš©í•˜ì—¬ ìµœì ì˜ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ ê²°ì •**

ì´ëŸ¬í•œ ê¸°ë²•ì„ ì ìš©í•˜ë©´, **Transformer ê¸°ë°˜ ëª¨ë¸ì—ì„œë„ Model Foldingì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ê²ƒ**ì´ë‹¤! ğŸš€