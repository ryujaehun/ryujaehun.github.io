---
title: "FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs"
date: 2024-12-23
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2410.16663v1)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

### 결론 요약
논문 *"FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs for Efficient Inference"*은 FlashAttention 시리즈의 성능을 NPU와 저자원 GPU로 확장하며, 초저자원 환경에서 Transformer 기반 모델의 추론 성능을 극적으로 향상시키는 새로운 기법을 제안합니다. FastAttention은 기존 FlashAttention 대비 **최대 15배 속도 향상**을 달성하며, 초장문 입력(최대 256K 토큰)까지도 효율적으로 처리할 수 있도록 설계되었습니다. 핵심 기여로는 **NPU와 GPU에 최적화된 새로운 타일링 전략**, **CPU-GPU 협업 전략**, 그리고 **메모리 효율성을 극대화하는 mask 생성 기법**이 포함됩니다.

---

### 강점과 독창성
1. **다양한 하드웨어 환경 지원**: 
   - 기존 FlashAttention은 고성능 GPU(Ampere, Hopper 등)에서만 동작하지만, FastAttention은 NPU(Ascend)와 저자원 GPU(Volta 기반)에서도 효율적으로 동작합니다.
   - 새로운 데이터 배치 설계와 SIMD 명령어 최적화를 통해 Volta 기반 GPU에서도 성능을 극대화했습니다.

2. **효율적인 타일링 전략**:
   - **이중 타일링(two-level tiling)**을 통해 NPU에서 동기화 오버헤드를 줄이고 L1/L2 캐시 활용도를 극대화합니다.
   - **Tiling-AllReduce** 전략을 통해 다중 NPU 환경에서 통신 오버헤드를 줄였습니다.

3. **CPU-GPU 협업 전략**:
   - 초장문 추론에서 GPU 메모리가 부족할 경우, KV 캐시를 CPU로 오프로드하여 GPU와 CPU가 협력하여 연산을 수행함으로써 처리 가능 토큰 수를 대폭 증가시켰습니다.

4. **압도적인 성능**:
   - **NPU**: 표준 Attention 대비 10.7배 속도 향상, Throughput 5.16배 증가.
   - **Volta GPU**: 기존 FlashAttention(xformers) 대비 최대 1.43배 성능 향상.

---

### 핵심 알고리즘: FastAttention의 동작 과정
아래는 FastAttention의 NPU 적용 과정과 이를 기반으로 예시를 설명합니다.

#### **1. 입력**
- 쿼리 \(Q\), 키 \(K\), 값 \(V\) 매트릭스를 입력으로 받습니다.
- 시퀀스 길이 \(S = 4\), 헤드 수 \(N = 2\)를 가정합니다.

#### **2. 단계별 과정**
1. **블록 분할 및 타일링**:
   - 이중 타일링 전략에 따라 \(Q\), \(K\), \(V\)를 블록 단위로 나눕니다. 
   - 1차 타일링에서는 큰 블록으로 메모리 접근을 최적화하고, 2차 타일링에서 작은 블록으로 세부 연산을 진행합니다.

2. **Softmax 계산**:
   - \(Q \times K^T\)의 연산을 블록 단위로 병렬 처리.
   - Softmax를 각 블록에서 계산한 후, 결과를 다음 단계로 전달.

3. **결과 결합**:
   - \(V\) 매트릭스를 곱하여 최종 Attention 출력을 생성.
   - 모든 블록 연산이 끝난 뒤 출력 결과를 합산.

#### **3. 성능 최적화**
- **L1/L2 캐시 활용**: 타일 크기 조정으로 메모리 대역폭 최적화.
- **동기화 오버헤드 최소화**: Cube 및 Vector 유닛 간의 파이프라인 설계.
- **메모리 절약**: \(M\)-mask 기반으로 Attention Mask 생성, 메모리 사용량 99% 절감.

---

### 한계점
1. **아키텍처 종속성**:
   - Volta 기반 GPU에 대해 특별한 데이터 배치 설계가 필요하며, 다른 저자원 GPU에서는 추가적인 최적화가 필요할 가능성이 큽니다.

2. **소프트웨어 호환성**:
   - 기존 모델에 FastAttention을 통합하려면 상당한 코드 수정이 필요할 수 있습니다.

3. **하드웨어 제약**:
   - FastAttention은 특정 하드웨어 특성을 최대한 활용하지만, 더 오래된 GPU나 특정 NPU 아키텍처에서는 효과가 제한적일 수 있습니다.

---

### 결론
FastAttention은 기존 FlashAttention을 NPU와 저자원 GPU로 성공적으로 확장하며, 초장문 입력 처리와 추론 속도에서 탁월한 성능을 보였습니다. 이 연구는 딥러닝 추론 환경에서 하드웨어 최적화를 논의할 때 중요한 참고 자료가 될 것입니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

### FastAttention 핵심 알고리즘 상세 설명

FastAttention은 FlashAttention 알고리즘을 기반으로 하여 NPU(Ascend) 및 저자원 GPU(Volta 기반)에서 최적의 성능을 발휘하도록 설계되었습니다. 아래는 알고리즘의 주요 과정을 예제와 함께 단계별로 설명하고 정리한 것입니다.

---

### 예제 입력
1. **입력 데이터**:
   - \( Q \): Query Matrix, \( S \times D \) 크기 (\( S = 4 \), \( D = 2 \))
   - \( K \): Key Matrix, \( S \times D \)
   - \( V \): Value Matrix, \( S \times D \)

   ```
   Q = [[1, 0], [0, 1], [1, 1], [0, 0]]
   K = [[1, 1], [0, 1], [1, 0], [0, 0]]
   V = [[1, 2], [3, 4], [5, 6], [7, 8]]
   ```

2. **목표**:
   Attention 매커니즘을 이용해 \( Q \), \( K \), \( V \)의 연산 결과 \( \text{Attention Output} \)을 구합니다.

---

### 단계별 과정

#### **1. 타일링(Tiling)**
- **이중 타일링 전략**:
  - \( Q, K, V \)를 작은 블록 단위로 분할해 처리합니다.
  - 1차 타일링: 큰 블록을 사용해 메모리 접근을 최적화.
  - 2차 타일링: 작은 블록으로 나누어 세부 계산 수행.

예를 들어, \( S = 4 \)를 두 개의 블록으로 나눕니다:
- 블록 1: \( Q_1 = [[1, 0], [0, 1]] \), \( K_1 = [[1, 1], [0, 1]] \), \( V_1 = [[1, 2], [3, 4]] \)
- 블록 2: \( Q_2 = [[1, 1], [0, 0]] \), \( K_2 = [[1, 0], [0, 0]] \), \( V_2 = [[5, 6], [7, 8]] \)

---

#### **2. \( Q \times K^T \) 연산**
각 블록에서 Query \( Q \)와 Key \( K \)의 전치행렬 \( K^T \)를 곱합니다:
1. 블록 1:
   \( Q_1 \times K_1^T \):
   \[
   [[1, 0], [0, 1]] \times [[1, 0], [1, 1]] = [[1, 1], [0, 1]]
   \]
2. 블록 2:
   \( Q_2 \times K_2^T \):
   \[
   [[1, 1], [0, 0]] \times [[1, 0], [0, 0]] = [[1, 0], [0, 0]]
   \]

---

#### **3. Softmax 계산**
Softmax를 사용해 Attention 점수를 정규화합니다:
1. 블록 1:
   \[
   \text{Softmax}([[1, 1], [0, 1]]) = [[0.5, 0.5], [0.27, 0.73]]
   \]
2. 블록 2:
   \[
   \text{Softmax}([[1, 0], [0, 0]]) = [[0.73, 0.27], [0.5, 0.5]]
   \]

---

#### **4. \( \text{Softmax}(Q \times K^T) \times V \) 연산**
Softmax 결과와 Value \( V \)를 곱합니다:
1. 블록 1:
   \[
   [[0.5, 0.5], [0.27, 0.73]] \times [[1, 2], [3, 4]] = [[2.0, 3.0], [2.46, 3.46]]
   \]
2. 블록 2:
   \[
   [[0.73, 0.27], [0.5, 0.5]] \times [[5, 6], [7, 8]] = [[5.54, 6.54], [6.0, 7.0]]
   \]

---

#### **5. 결과 병합**
모든 블록의 결과를 병합하여 최종 Attention 출력을 얻습니다:
\[
\text{Output} = [[2.0, 3.0], [2.46, 3.46], [5.54, 6.54], [6.0, 7.0]]
\]

---

### FastAttention의 최적화 포인트
1. **효율적인 타일링**:
   - L1/L2 캐시 활용을 극대화하며 동기화를 최소화.
2. **Tiling-Mask**:
   - 대규모 \( \text{attention\_mask} \)를 작은 블록 단위로 대체하여 메모리 사용량을 99% 감소.
3. **CPU-GPU 협력**:
   - GPU 메모리가 부족한 경우 CPU의 메모리와 연산 자원을 활용하여 초장문 입력(256K 토큰)을 처리 가능.

---

### 요약
FastAttention은 타일링 전략과 효율적인 연산 구조를 활용하여, 제한된 자원에서도 높은 성능과 메모리 효율성을 제공합니다. 이 알고리즘은 특히 NPU와 저자원 GPU 환경에서 초장문 입력 처리에 강점을 가지며, 기존 FlashAttention 대비 최대 15배 이상의 속도 향상을 달성합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### FastAttention 논문의 주요 결과 및 비교 분석

---

### 1. **주요 결과**
#### **NPU에서의 성능**:
- Ascend 910B NPU에서 FastAttention은 **최대 10.7배 속도 향상**을 달성.
- LLaMA-7B 모델에서 **5.16배 Throughput 증가**.
- **초장문 입력(최대 256K 토큰)** 처리 가능.

#### **Volta 기반 GPU에서의 성능**:
- FlashAttention2(xformers) 대비 **최대 1.43배 속도 향상**.
- Pangu-38B 모델에서 **1.46배 더 낮은 지연시간** 달성.

#### **CPU-GPU 협력 전략 결과**:
- 초장문 입력 처리 시 기존 오프로드 방식 대비 **1.27~1.48배 성능 향상**.
- GPU 메모리 부족 시 CPU를 활용하여 256K 입력 길이 지원.

---

### 2. **특출난 점**
#### **기존 방법론 대비 우위**
| **방법**               | **대상 하드웨어** | **초장문 처리 가능 여부** | **속도 향상**              | **메모리 효율성**         |
|-----------------------|----------------|-----------------------|-------------------------|-----------------------|
| FlashAttention       | 고성능 GPU(Ampere, Hopper) | 제한적                   | 기존 대비 3~5배 향상         | 효율적                  |
| **FastAttention**    | NPU, Volta GPU | 256K 입력 처리 가능        | NPU: 10.7배, GPU: 1.43배 | Mask 최적화로 메모리 99% 절약 |

1. **하드웨어 범용성**:
   - 기존 FlashAttention은 Ampere, Hopper와 같은 고성능 GPU에서만 동작 가능.
   - FastAttention은 NPU 및 저자원 GPU(Volta 기반)에서도 성능 향상 제공.

2. **초장문 처리**:
   - 256K 토큰 입력 처리로 초장문 처리 능력 강화.
   - 기존 FlashAttention은 대규모 입력 처리에 한계.

3. **성능 개선**:
   - NPU에서 10배 이상, GPU에서 약 1.5배 성능 향상.

4. **메모리 최적화**:
   - Mask 생성 최적화(Tiling-Mask 전략)으로 메모리 요구량 99% 절감.

---

### 3. **결과를 가능하게 한 방법론**
FastAttention이 높은 성능을 달성한 주요 원인은 다음과 같습니다:

#### **1. 이중 타일링 전략 (Two-level Tiling Strategy)**:
- **첫 번째 타일링**:
  - 큰 블록으로 메모리 대역폭 활용을 극대화하고 동기화 오버헤드를 줄임.
- **두 번째 타일링**:
  - 작은 블록으로 세분화해 연산을 병렬로 처리하며 캐시 활용도를 극대화.

#### **2. Tiling-Mask 전략**:
- \( M \)-Mask를 이용해 \( S \times S \) 크기의 Attention Mask를 블록 크기 (\( M \))로 축소.
- 메모리 사용량을 기존 대비 1% 수준으로 감소.
- 예: 64K 입력 처리 시 8GB 메모리가 필요했던 기존 방식이 FastAttention에서는 256KB로 감소.

#### **3. CPU-GPU 협력 전략**:
- GPU 메모리가 부족할 경우 일부 레이어의 연산(KV 캐시 생성)을 CPU로 오프로드.
- 다중 쓰레드 및 벡터화 연산을 통해 CPU의 계산 성능을 최적화.
- GPU와 CPU 간 데이터 교환을 최소화하여 PCIe 대역폭 한계를 극복.

#### **4. Volta GPU를 위한 데이터 레이아웃 재설계**:
- Volta의 제한된 MMA 명령어(m8n8k4)를 고려해 데이터를 효율적으로 배치.
- CuTe 라이브러리를 사용해 SRAM과 HBM 접근 효율 최적화.

---

### 4. **논문의 주장과 나의 분석**
#### 논문의 주장:
1. **NPU와 저자원 GPU에서도 FlashAttention의 성능을 구현할 수 있도록 설계했다.**
   - Ascend NPU와 Volta GPU의 구조적 차이를 분석하고 이를 반영한 타일링 및 데이터 배치 최적화.
   - Tiling-AllReduce로 다중 NPU 환경에서 통신 오버헤드 감소.

2. **초장문 입력을 효율적으로 처리한다.**
   - CPU-GPU 협력 전략과 메모리 최적화로 기존 초장문 처리 한계를 극복.

#### 나의 분석:
1. **설계의 일반성**:
   - 논문에서 제시한 Tiling 및 Mask 전략은 NPU와 GPU의 아키텍처적 차이를 효과적으로 반영.
   - 이론적으로 다른 하드웨어 플랫폼에도 적용 가능성을 제시.

2. **기술적 완성도**:
   - 실험을 통해 다양한 모델(LLaMA, PanGu 등)과 시퀀스 길이에서의 성능을 입증.
   - 단순한 성능 비교가 아니라 다양한 하드웨어 최적화 기술의 유기적 결합으로 결과를 도출.

3. **한계와 가능성**:
   - Volta 이전의 구형 GPU나 비슷한 구조의 NPU에 대한 추가 검증 필요.
   - FlashAttention과의 직접적인 비교가 불가능한 환경(NPU 등)에서 FastAttention의 절대적 우위를 입증하기 어려움.

---

### 결론
FastAttention은 효율적인 타일링 전략과 메모리 최적화를 통해 NPU 및 저자원 GPU에서도 기존 FlashAttention을 능가하는 성능을 보여줍니다. 논문에서 제시한 방법론은 하드웨어 제약을 극복하며 초장문 처리 능력을 크게 향상시켰습니다. 이는 대규모 LLM 추론에 있어 중요한 기여를 할 수 있는 연구라 판단됩니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘

# A :

### FastAttention 논문의 입력 데이터, 추론 과정, 모델 아키텍처 구성 및 자원 요구량에 대한 자세한 설명

---

### **1. 입력 데이터와 추론 과정**

#### **입력 데이터**
1. **Query (\(Q\))**, **Key (\(K\))**, **Value (\(V\))**:
   - 입력 데이터는 Transformer 구조의 Attention 메커니즘을 위한 3가지 행렬 \( Q, K, V \)입니다.
   - 차원:
     - \( Q \): \((\text{Batch size}, \text{Sequence length}, \text{Embedding dimension})\)
     - \( K \): \((\text{Batch size}, \text{Sequence length}, \text{Embedding dimension})\)
     - \( V \): \((\text{Batch size}, \text{Sequence length}, \text{Embedding dimension})\)

2. **예시**:
   - **Batch size (\(B\))**: 1
   - **Sequence length (\(S\))**: 4
   - **Embedding dimension (\(D\))**: 2
   ```
   Q = [[1, 0], [0, 1], [1, 1], [0, 0]]
   K = [[1, 1], [0, 1], [1, 0], [0, 0]]
   V = [[1, 2], [3, 4], [5, 6], [7, 8]]
   ```

---

#### **추론 과정**

##### **1. Attention 계산**
1. **\(Q \times K^T\)**:
   - \(Q\)와 \(K\)의 전치 행렬 \(K^T\)를 곱하여 유사도를 계산.
   - 예시:
     \[
     Q \times K^T =
     \begin{bmatrix}
     1 & 0 \\
     0 & 1 \\
     1 & 1 \\
     0 & 0
     \end{bmatrix}
     \times
     \begin{bmatrix}
     1 & 0 & 1 & 0 \\
     1 & 1 & 0 & 0
     \end{bmatrix}
     =
     \begin{bmatrix}
     1 & 0 & 1 & 0 \\
     0 & 1 & 0 & 0 \\
     2 & 1 & 1 & 0 \\
     0 & 0 & 0 & 0
     \end{bmatrix}
     \]

2. **Softmax 적용**:
   - 행별로 Softmax를 계산하여 정규화된 Attention 점수 생성.
   - 예시:
     \[
     \text{Softmax}(Q \times K^T) =
     \begin{bmatrix}
     0.5 & 0.1 & 0.5 & 0.1 \\
     0.1 & 0.8 & 0.1 & 0.1 \\
     0.4 & 0.3 & 0.2 & 0.1 \\
     0.25 & 0.25 & 0.25 & 0.25
     \end{bmatrix}
     \]

3. **Weighting \(V\)**:
   - Attention 점수를 \(V\)와 곱하여 최종 결과를 생성.
   - 예시:
     \[
     \text{Output} =
     \text{Softmax}(Q \times K^T) \times V =
     \begin{bmatrix}
     3.0 & 5.0 \\
     3.3 & 4.3 \\
     4.7 & 5.8 \\
     4.0 & 5.0
     \end{bmatrix}
     \]

---

### **2. 모델 아키텍처 구성**

#### **Transformer 블록 구조**
FastAttention은 Transformer 기반으로 설계된 모델에 적용됩니다. Transformer 모델의 주요 구성 요소:
1. **Multi-head Attention**:
   - \(Q, K, V\) 행렬 계산.
   - \(Q \times K^T\), Softmax, \(V\) 연산.
   - Head별 독립 연산 후 결합.

2. **Feed-Forward Network (FFN)**:
   - 선형 변환 및 활성화 함수 적용:
     - \( \text{ReLU}(X \cdot W_1 + b_1) \cdot W_2 + b_2 \)
   - 매트릭스 크기:
     - \( W_1: \text{(Embedding dimension)} \times (\text{Hidden dimension}) \)
     - \( W_2: (\text{Hidden dimension}) \times \text{(Embedding dimension)} \)

3. **Layer Normalization**:
   - 입력 데이터 정규화.

4. **Skip Connection**:
   - 입력과 출력의 잔차 연결.

---

### **3. 연산 및 자원 요구량**

#### **연산 요구량**
1. **Attention 연산**:
   - \(Q \times K^T\): \(\mathcal{O}(S^2 \cdot D)\)
   - Softmax: \(\mathcal{O}(S^2)\)
   - \(V\) 가중치 적용: \(\mathcal{O}(S^2 \cdot D)\)

2. **Feed-Forward Network**:
   - 연산 복잡도: \(\mathcal{O}(S \cdot D \cdot H + S \cdot H \cdot D)\)
     - \(H\): FFN의 Hidden Dimension.

---

#### **메모리 요구량**
1. **Attention**:
   - \(Q, K, V\): \(3 \cdot B \cdot S \cdot D\)
   - Intermediate Matrix (\(Q \times K^T\)): \(B \cdot S \cdot S\)
   - 최종 Output: \(B \cdot S \cdot D\)

2. **Feed-Forward Network**:
   - Weight: \(W_1, W_2\) 크기 포함.
   - Hidden Dimension \(H = 4 \cdot D\) 기준.

#### **예시**:
- 모델 크기: LLaMA-7B (\(D = 128, S = 16K\)):
  - \(Q, K, V\): 약 \(6.144\) GB.
  - Intermediate Attention Matrix: 약 \(4.096\) GB.

---

### **4. FastAttention이 자원 요구량을 최적화하는 방법**
1. **Tiling-Mask 전략**:
   - Attention Mask를 효율적으로 분할하여 메모리 사용량을 기존 대비 1% 수준으로 감소.
   - \(64K\) 입력 기준, 기존 8GB에서 256KB로 감소.

2. **이중 타일링**:
   - 메모리 접근 패턴을 최적화하여 캐시 히트율 증가.
   - 동기화 오버헤드 최소화.

3. **CPU-GPU 협력**:
   - GPU 메모리 부족 시 CPU의 메모리를 활용하여 초장문 입력(256K 토큰)도 처리 가능.

---

### 결론
FastAttention의 입력 데이터와 추론 과정은 Transformer 모델의 Attention 메커니즘을 기반으로 하며, 메모리와 연산량 최적화를 통해 기존 방법론 대비 우수한 성능과 효율성을 제공합니다. 특히, NPU와 저자원 GPU에서도 초장문 입력을 처리할 수 있는 능력이 큰 강점입니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :

### FastAttention 논문의 한계와 이를 극복하기 위한 연구 흐름

---

### **1. 논문의 한계**
FastAttention은 NPU와 저자원 GPU에서 효율적으로 작동하는 점에서 혁신적이지만, 다음과 같은 한계점이 존재합니다:

#### **1.1. 하드웨어 종속성**
- FastAttention은 특정 하드웨어(NPU, Volta 기반 GPU)에 최적화되어 있습니다.
- Volta 이전의 구형 GPU, AMD GPU, TPU 등 다른 하드웨어에 적용하려면 추가적인 최적화가 필요합니다.

#### **1.2. 범용성 부족**
- FastAttention이 구현된 최적화 기법(Tiling-Mask, Two-level Tiling 등)은 일부 Transformer 아키텍처에만 적합할 가능성이 있습니다.
- 비 Transformer 기반 모델(예: RNN, GNN 등)에 대한 확장은 논의되지 않았습니다.

#### **1.3. 초장문 입력 처리의 제한**
- 256K 토큰 처리로 성능을 크게 확장했지만, 초장문 추론에서 발생할 수 있는 동적 메모리 재할당 및 오버헤드 문제는 여전히 존재합니다.

#### **1.4. 동적 시퀀스 길이 문제**
- FastAttention은 고정된 블록 크기에 의존해 최적화되었기 때문에, 동적 입력 길이를 처리할 때 효율성이 떨어질 가능성이 있습니다.

#### **1.5. 다중 하드웨어 환경의 통합**
- NPU와 GPU가 혼합된 환경에서 최적화된 성능을 제공하기 위한 추가 설계가 부족합니다.

---

### **2. 한계를 극복하기 위한 연구 흐름**

#### **2.1. 다양한 하드웨어 플랫폼 지원**
1. **TPU 및 AMD GPU 지원 확장**
   - TPU는 구글에서 설계한 대규모 딥러닝 연산 최적화 하드웨어로, 기존 GPU와 다른 메모리 계층 구조를 가집니다.
   - AMD GPU는 ROCm 생태계를 기반으로 최적화가 가능하며, NPU와 비슷한 설계 원칙을 공유합니다.
   - 연구 방향:
     - 하드웨어 특성을 고려한 새로운 메모리 배치 및 타일링 전략 설계.
     - OpenCL 및 ROCm 기반으로 FastAttention 구현.

2. **범용 하드웨어 대응 알고리즘**
   - FastAttention의 타일링 및 Mask 전략을 하드웨어 비종속적으로 설계.
   - CUDA 이외의 연산 프레임워크(e.g., SYCL, HIP)를 활용.

---

#### **2.2. 범용 모델 적용**
1. **Transformer 외 모델 최적화**
   - FastAttention은 Transformer 기반 구조에 최적화되어 있지만, RNN이나 GNN과 같은 모델에도 확장 가능성을 모색할 수 있습니다.
   - 연구 방향:
     - Attention 메커니즘을 사용하는 모든 모델에서의 적용 가능성 탐구.
     - 예: GNN에서의 Graph Attention 연산 최적화.

2. **멀티모달 모델에의 확장**
   - 텍스트, 비전, 음성 데이터를 동시에 처리하는 멀티모달 모델(예: Flamingo, DALL-E)에 FastAttention 적용.
   - 연구 방향:
     - 다양한 모달리티의 데이터 처리에 적합한 타일링 전략 개발.

---

#### **2.3. 초장문 입력에서의 추가 최적화**
1. **동적 메모리 관리**
   - FastAttention은 정적인 타일 크기 및 블록 분할을 기반으로 설계되었습니다.
   - 초장문 입력에서는 입력 데이터의 특성에 따라 메모리 요구량이 변동되므로, 이를 동적으로 조정하는 기술이 필요합니다.
   - 연구 방향:
     - 메모리 사용 패턴을 실시간으로 모니터링하여 동적 재분할(dynamic tiling) 구현.

2. **메모리 오버헤드 최소화**
   - 초장문 입력 시 Offloading 전략을 더 세분화하여 GPU와 CPU 간의 데이터 교환을 줄이는 연구.
   - 연구 방향:
     - 압축 기법(예: 양자화, Pruning)과 결합한 FastAttention 설계.

---

#### **2.4. 동적 시퀀스 길이 지원**
1. **블록 크기 적응형 타일링**
   - 입력 데이터의 길이에 따라 블록 크기를 조정하는 방식 도입.
   - 연구 방향:
     - Sequence-to-sequence 모델에서 동적 시퀀스 길이를 지원하는 Attention 메커니즘 개발.

2. **Softmax 대체 기법**
   - Softmax는 시퀀스 길이가 길어질수록 계산 복잡도가 증가하는 문제가 있습니다.
   - 연구 방향:
     - Softmax 대체 알고리즘(예: Performer의 FAVOR+)를 FastAttention에 통합.

---

#### **2.5. 다중 하드웨어 환경 최적화**
1. **이기종 환경 통합**
   - GPU, NPU, CPU가 혼합된 환경에서의 통합 최적화 전략 개발.
   - 연구 방향:
     - 이기종 하드웨어 간의 데이터 교환을 최적화하는 새로운 AllReduce 알고리즘.
     - SDMA(System Direct Memory Access)를 확장하여 데이터 이동 최소화.

2. **분산 추론 환경에서의 확장**
   - 다중 노드 환경에서 FastAttention의 성능 최적화를 위한 추가 설계 필요.
   - 연구 방향:
     - Horovod와 같은 분산 추론 프레임워크와의 통합.

---

### **3. 정리**
FastAttention은 Transformer 기반 모델에서 매우 효율적인 Attention 메커니즘을 제공하지만, 특정 하드웨어와 고정된 시퀀스 구조에 제한된다는 한계가 있습니다. 이를 극복하기 위해서는 다음과 같은 연구 흐름이 유망합니다:
1. **다양한 하드웨어 플랫폼에 대응하는 범용 최적화 알고리즘** 개발.
2. **Transformer 외 모델** 및 **멀티모달 환경**으로 확장.
3. **동적 입력 및 초장문 처리**를 위한 추가 최적화.
4. **이기종 하드웨어 환경**과 **분산 추론** 최적화.

이러한 방향으로 연구가 진행된다면, FastAttention은 더욱 범용적이고 강력한 Attention 메커니즘으로 발전할 수 있을 것입니다.


# Q : FastAttention에서 제안된 이중 타일링(two-level tiling) 전략이 NPU와 Volta 기반 GPU에서 각각 어떻게 다른 방식으로 최적화되었는지, 그리고 이러한 최적화가 성능에 미친 영향을 구체적으로 설명할 수 있는가?

# A : 

### **이중 타일링(Two-level Tiling) 전략이 NPU와 Volta 기반 GPU에서의 최적화 방식**

---

### **1. NPU에서의 최적화**
Ascend NPU는 GPU와 다른 **디커플링 아키텍처**(Cube 유닛과 Vector 유닛 분리)와 **다층 메모리 계층 구조**를 가지고 있습니다. 이중 타일링 전략은 이러한 NPU의 특성을 활용하여 성능을 최적화합니다.

#### **1.1. 1차 타일링 (대형 블록)**
- **목표**:
  - 메모리 대역폭 활용 극대화 및 동기화 오버헤드 최소화.
- **방법**:
  - Cube 유닛이 큰 연속 데이터 블록을 처리하도록 설계.
  - 더 큰 블록 크기를 사용하여 **데이터 전송과 연산을 겹침(Double Buffering)**.
- **효과**:
  - L1/L2 캐시 활용을 극대화하고, 데이터 전송 시간을 줄임.

#### **1.2. 2차 타일링 (소형 블록)**
- **목표**:
  - Cube 유닛과 Vector 유닛 간의 작업 분할을 최적화.
- **방법**:
  - 큰 블록을 다시 소형 블록으로 분할하여 Cube 유닛과 Vector 유닛이 병렬로 작동.
  - Vector 유닛이 소형 블록의 Softmax 계산을 수행하는 동안 Cube 유닛은 다른 소형 블록에서 행렬 곱셈 수행.
- **효과**:
  - Cube와 Vector 간 **파이프라인 병렬성** 강화.
  - 동기화 오버헤드 감소 및 연산 효율 증가.

#### **1.3. 성능 영향**
- **성능 향상**:
  - Ascend 910B NPU에서 표준 Attention 대비 최대 **10.7배 속도 향상**.
  - Softmax 계산과 GEMM(General Matrix Multiplication)의 동시 실행을 통해 동작 효율 증대.
- **Throughput**:
  - LLaMA-7B 모델 기준, 기존 대비 **5.16배 Throughput 증가**.

---

### **2. Volta 기반 GPU에서의 최적화**
Volta 기반 GPU는 **공유 메모리(SRAM)**와 **Tensor Core(MMA 연산)**를 활용하며, NPU와는 다른 방식의 데이터 배치 및 연산 구조를 가집니다.

#### **2.1. 1차 타일링 (대형 블록)**
- **목표**:
  - Volta의 SRAM과 HBM 접근 효율 최적화.
- **방법**:
  - 대형 블록 크기를 사용하여 공유 메모리에서의 데이터 접근을 줄이고, 병렬 연산 수행.
  - Tensor Core의 행렬 연산(MMA 연산)을 최적화하기 위해 SRAM 내 데이터를 효율적으로 배치.
- **효과**:
  - SRAM에서의 **은행 충돌(Bank Conflict)** 최소화.
  - Tensor Core가 대형 연산을 병렬로 처리해 계산량 증가.

#### **2.2. 2차 타일링 (소형 블록)**
- **목표**:
  - Volta의 Warp 수준에서 MMA 연산의 데이터 흐름 최적화.
- **방법**:
  - 소형 블록을 Warp 단위로 분할하여 각 Warp가 독립적으로 연산.
  - CuTe 라이브러리를 활용해 데이터 레이아웃을 Volta의 MMA 명령어(m8n8k4)에 맞게 설계.
- **효과**:
  - MMA 연산에서 스레드 간 동기화 필요성을 줄이고 효율 극대화.

#### **2.3. 성능 영향**
- **성능 향상**:
  - FlashAttention2(xformers) 대비 **최대 1.43배 속도 향상**.
  - Pangu-38B 모델에서 **1.46배 낮은 지연시간**.
- **초장문 처리**:
  - GPU 메모리 최적화를 통해 최대 256K 입력 길이 지원.

---

### **3. NPU와 Volta 기반 GPU의 이중 타일링 비교**

| **특징**                   | **NPU (Ascend)**                          | **Volta 기반 GPU**                     |
|---------------------------|------------------------------------------|---------------------------------------|
| **1차 타일링 목표**         | 데이터 전송과 연산 겹침 (Double Buffering)    | SRAM 접근 효율 최적화                  |
| **2차 타일링 목표**         | Cube와 Vector 간 파이프라인 병렬성 강화       | Warp 수준 병렬성 극대화                |
| **메모리 계층 활용**        | L1/L2 캐시와 HBM                          | SRAM과 HBM                            |
| **최적화 방식**             | Cube와 Vector 병렬 작업 분할                | CuTe 기반 데이터 레이아웃 설계          |
| **성능 향상**               | 10.7배 속도 향상 (표준 대비)                 | 1.43배 속도 향상 (FlashAttention2 대비) |

---

### **4. 결론**
NPU와 Volta 기반 GPU에서 FastAttention의 이중 타일링 전략은 하드웨어 구조에 맞게 설계되어 성능을 극대화합니다. 
- NPU에서는 Cube와 Vector 간 병렬 작업을 강조해 동기화 오버헤드를 줄였고, 
- Volta GPU에서는 SRAM과 MMA 연산의 효율성을 강화했습니다. 
이 전략은 다양한 하드웨어 환경에서도 최적화 가능성을 시사하며, 특히 초장문 입력 처리에서 혁신적인 결과를 보여줍니다.


# Q : FastAttention의 Tiling-Mask 전략이 기존의 Attention Mask 처리 방식과 비교하여 메모리 절감 효과를 가져온 기작을 수식적으로 설명하고, 이를 초장문 입력(256K) 처리 시 어떻게 활용했는지 구체적인 사례를 들 수 있는가?

# A : 

### FastAttention의 Tiling-Mask 전략: 메모리 절감 기작과 초장문 입력 처리 사례

---

### **1. 기존 Attention Mask 처리 방식**
1. **Attention Mask 정의**:
   - Attention 계산은 특정 위치를 "마스킹(mask)"하여 무시할 수 있습니다.
   - 일반적으로 **Lower Triangular Matrix** 형태로 구현되며, 크기는 \( S \times S \) (Sequence Length \( S \))입니다.
   - 예: \( S = 4 \)
     \[
     \text{Mask} =
     \begin{bmatrix}
     1 & 0 & 0 & 0 \\
     1 & 1 & 0 & 0 \\
     1 & 1 & 1 & 0 \\
     1 & 1 & 1 & 1 \\
     \end{bmatrix}
     \]

2. **메모리 요구량**:
   - Mask 매트릭스의 크기는 \( S^2 \)이며, 초장문 입력에서는 큰 메모리 사용량 초래.
   - 예: \( S = 64K \)일 경우, \( \text{Mask Size} = (64K)^2 = 4.096 \times 10^9 \) (약 4GB).

---

### **2. Tiling-Mask 전략**
FastAttention의 Tiling-Mask 전략은 기존 방식의 **메모리 사용량을 블록 크기 \( M \)로 감소**시킵니다.

1. **Tiling-Mask 매트릭스 생성**:
   - 전체 Mask 매트릭스를 저장하지 않고, 필요한 블록 단위로 **작은 마스크(\( M \)-Mask)**만 생성.
   - \( M \)-Mask는 크기가 \( (2M) \times (2M) \)이며, \( S \times S \) 크기를 대체.
   - 예: \( M = 512 \)일 경우, \( M \)-Mask 크기는 \( (2 \times 512)^2 = 1MB \).

2. **블록 단위 Attention 연산**:
   - Attention 연산은 매트릭스를 블록으로 나누어 수행하며, 각 블록에 대응하는 \( M \)-Mask를 사용.
   - 예: \( S = 64K \), \( M = 512 \)일 경우, 전체 매트릭스는 \( 64K / 512 = 128 \)개의 블록으로 나뉘며 각 블록은 \( M \)-Mask로 처리.

3. **수식적 표현**:
   - 기존 방식:
     \[
     \text{Attention Output} = \text{Softmax}(Q \cdot K^T + \text{Mask}) \cdot V
     \]
   - Tiling-Mask:
     \[
     \text{Attention Output}_{\text{Block}} = \text{Softmax}(Q_B \cdot K_B^T + M_B) \cdot V_B
     \]
     - \( Q_B, K_B, V_B \): 블록 단위 Query, Key, Value.
     - \( M_B \): 블록 크기 \( M \)에 해당하는 마스크.

4. **메모리 절감**:
   - \( S = 64K \), \( M = 512 \)일 경우:
     - 기존 Mask 크기: \( S^2 = 4GB \).
     - Tiling-Mask 크기: \( (2M)^2 = 256KB \).
   - 메모리 절감율:
     \[
     \text{절감율} = 1 - \frac{(2M)^2}{S^2} = 1 - \frac{(2 \cdot 512)^2}{(64K)^2} \approx 0.9999 \, (99.99\%)
     \]

---

### **3. 초장문 입력(256K) 처리 사례**
1. **기존 방식의 한계**:
   - \( S = 256K \)일 경우:
     - 기존 Mask 크기: \( S^2 = (256K)^2 = 65.536 \times 10^9 \) (약 65GB).
     - 단일 GPU 메모리 한계를 초과.

2. **Tiling-Mask를 활용한 처리**:
   - \( M = 512 \)일 경우:
     - Tiling-Mask 크기: \( (2M)^2 = (2 \cdot 512)^2 = 256KB \).
   - 메모리 절감율:
     \[
     \text{절감율} = 1 - \frac{256KB}{65GB} \approx 99.99\%
     \]

3. **GPU 메모리 내 연산 가능**:
   - 초장문 입력 데이터를 블록 단위로 분할하여 연산을 수행.
   - 각 블록은 \( M \)-Mask를 사용해 효율적으로 Attention 연산 수행.

4. **성능 비교**:
   - 기존 방식: \( S = 256K \) 처리 불가.
   - Tiling-Mask: \( S = 256K \) 처리 가능, GPU 메모리 요구량 감소로 추론 효율 증가.

---

### **4. 요약**
Tiling-Mask 전략은 초장문 입력의 Attention Mask 처리에서 메모리 사용량을 대폭 줄여줍니다. \( S^2 \) 크기의 기존 매트릭스를 블록 크기 \( M \)에 비례하는 크기로 대체하여, 메모리 사용량을 \( 99.99\% \) 이상 절감할 수 있습니다. 이러한 방식은 GPU 메모리가 부족한 환경에서 **256K 이상의 초장문 입력**을 효율적으로 처리할 수 있도록 하며, FastAttention이 초장문 처리에서 뛰어난 성능을 보이는 핵심 요소 중 하나로 작용합니다.

# Q : CPU-GPU 협력 전략이 초장문 입력 처리에서 기존 오프로드 방식보다 우수한 성능을 보이는 이유를 데이터 전송, 연산 분배, 동기화 메커니즘 측면에서 설명할 수 있는가?

# A : 

### CPU-GPU 협력 전략이 기존 오프로드 방식보다 우수한 성능을 보이는 이유

---

### **1. 주요 차별점**
FastAttention의 CPU-GPU 협력 전략은 기존의 단순 오프로드 방식과 달리 **데이터 전송**, **연산 분배**, **동기화 메커니즘**을 최적화하여 초장문 입력 처리 성능을 크게 향상시켰습니다. 이를 구체적으로 설명하면 다음과 같습니다.

---

### **2. 데이터 전송 측면**

#### **기존 오프로드 방식의 문제**
1. GPU 메모리가 부족한 경우, **KV 캐시(Key-Value Cache)**를 CPU로 오프로드합니다.
2. 필요 시 GPU로 다시 업로드하는데, 이는 **전체 데이터 크기(S \times H \times B)**를 전송해야 함.
   - \( S \): 시퀀스 길이, \( H \): 헤드 크기, \( B \): 배치 크기.
3. GPU와 CPU 간의 데이터 전송은 **PCIe 대역폭**으로 제한되며, 대규모 데이터 전송 시 병목현상이 발생.

#### **FastAttention의 최적화**
1. **소규모 데이터 전송**:
   - CPU와 GPU 간 데이터 전송을 최소화하기 위해, **QKV 매트릭스 일부**와 연산 결과만 교환.
   - 업로드 데이터 크기:
     - 기존 방식: \( \text{O}(S \times H \times B) \)
     - FastAttention: \( \text{O}(H \times B) \) (블록 단위로 제한)
2. **비동기 데이터 처리**:
   - CPU와 GPU 간 데이터 전송이 비동기로 이루어지며, 연산과 겹치도록 설계.

#### **결과**:
- PCIe 병목현상이 크게 감소.
- 초장문 입력(예: \( S = 256K \))에서도 데이터 전송 시간이 전체 처리 시간에서 차지하는 비중 감소.

---

### **3. 연산 분배 측면**

#### **기존 오프로드 방식의 문제**
1. 전체 연산이 GPU에 집중되거나 CPU로 전환됨.
2. GPU로 데이터를 업로드한 후 계산을 진행해야 하므로, **GPU 대기 시간**이 길어짐.

#### **FastAttention의 최적화**
1. **CPU와 GPU 연산 분배**:
   - 전체 Transformer Layer를 GPU와 CPU로 분리:
     - **GPU 처리 Layer (\( L_{\text{GPU}} \))**: GPU 메모리에 상주 가능한 레이어.
     - **CPU 처리 Layer (\( L_{\text{CPU}} \))**: 초과하는 KV 캐시는 CPU 메모리에 저장.
2. **실시간 오프로드 및 계산**:
   - GPU가 **초기 레이어**(\( L_{\text{CPU}} \))의 연산을 수행하는 동안, CPU는 KV 캐시 계산과 저장을 병렬 처리.
   - CPU 연산 후, 필요한 경우 결과만 GPU로 전송.

#### **결과**:
- GPU 연산이 연속적으로 진행되어 대기 시간 감소.
- CPU 연산이 GPU 메모리 부족을 보완하면서 추가적인 처리 능력 제공.

---

### **4. 동기화 메커니즘 측면**

#### **기존 오프로드 방식의 문제**
1. GPU와 CPU 간 데이터 전송이 동기적으로 이루어짐.
2. 데이터 전송이 완료될 때까지 GPU 연산이 대기 상태에 놓이게 됨.

#### **FastAttention의 최적화**
1. **비동기 처리**:
   - CPU 연산과 데이터 전송을 동시에 실행:
     - CPU에서 연산이 끝나는 즉시, 결과를 GPU로 전송.
     - GPU는 현재 처리 중인 블록이 완료된 후 전송된 데이터를 이어받아 처리.
2. **SDMA 활용**:
   - **System Direct Memory Access (SDMA)**를 통해 CPU와 GPU 간 데이터 교환 및 연산이 병렬로 실행.
   - 연산과 통신이 겹치도록 하여 동기화 오버헤드를 제거.

#### **결과**:
- GPU 연산 중단 없이 연속 처리 가능.
- CPU와 GPU 간 효율적인 자원 활용으로 처리 성능 극대화.

---

### **5. 사례: 초장문 입력(256K) 처리**

#### **기존 방식**:
1. \( S = 256K \), \( H = 128 \), \( B = 1 \)일 경우:
   - GPU 메모리 부족 → 모든 KV 캐시를 CPU로 오프로드.
   - GPU에서 연산 시 KV 캐시를 다시 업로드 → **PCIe 병목** 발생.
2. 데이터 전송 및 대기 시간 증가로 인해 처리 불가능하거나 매우 비효율적.

#### **FastAttention 방식**:
1. \( L_{\text{CPU}} \): CPU가 처리하는 레이어 수.
   \[
   L_{\text{CPU}} = L - L_{\text{GPU}}
   \]
   - \( L_{\text{GPU}} = \frac{\text{GPU 메모리 - 기타 메모리}}{\text{Layer 메모리 사용량}} \).
2. CPU는 KV 캐시를 계산하여 필요할 때 GPU로 비동기 전송.
3. GPU는 현재 블록을 처리하면서 CPU로부터 데이터를 받아 이어서 처리.

#### **결과**:
- GPU 메모리 제한에도 불구하고 \( S = 256K \) 입력 처리 가능.
- 기존 방식 대비 **1.27~1.48배 성능 향상**.

---

### **6. 요약**
FastAttention의 CPU-GPU 협력 전략은 기존 오프로드 방식의 주요 병목을 해결하며, 초장문 입력 처리에서 뛰어난 성능을 제공합니다:
1. **데이터 전송**: GPU와 CPU 간 전송 데이터를 최소화하고 비동기로 처리.
2. **연산 분배**: GPU와 CPU 간 작업을 효율적으로 분배하여 처리 병렬성 극대화.
3. **동기화 메커니즘**: SDMA를 활용해 데이터 전송과 연산을 병렬로 수행하여 동기화 오버헤드 제거.

이로 인해 FastAttention은 초장문 입력에서도 기존 방식 대비 탁월한 성능을 보이며, 대규모 모델 추론의 효율성을 크게 향상시킵니다.