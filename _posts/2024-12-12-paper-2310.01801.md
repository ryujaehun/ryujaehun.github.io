---
title: "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
date: 2024-12-12
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2310.01801)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

### **논문 요약 및 분석**

논문 **"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"**는 대규모 언어 모델(LLMs)의 효율적인 추론을 위한 **KV 캐시 압축(FastGen)** 방법을 제안합니다. 이 방법은 메모리 사용량을 줄이고 추론 속도를 향상시키며, 기존의 캐시 압축 방식과 비교하여 **질적 손실 없이 메모리 효율성을 높이는 점**에서 독창적입니다.

---

### **1. 논문의 강점 및 독창성**

#### **1.1 강점**
1. **메모리 최적화**:
   - FastGen은 **KV 캐시 메모리**를 최대 50% 압축하면서도 **95% 이상의 주의력 맵 복원율**을 달성합니다.
   - 이는 GPU 메모리 부족 문제를 완화하여 긴 시퀀스 생성 시 유리합니다.

2. **효율성**:
   - FastGen은 **사전 학습 없이** 기존 모델에 적용할 수 있는 **Plug-and-Play** 방식으로 구현.
   - 경량화된 프로파일링 단계로 추가 연산 오버헤드를 최소화.

3. **다양한 압축 정책**:
   - 주의력 모듈의 구조적 패턴(특정 토큰, 지역성 등)을 활용하여 **압축 정책을 맞춤형으로 적용**.
   - 주의력 맵에 따라 "특수 토큰", "구두점", "지역성", "빈도 기반" 등 다양한 KV 캐시 정책 적용.

#### **1.2 독창적인 지점**
1. **적응형 KV 캐시**:
   - FastGen은 **각 주의력 헤드의 구조를 분석**하여 적합한 압축 정책을 선택하는 **이중 단계 알고리즘**을 도입.
   - 기존의 고정된 KV 캐시 압축 방식과 달리, 각 레이어와 헤드에 최적화된 정책을 적용.

2. **무손실 성능 유지**:
   - 대부분의 기존 방법이 캐시 압축으로 인해 성능 저하를 초래하는 반면, FastGen은 성능 손실 없이도 압축 가능.

3. **범용성**:
   - 다양한 LLM (LLaMA, GPT 등) 및 작업(수학, 코딩, 질문 응답)에 적용 가능.
   - **Fine-tuning 없이도 성능 향상**이 가능해 실용성이 높음.

---

### **2. 핵심 알고리즘: FastGen**

#### **2.1 주요 개념**
1. **Prompt Encoding**:
   - 모델은 입력 프롬프트를 기반으로 초기 KV 캐시를 생성.
   - KV 캐시란, 이전 토큰의 Key/Value 벡터를 저장하여 반복 연산을 방지하는 구조입니다.

2. **Token Generation**:
   - 토큰을 생성할 때마다 새 KV 캐시를 갱신하며, 이를 압축된 상태로 유지.

3. **KV 캐시 압축 정책**:
   - 네 가지 기본 압축 정책과 이들의 조합을 사용:
     - **특수 토큰(Cspecial)**: 문장 시작/명령어 토큰 등 중요한 토큰만 유지.
     - **구두점(Cpunct.)**: 구두점 토큰만 유지.
     - **지역성(Clocal)**: 현재 토큰과 멀리 떨어진 토큰 제거.
     - **빈도 기반(Cfrequent)**: 주의력 점수가 높은 토큰만 유지.

---

#### **2.2 예시와 과정**

**입력 프롬프트**:
```
Prompt: "Translate the following sentence to French: 'How are you today?'"
```

**1단계: Prompt Encoding**
1. 입력 문장을 처리하여 초기 Key (\(K\)) 및 Value (\(V\)) 벡터를 생성.
2. 각 주의력 헤드에 대해 압축 정책 적용:
   - 특수 토큰 (\<s\>, [INST])만 보존: \(C_{special}\)
   - 구두점(예: ".")만 보존: \(C_{punct.}\)

**2단계: Token Generation**
1. 이전 토큰의 \(K, V\) 벡터를 이용하여 현재 토큰 생성.
2. 새로 생성된 토큰의 \(K, V\) 벡터 추가.
3. 압축 정책에 따라 캐시 크기 조정:
   - 멀리 떨어진 컨텍스트를 제거(Clocal).
   - 주의력이 낮은 토큰 제거(Cfrequent).

**출력**:
```
Output: "Comment allez-vous aujourd'hui?"
```

---

### **3. 논문의 한계**

1. **작업 편중**:
   - 실험은 LLaMA 모델과 일부 데이터셋에 집중되어 있어, 다른 모델이나 작업에서는 일반화가 불분명.

2. **하이퍼파라미터 의존성**:
   - 압축 정책의 성능은 하이퍼파라미터(압축 비율 \(rl\), \(rf\))에 따라 변동.

3. **추가 메모리 사용**:
   - 압축을 위한 주의력 점수 누적에 추가 메모리가 필요하며, 매우 대규모 모델에서는 제한적일 수 있음.

4. **복잡성 증가**:
   - 여러 압축 정책을 조합하는 과정에서 계산 복잡도가 증가할 가능성.

---

### **4. 결론 및 향후 연구 방향**

#### **4.1 강점 요약**
- FastGen은 **경량화된 프로파일링**과 **적응형 정책**을 통해 메모리 사용량을 줄이면서도 성능 저하를 최소화하는 데 성공.

#### **4.2 향후 연구 방향**
1. **다양한 모델 테스트**:
   - GPT-4, PaLM 등 다양한 모델에 FastGen 적용해 범용성 검증.

2. **효율성 향상**:
   - 프로파일링 및 압축 과정을 병렬화하여 계산 속도 향상.

3. **정교한 정책 개발**:
   - 더 세밀한 주의력 분석을 통해 고도화된 압축 정책 개발.

4. **멀티모달 확장**:
   - 이미지-텍스트 통합 모델과 같은 멀티모달 모델에 적용 가능성 탐구.

FastGen은 LLM 추론의 메모리 병목 문제를 해결하는 실용적인 방법으로, 미래의 대규모 언어 모델에서 널리 활용될 잠재력이 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

### **논문의 핵심 알고리즘: FastGen**

FastGen은 대규모 언어 모델(LLM)의 **KV 캐시(KV Cache)**를 **적응형으로 압축**하여 메모리 사용량을 줄이고 추론 속도를 향상시키는 알고리즘입니다. 이를 이해하기 위해 단계별로 과정을 예시와 함께 자세히 설명하겠습니다.

---

### **1. 핵심 아이디어**

#### **KV 캐시란?**
- KV 캐시는 모델이 **토큰 생성 과정**에서 이전 토큰의 Key와 Value 벡터를 저장하여 **반복 연산을 방지**하는 구조입니다.
- 문제점: 시퀀스 길이가 길어질수록 KV 캐시의 메모리 사용량이 선형적으로 증가하여 **GPU 메모리 병목 현상**이 발생.

#### **FastGen의 목표**
- 모든 KV 벡터를 저장하지 않고, 주의력(Attention) 구조를 분석해 **필요한 데이터만 저장**.
- 특정 정책에 따라 KV 캐시를 선택적으로 압축하여 메모리 사용을 줄임.

---

### **2. FastGen의 동작 과정**

#### **2.1 단계 1: Prompt Encoding (프로파일링)**
Prompt(입력 프롬프트)를 기반으로 각 **주의력 헤드**의 구조를 분석하여 적합한 압축 정책을 선택합니다.

1. **입력 프롬프트**:
   ```
   Prompt: "The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more, how many apples are left?"
   ```

2. **주의력 맵 분석**:
   - 각 Attention Head가 어떤 토큰에 주의를 두는지 분석.
   - 주의력 구조에 따라 다음 네 가지 정책 중 하나를 선택:
     - **Cspecial**: 문장의 특수 토큰만 저장. (예: `<s>`, `[INST]`)
     - **Cpunct.**: 구두점 토큰만 저장. (예: ".", ",", "?")
     - **Clocal**: 현재 토큰과 가까운 문맥 토큰만 유지.
     - **Cfrequent**: 주의력이 높은 토큰만 저장.

3. **결과**:
   - Head 1: "특수 토큰" 주의 → **Cspecial** 적용.
   - Head 2: "구두점" 주의 → **Cpunct.** 적용.
   - Head 3: "가까운 문맥" 주의 → **Clocal** 적용.
   - Head 4: "전체 토큰" 주의 → 압축하지 않음(**Cfull**).

#### **2.2 단계 2: Token Generation**
1. **현재 토큰 생성**:
   - 이전 토큰의 Key/Value 벡터를 이용하여 새 토큰을 생성.
   - 예: 
     ```
     Step 1: "The cafeteria had 23 apples."
     Step 2: "If they used 20 for lunch."
     Step 3: "and bought 6 more."
     ```

2. **KV 캐시 갱신 및 압축**:
   - 새로 생성된 토큰의 KV 벡터를 추가하되, 선택된 압축 정책에 따라 일부 벡터를 삭제.
   - 예:
     - Head 1: `<s>`와 같은 특수 토큰만 유지.
     - Head 2: ".", ","와 같은 구두점만 유지.
     - Head 3: 현재 토큰 기준으로 가까운 문맥만 유지.

3. **출력 생성**:
   - 최종 출력: `"how many apples are left?"`

---

### **3. 알고리즘 요약**

#### **3.1 알고리즘 구조**
##### **Prompt Encoding (Algorithm 1)**
1. 입력: 주어진 프롬프트 \( P \)
2. 출력: 적응형 KV 캐시 \( \{C, \hat{K}, \hat{V}\} \)
3. 각 Attention Head \( H_i \)에 대해:
   - \( Q_i, K_i, V_i \) 벡터 계산.
   - 주의력 맵 \( A_i = \text{softmax}(Q_i K_i^T) \) 생성.
   - \( A_i \)에 가장 적합한 압축 정책 \( C_i \) 선택.
   - 압축된 KV 캐시 \( \hat{K}_i, \hat{V}_i \) 생성.

##### **Token Generation (Algorithm 2)**
1. 입력: 압축된 KV 캐시 \( \{C, \hat{K}, \hat{V}\} \)
2. 출력: 생성된 텍스트.
3. 반복:
   - 이전 토큰 \( z_{j-1} \)와 \( \hat{K}, \hat{V} \)를 사용해 새 토큰 생성.
   - 압축 정책 \( C_i \)에 따라 KV 캐시 갱신.

#### **3.2 네 가지 압축 정책**
1. **Cspecial**:
   - 특수 토큰만 유지(예: `<s>`, `[INST]`).
   - 메모리 비용이 낮음.

2. **Cpunct.**:
   - 구두점 토큰만 유지(예: ".", ",", "?").
   - 문장 구조를 보존.

3. **Clocal**:
   - 현재 토큰과 가까운 문맥만 유지.
   - 먼 문맥 제거.

4. **Cfrequent**:
   - 주의력이 높은 토큰만 유지.
   - 주의 점수 누적 기반.

---

### **4. 예시**

#### **4.1 입력**
```
Prompt: "What is the sum of 15 and 25?"
```

#### **4.2 동작**
1. **Prompt Encoding**:
   - 특수 토큰 `<s>` 및 질문 구조 분석.
   - 각 Attention Head에 정책 할당:
     - Head 1: 특수 토큰 → Cspecial.
     - Head 2: 구두점 → Cpunct.

2. **Token Generation**:
   - `"The sum is 40."` 생성 과정에서 압축된 KV 캐시 유지.

#### **4.3 출력**
```
"The sum is 40."
```

---

### **5. 정리**

FastGen은 **주의력 구조에 기반한 적응형 KV 캐시 압축**으로 메모리 사용량을 줄이고 성능을 유지합니다. 이를 통해 긴 시퀀스 생성에서도 효율적으로 작동하며, Plug-and-Play 방식으로 범용성을 가집니다.

# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### **논문에서 제시한 결과**

**"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"**는 대규모 언어 모델(LLMs)의 KV 캐시를 압축하는 FastGen 알고리즘을 통해 **메모리 사용량 감소**와 **성능 손실 최소화**라는 두 가지 목표를 동시에 달성했습니다. 주요 결과는 다음과 같습니다:

---

### **1. 주요 결과**

#### **1.1 메모리 사용량 감소**
- **FastGen은 KV 캐시를 최대 50% 압축하면서도 성능 손실을 최소화**했습니다.
- LLaMA 모델(7B~65B)에 대해 테스트:
  - **65B 모델**: KV 캐시를 약 44.9% 압축하여 메모리 사용량을 약 11.8GB로 감소.
  - **7B 모델**: KV 캐시를 약 30.8% 압축하며 메모리 사용량 1.9GB.

#### **1.2 성능 손실 최소화**
- FastGen은 95% 이상의 주의력 맵 복원률을 유지하며, 다른 고정 압축 방식 대비 성능 유지율이 높았습니다.
- 실험 결과:
  - **LLaMA 65B**에서 KV 캐시 44.9% 압축 시, 성능 손실이 거의 없는 "Win Rate 45%"를 기록.

#### **1.3 추론 속도 향상**
- FastGen은 GPU 메모리 병목 현상을 줄여 **추론 속도**를 향상:
  - Hugging Face의 기본 추론 방식 대비 최대 **55%** 속도 증가.
  - DeepSpeed 대비 최대 **17.4%** 속도 향상.

#### **1.4 다른 방법과 비교한 결과**
- 고정된 압축 방법(Frequency + Local)과 비교 시:
  - **FastGen**은 모든 모델 크기에서 더 높은 메모리 절약 비율 및 성능 유지율을 달성.

---

### **2. 다른 방법론과의 비교: FastGen의 차별성**

FastGen은 기존 방법론에 비해 다음과 같은 독특한 강점을 가지고 있습니다:

#### **2.1 적응형 압축**
- 기존의 고정된 KV 캐시 압축 방법과 달리, FastGen은 **모델 내부 Attention Head의 구조적 패턴을 분석**하여 각 헤드에 맞는 압축 정책을 동적으로 적용합니다.
  - **특수 토큰(Cspecial)**: 문장 시작 또는 명령어 토큰만 유지.
  - **구두점(Cpunct.)**: 문법적으로 중요한 구두점만 유지.
  - **지역성(Clocal)**: 현재 토큰과 가까운 문맥만 유지.
  - **빈도 기반(Cfrequent)**: 주의력이 높은 토큰만 유지.

#### **2.2 무손실 성능 유지**
- 기존 방법은 캐시 압축 과정에서 주의력 맵의 손실로 인해 성능이 저하되는 경우가 많았으나, FastGen은 95% 이상의 주의력 맵 복원율을 달성.

#### **2.3 Plug-and-Play 방식**
- FastGen은 사전 학습이나 Fine-tuning 없이 기존 모델에 **즉시 적용 가능**.
- 이는 추가 학습 비용을 피할 수 있어 실용성이 매우 높습니다.

---

### **3. FastGen의 성공 요인 (논문에서 제시한 이유)**

#### **3.1 주의력 구조의 다양성**
- 주의력 헤드가 각기 다른 역할(특수 토큰, 구두점, 전체 문맥 등)을 수행한다는 점에 착안하여 **헤드별 맞춤 압축 정책**을 적용.
- 이는 고정된 압축 방식이 모든 헤드에 동일한 정책을 적용했던 문제를 해결.

#### **3.2 경량화된 프로파일링**
- 입력 프롬프트만을 기반으로 각 Attention Head의 구조를 분석하여 **최적의 압축 정책**을 선택.
- 모델 성능을 보존하면서 메모리 절약.

#### **3.3 하이브리드 정책**
- 단일 정책이 아닌, 네 가지 압축 정책의 조합을 사용하여 최적화:
  - 예: 특수 토큰(Cspecial)과 구두점(Cpunct.)을 동시에 유지하는 혼합 정책.

#### **3.4 실험적 검증**
- LLaMA 모델(7B, 13B, 30B, 65B)과 다양한 작업(수학 문제, 코딩, 질문 응답)을 통해 FastGen의 범용성과 효과를 검증.
- 실험에서 50% 압축 시에도 성능 유지율이 높음을 입증.

---

### **4. 내 생각: FastGen의 의의와 향후 방향**

#### **4.1 FastGen의 의의**
1. **효율성과 성능의 균형**:
   - 메모리 최적화와 성능 손실 최소화의 균형을 이룬 FastGen은 LLM 추론 효율화를 위한 중요한 돌파구를 제공합니다.

2. **적응형 학습의 가능성**:
   - Attention 구조를 실시간으로 분석해 최적의 정책을 적용하는 FastGen의 접근법은 **다른 영역**(예: 데이터 증강, 모델 압축)에서도 응용 가능.

3. **실용성**:
   - Fine-tuning이 필요 없고, Plug-and-Play 방식으로 기존 LLM에 쉽게 통합 가능.

#### **4.2 향후 연구 방향**
1. **다양한 모델과 작업 확장**:
   - FastGen을 GPT-4, PaLM 등 더 큰 모델과 다양한 도메인 작업에 적용하여 범용성을 확인할 필요가 있음.

2. **정교한 정책 탐색**:
   - 현재 사용된 네 가지 정책 외에도 더 정교한 패턴 탐색 및 압축 방법을 개발.

3. **다양한 효율화 기법과 통합**:
   - FastGen을 **양자화(Quantization)**, **지식 증류(Distillation)**와 결합하여 더 큰 효율성을 달성.

4. **멀티모달 LLM 확장**:
   - 텍스트뿐 아니라 이미지, 오디오와 같은 멀티모달 입력에서도 FastGen이 적용될 수 있는지 연구.

---

### **5. 결론**

FastGen은 LLM의 KV 캐시 압축을 통해 **GPU 메모리 병목 현상**을 해결하고, **성능 손실 없이** 추론 효율성을 극대화하는 혁신적인 방법입니다. 이 알고리즘은 적응형 접근법과 실용성을 결합하여, 대규모 언어 모델이 요구하는 높은 연산 및 메모리 자원 문제를 완화하는 데 중요한 기여를 합니다. 

이를 통해 LLM의 접근성을 높이고, 다양한 응용 분야에서 비용 효율적인 활용 가능성을 제시합니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘

# A :

### **논문의 입력 데이터와 추론 과정**

#### **1. 입력 데이터**
이 논문에서 사용하는 입력 데이터는 주로 **텍스트 기반**이며, 주의력(Attention) 구조를 분석하고 적응형 KV 캐시 압축을 적용하기 위해 필요합니다.

##### **입력 데이터의 형태**
- **프롬프트(Prompt)**: 모델이 텍스트 생성을 시작하기 위해 제공받는 초기 텍스트.
  - 예: `"The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more, how many apples are left?"`

---

#### **2. 추론 과정**

FastGen은 입력 데이터를 기반으로 KV 캐시(Key-Value Cache)를 생성 및 압축하여 추론을 수행합니다. 이 과정을 단계별로 설명하겠습니다.

---

##### **2.1 Prompt Encoding (KV 캐시 생성)**

1. **입력 토큰화(Tokenization)**:
   - 텍스트 입력을 토큰 단위로 변환.
   - 예: `"The cafeteria had 23 apples"` → `[101, 2026, 3893, 2097, 23, 1066]`.

2. **KV 벡터 생성**:
   - 각 입력 토큰은 모델의 Transformer 블록에서 Key와 Value 벡터로 변환.
   - 예: 
     - \(K = [\mathbf{k_1}, \mathbf{k_2}, \mathbf{k_3}, ...]\),
     - \(V = [\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3}, ...]\).

3. **주의력 맵 계산**:
   - Attention Score \(A\) 계산:
     \[
     A = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}}\right)
     \]
     여기서 \(Q\): Query 벡터, \(K\): Key 벡터, \(d\): 차원 수.

---

##### **2.2 KV 캐시 압축 (FastGen 적용)**

1. **주의력 맵 분석**:
   - 주의력 \(A\)를 기반으로 각 Attention Head가 가장 중요한 정보를 담고 있는 패턴을 분석.
   - 예:
     - Head 1: 특수 토큰만 중요 → **Cspecial**.
     - Head 2: 구두점 중요 → **Cpunct.**.
     - Head 3: 가까운 문맥 중요 → **Clocal**.

2. **압축 정책 적용**:
   - 선택된 정책에 따라 불필요한 \(K\)와 \(V\) 벡터를 제거.
   - 예:
     - \(Cspecial\): `<s>`와 같은 특수 토큰만 유지.
     - \(Clocal\): 최근 \(n\)개의 토큰만 유지.

---

##### **2.3 Token Generation (추론)**

1. **새로운 토큰 생성**:
   - 이전 토큰의 \(K, V\)와 현재 Query \(Q\)를 사용해 Attention Score 계산.
   - 예:
     ```
     Input: "The cafeteria had 23 apples."
     Generated Token: "If"
     ```

2. **KV 캐시 갱신**:
   - 새로 생성된 토큰의 \(K, V\)를 추가.
   - 압축 정책에 따라 오래되거나 중요도가 낮은 \(K, V\)를 삭제.

3. **출력**:
   - 최종적으로 모델이 "23 - 20 + 6 = 9"라는 과정을 추론하여 답변 생성.
   ```
   Output: "The cafeteria has 9 apples left."
   ```

---

### **모델 아키텍처 구성**

#### **1. Transformer 기반 아키텍처**
FastGen은 LLaMA, GPT 등의 Transformer 기반 아키텍처에서 작동합니다.

1. **Self-Attention 블록**:
   - 주요 구성 요소:
     - Query (\(Q\)), Key (\(K\)), Value (\(V\)) 벡터 생성.
     - Attention Score 계산 및 Context Vector 생성.
   - Attention Score:
     \[
     A = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}}\right)
     \]

2. **Feed-Forward Network (FFN)**:
   - 각 Attention 헤드의 출력을 비선형 변환.
   - Fully Connected Layer와 활성화 함수로 구성.

3. **Multi-Head Attention**:
   - 여러 Attention Head를 병렬로 처리하여 각 Head가 고유한 정보 패턴을 학습.

---

### **모델 연산 및 요구량 분석**

#### **1. 연산량 (FLOPs)**

##### **1.1 Attention 계산**
- Attention Score 연산:
  \[
  O(n^2 \cdot d)
  \]
  여기서 \(n\): 입력 시퀀스 길이, \(d\): 모델 차원.
  - 예: \(n=2048, d=512\)일 때, 한 Attention Layer에서 약 \(2.1 \times 10^6\) FLOPs.

##### **1.2 KV 캐시 압축**
- FastGen 압축은 정책에 따라 일부 벡터를 삭제하므로 연산량 감소.
- 정책 예:
  - \(Clocal\): 최근 \(m\)개의 토큰만 유지 → \(m \ll n\)이므로 \(O(m^2 \cdot d)\)로 감소.

---

#### **2. 메모리 요구량**

##### **2.1 기본 KV 캐시**
- 각 토큰당 \(K, V\) 벡터는 모델 차원 \(d\)를 가짐.
- 총 메모리 사용량:
  \[
  \text{Memory} = 2 \cdot n \cdot d \cdot \text{float size}
  \]
  - 예: \(n=2048, d=512, \text{float size}=4 \text{bytes}\)일 때 약 8MB.

##### **2.2 압축 후 KV 캐시**
- FastGen 압축으로 불필요한 \(K, V\) 벡터를 삭제하여 메모리 사용량 감소.
- 실험 결과:
  - LLaMA-65B 기준으로 약 45% 메모리 절약.

---

#### **3. 컴퓨팅 요구량**

##### **3.1 GPU 메모리**
- 기본 LLaMA-65B: 약 22.1GB GPU 메모리 필요.
- FastGen 압축 적용 시:
  - 약 44.9% 압축 → 메모리 사용량 12.2GB로 감소.

##### **3.2 추론 속도**
- 압축 적용 시, 불필요한 KV 캐시를 삭제하므로 **속도 증가**.
- 실험 결과:
  - Hugging Face 추론 대비 최대 **55% 속도 향상**.

---

### **결론**

- **입력 데이터와 추론 과정**: FastGen은 KV 캐시의 압축 정책을 각 Attention Head에 맞게 적용하여 불필요한 정보를 제거하고, 긴 문맥에서도 효율적으로 추론.
- **모델 아키텍처**: Transformer의 Self-Attention 구조를 기반으로 작동하며, Multi-Head Attention과 Feed-Forward Network를 활용.
- **연산 및 메모리 요구량**: KV 캐시 압축을 통해 메모리 사용량과 컴퓨팅 비용을 줄이며, 추론 속도를 대폭 향상.

FastGen은 실용적이고 확장 가능한 방식으로 LLM의 메모리 병목 문제를 해결하며, 다양한 환경에서 효율적으로 작동할 수 있습니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :
