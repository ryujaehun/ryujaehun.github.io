---
title: "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention"
date: 2025-04-14
categories:
 - paper-review
 - with-gpt
---

{% capture time %}{{ content | reading_time }}{% endcapture %}
   <p>ì´ í¬ìŠ¤íŠ¸ë¥¼ ì½ëŠ”ë° ì•½ {{ time }} {% if time == '1' %}ë¶„{% else %}ë¶„{% endif %} ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2312.07987v3)

# Q : ì´ ë…¼ë¬¸ì„ ì•„ì£¼ ìì„¸í•˜ê²Œ ì½ê³  ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì ì„ ì„¤ëª…í•´ì£¼ê³  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œ ì…ë ¥ì„ ë“¤ì–´ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜ ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ì˜ í•œê³„ì ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜



ë…¼ë¬¸ ã€ŒSwitchHead: Accelerating Transformers with Mixture-of-Experts Attentionã€ì„ ë§¤ìš° ìì„¸í•˜ê²Œ ì½ê³  ë¶„ì„í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì , í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì˜ ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì˜ˆì‹œ ì…ë ¥ì„ ì´ìš©í•œ ë™ì‘ ê³¼ì •ì„ ì†Œê°œí•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ í•œê³„ì ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

---

## 1. ë…¼ë¬¸ì˜ í•µì‹¬ ë° ê²°ë¡  ìš”ì•½:

**SwitchHeadëŠ” Transformerì˜ Self-Attentionì¸µì— Mixture of Experts(MoE)ë¥¼ ì ìš©í•´ ê³„ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì ˆì•½í•˜ëŠ” ê¸°ë²•ì´ë‹¤.**  
- ê¸°ì¡´ Transformer ëª¨ë¸ì˜ Self-Attentionì€ ë§ì€ ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ì„ ìš”êµ¬í•œë‹¤.  
- SwitchHeadëŠ” Attention ë‚´ì˜ Valueì™€ Output projectionì„ ì—¬ëŸ¬ Expertsë¡œ êµ¬ì„±í•˜ì—¬, ì†Œìˆ˜ì˜ Attention Matrixë§Œ ê³„ì‚°í•˜ê³ ë„ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.
- ì‹¤í—˜ì ìœ¼ë¡œ ìµœëŒ€ **8ë°° ë” ì ì€ Attention Matrix** ê³„ì‚°ìœ¼ë¡œ **ê¸°ì¡´ Transformer ëª¨ë¸ì˜ ì„±ëŠ¥(Perplexity)ì„ ìœ ì§€**í–ˆë‹¤.
- 262M í¬ê¸°ì˜ ëª¨ë¸ ê¸°ì¤€, ê¸°ì¡´ Transformerì˜ **44%ì˜ ê³„ì‚°ëŸ‰**ê³¼ **27%ì˜ ë©”ëª¨ë¦¬**ë§Œ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

---

## 2. ë…¼ë¬¸ì˜ ê°•ì  ë° ë…ì°½ì  ê¸°ì—¬:

- **Attentionì¸µì˜ íš¨ìœ¨ì  MoE ì ìš©**: ê¸°ì¡´ ì—°êµ¬ëŠ” Feed-forwardì¸µì—ë§Œ MoEë¥¼ ì ìš©í•˜ëŠ” ë° ì§‘ì¤‘í–ˆë˜ ë°˜ë©´, SwitchHeadëŠ” Attention ì¸µ ìì²´ë¥¼ MoEë¡œ êµ¬ì„±í•´ ë§¤ìš° í° íš¨ìœ¨ì„± ì¦ëŒ€ë¥¼ ì´ë¤˜ë‹¤.
- **ë…ë¦½ì ì´ê³  ì•ˆì •ì ì¸ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜**: ë¹„ê²½ìŸì ì¸ sigmoid ê¸°ë°˜ì˜ expert ì„ íƒ ë°©ì‹(Ïƒ-MoE)ì„ ì ìš©í•˜ì—¬ ë³„ë„ì˜ Regularization ì—†ì´ ì•ˆì •ì ì¸ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤.
- **íš¨ìœ¨ì ì´ê³  ê°„ê²°í•œ ì„¤ê³„**: Keyì™€ Queryë¥¼ ê³µìœ í•˜ê³ , Valueì™€ Outputë§Œ MoEë¡œ êµ¬ì„±í•¨ìœ¼ë¡œì¨ ë¶ˆí•„ìš”í•œ ë³µì¡ì„±ì„ ìµœì†Œí™”í–ˆë‹¤.
- **í˜„ì‹¤ì  ë²¤ì¹˜ë§ˆí¬ ë° í‰ê°€**: C4, Wikitext-103, Enwik8 ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œì˜ ì‹¤í—˜ì„ í†µí•´ ë²”ìš©ì„±ì„ ì…ì¦í–ˆë‹¤. ë˜í•œ zero-shot ì„±ëŠ¥ í‰ê°€ì—ì„œë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.

---

## 3. í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… ë° ì˜ˆì‹œ:

SwitchHeadì˜ ì•Œê³ ë¦¬ì¦˜ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

### Step-by-step ì•Œê³ ë¦¬ì¦˜ ë™ì‘:

1. ì…ë ¥ \(x \in \mathbb{R}^{T\times d_{model}}\)ë¥¼ ë°›ëŠ”ë‹¤.
2. ê° Attention head \(h\) ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ Expert ì„ íƒì„ ìœ„í•œ gate ê°’ì„ ê³„ì‚°í•œë‹¤.

   - Source (Value) side gate \(s_S^h\):
     \[
     s_S^h = \sigma(x W_S^h) \quad,\quad W_S^h \in \mathbb{R}^{d_{model}\times E}
     \]

   - Destination (Output) side gate \(s_D^h\):
     \[
     s_D^h = \sigma(x W_D^h) \quad,\quad W_D^h \in \mathbb{R}^{d_{model}\times E}
     \]

3. Gate ê°’ì´ ë†’ì€ ìƒìœ„ \(k\)ê°œì˜ Expertë¥¼ ì„ íƒí•˜ì—¬ Valueì™€ Output projection ìˆ˜í–‰:
   - Value projection \(V^h\):
     \[
     V^h = \sum_{e\in E_S^h} s_S^h[e] \cdot (xW_V^{h,e}), \quad E_S^h = \text{arg top-k}(s_S^h)
     \]

   - Key, QueryëŠ” ê³µìœ ë˜ì–´ \(K^h = xW_K^h,\; Q^h = xW_Q^h\)ë¡œ ê³„ì‚°ë¨.

4. Attention í–‰ë ¬ ê³„ì‚°:
   \[
   A^h = \text{softmax}\left(\frac{Q^h (K^h)^T}{\sqrt{d_{head}}}\right)
   \]

5. ìµœì¢… Output ê³„ì‚°:
   \[
   y = \sum_{h=0}^{n_{heads}-1}\sum_{e\in E_D^h} s_D^h[e] \cdot A^h V^h W_O^{h,e}
   \]

### ê°„ë‹¨í•œ ì˜ˆì‹œ ì…ë ¥ ë° ê²°ê³¼:

- ì…ë ¥: \(x\) (ê°„ëµí•œ ì˜ˆì‹œ)
- Gate ê³„ì‚°: ê°ê° \(s_S^h, s_D^h\)ì—ì„œ ë†’ì€ ê°’ìœ¼ë¡œ ìƒìœ„ Expert ì„ íƒ (ì˜ˆ: 5ê°œ ì¤‘ 2ê°œ ì„ íƒ)
- ì„ íƒëœ Expertë§Œì„ ì´ìš©í•˜ì—¬ Value ë° Output Projection ìˆ˜í–‰
- í•˜ë‚˜ì˜ Attention í–‰ë ¬ \(A^h\) ê³„ì‚° ë° ìµœì¢… Output ì¶œë ¥

ì´ ë°©ì‹ìœ¼ë¡œ, Attention Matrix ê³„ì‚°ì´ í¬ê²Œ ê°ì†Œí•˜ê³ , ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ê·¹ëŒ€í™”ë¨.

---

## 4. ë¹„êµ í‰ê°€ (ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ë¹„êµ):

| Model                      | Params | Perplexity (â†“) | MACs (ê³„ì‚°ëŸ‰)  | Memory Usage   |
| -------------------------- | ------ | -------------- | -------------- | -------------- |
| Transformer (Dense)        | 262M   | 9.66           | 5.4G           | 21.0M          |
| SwitchHead                 | 262M   | **9.55**       | **2.0G (44%)** | **2.9M (27%)** |
| MoA (Mixture of Attention) | 262M   | 9.50           | 2.9G           | 9.9M           |

SwitchHeadëŠ” ë™ê¸‰ ì„±ëŠ¥ì—ì„œ ê°€ì¥ ë‚®ì€ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë³´ì—¬ì¤€ë‹¤.

---

## 5. ë…¼ë¬¸ì˜ í•œê³„ì :

- **ê·œëª¨ í™•ì¥ì„± ê²€ì¦ ë¶€ì¡±**: ë…¼ë¬¸ì€ 262M í¬ê¸° ëª¨ë¸ê¹Œì§€ ê²€ì¦í–ˆì§€ë§Œ, GPT-3ë‚˜ GPT-4ì™€ ê°™ì€ ì´ˆëŒ€í˜• ëª¨ë¸ì—ì„œì˜ ì„±ëŠ¥ì´ ì¶”ê°€ì ìœ¼ë¡œ ê²€ì¦ë˜ì–´ì•¼ í•œë‹¤.
- **í•˜ë“œì›¨ì–´ ìµœì í™” í•„ìš”ì„±**: MoE ì—°ì‚°ì€ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì†ë„ í¸ì°¨ê°€ í¬ë©°, ì „ìš© ì—°ì‚° ì»¤ë„ì´ ì—†ìœ¼ë©´ íš¨ìœ¨ì„±ì´ ì œí•œë  ìˆ˜ ìˆë‹¤. Triton kernelì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì—¬ì „íˆ cuBLAS ëŒ€ë¹„ 60% ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì´ë‹¤.
- **ëª¨ë¸ ë³‘ë ¬í™” êµ¬í˜„ì˜ ë³µì¡ì„±**: ì „ë¬¸ê°€(expert)ë“¤ì´ ì—¬ëŸ¬ GPUì— ë¶„ì‚°ë  ê²½ìš° ë™ì  ë¡œë“œ ë°¸ëŸ°ì‹±ì´ í•„ìš”í•´ êµ¬í˜„ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆë‹¤.

---

## ê²°ë¡  ë° ì¶”ê°€ ë…¼ì˜:

SwitchHeadëŠ” Self-Attention ì¸µì˜ MoE ì„¤ê³„ë¥¼ ê°„ê²°í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í˜„í•œ ë§¤ìš° ë…ì°½ì ì¸ ë…¼ë¬¸ì´ë‹¤. íŠ¹íˆ Valueì™€ Output projectionë§Œ Expertë¡œ êµ¬ì„±í•˜ê³ , Keyì™€ QueryëŠ” ê³µìœ í•˜ëŠ” ì„¤ê³„ëŠ” ë…íŠ¹í•˜ë©°, ì‹¤í—˜ì ìœ¼ë¡œë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ ì‹¤ìš©ì ìœ¼ë¡œë„ ë§¤ìš° ê°€ì¹˜ê°€ ìˆë‹¤.

ë‹¤ë§Œ, ì´ˆëŒ€í˜• ëª¨ë¸ì—ì„œì˜ ì„±ëŠ¥ ë° í•˜ë“œì›¨ì–´ íŠ¹í™” êµ¬í˜„ì€ ì¶”ê°€ ì—°êµ¬ê°€ ìš”êµ¬ëœë‹¤. ì¶”í›„ ì—°êµ¬ì—ì„œëŠ” ë” í° ëª¨ë¸ê³¼ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê³ , ì—°ì‚° ìµœì í™” ë°©ì•ˆì„ ì‹¬í™” ì—°êµ¬í•´ì•¼ í•  ê²ƒì´ë‹¤.

# Q : ë…¼ë¬¸ì˜ í•µì‹¬ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ì •ë¦¬í•´ ì¤˜



ë…¼ë¬¸ ã€SwitchHead: Accelerating Transformers with Mixture-of-Experts Attentionã€ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì•„ë˜ì—ì„œ ë§¤ìš° ìì„¸í•˜ê³  ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì˜ˆì‹œë¥¼ í†µí•´ ì „ì²´ì ì¸ ê³¼ì •ì„ ëª…í™•íˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

---

## ğŸ“Œ **SwitchHeadì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê°œë…**

SwitchHeadì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **"í•˜ë‚˜ì˜ Attention head ë‚´ì—ì„œ, ì—¬ëŸ¬ ê°œì˜ Value ë° Output projectionì„ ë¯¸ë¦¬ ì¤€ë¹„í•˜ê³ , ì…ë ¥ì— ë”°ë¼ ê·¸ ì¤‘ ì¼ë¶€ì˜ Expertë§Œ ì„ íƒí•˜ì—¬ ê³„ì‚°í•œë‹¤"** ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ Transformer Attentionì€ ê° headë§ˆë‹¤ í•œ ìŒì˜ ê³ ì •ëœ projectionì´ ìˆì§€ë§Œ, SwitchHeadëŠ” **ë™ì  ì„ íƒì„ í†µí•´ ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.** 

êµ¬ì²´ì ìœ¼ë¡œ ê° headëŠ” ë‹¤ìŒì˜ ìš”ì†Œë¥¼ ê°€ì§‘ë‹ˆë‹¤:

- í•˜ë‚˜ì˜ Query projection
- í•˜ë‚˜ì˜ Key projection
- **ì—¬ëŸ¬ ê°œì˜ (Eê°œì˜) Value projection** ì¤‘ ì¼ë¶€ (kê°œ) ì„ íƒ
- **ì—¬ëŸ¬ ê°œì˜ (Eê°œì˜) Output projection** ì¤‘ ì¼ë¶€ (kê°œ) ì„ íƒ

---

## ğŸ“Œ **ìƒì„¸ ì•Œê³ ë¦¬ì¦˜ ê³¼ì • (Step-by-Step)**

### ğŸ” **Step 0: ì…ë ¥ ì„¤ì • (Example)**

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ìˆë‹¤ê³  í•˜ê² ìŠµë‹ˆë‹¤:

- ì…ë ¥ ë²¡í„° \( x \in \mathbb{R}^{T \times d_{model}} \)  
  - \( T = 3 \) (ë¬¸ì¥ ê¸¸ì´)
  - \( d_{model} = 4 \) (ëª¨ë¸ ì°¨ì›)

ì‹¤ì œ ì˜ˆì‹œ (ì„ì˜ ê°’):

\[
x = \begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\[6pt]
0.5 & 0.6 & 0.7 & 0.8 \\[6pt]
0.9 & 1.0 & 1.1 & 1.2
\end{bmatrix}
\]

---

### ğŸ” **Step 1: Gate ê³„ì‚° (Expert ì„ íƒ)**

ê° head(\(h\))ë§ˆë‹¤ expertë¥¼ ì„ íƒí•˜ëŠ” Gateê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

- Source-side(Value) gate \( s_S^h \in \mathbb{R}^{T\times E} \)  
- Destination-side(Output) gate \( s_D^h \in \mathbb{R}^{T\times E} \)  

ê°„ëµí•œ ì˜ˆë¡œ headê°€ 1ê°œ(h=1), Expertê°€ 3ê°œ(E=3)ì¸ ê²½ìš°:

- \( W_S^h, W_D^h \in \mathbb{R}^{d_{model} \times E} = (4\times3) \)

Gate ê°’ ê³„ì‚°ì‹ì€:

\[
s_S^h = \sigma(xW_S^h), \quad s_D^h = \sigma(xW_D^h)
\]

ê³„ì‚° í›„ì˜ ì˜ˆì‹œ gate ê°’ (ì„ì˜ì˜ sigmoid ì¶œë ¥ê°’ ì˜ˆ):

| Token | \( s_S^h \) (Value-side gate) | \( s_D^h \) (Output-side gate) |
| ----- | ----------------------------- | ------------------------------ |
| 1     | [0.8, 0.1, 0.3]               | [0.2, 0.9, 0.5]                |
| 2     | [0.2, 0.7, 0.4]               | [0.6, 0.1, 0.7]                |
| 3     | [0.5, 0.4, 0.9]               | [0.3, 0.8, 0.2]                |

ìœ„ ê°’ì—ì„œ k=2ê°œì˜ ìƒìœ„ gate ê°’ì„ ê°€ì§„ Expertë¥¼ ê° í† í°ë³„ë¡œ ì„ íƒí•©ë‹ˆë‹¤:

- Value-side ì„ íƒ: í† í° 1 (Expert 1,3), í† í° 2 (Expert 2,3), í† í° 3 (Expert 1,3)
- Output-side ì„ íƒ: í† í° 1 (Expert 2,3), í† í° 2 (Expert 1,3), í† í° 3 (Expert 2,1)

---

### ğŸ” **Step 2: Key, Query, Value projection ê³„ì‚°**

- Queryì™€ KeyëŠ” ì¼ë°˜ì ì¸ Transformerì™€ ë™ì¼í•˜ê²Œ ê³ ì • projection í–‰ë ¬ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤:
\[
Q^h = xW_Q^h, \quad K^h = xW_K^h
\]

- ValueëŠ” Expert projectionì„ ì‚¬ìš©í•˜ë©°, ì„ íƒëœ Expertë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:
\[
V^h = \sum_{e \in E_S^h} s_S^h[e]\cdot(xW_V^{h,e})
\]

(ê°ê° \(W_Q^h, W_K^h, W_V^{h,e}\)ëŠ” \(d_{model}\times d_{head}\) ì°¨ì›ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.)

---

### ğŸ” **Step 3: Attention í–‰ë ¬ ê³„ì‚°**

ì¼ë°˜ì ì¸ Transformerì™€ ë™ì¼í•œ Attention Matrixë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
\[
A^h = \text{softmax}\left(\frac{Q^h(K^h)^T}{\sqrt{d_{head}}}\right)
\]

ì—¬ê¸°ì„œ \(A^h\)ëŠ” (3Ã—3) í–‰ë ¬ì…ë‹ˆë‹¤ (ì…ë ¥ ê¸¸ì´ 3 ê¸°ì¤€).

---

### ğŸ” **Step 4: ìµœì¢… ì¶œë ¥ (Output projection) ê³„ì‚°**

Output projection ì—­ì‹œ ì„ íƒëœ Expertë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:
\[
y = \sum_{e \in E_D^h} s_D^h[e]\cdot(A^hV^hW_O^{h,e})
\]

ì´ë¡œì¨ ìµœì¢… ì¶œë ¥ \(y \in \mathbb{R}^{T\times d_{model}}\) ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

---

## ğŸ“Œ **ì „ì²´ ì•Œê³ ë¦¬ì¦˜ ìš”ì•½ ì •ë¦¬**

| ë‹¨ê³„   | ê³¼ì • ìš”ì•½                                            | ê³„ì‚°ë°©ì‹ ë° íŠ¹ì§•                   |
| ------ | ---------------------------------------------------- | ---------------------------------- |
| Step 1 | Gate ê³„ì‚° ë° Expert ì„ íƒ                             | ì…ë ¥ë³„ ë…ë¦½ì  sigmoid gate ê³„ì‚°    |
| Step 2 | Key, Query(ê³ ì •), Value(Expert ì„ íƒ) projection ê³„ì‚° | ì„ íƒëœ Expertë§Œ ê³„ì‚° (íš¨ìœ¨ì„± ì¦ëŒ€) |
| Step 3 | Attention í–‰ë ¬ ê³„ì‚°                                  | ê¸°ì¡´ Attentionê³¼ ë™ì¼              |
| Step 4 | Output projection (Expert ì„ íƒ) ë° ìµœì¢… ê²°ê³¼ ê³„ì‚°    | ì„ íƒëœ Expertë§Œìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì‚°ì¶œ |

---

## ğŸ“Œ **SwitchHeadì˜ ì´ì  ë° ê¸°ì—¬**

- **íš¨ìœ¨ì  ì—°ì‚°**: ëª¨ë“  Expertë¥¼ ë‹¤ ê³„ì‚°í•˜ì§€ ì•Šê³  ì…ë ¥ì— ë”°ë¼ ì¼ë¶€ Expertë§Œ ë™ì ìœ¼ë¡œ ì„ íƒ.
- **ìì› ì ˆì•½**: Transformerì˜ Attention ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ íšê¸°ì ìœ¼ë¡œ ê°ì†Œì‹œí‚´.
- **ì„±ëŠ¥ ìœ ì§€**: ì ì€ ì—°ì‚°ëŸ‰ìœ¼ë¡œë„ ì„±ëŠ¥(perplexity)ì€ ê¸°ì¡´ì˜ dense ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ ìœ ì§€.

---

ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ SwitchHeadëŠ” íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œë„, Transformer ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” í˜ì‹ ì ì¸ Mixture-of-Experts ê¸°ë°˜ Attention ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.



# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë¥¼ ìì„¸í•˜ê²Œ ë³´ê³  ë‹¤ë¥¸ ë°©ë²•ë¡ ì— ë¹„í•˜ì—¬ íŠ¹ì¶œë‚œ ì ê³¼ ë…¼ë¬¸ì—ì„œ ì œê¸°í•˜ëŠ” ì–´ë– í•œ ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ì´ìœ ì™€ ë„ˆì˜ ìƒê°ì„ ì•Œë ¤ì¤˜



ë…¼ë¬¸ ã€SwitchHead: Accelerating Transformers with Mixture-of-Experts Attentionã€ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì•„ë˜ì—ì„œ ë§¤ìš° ìì„¸í•˜ê³  ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì˜ˆì‹œë¥¼ í†µí•´ ì „ì²´ì ì¸ ê³¼ì •ì„ ëª…í™•íˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

---

## ğŸ“Œ **SwitchHeadì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê°œë…**

SwitchHeadì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **"í•˜ë‚˜ì˜ Attention head ë‚´ì—ì„œ, ì—¬ëŸ¬ ê°œì˜ Value ë° Output projectionì„ ë¯¸ë¦¬ ì¤€ë¹„í•˜ê³ , ì…ë ¥ì— ë”°ë¼ ê·¸ ì¤‘ ì¼ë¶€ì˜ Expertë§Œ ì„ íƒí•˜ì—¬ ê³„ì‚°í•œë‹¤"** ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ Transformer Attentionì€ ê° headë§ˆë‹¤ í•œ ìŒì˜ ê³ ì •ëœ projectionì´ ìˆì§€ë§Œ, SwitchHeadëŠ” **ë™ì  ì„ íƒì„ í†µí•´ ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.** 

êµ¬ì²´ì ìœ¼ë¡œ ê° headëŠ” ë‹¤ìŒì˜ ìš”ì†Œë¥¼ ê°€ì§‘ë‹ˆë‹¤:

- í•˜ë‚˜ì˜ Query projection
- í•˜ë‚˜ì˜ Key projection
- **ì—¬ëŸ¬ ê°œì˜ (Eê°œì˜) Value projection** ì¤‘ ì¼ë¶€ (kê°œ) ì„ íƒ
- **ì—¬ëŸ¬ ê°œì˜ (Eê°œì˜) Output projection** ì¤‘ ì¼ë¶€ (kê°œ) ì„ íƒ

---

## ğŸ“Œ **ìƒì„¸ ì•Œê³ ë¦¬ì¦˜ ê³¼ì • (Step-by-Step)**

### ğŸ” **Step 0: ì…ë ¥ ì„¤ì • (Example)**

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ìˆë‹¤ê³  í•˜ê² ìŠµë‹ˆë‹¤:

- ì…ë ¥ ë²¡í„° \( x \in \mathbb{R}^{T \times d_{model}} \)  
  - \( T = 3 \) (ë¬¸ì¥ ê¸¸ì´)
  - \( d_{model} = 4 \) (ëª¨ë¸ ì°¨ì›)

ì‹¤ì œ ì˜ˆì‹œ (ì„ì˜ ê°’):

\[
x = \begin{bmatrix}
0.1 & 0.2 & 0.3 & 0.4 \\[6pt]
0.5 & 0.6 & 0.7 & 0.8 \\[6pt]
0.9 & 1.0 & 1.1 & 1.2
\end{bmatrix}
\]

---

### ğŸ” **Step 1: Gate ê³„ì‚° (Expert ì„ íƒ)**

ê° head(\(h\))ë§ˆë‹¤ expertë¥¼ ì„ íƒí•˜ëŠ” Gateê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

- Source-side(Value) gate \( s_S^h \in \mathbb{R}^{T\times E} \)  
- Destination-side(Output) gate \( s_D^h \in \mathbb{R}^{T\times E} \)  

ê°„ëµí•œ ì˜ˆë¡œ headê°€ 1ê°œ(h=1), Expertê°€ 3ê°œ(E=3)ì¸ ê²½ìš°:

- \( W_S^h, W_D^h \in \mathbb{R}^{d_{model} \times E} = (4\times3) \)

Gate ê°’ ê³„ì‚°ì‹ì€:

\[
s_S^h = \sigma(xW_S^h), \quad s_D^h = \sigma(xW_D^h)
\]

ê³„ì‚° í›„ì˜ ì˜ˆì‹œ gate ê°’ (ì„ì˜ì˜ sigmoid ì¶œë ¥ê°’ ì˜ˆ):

| Token | \( s_S^h \) (Value-side gate) | \( s_D^h \) (Output-side gate) |
| ----- | ----------------------------- | ------------------------------ |
| 1     | [0.8, 0.1, 0.3]               | [0.2, 0.9, 0.5]                |
| 2     | [0.2, 0.7, 0.4]               | [0.6, 0.1, 0.7]                |
| 3     | [0.5, 0.4, 0.9]               | [0.3, 0.8, 0.2]                |

ìœ„ ê°’ì—ì„œ k=2ê°œì˜ ìƒìœ„ gate ê°’ì„ ê°€ì§„ Expertë¥¼ ê° í† í°ë³„ë¡œ ì„ íƒí•©ë‹ˆë‹¤:

- Value-side ì„ íƒ: í† í° 1 (Expert 1,3), í† í° 2 (Expert 2,3), í† í° 3 (Expert 1,3)
- Output-side ì„ íƒ: í† í° 1 (Expert 2,3), í† í° 2 (Expert 1,3), í† í° 3 (Expert 2,1)

---

### ğŸ” **Step 2: Key, Query, Value projection ê³„ì‚°**

- Queryì™€ KeyëŠ” ì¼ë°˜ì ì¸ Transformerì™€ ë™ì¼í•˜ê²Œ ê³ ì • projection í–‰ë ¬ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤:
\[
Q^h = xW_Q^h, \quad K^h = xW_K^h
\]

- ValueëŠ” Expert projectionì„ ì‚¬ìš©í•˜ë©°, ì„ íƒëœ Expertë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:
\[
V^h = \sum_{e \in E_S^h} s_S^h[e]\cdot(xW_V^{h,e})
\]

(ê°ê° \(W_Q^h, W_K^h, W_V^{h,e}\)ëŠ” \(d_{model}\times d_{head}\) ì°¨ì›ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.)

---

### ğŸ” **Step 3: Attention í–‰ë ¬ ê³„ì‚°**

ì¼ë°˜ì ì¸ Transformerì™€ ë™ì¼í•œ Attention Matrixë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:
\[
A^h = \text{softmax}\left(\frac{Q^h(K^h)^T}{\sqrt{d_{head}}}\right)
\]

ì—¬ê¸°ì„œ \(A^h\)ëŠ” (3Ã—3) í–‰ë ¬ì…ë‹ˆë‹¤ (ì…ë ¥ ê¸¸ì´ 3 ê¸°ì¤€).

---

### ğŸ” **Step 4: ìµœì¢… ì¶œë ¥ (Output projection) ê³„ì‚°**

Output projection ì—­ì‹œ ì„ íƒëœ Expertë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:
\[
y = \sum_{e \in E_D^h} s_D^h[e]\cdot(A^hV^hW_O^{h,e})
\]

ì´ë¡œì¨ ìµœì¢… ì¶œë ¥ \(y \in \mathbb{R}^{T\times d_{model}}\) ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

---

## ğŸ“Œ **ì „ì²´ ì•Œê³ ë¦¬ì¦˜ ìš”ì•½ ì •ë¦¬**

| ë‹¨ê³„   | ê³¼ì • ìš”ì•½                                            | ê³„ì‚°ë°©ì‹ ë° íŠ¹ì§•                   |
| ------ | ---------------------------------------------------- | ---------------------------------- |
| Step 1 | Gate ê³„ì‚° ë° Expert ì„ íƒ                             | ì…ë ¥ë³„ ë…ë¦½ì  sigmoid gate ê³„ì‚°    |
| Step 2 | Key, Query(ê³ ì •), Value(Expert ì„ íƒ) projection ê³„ì‚° | ì„ íƒëœ Expertë§Œ ê³„ì‚° (íš¨ìœ¨ì„± ì¦ëŒ€) |
| Step 3 | Attention í–‰ë ¬ ê³„ì‚°                                  | ê¸°ì¡´ Attentionê³¼ ë™ì¼              |
| Step 4 | Output projection (Expert ì„ íƒ) ë° ìµœì¢… ê²°ê³¼ ê³„ì‚°    | ì„ íƒëœ Expertë§Œìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì‚°ì¶œ |

---

## ğŸ“Œ **SwitchHeadì˜ ì´ì  ë° ê¸°ì—¬**

- **íš¨ìœ¨ì  ì—°ì‚°**: ëª¨ë“  Expertë¥¼ ë‹¤ ê³„ì‚°í•˜ì§€ ì•Šê³  ì…ë ¥ì— ë”°ë¼ ì¼ë¶€ Expertë§Œ ë™ì ìœ¼ë¡œ ì„ íƒ.
- **ìì› ì ˆì•½**: Transformerì˜ Attention ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ íšê¸°ì ìœ¼ë¡œ ê°ì†Œì‹œí‚´.
- **ì„±ëŠ¥ ìœ ì§€**: ì ì€ ì—°ì‚°ëŸ‰ìœ¼ë¡œë„ ì„±ëŠ¥(perplexity)ì€ ê¸°ì¡´ì˜ dense ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ ìœ ì§€.

---

ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ SwitchHeadëŠ” íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œë„, Transformer ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” í˜ì‹ ì ì¸ Mixture-of-Experts ê¸°ë°˜ Attention ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.



# Q : ì´ ë…¼ë¬¸ì˜ ì…ë ¥ë°ì´í„°ì™€ ì¶”ë¡  ê³¼ì •ì— ëŒ€í•´ì„œ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì£¼ê² ë‹ˆ? ì¶”ê°€ì ìœ¼ë¡œ ëª¨ë¸ì•„í‚¤í…ì²˜ì˜ êµ¬ì„± ë° ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ì—°ì‚°ê³¼ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì»´í“¨íŒ… ìš”êµ¬ëŸ‰ ëŒ€í•´ì„œë„ ì„¤ëª…í•´ì¤˜



ì¢‹ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” Transformerì˜ Attention Layerë¥¼ Mixture-of-Experts(MoE)ë¡œ í™•ì¥í•œ **SwitchHead** ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ë©°, íŠ¹íˆ **ì…ë ¥ ì²˜ë¦¬, ì¶”ë¡  ê³¼ì •, ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì„±, ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰**ì„ ëª¨ë‘ ì •ëŸ‰ì ìœ¼ë¡œ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì˜ˆì‹œ ê¸°ë°˜ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## ğŸ“Œ 1. ì…ë ¥ ë°ì´í„° (ì˜ˆì‹œ í¬í•¨)

### ğŸ”· ë°ì´í„° í˜•ì‹

ë…¼ë¬¸ì—ì„œëŠ” ì¼ë°˜ì ì¸ ìì—°ì–´ì²˜ë¦¬ ë°ì´í„°ì…‹(C4, Wikitext103 ë“±)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆì‹œë¡œ í† í° ì‹œí€€ìŠ¤:

```plaintext
Input sequence: ["The", "cat", "sat", "on", "the", "mat", "."]
```

ì´ë¥¼ subword tokenizer (SentencePiece, vocab size=8k)ë¡œ ë¶„ì ˆí•˜ë©´:

```plaintext
Tokens: [312, 532, 771, 128, 312, 949, 9]
```

ì´ê²ƒì€ embeddingì„ í†µí•´ ë²¡í„°ë¡œ ë³€í™˜ë©ë‹ˆë‹¤:

- ê° í† í° â†’ \( d_{model} \)-ì°¨ì›ì˜ ë²¡í„° (ì˜ˆ: \( d_{model} = 512 \))
- ê²°ê³¼: \( x \in \mathbb{R}^{T \times d_{model}} \), ì˜ˆ: \( x \in \mathbb{R}^{7 \times 512} \)

---

## ğŸ“Œ 2. ì¶”ë¡  ê³¼ì • (ì˜ˆì‹œ ê¸°ë°˜ Step-by-Step)

ì•„ë˜ì—ì„œëŠ” SwitchHead attentionì˜ í•œ layerê°€ ì‘ë™í•˜ëŠ” ê³¼ì •ì„ ì˜ˆì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

### âš™ï¸ ì „ì œ ì¡°ê±´ (ì„¤ì • ì˜ˆì‹œ)
- ì…ë ¥ ê¸¸ì´ \( T = 7 \)
- \( d_{model} = 512 \), \( d_{head} = 128 \)
- Head ìˆ˜ \( n_{heads} = 2 \), Expert ìˆ˜ \( E = 4 \), Top-k = 2

---

### ğŸ”¶ [1] **Query, Key ê³„ì‚° (ê³ ì • projection)**

- ê° head \( h \in \{1, 2\} \)ì— ëŒ€í•´ ê³ ì • projection í–‰ë ¬ \( W_Q^h, W_K^h \in \mathbb{R}^{512 \times 128} \) ì ìš©

\[
Q^h = xW_Q^h, \quad K^h = xW_K^h
\]

â†’ \( Q^h, K^h \in \mathbb{R}^{7 \times 128} \)

---

### ğŸ”¶ [2] **Expert gate ê³„ì‚° ë° ì„ íƒ (MoE for Value/Output)**

- Value side gate: \( s_S^h = \sigma(xW_S^h) \), \( W_S^h \in \mathbb{R}^{512 \times E} \)
- Output side gate: \( s_D^h = \sigma(xW_D^h) \), \( W_D^h \in \mathbb{R}^{512 \times E} \)

â†’ ì˜ˆë¥¼ ë“¤ì–´ í† í° 3ì— ëŒ€í•´:

```plaintext
s_S^h = [0.1, 0.8, 0.05, 0.7] â†’ top-k = {2, 4}
s_D^h = [0.6, 0.3, 0.9, 0.2] â†’ top-k = {1, 3}
```

---

### ğŸ”¶ [3] **Value Projection (Weighted Expert Mix)**

ì„ íƒëœ expertì— ëŒ€í•´ì„œë§Œ value projection:

\[
V^h = \sum_{e \in E_S^h} s_S^h[e] \cdot (x W_V^{h,e}), \quad W_V^{h,e} \in \mathbb{R}^{512 \times 128}
\]

â†’ ìµœì¢… \( V^h \in \mathbb{R}^{7 \times 128} \)

---

### ğŸ”¶ [4] **Attention ê³„ì‚°**

\[
A^h = \text{softmax}\left(\frac{Q^h (K^h)^T}{\sqrt{d_{head}}}\right) \in \mathbb{R}^{7 \times 7}
\]

â†’ ê° headë§ˆë‹¤ ë‹¨ í•˜ë‚˜ì˜ attention matrixë§Œ ê³„ì‚°í•¨.

---

### ğŸ”¶ [5] **Output projection (Weighted Expert Mix)**

\[
y = \sum_{e \in E_D^h} s_D^h[e] \cdot (A^h V^h W_O^{h,e})
\quad W_O^{h,e} \in \mathbb{R}^{128 \times 512}
\]

â†’ ìµœì¢… \( y \in \mathbb{R}^{7 \times 512} \)

---

## ğŸ“Œ 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì„±

SwitchHeadëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Transformer-XL ë˜ëŠ” RoPE ê¸°ë°˜ standard Transformer êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜:

### ğŸ”§ í•µì‹¬ ë³€ê²½ì :
| Layer êµ¬ì„± ìš”ì†Œ       | ë³€ê²½ ì „ (ê¸°ì¡´ Transformer) | SwitchHead ë³€ê²½ì‚¬í•­                   |
| --------------------- | -------------------------- | ------------------------------------- |
| Query/Key projection  | ê³ ì •, headë³„ projection    | ë™ì¼                                  |
| **Value projection**  | ê³ ì •                       | **headë³„ MoE ì„ íƒì  projection**      |
| **Output projection** | ê³ ì •                       | **headë³„ MoE ì„ íƒì  projection**      |
| Attention ê³„ì‚°        | ê° headë§ˆë‹¤ 1ê°œ            | ë™ì¼ (ì¤‘ìš”: expertë§ˆë‹¤ X, headë§ˆë‹¤ O) |
| Feedforward layer     | MLP (ê³ ì •)                 | Ïƒ-MoEë¡œ ëŒ€ì²´ ê°€ëŠ¥ (SwitchAllì˜ ê²½ìš°)  |

---

## ğŸ“Œ 4. ì—°ì‚°ëŸ‰ ë° ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ë¶„ì„

ë…¼ë¬¸ì—ì„œëŠ” Multiply-Accumulate operations (MACs) ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.

### âš™ï¸ ì—°ì‚°ëŸ‰(MACs): ê¸°ë³¸ Transformer vs SwitchHead ë¹„êµ

| í•­ëª©                  | Transformer                               | SwitchHead (value/output MoE)                                    |
| --------------------- | ----------------------------------------- | ---------------------------------------------------------------- |
| Projection ì—°ì‚°ëŸ‰     | \(4T d_{model} d_{head} \cdot n_{heads}\) | \(2T d_{model} d_{head} \cdot n_{heads} + 2Tkd_{model}d_{head}\) |
| Attention ì—°ì‚°ëŸ‰      | \(2T^2 d_{head} \cdot n_{heads}\)         | ë™ì¼                                                             |
| Output readout ì—°ì‚°ëŸ‰ | \(T d_{head} d_{model} \cdot n_{heads}\)  | \(2Tk d_{head} d_{model}\)                                       |

â†’ ì „ì²´ MACs ë¹„êµ:

\[
\text{SwitchHead MACs} \approx 40\% \sim 50\% ìˆ˜ì¤€
\]

### ğŸ“¦ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| í•­ëª©             | Transformer                          | SwitchHead                       |
| ---------------- | ------------------------------------ | -------------------------------- |
| Attention matrix | \(n_{heads} \cdot T^2\)              | ë™ì¼                             |
| Projection cache | \(n_{heads} \cdot T \cdot d_{head}\) | ë™ì¼                             |
| Expert weights   | ì—†ìŒ                                 | **í™œì„± expertì— ëŒ€í•œ ê°’ë§Œ ì €ì¥** |

â†’ ë…¼ë¬¸ ì‹¤í—˜ ê¸°ì¤€ ìµœëŒ€ **73% ë©”ëª¨ë¦¬ ì ˆê°**  
(ex: 262M ëª¨ë¸ì—ì„œ 21M â†’ 2.9M float ìˆ˜ì¹˜ë¡œ ê°ì†Œ)

---

## ğŸ“Œ ìš”ì•½ ì •ë¦¬

| í•­ëª©                | SwitchHeadì˜ íŠ¹ì§•                                                              |
| ------------------- | ------------------------------------------------------------------------------ |
| ì…ë ¥ í˜•íƒœ           | Subword token sequence, embedding: \( x \in \mathbb{R}^{T \times d_{model}} \) |
| Attention êµ¬ì¡° ë³€ê²½ | Query/Key: ê³ ì •, Value/Output: MoE expert ì„ íƒ                                 |
| Attention Matrix ìˆ˜ | Headë§ˆë‹¤ 1ê°œë§Œ ê³„ì‚° (ê¸°ì¡´ MoAë³´ë‹¤ í¬ê²Œ ì ˆê°)                                   |
| Expert ì„ íƒ ë°©ì‹    | Sigmoid-based ë¹„ê²½ìŸ gate (Ïƒ-MoE)                                              |
| ì—°ì‚°ëŸ‰ (MAC ê¸°ì¤€)   | Dense ëŒ€ë¹„ ìµœëŒ€ 60% ì´ìƒ ì ˆê°                                                  |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰       | Dense ëŒ€ë¹„ ìµœëŒ€ 73% ê°ì†Œ                                                       |
| ì¶”ë¡  ì†ë„           | Wall-clock ê¸°ì¤€ 1.5ë°° ì´ìƒ ë¹ ë¦„ (ì‹¤í—˜ì—ì„œ í™•ì¸ë¨)                              |

---

SwitchHeadëŠ” MoE Attention ì¤‘ì—ì„œë„ ê°€ì¥ "í˜„ì‹¤ì ì´ê³  ì•ˆì •ì ì´ë©° ê³„ì‚° íš¨ìœ¨ì ì¸ ë°©ì‹"ìœ¼ë¡œ, íŠ¹íˆ inference-time costì™€ memory footprintê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œ ê°•ë ¥í•œ ì„ íƒì§€ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë°”ì¼/ì—£ì§€ ë””ë°”ì´ìŠ¤, ì¥ê¸° ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë“±ì—ì„œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.

# Q : ì´ ë…¼ë¬¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì–´ë–¤ ì—°êµ¬íë¦„ì´ ìˆëŠ”ì§€ ì •ë¦¬í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì¤˜



ì´ ë…¼ë¬¸ ã€SwitchHead: Accelerating Transformers with Mixture-of-Experts Attentionã€ì€ Attention Layerì— MoEë¥¼ ì ìš©í•˜ì—¬ ìì› íš¨ìœ¨ì„±ì„ í¬ê²Œ ë†’ì¸ í˜ì‹ ì ì¸ ì—°êµ¬ì§€ë§Œ, ì—¬ì „íˆ ëª‡ ê°€ì§€ **êµ¬ì¡°ì  í•œê³„**ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ìµœê·¼ ì—°êµ¬ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

---

## âœ… ë…¼ë¬¸ì˜ ì£¼ìš” í•œê³„ ì •ë¦¬

| í•œê³„ì                       | ìƒì„¸ ì„¤ëª…                                                                     |
| --------------------------- | ----------------------------------------------------------------------------- |
| â‘  ëª¨ë¸ í¬ê¸° ì œí•œ            | ì‹¤í—˜ ëª¨ë¸ì´ 262M ê·œëª¨ë¡œ, GPT-3 ìˆ˜ì¤€(10B~175B)ì˜ ì´ˆëŒ€í˜• ëª¨ë¸ì—ì„œëŠ” íš¨ê³¼ ë¯¸í™•ì¸ |
| â‘¡ MoE Load Balancing ë¯¸ì§€ì› | Expert ê°„ ë¶€í•˜ ë¶ˆê· í˜• ë°œìƒ ê°€ëŠ¥ì„± ì¡´ì¬. ì‹¤ì œ ëŒ€ê·œëª¨ ë¶„ì‚°ì—ì„œëŠ” ë³‘ëª© ê°€ëŠ¥ì„±    |
| â‘¢ Triton kernel ì„±ëŠ¥ ë¯¸í¡   | í˜„ì¬ êµ¬í˜„ëœ ì»¤ë„ì´ cuBLAS ëŒ€ë¹„ 60% ìˆ˜ì¤€ìœ¼ë¡œ suboptimal                        |
| â‘£ Layer-level sparsity ë¶€ì¡± | Attention ë‚´ sparsityë§Œ ì ìš©, ì „ì²´ layer êµ¬ì¡° ìˆ˜ì¤€ì—ì„œ sparsity ì„¤ê³„ ë¶€ì¡±     |
| â‘¤ Expert ì„ íƒì˜ ì§€ì—­ì„± ê²°ì—¬ | í† í° ê°„ ìƒê´€ê´€ê³„ ë°˜ì˜ ì—†ì´ expert ì„ íƒ â†’ ì •ë³´ íë¦„ ì™œê³¡ ê°€ëŠ¥ì„±                |

---

## ğŸ” ìœ„ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ì£¼ìš” ì—°êµ¬ íë¦„

ì•„ë˜ì—ì„œëŠ” ê° í•œê³„ì ì— ëŒ€ì‘í•˜ëŠ” í˜„ì¬ì˜ ì—°êµ¬ íë¦„ì„ êµ¬ì²´ì  ë°©ë²•ë¡ ê³¼ í•¨ê»˜ ì •ë¦¬í•©ë‹ˆë‹¤.

---

### ğŸ”¹ [1] **ì´ˆëŒ€í˜• ëª¨ë¸ í™•ì¥ì„ ìœ„í•œ Sparse Routing ìµœì í™”**

#### ğŸ“˜ ê´€ë ¨ íë¦„:
- **GShard, Switch Transformer (Fedus et al.)**
- **Unified Routing (Clark et al., 2022)**

#### ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´:
- Expert ê°„ ë¼ìš°íŒ…ì„ í•™ìŠµ ê°€ëŠ¥í•œ ì •ì±…ìœ¼ë¡œ ì„¤ê³„ (e.g., RL ê¸°ë°˜, differentiable top-k)
- MoE expertë“¤ì„ **ë‹¤ì¤‘ ë ˆë²¨ ê³„ì¸µ êµ¬ì¡°**ë¡œ êµ¬ì„±í•´ ëŒ€ê·œëª¨ ëª¨ë¸ì—ë„ ì í•©í•˜ê²Œ ì„¤ê³„

#### ğŸ§  ì ìš© ê°€ëŠ¥ ë°©í–¥:
- SwitchHeadì—ì„œë„ expertë¥¼ ê° layerë§ˆë‹¤ ê³ ì •ëœ êµ¬ì¡°ë¡œ ë‘˜ ê²ƒì´ ì•„ë‹ˆë¼, **hierarchical expert routing**ì„ ì ìš©í•˜ë©´ ì´ˆëŒ€í˜• ëª¨ë¸ë¡œ í™•ì¥ ê°€ëŠ¥

---

### ğŸ”¹ [2] **Load Balancing ë° Expert Activation ê· í˜•í™”**

#### ğŸ“˜ ê´€ë ¨ íë¦„:
- **BASE Layers (Lewis et al., 2021)**
- **Soft Mixture of Experts (Zhou et al., 2022)**

#### ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´:
- Routing networkê°€ íŠ¹ì • expertì— ê³¼ë„í•˜ê²Œ ì§‘ì¤‘ë˜ì§€ ì•Šë„ë¡, regularization ë˜ëŠ” auxiliary loss (e.g., entropy loss, usage loss)ë¥¼ ì¶”ê°€
- Soft routingì„ í†µí•´ í™œì„±í™” scoreë¥¼ ë¶€ë“œëŸ½ê²Œ ë¶„ì‚°ì‹œí‚´

#### ğŸ§  ì ìš© ê°€ëŠ¥ ë°©í–¥:
- SwitchHeadì˜ Ïƒ-MoEëŠ” ì•ˆì •ì ì´ì§€ë§Œ **expert ê°„ activation í¸í–¥** ë¬¸ì œëŠ” ì—¬ì „íˆ ì¡´ì¬  
â†’ ì´ë¥¼ ìœ„í•´ soft usage lossë‚˜ **entropy-based expert dispersion loss** ì¶”ê°€ê°€ ìœ íš¨

---

### ğŸ”¹ [3] **í•˜ë“œì›¨ì–´ ìµœì í™”ë¥¼ ìœ„í•œ ì»¤ë„ ê°œì„ **

#### ğŸ“˜ ê´€ë ¨ íë¦„:
- **Flash-MoE (Zhang et al., 2023)**
- **FlashAttention-2 (Dao et al., 2023)**

#### ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´:
- GPU ë©”ëª¨ë¦¬ bandwidthì™€ compute tradeoffë¥¼ ê°ì•ˆí•˜ì—¬, kernel ë‚´ attention & projectionì„ **fused ì—°ì‚°**ìœ¼ë¡œ ì²˜ë¦¬
- CUDA-aware/IO-aware optimization ë„ì…

#### ğŸ§  ì ìš© ê°€ëŠ¥ ë°©í–¥:
- SwitchHeadì˜ MoE projectionì„ Tritonì´ ì•„ë‹Œ **fused CUDA kernel**ë¡œ ì‘ì„± â†’ cuBLAS ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„  (ìµœëŒ€ 2ë°°)

---

### ğŸ”¹ [4] **Layer-Level Sparse ëª¨ë“ˆê³¼ì˜ ê²°í•©**

#### ğŸ“˜ ê´€ë ¨ íë¦„:
- **SparseMixture (Lepikhin et al., 2021)**  
- **LayerDrop + Sparse Routing** (Zhang et al., 2023)

#### ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´:
- íŠ¹ì • layer ì „ì²´ë¥¼ í† í°ë§ˆë‹¤ skipí•˜ê±°ë‚˜ expertë¡œ ë¼ìš°íŒ…í•¨ìœ¼ë¡œì¨ **ì „ì²´ ë ˆë²¨ì—ì„œ sparsity ì œì–´**
- ê° LayerëŠ” MoE-FF, MoE-Attn, Dense ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©

#### ğŸ§  ì ìš© ê°€ëŠ¥ ë°©í–¥:
- SwitchHeadë¥¼ ë‹¨ë…ìœ¼ë¡œ ì“°ê¸°ë³´ë‹¤ëŠ” SwitchFF, SwitchDense, SparseLayer ë“±ê³¼ ê²°í•©í•œ **"Fully Conditional Sparse Stack"** êµ¬ì„± ê°€ëŠ¥

---

### ğŸ”¹ [5] **Locality-aware Expert Routing / Token Clustering ê¸°ë°˜ ë¶„ì‚°**

#### ğŸ“˜ ê´€ë ¨ íë¦„:
- **Token-level Routing (Shazeer et al., 2017)**  
- **Dynamic MoE Routing with Token Clusters (2023)**

#### ğŸ“Œ í•µì‹¬ ì•„ì´ë””ì–´:
- Query ê°„ ìœ ì‚¬ë„ì— ê¸°ë°˜í•œ **grouping í›„ expert ì„ íƒ**
- ì˜ˆ: ë¹„ìŠ·í•œ í† í°ë“¤ì€ ë™ì¼ expertì— í• ë‹¹ë˜ì–´ í•™ìŠµ íš¨ìœ¨ ì¦ê°€

#### ğŸ§  ì ìš© ê°€ëŠ¥ ë°©í–¥:
- í˜„ì¬ SwitchHeadëŠ” **token-local gate**ë§Œ ì‚¬ìš©  
â†’ Attention head ë‚´ì—ì„œ Query ìœ ì‚¬ë„ ê¸°ë°˜ **context-aware expert routing** ì ìš©í•˜ë©´ ì„±ëŠ¥ ë° ì •ë³´ íë¦„ ê°•í™” ê°€ëŠ¥

---

## ğŸ“Œ í†µí•©ì  ì ìš© ì˜ˆì‹œ: Future SwitchHead++

| ì ìš© ê¸°ìˆ                          | ê¸°ëŒ€ íš¨ê³¼                                        |
| --------------------------------- | ------------------------------------------------ |
| Hierarchical MoE                  | ì´ˆëŒ€í˜• ëª¨ë¸ í™•ì¥                                 |
| Load Balancing Loss               | expert ê°„ í¸í–¥ ìµœì†Œí™”                            |
| FlashAttention-2 ê¸°ë°˜ CUDA kernel | í•™ìŠµ/ì¶”ë¡  ì†ë„ ê°œì„  (1.5x ì´ìƒ)                  |
| Layer-wise MoE (SwitchAll++)      | ëª¨ë¸ ì „ì²´ì—ì„œ sparsity ê·¹ëŒ€í™”                    |
| Token Clustering Routing          | ì˜ë¯¸ ê¸°ë°˜ expert ì„ íƒ â†’ ì •ë³´ ë³´ì¡´ ë° í‘œí˜„ë ¥ ì¦ê°€ |

---

## âœ… ê²°ë¡ 

SwitchHeadëŠ” Transformer attention íš¨ìœ¨í™”ë¥¼ ì´ëˆ ë§¤ìš° ê°•ë ¥í•œ ê¸°ì´ˆ êµ¬ì¡°ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ìŒì˜ ë°©í–¥ìœ¼ë¡œ í™•ì¥ ì‹œ ë”ìš± ì‹¤ìš©ì ì´ê³  ê°•ë ¥í•œ ì‹œìŠ¤í…œì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ì´ˆëŒ€í˜• ëª¨ë¸ ì ìš© ê°€ëŠ¥ì„± í™•ë³´ (scalability)**
- **expert ê°„ í•™ìŠµ/ì¶”ë¡  ë³‘ë ¬ì„± ìµœì í™” (load balance)**
- **context-aware routingìœ¼ë¡œ í‘œí˜„ë ¥ ìœ ì§€**
- **Flash ê¸°ë°˜ ìµœì  ì»¤ë„ ì—°ì‚° ì ìš©**

ì•ìœ¼ë¡œì˜ ì—°êµ¬ì—ì„œëŠ” SwitchHead êµ¬ì¡°ë¥¼ **í•˜ë“œì›¨ì–´ ì¹œí™”ì ì´ê³  context-adaptiveí•œ ì „ì²´ Sparse LLM ìŠ¤íƒì˜ ì¼ë¶€ë¡œ í†µí•©**í•˜ëŠ” ë°©í–¥ì´ ìœ ë§í•©ë‹ˆë‹¤.


# Q : SwitchHeadëŠ” ì™œ Queryì™€ Key projectionì€ ê³ ì •ì‹œí‚¤ê³  Valueì™€ Output projectionë§Œ MoEë¡œ êµ¬ì„±í–ˆì„ ë•Œ ê°€ì¥ íš¨ìœ¨ì ì¸ ê²°ê³¼ë¥¼ ë³´ì˜€ëŠ”ê°€? ì´ëŠ” Attention ë©”ì»¤ë‹ˆì¦˜ì˜ êµ¬ì¡°ì  íŠ¹ì„±ê³¼ ì–´ë–¤ ê´€ë ¨ì´ ìˆì„ê¹Œ?

 

ì´ ì§ˆë¬¸ì€ SwitchHeadì˜ í•µì‹¬ ì„¤ê³„ ì„ íƒì— ëŒ€í•œ í†µì°°ì„ ìš”êµ¬í•˜ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤. ê²°ë¡ ë¶€í„° ë§í•˜ë©´:

**Queryì™€ KeyëŠ” Attention Matrix ìƒì„±ì„ ìœ„í•œ â€œê³µí†µ ì—°ì‚°â€ì´ë¯€ë¡œ, ì´ë¥¼ ê³ ì •ì‹œí‚¤ê³  Valueì™€ Outputë§Œ MoEë¡œ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì—°ì‚° íš¨ìœ¨ì„±ê³¼ í•™ìŠµ ì•ˆì •ì„± ì–‘ë©´ì—ì„œ ìœ ë¦¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.**

ì•„ë˜ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ êµ¬ì¡°ì  ì´ìœ ì™€ ì‹¤í—˜ì  ì¦ê±°ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

---

## âœ… ì™œ Query/KeyëŠ” ê³ ì •í•˜ê³ , Value/Outputë§Œ MoEë¡œ í•´ì•¼ íš¨ìœ¨ì ì¸ê°€?

### 1. **Query-KeyëŠ” Attention Matrixë¥¼ ìƒì„±í•˜ëŠ” ìƒí˜¸ì‘ìš©ì˜ â€œí•µì‹¬â€**
- Attention í–‰ë ¬ \( A = \text{softmax}(QK^T / \sqrt{d}) \)ëŠ” Queryì™€ Keyì˜ ë‚´ì ì„ í†µí•´ ìƒì„±ë¨
- ë§Œì•½ Qì™€ Kê°€ ê°ê° MoE êµ¬ì¡°ë¡œ tokenë§ˆë‹¤ ë‹¤ë¥´ê²Œ ìƒì„±ë˜ë©´, ê° tokenë§ˆë‹¤ **ë‹¤ë¥¸ attention í–‰ë ¬**ì„ ê³„ì‚°í•´ì•¼ í•¨
  â†’ ì´ëŠ” ê³§ **ëª¨ë“  token ê°„ ì¡°í•©ì— ëŒ€í•´ attention matrixë¥¼ ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ì—°ì‚°ëŸ‰ì´ í­ì¦**

ì˜ˆì‹œ:
- \( T \)ê°œì˜ tokenì´ ìˆë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œ \( T \times T \) attention matrix 1ê°œë©´ ë˜ì§€ë§Œ,
- Q, Kê°€ MoEë¼ë©´ í† í°ë§ˆë‹¤ K/Qê°€ ë‹¤ë¥´ë¯€ë¡œ \( T \times T \) ê°œ attention scoreê°€ ê°ê¸° ë‹¤ë¥´ê²Œ ê³„ì‚°ë˜ì–´ì•¼ í•¨

> âœ… **ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ â€œsource/destination routingì´ attention matrix ê³„ì‚°ì„ ë°©í•´í•œë‹¤â€ê³  í‘œí˜„í•©ë‹ˆë‹¤.**

---

### 2. **ë°˜ë©´, Value/Output projectionì€ Attention ê²°ê³¼ ì´í›„ ë‹¨ê³„ â†’ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬ ê°€ëŠ¥**

- Attention matrixê°€ ì´ë¯¸ ê³ ì •ë˜ì–´ ìˆë‹¤ë©´,
  - ê° í† í°ì— ëŒ€í•´ ì„ íƒëœ Value expertë¡œë¶€í„° context vector \( A \cdot V \)ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ token-localí•œ ì‘ì—…
  - Output projectionë„ tokenë§ˆë‹¤ ë‹¤ë¥¸ expertë¥¼ ì ìš© ê°€ëŠ¥ (ì„ íƒëœ \( W_O^e \))

> âœ… **Value/Outputì€ Attention ì´í›„ ë‹¨ê³„ì´ë¯€ë¡œ expertë§ˆë‹¤ ë…ë¦½ì  ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê³ , ê³µìœ ëœ attention matrixë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥**

---

## âœ… ë…¼ë¬¸ ë‚´ ì‹¤í—˜ ê²°ê³¼ì™€ ê·¼ê±°

ë…¼ë¬¸ Table 6 (Sec. A.3)ì—ì„œ ë‹¤ì–‘í•œ ì¡°í•©ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµë¥¼ ì œì‹œí•©ë‹ˆë‹¤:

| V   | K   | Q   | O   | Perplexity â†“          |
| --- | --- | --- | --- | --------------------- |
| Y   | N   | N   | Y   | **12.27** (ìµœê³  ì„±ëŠ¥) |
| N   | N   | N   | Y   | 12.30                 |
| Y   | Y   | Y   | Y   | 12.57                 |
| Y   | Y   | N   | Y   | 12.37                 |
| N   | Y   | N   | Y   | 12.36                 |
| Y   | N   | Y   | N   | 12.51                 |
| N   | Y   | Y   | N   | 12.90                 |

â†’ **Query/Keyë¥¼ MoEë¡œ ë°”ê¾¸ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ í•˜ë½í•˜ë©°, ë¶ˆí•„ìš”í•œ ì—°ì‚°ë§Œ ì¦ê°€**

---

## âœ… êµ¬ì¡°ì  íŠ¹ì„±ê³¼ì˜ ì •ë¦¬ëœ ì—°ê´€ì„±

| ìš”ì†Œ       | ì—­í•                            | MoE ì í•©ì„± | ì´ìœ                                     |
| ---------- | ------------------------------ | ---------- | --------------------------------------- |
| Query (Q)  | ê° í† í°ì˜ ì •ë³´ ìˆ˜ì§‘ ëŒ€ìƒ ê²°ì •  | âœ– ë¶€ì í•©   | ëª¨ë“  Kì™€ì˜ ìƒí˜¸ì‘ìš© í•„ìš” â†’ global       |
| Key (K)    | ëª¨ë“  í† í°ì˜ feature ì œê³µ       | âœ– ë¶€ì í•©   | ëª¨ë“  Qì™€ì˜ ìƒí˜¸ì‘ìš© í•„ìš” â†’ global       |
| Value (V)  | Attentionìœ¼ë¡œ ê°€ì¤‘ í‰ê· ë  ì •ë³´ | âœ” ì í•©     | ê°€ì¤‘í•© ëŒ€ìƒì´ë¯€ë¡œ token-local           |
| Output (O) | ìµœì¢… representation ê³„ì‚°       | âœ” ì í•©     | ë…ë¦½ì ì¸ í›„ì²˜ë¦¬ â†’ expert selection ê°€ëŠ¥ |

---

## âœ… ìš”ì•½ (í•µì‹¬ ì •ë¦¬)

- **Query/KeyëŠ” token-to-token ê°„ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì „ì—­ ì—°ì‚°ì´ë¯€ë¡œ, MoEë¡œ ë§Œë“¤ ê²½ìš° attention matrix ê³µìœ ê°€ ë¶ˆê°€ëŠ¥í•´ì§€ê³  ì—°ì‚°ëŸ‰ì´ í­ì¦**
- **Value/Outputì€ attention ì´í›„ì˜ token-local ì—°ì‚°ì´ë¯€ë¡œ, MoE ì ìš© ì‹œ ì„ íƒì  sparsityë¥¼ í™œìš©í•´ íš¨ìœ¨ì„± í–¥ìƒ**
- **SwitchHeadëŠ” ì´ êµ¬ì¡°ì  íŠ¹ì„±ì„ í™œìš©í•´ ìµœì†Œí•œì˜ ì—°ì‚°ìœ¼ë¡œ ìµœëŒ€ì˜ íš¨ê³¼ë¥¼ ë‚´ëŠ” ì„¤ê³„ë¥¼ íƒí–ˆë‹¤**

ë”°ë¼ì„œ, ì´ ì„ íƒì€ ë‹¨ìˆœí•œ ì‹¤í—˜ì  ê²°ê³¼ê°€ ì•„ë‹ˆë¼ **Transformerì˜ attention ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™ì  êµ¬ì¡°ì— ê¸°ì´ˆí•œ ì„¤ê³„ ìµœì í™”**ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# Q : ê¸°ì¡´ MoE ë°©ì‹(ì˜ˆ: MoA, Switch Transformer ë“±)ê³¼ ë¹„êµí•´ SwitchHeadê°€ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ ì—†ì´ í•™ìŠµ ê°€ëŠ¥í•œ í•µì‹¬ ìš”ì¸ì€ ë¬´ì—‡ì´ë©°, ì´ë¥¼ ë‹¤ë¥¸ êµ¬ì¡°(ì˜ˆ: Decoder-only LLM)ì—ë„ í™•ì¥ ê°€ëŠ¥í• ê¹Œ?

 

ê²°ë¡ ë¶€í„° ë§í•˜ë©´, **SwitchHeadê°€ ê¸°ì¡´ MoE ë°©ì‹ë³´ë‹¤ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ ì—†ì´ í•™ìŠµ ê°€ëŠ¥í•œ ì´ìœ ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ì„¤ê³„ ë•ë¶„ì…ë‹ˆë‹¤**:

1. **Attention matrixëŠ” headë§ˆë‹¤ 1ê°œë§Œ ê³„ì‚° (MoA ëŒ€ë¹„ í° ì ˆê°)**
2. **ë¹„ê²½ìŸì  expert ì„ íƒ êµ¬ì¡°(Ïƒ-MoE)ë¡œ ì¸í•´ í•™ìŠµ ì•ˆì •ì„±ì´ ë†’ê³  regularizationì´ ë¶ˆí•„ìš”**

ì´ëŸ¬í•œ ì„¤ê³„ëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ë§¤ìš° ì¼ë°˜ì ì´ê¸° ë•Œë¬¸ì—, **Decoder-only LLM êµ¬ì¡°(ì˜ˆ: GPT ê³„ì—´)**ì—ë„ ì¶©ë¶„íˆ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## âœ… SwitchHeadê°€ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ë™ì‹œì— í™•ë³´í•œ í•µì‹¬ ìš”ì¸ 2ê°€ì§€

### â‘  Attention ê³„ì‚° ë³‘ëª© í•´ì†Œ: "ë‹¨ í•˜ë‚˜ì˜ attention matrixë§Œ ê³„ì‚°"
- ê¸°ì¡´ MoE attention ë°©ì‹ (MoA: Mixture of Attention Heads)ì€ í† í°ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ expertì—ì„œ projectionì„ í•˜ë¯€ë¡œ, **ê° headë§ˆë‹¤ ì—¬ëŸ¬ attention matrixë¥¼ ë”°ë¡œ ê³„ì‚°í•´ì•¼ í•¨**.
  - ì˜ˆ: top-k=4ì¸ ê²½ìš°, headë‹¹ 4ê°œì˜ attention matrix
- **SwitchHeadëŠ” headë§ˆë‹¤ 1ê°œì˜ shared attention matrixë§Œ ì‚¬ìš©**  
  â†’ MoEëŠ” Query/Keyê°€ ì•„ë‹ˆë¼ Value/Outputì—ë§Œ ì ìš©í•˜ë¯€ë¡œ attention ê³„ì‚°ëŸ‰ ì ˆê°

| ë¹„êµ í•­ëª©      | MoA                             | SwitchHead                 |
| -------------- | ------------------------------- | -------------------------- |
| Attention ê³„ì‚° | head Ã— top-k ê°œ                 | head ìˆ˜ë§Œí¼ (top-kì™€ ë¬´ê´€) |
| ì—°ì‚°ëŸ‰ (MACs)  | ë†’ìŒ                            | ìµœëŒ€ 60% ê°ì†Œ              |
| ë©”ëª¨ë¦¬         | attention matrix ë³µìˆ˜ ì €ì¥ í•„ìš” | 1ê°œ per headë§Œ ì €ì¥í•˜ë©´ ë¨ |

â†’ íŠ¹íˆ ì‹œí€€ìŠ¤ ê¸¸ì´ \(T\)ê°€ ê¸´ ê²½ìš° \(O(T^2)\) ë¹„ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆì–´ ë§¤ìš° íš¨ê³¼ì ì„.

---

### â‘¡ Ïƒ-MoE ê¸°ë°˜ ë¹„ê²½ìŸ Expert ì„ íƒìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ

- ê¸°ì¡´ MoE (ì˜ˆ: Switch Transformer)ëŠ” Softmax ê¸°ë°˜ routingì„ ì‚¬ìš©í•˜ì—¬ Expert ê°„ ê²½ìŸ ìœ ë„
  â†’ ë¬¸ì œ: **Expert collapse, high variance**, Regularization í•„ìš”
- **SwitchHeadëŠ” sigmoid ê¸°ë°˜ ë¹„ê²½ìŸ selection(Ïƒ-MoE)** ì‚¬ìš©:
  - Top-k selectionì„ ê° Expertì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰
  - Regularization ì—†ì´ë„ expert ë‹¤ì–‘ì„±ì´ ìœ ì§€ë¨

| ë¹„êµ í•­ëª©           | Switch Transformer | SwitchHead          |
| ------------------- | ------------------ | ------------------- |
| Routing ë°©ì‹        | Softmax ê²½ìŸ ì„ íƒ  | Sigmoid ë¹„ê²½ìŸ ì„ íƒ |
| Regularization í•„ìš” | í•„ìˆ˜               | ë¶ˆí•„ìš”              |
| Collapse ìœ„í—˜       | ìˆìŒ               | ê±°ì˜ ì—†ìŒ           |

â†’ ê²°ê³¼ì ìœ¼ë¡œ í•™ìŠµ ì•ˆì •ì„±ì´ ë†’ê³ , hyperparameter tuning ë¶€ë‹´ë„ ë‚®ìŒ

---

## âœ… Decoder-only LLMì— ì ìš© ê°€ëŠ¥ì„± ë¶„ì„

### SwitchHead êµ¬ì¡°ì˜ ì¼ë°˜ì„±

SwitchHeadëŠ” **Self-Attentionì˜ ê³„ì‚° íŒ¨í„´ë§Œì„ ìˆ˜ì •í•˜ëŠ” êµ¬ì¡°**ë¡œì„œ,
- Encoder/Decoder/Decoder-only ëª¨ë‘ì—ì„œ **Self-Attention block**ë§Œ ì¡´ì¬í•˜ë©´ ì ìš© ê°€ëŠ¥
- ì¦‰, GPT ê³„ì—´ì˜ **Decoder-only êµ¬ì¡°ì—ë„ ì§ì ‘ í™•ì¥ ê°€ëŠ¥**í•¨

#### ì ìš© ì˜ˆ:
- GPT-style blockì—ì„œ Multi-Head Attention ë¶€ë¶„ì„ SwitchHeadë¡œ êµì²´
- Feedforward(MLP) ë¶€ë¶„ì€ ê¸°ì¡´ Ïƒ-MoE ë˜ëŠ” dense ì‚¬ìš© ê°€ëŠ¥

```plaintext
GPT Block:
[LayerNorm] â†’ [SwitchHead Attention] â†’ [Add & Norm] â†’ [MLP / Ïƒ-MoE] â†’ [Add & Norm]
```

### ì ìš© ì‹œ ìœ ì˜ì 

| ê³ ë ¤ ìš”ì†Œ      | ì˜í–¥ ë° ëŒ€ì‘ ì „ëµ                                                     |
| -------------- | --------------------------------------------------------------------- |
| Causal Masking | Attention í–‰ë ¬ \(A = \text{softmax}(QK^T)\) ê³„ì‚° ì‹œ ë™ì¼ ì ìš© ê°€ëŠ¥    |
| KV Cache       | Value MoEê°€ token-dependent â†’ **ì¶”ë¡  ì‹œ expert activation ì €ì¥ í•„ìš”** |
| Memory íš¨ìœ¨    | GPT ì¶”ë¡  ì‹œ í† í°ë‹¹ expert ì„ íƒë§Œ ì €ì¥í•˜ë©´ ë˜ë¯€ë¡œ **í¬ê²Œ ë¬¸ì œ ì—†ìŒ**   |

â†’ ì‹¤ì œë¡œ **GPT-style LLM + SwitchHeadëŠ” ì‹¤ìš©ì  êµ¬ì„±**ì´ë©°, íŠ¹íˆ ì¥ê¸° ë¬¸ë§¥, ëª¨ë°”ì¼ í™˜ê²½ ë“±ì— ë§¤ìš° ì í•©

---

## âœ… ê²°ë¡  ìš”ì•½

| í•­ëª©                         | SwitchHeadì˜ ì´ì                                 |
| ---------------------------- | ------------------------------------------------ |
| Attention Matrix ìˆ˜          | headë‹¹ 1ê°œ (MoA ëŒ€ë¹„ ìµœëŒ€ 8Ã— ì ˆê°)               |
| Routing ë°©ì‹                 | Ïƒ-MoE: ë¹„ê²½ìŸ, regularization ë¶ˆí•„ìš”             |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰                | ìµœëŒ€ 73% ì ˆê° (ë…¼ë¬¸ ê¸°ì¤€)                        |
| ì„±ëŠ¥ ìœ ì§€                    | perplexity ìœ ì§€ ë˜ëŠ” ê°œì„ ë¨ (Table 1, 2, 4 ì°¸ì¡°) |
| Decoder-only LLM ì ìš© ê°€ëŠ¥ì„± | ë†’ìŒ: Self-Attn blockë§Œ ì¡´ì¬í•˜ë©´ ì ìš© ê°€ëŠ¥       |
| ì¶”ê°€ êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­        | KV ìºì‹œ ê´€ë¦¬, expert activation ìºì‹œ í•„ìš”        |

ë”°ë¼ì„œ, SwitchHeadëŠ” **ê¸°ì¡´ MoEë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì´ê³  ì•ˆì •ì ì´ë©°**, **GPTë¥˜ LLMì— ì‰½ê²Œ í†µí•©ë  ìˆ˜ ìˆëŠ” í™•ì¥ì„± ë†’ì€ Attention êµ¬ì¡°**ë¼ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# Q : í˜„ì¬ SwitchHeadëŠ” token-local gateì— ì˜ì¡´í•´ expertë¥¼ ì„ íƒí•˜ëŠ”ë°, token ê°„ì˜ ìƒí˜¸ì‘ìš© ì •ë³´(ì˜ˆ: attention weight, query ìœ ì‚¬ë„ ë“±)ë¥¼ í™œìš©í•œ context-aware routingì´ ì¶”ê°€ëœë‹¤ë©´ ì–´ë–¤ ì´ì ê³¼ ìœ„í—˜ì´ ìˆì„ê¹Œ?

 

ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤. í˜„ì¬ SwitchHeadëŠ” **token-local gate**ë§Œì„ ì‚¬ìš©í•´ expertë¥¼ ì„ íƒí•˜ëŠ” êµ¬ì¡°ì¸ë°, ì—¬ê¸°ì— **token ê°„ ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ í™œìš©í•œ context-aware routing**ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì€ êµ¬ì¡°ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í™•ì¥ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ **ì´ì ê³¼ í•¨ê»˜ êµ¬ì¡°ì , ê³„ì‚°ì  ìœ„í—˜ë„ ë™ë°˜**ë˜ë¯€ë¡œ ì‹ ì¤‘í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì•„ë˜ì—ì„œ êµ¬ì²´ì ì¸ ì¥ë‹¨ì  ë° ì ìš© ê°€ëŠ¥ì„±, ì˜ˆì‹œ, ëŒ€ì•ˆê¹Œì§€ ìì„¸íˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

---

## âœ… ê¸°ë³¸ ì „ì œ: í˜„ì¬ SwitchHeadì˜ Routing ë°©ì‹

í˜„ì¬ SwitchHeadì˜ expert ì„ íƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **í† í°ë³„ë¡œ ë…ë¦½ì ì¸ gate (sigmoid(xW))**ë¥¼ ì‚¬ìš©
- Top-k expertë¥¼ ì„ íƒí•˜ê³ , ì„ íƒëœ expertë§Œ projectionì„ ìˆ˜í–‰
- ì´ ì„ íƒì€ **ë¬¸ë§¥ê³¼ ë¬´ê´€í•˜ë©°, token ìì²´ì˜ embedding ê°’ë§Œ ì‚¬ìš©**

---

## âœ… ë§Œì•½ context-aware routingì„ ì¶”ê°€í•œë‹¤ë©´?

ì˜ˆì‹œì ì¸ ë³€í™”:
- ê¸°ì¡´ gateë¥¼ \( g_i = \sigma(x_iW) \)ì—ì„œ  
â†’ **\( g_i = \sigma(\text{f}(x_i, c_i)) \)** ë¡œ ë³€ê²½
  - ì—¬ê¸°ì„œ \( c_i \)ëŠ” ì£¼ë³€ í† í°ì˜ ì •ë³´ (ì˜ˆ: attention score, query ìœ ì‚¬ë„ ë“±)
  - ì˜ˆ: \( c_i = \sum_j A_{ij}x_j \) (attention-weighted context)

---

## âœ… ì´ì  (ì¥ì )

### 1. **ë¬¸ë§¥ ê¸°ë°˜ expert ì„ íƒ â†’ ë” í‘œí˜„ë ¥ ë†’ì€ ë¶„ê¸°**

- ì˜ˆ: "The bank"ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œ,
  - í˜„ì¬ token "bank"ì˜ ì˜ë¯¸ëŠ” ë¬¸ë§¥ì— ë”°ë¼ "financial" vs "river"ë¡œ ë‹¬ë¼ì§
  - context-aware routingì€ ì£¼ë³€ ë‹¨ì–´ë¡œë¶€í„° ì˜ë¯¸ë¥¼ íŒŒì•…í•´ **ì ì ˆí•œ expertë¥¼ ì„ íƒ** ê°€ëŠ¥

â†’ **ë¬¸ë§¥ì„ ë°˜ì˜í•œ ì˜ë¯¸ì  ë¶„ê¸° êµ¬ì¡° í•™ìŠµ ê°€ëŠ¥**  
â†’ ê° expertì˜ ì˜ë¯¸ì  specialization ê°€ëŠ¥ì„± ì¦ê°€

---

### 2. **Expert redundancy ê°ì†Œ ë° ì••ì¶• í–¥ìƒ**

- í† í° ê°„ ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ê²½ìš° **ê°™ì€ expertë¥¼ ê³µìœ ** ê°€ëŠ¥
- token-local gateëŠ” ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë¼ë„ ì„œë¡œ ë‹¤ë¥¸ expertë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŒ
- context-aware routingì€ ìœ ì‚¬ í† í°ë¼ë¦¬ ê³µìœ í•˜ëŠ” ê²½í–¥ì„ ìœ ë„í•˜ì—¬ **ëª¨ë¸ ì••ì¶•ê³¼ generalizationì— ìœ ë¦¬**

---

### 3. **ì¶”ë¡  ì‹œ interpretability í–¥ìƒ ê°€ëŠ¥**

- attention ê¸°ë°˜ìœ¼ë¡œ expert ì„ íƒì´ ì´ë£¨ì–´ì§€ë©´ **"ì™œ ì´ expertë¥¼ ì¼ëŠ”ê°€?"**ë¥¼ attention mapìœ¼ë¡œ ì‹œê°í™” ê°€ëŠ¥
- ì´ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ í•´ì„ì„±ê³¼ ë””ë²„ê¹…ì— ìœ ë¦¬

---

## âŒ ìœ„í—˜ (ë‹¨ì )

### 1. **ì—°ì‚°ëŸ‰ ì¦ê°€: routing ê³„ì‚°ì´ ë¹„ì„ í˜•í™”ë¨**

- ê¸°ì¡´ gate: ë‹¨ì¼ matmul + sigmoid
- context-aware gate:
  - attention or query ìœ ì‚¬ë„ ê³„ì‚° í•„ìš” â†’ ì¶”ê°€ ì—°ì‚° \( O(T^2) \) ë˜ëŠ” ìµœì†Œ \( O(Td) \)
  - íŠ¹íˆ ê¸¸ì´ê°€ ê¸´ ì…ë ¥ ì‹œ routing ê³„ì‚° ìì²´ê°€ ë³‘ëª© ê°€ëŠ¥

### 2. **attentionê³¼ routing ê°„ì˜ dependency loop ë°œìƒ ê°€ëŠ¥ì„±**

- ë§Œì•½ attention â†’ routing â†’ attention ì´ ë°˜ë³µëœë‹¤ë©´, **ëª¨ë¸ ê³„ì‚° ê·¸ë˜í”„ê°€ ìˆœí™˜ ì˜ì¡´ì„±** ê°€ì§ˆ ìˆ˜ ìˆìŒ
- ì´ë¥¼ í•´ê²°í•˜ë ¤ë©´ routingì„ ì´ì „ layerì˜ attentionì—ë§Œ ì˜ì¡´ì‹œí‚¤ê±°ë‚˜ stop-gradient ì„¤ê³„ í•„ìš”

### 3. **expert collapse/overfitting ìœ„í—˜ ì¦ê°€**

- context-aware routingì€ ì‘ì€ attention ë³€í™”ì—ë„ expert ì„ íƒì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ **unstable**
- íŠ¹íˆ í•™ìŠµ ì´ˆê¸°ì— íŠ¹ì • attention ë¶„í¬ê°€ í¸í–¥ë  ê²½ìš°, íŠ¹ì • expertë§Œ ê³¼ë„í•˜ê²Œ í™œì„±í™”ë  ìˆ˜ ìˆìŒ

â†’ ê¸°ì¡´ softmax ê¸°ë°˜ MoEê°€ ê²ªë˜ collapse ë¬¸ì œì™€ ìœ ì‚¬ í˜„ìƒ ë°œìƒ ê°€ëŠ¥

---

## ğŸ”„ ê°€ëŠ¥í•œ êµ¬í˜„ ë°©ì‹ ë° ì™„í™” ì „ëµ

| ì „ëµ                    | ì„¤ëª…                                                         | ìœ„í—˜ ì™„í™”            |
| ----------------------- | ------------------------------------------------------------ | -------------------- |
| delayed routing         | ì´ì „ layerì˜ attention ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ routing               | dependency loop ë°©ì§€ |
| routing gradient stop   | routingì— gradient ì „íŒŒ ì°¨ë‹¨                                 | collapse ë°©ì§€        |
| context-compressed gate | context vectorë¥¼ ë‹¨ì¼ ìš”ì•½ ë²¡í„°ë¡œ ì••ì¶• í›„ ì‚¬ìš©               | ì—°ì‚°ëŸ‰ ê°ì†Œ          |
| Gated Fusion            | ê¸°ì¡´ token-local gateì™€ context-aware gateë¥¼ gatingìœ¼ë¡œ í˜¼í•© | ì•ˆì •ì„± + í‘œí˜„ë ¥ ê· í˜• |

---

## âœ… ê²°ë¡ : ì–¸ì œ context-aware routingì„ ê³ ë ¤í•  ìˆ˜ ìˆëŠ”ê°€?

| ì¡°ê±´                                                     | ì ìš© ì—¬ë¶€                       |
| -------------------------------------------------------- | ------------------------------- |
| ë¬¸ë§¥ì— ë”°ë¼ ì˜ë¯¸ê°€ í¬ê²Œ ë°”ë€ŒëŠ” ì‘ì—… (ex: QA, multi-task) | ë§¤ìš° íš¨ê³¼ì                      |
| ê¸´ sequence ì²˜ë¦¬ (ex: document, code)                    | ìœ ë¦¬ (ì „ì—­ ì •ë³´ í™œìš© ê°€ëŠ¥)      |
| ì—°ì‚° ë¦¬ì†ŒìŠ¤ê°€ ì—¬ìœ ë¡œìš´ í™˜ê²½ (ì„œë²„ ì¶”ë¡ )                  | ì ìš© ê°€ëŠ¥                       |
| ì‹¤ì‹œê°„ inference, ëª¨ë°”ì¼ í™˜ê²½                            | **ë¹„ì¶”** (routing latency ì¦ê°€) |

---

## ğŸ“Œ ìš”ì•½

- **ì´ì **: í‘œí˜„ë ¥ í–¥ìƒ, ì˜ë¯¸ ê¸°ë°˜ expert specialization, redundancy ê°ì†Œ
- **ìœ„í—˜**: ì—°ì‚°ëŸ‰ ì¦ê°€, ìˆœí™˜ ì˜ì¡´ì„±, expert collapse ê°€ëŠ¥ì„±
- **ëŒ€ì•ˆ/ë³´ì™„ì±…**: delayed routing, gradient stop, gated fusion ë“±

> ğŸ” **ê²°ë¡ ì ìœ¼ë¡œ, context-aware routingì€ SwitchHeadì˜ í‘œí˜„ë ¥ì„ ë”ìš± ë†’ì¼ ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ í™•ì¥ì´ ë  ìˆ˜ ìˆìœ¼ë‚˜, ì—°ì‚°ëŸ‰ê³¼ í•™ìŠµ ì•ˆì •ì„± ì´ìŠˆë¥¼ í•´ê²°í•˜ëŠ” ì¶”ê°€ ë©”ì»¤ë‹ˆì¦˜ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.**