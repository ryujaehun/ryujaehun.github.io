---
title: "SELF-DATA DISTILLATION FOR RECOVERING QUALITY IN PRUNED LARGE LANGUAGE MODELS"
date: 2025-03-25
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2410.09982)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

논문의 핵심을 정리하여 결론부터 간략히 제시한 후, 구체적인 수치를 통해 강점 및 독창적인 지점을 설명하고, 논문에서 제안한 핵심 알고리즘을 예시와 함께 설명하며, 논문의 한계점을 논의하겠습니다.

---

## 📌 논문의 요약된 결론:

이 논문은 구조적 프루닝(structured pruning)을 수행한 대형 언어 모델(LLM)에서 발생하는 성능 저하를 극복하기 위해 **Self-Data Distillation (자체 데이터 증류)** 기법을 제안했습니다. 자체 데이터 증류를 통해 프루닝된 모델은 원래의 미프루닝 모델로부터 생성한 증류 데이터로 추가 미세조정하여 성능을 회복하며, 특히 다단계 추론(multi-step reasoning)이 중요한 작업에서 성능 향상이 두드러졌습니다. 이를 통해 Llama3.1-8B 모델에서 6개의 디코더 블록(32→26)을 프루닝했을 때, 표준 미세조정(SFT) 대비 정확도를 **81.7% → 91.2%**로 향상시켰습니다.

---

## 📌 논문의 강점 및 독창성:

이 논문의 주요 강점은 다음과 같습니다.

1. **성능 회복의 탁월성**
   - Llama3.1-8B 모델 기준, 6개 레이어 프루닝 시 평균 정확도 회복률이 **81.7% (기존 SFT)**에서 **91.2% (자체 데이터 증류)**로 10% 가까이 상승했습니다.
   - 실제 환경에서 FLOPs를 16.3% 절약하면서도 높은 성능을 유지합니다.

2. **Catastrophic Forgetting 감소**
   - 자체 데이터 증류는 원본 모델의 출력 분포와 매우 유사한 데이터를 생성함으로써 분포 이동(distribution shift)을 최소화하고, catastrophic forgetting 문제를 효과적으로 완화합니다.

3. **Speculative Decoding과의 결합을 통한 효율성 극대화**
   - 자체 데이터 증류 모델을 speculative decoding의 Draft 모델로 활용 시, 큰 타겟 모델과의 높은 정렬(alignment)을 유지하여 토큰 수락률을 높이고 효율적인 추론이 가능하도록 합니다.
   - Speculative decoding 시, 프루닝된 모델(블록 크기=10)을 Draft로 활용하면 평균 승인 토큰 길이가 기존 대비 최대 1.70 토큰 증가합니다.

---

## 📌 핵심 알고리즘 설명 (예시 입력을 통한 전체 과정):

### 알고리즘 흐름:

알고리즘은 크게 두 단계로 구성됩니다.

### ① 구조적 프루닝 알고리즘 (Layer-Pruning):

> 입력: 원본 모델 \(M\), 프루닝할 레이어 수 \(n\), 대표 데이터셋 \(D\)

1. 모델 각 레이어 \( ℓ \)에서 데이터셋 \(D\)를 통해 활성화 벡터 계산: \(h^{(ℓ)}(D)\)
2. 두 레이어 사이의 출력 활성화 간 코사인 각도 거리(angular cosine distance)를 계산하여 최소화하는 블록(연속된 \(n\)개 레이어)을 찾아 제거합니다.

예시: 32개 레이어 중 6개를 프루닝할 때,

- 레이어 22~27 사이의 활성화가 매우 유사(낮은 angular distance)하여 프루닝할 블록으로 선정
- 이 레이어 블록을 제거하고, 21번 레이어의 출력을 28번 레이어의 입력으로 연결합니다.

### ② 자체 데이터 증류(Self-Data Distillation):

> 입력: 원본 모델 \(M\), 프루닝된 모델 \(M'\), task-specific 데이터셋 \((x, y)\)

- 원본 모델 \(M\)을 사용하여 원본 task 데이터 \((x, y)\)에 대해 증류 데이터 \(ỹ\) 생성.
- 생성된 데이터가 원본 모델의 분포를 유지하도록 하면서, 특히 정확성이 중요한 데이터는 조건적으로 선택(정답이 일치하지 않을 경우 원본 \(y\)를 선택)
- 프루닝된 모델 \(M'\)은 증류 데이터 \((x, ỹ)\)로 미세조정하여 원본 모델 분포와의 정렬을 유지하면서 성능 회복.

예시:
- 입력 \(x\): `"Calculate the sum: 23 + 57"`
- 원본 모델 출력 \(ỹ\): `"23 + 57 = 80"`
- 증류 데이터 생성 시, 모델의 출력이 정답과 일치하지 않으면 원본 정답(80)을 유지.

---

## 📌 성능 비교 테이블 (프루닝 블록 사이즈=6 기준):

| 방식                | ARC-C (추론) | GSM8k (수학) | MMLU (종합지식) | 평균 정확도 | 정확도 회복률 |
|---------------------|--------------|--------------|-----------------|------------|--------------|
| 미프루닝 Baseline   | 58.70%       | 63.15%       | 67.40%          | 63.08%     | 100.0%       |
| No Fine-Tuning      | 49.49%       | 0.00%        | 67.42%          | 48.93%     | 70.5%        |
| SFT                 | 46.93%       | 43.82%       | 59.98%          | 50.91%     | 80.7%        |
| Self-Data Distill   | **50.00%**   | **66.64%**   | **64.96%**      | **60.53%** | **95.96%**   |

---

## 📌 논문의 한계점:

1. **증류 데이터 생성 비용**
   - Self-Data Distillation 방식은 원본 대형 모델을 활용하여 데이터를 생성하는 단계가 필수적입니다. 이는 별도의 추가 계산 자원과 시간 소요를 요구합니다.

2. **데이터 의존성**
   - 증류 데이터의 품질이 최종 성능에 직결되므로, 증류 데이터 생성에 사용하는 원본 task 데이터가 충분히 다양하지 않거나, 원본 모델의 출력 품질이 좋지 않을 경우 성능 회복 효과가 제한적일 수 있습니다.

3. **높은 추론 복잡도의 한계**
   - 극도로 복잡한 추론이 요구되는 작업에서는 자체 데이터 증류로 인한 성능 회복이 완벽하지 않으며, 원본 모델의 추론 품질에 의존적입니다.

---

📍 **결론**:  
이 논문은 자체 데이터 증류를 통해 구조적 프루닝 후 대형 언어 모델의 성능 저하 문제를 효과적으로 해결한 독창적인 연구이며, 특히 다단계 추론 작업에서 높은 성능 회복력을 보였습니다. 그러나 계산 비용 및 데이터 의존성이라는 현실적 한계점도 함께 존재합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

논문에서 제안한 핵심 알고리즘은 크게 두 단계로 구성되어 있습니다:

1. **구조적 레이어 프루닝(Structured Layer Pruning)**  
2. **자체 데이터 증류 기반 미세조정(Self-Data Distillation Fine-tuning)**

각 단계를 상세히 예시와 함께 구체적으로 설명하겠습니다.

---

## 🚩 1단계: 구조적 레이어 프루닝 (Structured Layer Pruning)

구조적 프루닝의 핵심은 모델의 여러 연속된 레이어 중에서 **중복되거나 비슷한 출력을 만드는 블록**을 선택해 제거하는 것입니다.

### **단계별 과정**

논문의 알고리즘은 다음과 같은 순서로 진행됩니다:

> **입력:**
- 원본 모델 \( M \) (레이어 수: \( L \))
- 제거할 연속된 레이어 수 \( n \)
- 대표적인 입력 데이터셋 \( D \)

> **출력:**  
- 프루닝된 모델 \( M' \)

### **예시 상황**:  
원본 모델 \(M\): 32개의 레이어로 이루어진 Transformer 기반 모델(Llama3.1-8B)  
프루닝할 연속된 레이어 수 \(n\): 6개  
데이터셋 \(D\): RedPajama 데이터의 샘플 128개(논문에서 사용된 실제 예시)

---

### **구체적인 프루닝 절차**

① 각 레이어의 출력 활성화를 구한다.

모델의 레이어에서 입력 데이터 \(D\)의 마지막 토큰(T번째)의 활성화 출력을 다음과 같이 나타냅니다.

\[
h^{(\ell)}(D): \text{ℓ번째 레이어의 마지막 토큰 활성화 벡터}
\]

예시:  
- 21번 레이어의 활성화 벡터: \( h^{(21)}(D) \)  
- 22번 레이어의 활성화 벡터: \( h^{(22)}(D) \)  
- … (마찬가지로 32번 레이어까지 활성화 계산)

② 특정 두 레이어 사이 활성화 출력의 각도 거리(Angular distance)를 계산한다.

각도 거리를 통해 레이어의 출력 유사성을 평가합니다.

각도 거리 식:
\[
d\left(h^{(\ell)}(D), h^{(\ell+n)}(D)\right) = \frac{1}{\pi}\arccos\left(\frac{h^{(\ell)}(D)\cdot h^{(\ell+n)}(D)}{\|h^{(\ell)}(D)\|\|h^{(\ell+n)}(D)\|}\right)
\]

예시:  
- 예를 들어, 22번과 28번 레이어 간 활성화의 각도 거리가 계산값 중 가장 낮다면, 이 구간이 가장 중복성이 높다는 의미입니다.

③ 가장 작은 각도 거리를 가진 연속된 블록 선택 및 제거:

모든 연속된 레이어 블록 중 가장 작은 각도 거리(즉, 출력이 가장 유사한 구간)를 찾고, 그 구간을 프루닝합니다.

논문의 실제 예시에서 가장 작은 각도 거리를 가진 블록은 22번에서 27번 레이어였습니다.  
따라서, 22~27번 레이어(6개 레이어)를 제거하고, 21번 레이어의 출력을 28번 레이어로 바로 연결하여 프루닝 완료합니다.

---

## 🚩 2단계: 자체 데이터 증류 기반 미세조정 (Self-Data Distillation Fine-tuning)

구조적 프루닝 후 모델 성능이 하락하므로, 프루닝된 모델을 원래 모델의 지식을 보존한 데이터로 미세조정하여 성능을 회복하는 단계입니다.

### **단계별 과정**

> **입력:**
- 원본 모델 \( M \) (미프루닝 모델)
- 프루닝된 모델 \( M' \)
- 특정 작업의 데이터셋 \((x, y)\)

> **출력:**  
- 성능이 회복된 프루닝된 모델 \( M'_{\text{fine-tuned}} \)

---

### **구체적인 증류 절차**

① 원본 모델을 활용하여 증류 데이터 생성:

원본 모델 \( M \)을 사용하여 task 데이터 \((x,y)\)를 기반으로 증류 데이터를 생성합니다.

- 입력 \( x \)가 주어지면 원본 모델의 출력 \( ỹ \)를 생성합니다.
- 이 출력 \( ỹ \)는 원본 모델이 학습한 데이터 분포를 가장 잘 유지하는 문장입니다.

예시 입력:
- 원본 데이터:  
```
x = "Calculate the product: 15 × 7"
y = "105"
```

- 원본 모델로부터 생성된 증류 출력 \( ỹ \):  
```
"15 multiplied by 7 equals 105"
```

② 조건부 데이터 선택 (Conditional Selection):

모델이 생성한 \( ỹ \)이 정확성과 일관성을 보장하지 못할 경우 원본의 레이블 \( y \)를 유지하는 전략입니다.

논문의 예시 공식:
\[
ỹ' =
\begin{cases}
ỹ & \text{if Extract}(ỹ)=y\\[6pt]
y & \text{otherwise}
\end{cases}
\]

즉, 생성된 응답이 올바르지 않을 때만 원본 레이블을 유지합니다.

예시 적용:
- 모델 출력: `"15 multiplied by 7 equals 105"`  
- 원본 레이블과 비교한 결과, 정답(105)이 정확히 일치하면 \(ỹ'\)은 `"15 multiplied by 7 equals 105"` 유지
- 만약 `"15 multiplied by 7 equals 100"` 같이 오류가 있는 경우 원본의 정답 `105`를 유지

③ 생성된 증류 데이터셋으로 미세조정(Fine-tuning):

프루닝된 모델 \(M'\)를 조건부로 선택된 증류 데이터셋 \((x, ỹ')\)을 사용해 미세조정하여 원본 모델의 지식을 최대한 보존하며 성능을 회복시킵니다.

---

## ✅ 전체 알고리즘을 정리하면 다음과 같습니다:

| 단계               | 핵심 과정                                                | 목적                                    |
|--------------------|----------------------------------------------------------|-----------------------------------------|
| **구조적 프루닝**  | 각 레이어의 활성화를 비교해 가장 중복된 연속된 레이어 제거  | 중복성 제거 및 모델 효율성 향상           |
| **자체 데이터 증류**| 원본 모델이 생성한 task별 데이터로 프루닝 모델 추가 미세조정 | 성능 회복 및 원본 지식 보존 (Catastrophic forgetting 방지) |

---

## 🚨 **핵심적인 이유**:  
이러한 두 가지 절차(프루닝 → 자체 데이터 증류)를 통해, 효율적으로 모델 크기를 줄이면서도 성능을 극대화하여 실질적인 배포 환경에서 유용한 모델을 유지할 수 있게 됩니다. 특히 다단계 추론(multi-step reasoning)이 필요한 경우에 우수한 성능을 유지하게 됩니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

논문에서 제시한 결과를 상세히 분석하고, 다른 방법론(Supervised Fine-tuning, No Fine-tuning) 대비 특출난 성과를 낸 핵심 요인을 명확히 짚어보겠습니다. 논문이 주장하는 이유와 더불어 저의 견해도 함께 덧붙입니다.

---

## 📌 논문의 결과에서 나타나는 특출난 점 (정량적 분석)

이 논문은 **Self-Data Distillation (자체 데이터 증류)** 를 통해 프루닝된 LLM(Large Language Model)의 성능 저하를 매우 효과적으로 완화합니다. 특히 표준 supervised fine-tuning (SFT) 및 미세조정이 없는(no fine-tuning) 방식과 비교할 때, 프루닝된 모델의 성능 회복률이 뚜렷하게 높아진 것이 큰 특징입니다.

### 📝 논문에 보고된 핵심 성과 (Llama3.1-8B, Block Size=6):

| 방법론                           | ARC-C | GSM8k | MMLU | 평균 정확도 | 회복률(%) |
|---------------------------------|--------|--------|--------|-----------|----------|
| 원본 Baseline (미프루닝 모델)      | 58.70  | 63.15  | 67.40  | 63.08     | 100.00   |
| No Fine-Tuning                  | 49.49  | 0.00   | 67.42  | 48.93     | 70.50    |
| Supervised Fine-tuning (SFT)    | 46.93  | 43.82  | 59.98  | 50.91     | 80.71    |
| **Self-Data Distillation**      | **50.00** | **66.64** | **64.96** | **60.53** | **95.96** |

이러한 수치를 통해 다음과 같은 특출난 점을 볼 수 있습니다.

- **Self-Data Distillation**은 표준 SFT에 비해 평균 정확도가 **약 10% 이상 증가**했습니다.
- 특히 **GSM8k(수학 추론)** 같은 다단계 추론이 중요한 작업에서 정확도가 크게 향상되어(43.82% → 66.64%), 거의 원본 모델과 비슷한 수준까지 회복됐습니다.
- 전반적인 정확도 회복률에서도 Self-Data Distillation이 압도적으로 우세했습니다.

---

## 📌 논문에서 제시하는 Self-Data Distillation이 좋은 결과를 도출한 이유

논문은 자체 데이터 증류 방법론이 다른 방법 대비 우수한 결과를 도출한 이유로 다음 세 가지 핵심 원인을 지적합니다:

### ① 원본 모델의 분포 유지 (Distribution Alignment)

- Self-Data Distillation은 **원본(unpruned) 모델에서 직접 생성된 증류 데이터를 활용**합니다. 이를 통해 fine-tuning 과정에서 발생하는 분포 이동(distribution shift)을 최소화하여, 미세조정 시 원본 모델의 지식과 분포를 매우 잘 보존합니다.
- 이로 인해 catastrophic forgetting(파괴적 망각)이 최소화되며, 프루닝 과정에서도 원본 모델의 강력한 일반화 능력을 그대로 유지할 수 있습니다.

### ② 데이터 품질 및 task-specific correctness 유지

- 조건부 선택(Conditional selection) 전략을 사용하여 모델이 생성한 응답이 정확하지 않을 때 원본 task 데이터의 정답을 유지합니다.
- 이로 인해 수학이나 논리적 추론 등 정확한 결과값이 중요한 task에서 프루닝 후 성능 저하를 현저히 줄일 수 있었습니다.

### ③ 모델 간 병합(Model Merging)을 통한 일반화 성능 향상

- 논문은 서로 다른 데이터로 학습된 모델 간 Spherical Linear Interpolation(SLERP)을 사용한 병합(merge)을 통해 서로 보완적인 지식을 결합하여 일반화 성능을 추가로 향상시켰습니다.
- 예컨대 OpenMathInstruct 데이터와 Alpaca 데이터로 각각 학습된 모델을 병합하면 두 데이터의 강점(수학 추론 능력 + 일반 지식 추론 능력)이 결합되어 최종적인 성능 회복률을 더욱 높였습니다.

---

## 📌 개인적 견해: Self-Data Distillation 방법론이 뛰어난 이유에 대한 추가적 분석

저는 논문에서 제안한 방법이 우수한 성능을 달성한 이유에 대해 추가로 다음과 같은 견해를 덧붙이고자 합니다:

### 🧩 (1) **모델의 내재된 잠재지식(latent knowledge)을 명시적으로 활용**

- 프루닝 과정에서 제거되는 정보는 단순히 레이어 제거로 인한 중간 표현의 손실입니다. 그러나 원본 모델 자체가 지닌 내재적 잠재지식(latent knowledge)은 남아있습니다.
- 자체 데이터 증류는 이 잠재지식을 명시적으로 다시 데이터 형태로 만들어 프루닝된 모델에 주입함으로써 모델의 잠재적 역량을 다시 활성화합니다. 이는 표준적인 supervised fine-tuning 방식에서는 수행하지 못하는 독창적인 접근법입니다.

### 🧩 (2) **학습 데이터의 다양성 및 스케일 효과**

- 논문은 특히 큰 스케일의 자체 데이터(OpenMathInstruct, 50k 샘플)를 사용했을 때 성능 회복이 극대화되는 점을 발견했습니다.
- 이는 모델의 성능 회복이 데이터의 다양성 및 분포의 정확한 재현에 크게 의존함을 시사하며, 자체 데이터 증류의 성능이 단순한 Fine-tuning과 달리 데이터의 질과 양에 강하게 영향을 받는다는 것을 암시합니다.

### 🧩 (3) **Speculative Decoding 등과의 높은 상호호환성**

- 자체 데이터 증류를 거친 프루닝 모델은 speculative decoding의 draft 모델로서 더 높은 token acceptance rate를 보입니다.
- 이는 자체 데이터 증류가 모델의 예측 분포를 원본 모델과 매우 유사하게 유지시키기 때문이며, 실제 서비스 환경에서의 효율성을 극대화할 수 있는 추가적 이점입니다.

---

## 📌 정리된 결론

| 이유 구분                       | 논문이 주장하는 핵심 이유                                    | 개인적으로 추가하는 견해                                |
|-------------------------------|---------------------------------------------------|---------------------------------------------------|
| 모델 지식 보존                   | 분포 이동 최소화로 원본 모델 지식 보존                      | 모델의 잠재적 역량(latent knowledge)을 데이터화하여 보존  |
| 데이터 품질 확보                 | 조건부 선택으로 정확한 데이터 유지                          | 대규모 자체 데이터 증류가 품질 및 분포 일치도 극대화      |
| 모델 간 결합으로 일반화 능력 향상  | SLERP을 통한 모델 병합                                   | Speculative decoding 등 실제 환경과의 높은 상호호환성  |

---

## 💡 결론적으로  
이 논문의 **Self-Data Distillation** 방법론은 모델의 내재된 지식을 명시적 데이터 형태로 보존·활용하여 프루닝으로 인한 성능 손실을 극복하는 창의적인 접근법입니다.  
특히, 다단계 추론 및 정확성이 중요한 작업에서 다른 방식(SFT, No FT)에 비해 현저히 우수한 결과를 얻었으며, 실제 서비스 배포 환경에서도 추가적 이점을 제공하는 매우 실용적인 방법입니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :


논문에서 제시한 방법의 주요 한계점을 바탕으로, 최근 연구 흐름에서 이러한 한계를 어떻게 극복하고자 하는지 구체적으로 정리하여 소개하겠습니다.

논문에서 제기된 주요 한계점은 다음과 같습니다:

1. **데이터 증류 과정에서의 추가적인 계산 비용 및 시간**
2. **증류 데이터 품질에 대한 의존성 (원본 모델의 출력 정확성 문제)**
3. **높은 복잡도의 다단계 추론에서 완벽한 성능 회복이 어려움**

아래에서는 각각의 한계를 극복하기 위한 최신 연구 흐름과 접근법을 구체적으로 소개합니다.

---

## 🔍 한계 1: 데이터 증류 과정의 높은 계산 비용 극복 연구

논문의 방법은 원본의 대형 모델을 이용하여 별도 증류 데이터를 생성해야 하는 문제 때문에 추가적인 계산 비용과 시간이 필요합니다.

이를 극복하기 위한 연구 흐름은 크게 두 가지입니다.

### ✅ 연구 흐름 ①: **Low-rank Adaptation 및 Parameter-efficient Tuning**

- **LoRA (Low-rank Adaptation)** *(Hu et al., 2022)*  
  모델 전체를 미세조정하지 않고, 일부 매개변수만 업데이트하는 Low-rank 행렬을 적용하여, 계산 비용과 메모리 소모를 크게 줄이는 방법입니다.

- **QLoRA (Quantized LoRA)** *(Dettmers et al., 2023)*  
  LoRA에 양자화를 적용하여 GPU 메모리 사용을 최소화하고, 미세조정 효율성을 크게 높이는 방법으로 최근 연구에서 널리 활용됩니다.

이러한 흐름은 추가적인 계산 비용을 절감하여 자체 데이터 증류 방식을 더욱 효율적으로 활용 가능하게 합니다.

### ✅ 연구 흐름 ②: **추론 단계에서의 효율성 향상 (Speculative Decoding)**

- **Speculative Sampling** *(Leviathan et al., 2023; Xia et al., 2023)*  
  작은 모델로 초안을 생성하고 큰 모델이 이를 검증하는 투-스테이지 방식으로, 자체 데이터 증류와 결합하면 증류 과정의 계산 비용을 실질적으로 줄일 수 있습니다.

---

## 🔍 한계 2: 증류 데이터 품질의 원본 모델 출력 정확성에 의존성 극복 연구

원본 모델이 부정확한 출력을 생성하면, 이를 활용한 증류 데이터도 부정확해져 최종 모델 성능에 악영향을 미칩니다. 이를 극복하는 최근의 연구 흐름은 다음과 같습니다.

### ✅ 연구 흐름 ①: **자기 교정(Self-Correction) 방법론**

- **Reflexion** *(Shinn et al., 2023)*  
  모델 스스로 초기 출력을 다시 평가하고 재작성하는 자기-반성(reflection) 기반 접근법입니다. 자체 데이터 증류 시 생성된 데이터를 한번 더 교정하여 정확성을 높입니다.

- **Self-Refine** *(Madaan et al., 2023)*  
  최초 생성된 출력을 반복적으로 개선하며, 이를 통해 자체 데이터 증류의 데이터 품질을 한층 더 높이는 접근법입니다.

### ✅ 연구 흐름 ②: **지식 증류 및 앙상블 기반 데이터 정제**

- **Ensemble Distillation** *(Gu et al., 2024)*  
  여러 모델(또는 서로 다른 프롬프트 결과)을 앙상블로 묶어 증류 데이터 생성 시 정확성과 일관성을 높이는 방법론입니다.

- **Selective Data Distillation** *(Yang et al., 2024)*  
  생성된 데이터 중에서 정확한 데이터를 선별적으로 사용하는 전략으로, 부정확한 증류 데이터의 영향력을 줄입니다.

---

## 🔍 한계 3: 복잡한 다단계 추론 작업의 성능 완벽 회복을 위한 연구

자체 데이터 증류가 다단계 추론에서 완벽한 성능 회복을 이루기 어렵다는 한계를 극복하기 위한 최신 흐름은 다음과 같습니다.

### ✅ 연구 흐름 ①: **Chain-of-Thought(CoT) 및 Reasoning Data 활용**

- **Chain-of-Thought (Wei et al., 2022)**  
  모델이 단계를 나누어 점진적으로 추론하는 방식이며, 자체 데이터 증류 시 추론 과정을 포함한 데이터를 추가로 생성하여 복잡한 추론 성능을 높입니다.

- **Self-Consistency (Wang et al., 2023)**  
  모델이 여러 번 다양한 방식으로 추론을 진행하고, 이를 최종 앙상블로 병합하여 정확성을 높이는 방식으로 자체 데이터 증류에도 효과적으로 적용할 수 있습니다.

### ✅ 연구 흐름 ②: **추론 특화 어댑터(Reasoning-specific Adapters)**

- **Reasoning Adapters (Li et al., 2023)**  
  추론 능력 특화된 task-specific adapter를 추가하여 다단계 추론 작업의 정확성을 직접적으로 높이고, 자체 데이터 증류 후 미세조정 단계에서 이 adapter만 별도로 조정하는 접근입니다.

- **Expert Modules (MoE)** *(Shazeer et al., 2017; Fedus et al., 2022)*  
  추론과 같은 복잡한 작업에 특화된 전문가(expert) 레이어를 별도로 구성하여, 복잡한 추론 능력을 유지하는 방식입니다.

---

## 📌 위 연구 흐름을 논문의 한계와 연결하여 정리한 테이블

| 논문의 한계점 | 극복 연구 흐름 | 대표적 연구 및 방법론 |
|-------------|---------------|----------------------|
| 계산 비용 증가 | Parameter-efficient Tuning | LoRA (Hu et al., 2022), QLoRA (Dettmers et al., 2023) |
|            | 효율적 추론 전략 | Speculative Decoding (Leviathan et al., 2023; Xia et al., 2023) |
| 증류 데이터 정확성 문제 | 자기 교정 방법 | Reflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023) |
|                  | 앙상블 데이터 정제 | Ensemble Distillation (Gu et al., 2024), Selective Data Distillation (Yang et al., 2024) |
| 복잡한 다단계 추론의 한계 | 단계적 추론 방식(CoT) | Chain-of-Thought (Wei et al., 2022), Self-Consistency (Wang et al., 2023) |
|                       | 추론 특화 Adapter | Reasoning Adapters (Li et al., 2023), Expert Modules(MoE) (Fedus et al., 2022) |

---

## 🚩 결론적 정리

이 논문의 제안 방법론이 가지는 한계점들은 현재 활발히 연구되는 다양한 방법론과 결합되면 효과적으로 극복 가능합니다.  
특히, **자기 교정(self-correction)**, **chain-of-thought** 등의 추론 방법과 결합하거나, 계산 비용을 절감하는 **parameter-efficient tuning**과의 결합을 통해 본 논문 방법론이 더욱 강력한 성능을 발휘할 수 있을 것으로 기대됩니다.


# Q :  이 논문의 방법론(Self-Data Distillation)이 표준 supervised fine-tuning(SFT)에 비해 특히 다단계 추론(GSM8k 등) 작업에서 성능 회복이 더 뛰어난 이유는 무엇인가?

# A : 

이 논문에서 제시한 Self-Data Distillation 방법론이 표준 Supervised Fine-tuning(SFT) 대비 다단계 추론(GSM8k 등) 작업에서 성능 회복이 더 뛰어난 이유는 다음과 같습니다.

첫 번째 이유는, 자체 데이터 증류가 원본(미프루닝) 모델의 데이터 분포를 매우 효과적으로 보존하기 때문입니다. 다단계 추론 작업은 여러 단계를 거쳐 논리적 일관성을 유지해야 하는데, 프루닝 후 표준 supervised fine-tuning 방식으로 미세조정하면 데이터의 분포가 원본 모델에서 학습한 지식 분포와 크게 달라져 논리적 연결성 및 일관성이 깨지기 쉽습니다. 반면, Self-Data Distillation은 원본 모델의 출력을 직접적으로 활용하여 증류 데이터를 생성하므로, 원본 모델의 내재적 추론 논리 구조 및 의미적 일관성을 보다 잘 유지할 수 있습니다.

두 번째 이유는, Self-Data Distillation의 조건부 데이터 선택(conditional selection)이 정확한 추론을 위한 데이터 품질을 더욱 엄격하게 보장하기 때문입니다. GSM8k와 같은 다단계 추론 작업은 각 단계의 결과 정확성이 다음 단계의 추론 결과에 연쇄적으로 영향을 미칩니다. 조건부 선택을 통해 생성된 데이터가 정확하지 않은 경우 원본의 정확한 정답을 유지함으로써, 최종적으로 프루닝된 모델이 각 추론 단계마다 올바른 논리적 흐름을 학습하도록 합니다.

세 번째 이유는, 다단계 추론 능력이 원본 모델의 지식 분포 및 추론 흐름과 매우 밀접하게 연결되어 있다는 특성 때문입니다. Self-Data Distillation을 이용한 증류 데이터는 원본 모델의 추론 과정을 반영하여 생성되기 때문에, 모델이 다단계 추론에서 필요로 하는 중간 단계의 추론 패턴과 논리적 전개 흐름을 훨씬 더 효과적으로 전달해 줄 수 있습니다. 이는 표준 supervised fine-tuning에서 일반적인 인간 생성 데이터셋을 사용할 때는 재현하기 어려운 원본 모델의 추론 패턴을 프루닝된 모델에 명확히 전달하여, 추론 성능 회복에 크게 기여합니다.

종합적으로, 자체 데이터 증류(Self-Data Distillation)는 원본 모델의 분포와 논리적 일관성을 더 정확히 유지하고, 조건부 데이터 선택을 통해 정확한 추론을 위한 데이터 품질을 강화하기 때문에, 표준 supervised fine-tuning 방식보다 다단계 추론 작업에서 성능 회복에 더욱 효과적입니다.

# Q :  프루닝 과정에서 각도 거리(Angular Distance)로 레이어의 중복성을 측정하는 방식이 다른 중요도 측정 방식(Block Influence 등)보다 효과적인 이유는 무엇인가?

# A : 

프루닝 과정에서 Angular Distance(각도 거리)가 Block Influence 등 다른 중요도 측정 방식보다 효과적인 이유는 논문에서 다음과 같이 명확히 설명하고 있습니다.

첫째, Angular Distance는 레이어 출력 활성화 벡터 간의 기하학적 유사성을 명시적으로 측정합니다. Angular Distance는 두 벡터의 방향적 유사성에 초점을 맞추고 있으며, 레이어 간 출력 활성화가 얼마나 서로 중복되거나 유사한 표현을 생성하는지 직접적으로 나타냅니다. 다시 말해, 두 레이어의 출력 활성화 벡터가 각도적으로 가까울수록 두 레이어가 거의 동일한 정보(중복 정보)를 제공한다는 의미로, 제거해도 성능 저하가 적습니다.

둘째, Angular Distance 방식은 계산 효율성이 높습니다. Block Influence(BI)와 같은 다른 방법은 레이어의 입력과 출력 간의 모든 hidden state 변화를 계산해야 하므로 전체 시퀀스 길이에 대해 계산을 반복해야 합니다. 반면, Angular Distance는 활성화 벡터의 마지막 토큰만을 기반으로 코사인 유사도를 계산하여, 계산 비용이 낮고 효율적입니다. 즉, 프루닝 대상을 빠르고 정확하게 선정할 수 있습니다.

셋째, 논문의 실험 결과에 따르면 Angular Distance와 Block Influence의 성능 자체는 거의 유사하지만, Angular Distance가 계산의 효율성과 해석의 명료성 면에서 더욱 뛰어나므로 최종적으로 이를 선택했습니다. 특히 Transformer 기반 언어 모델에서 활성화 벡터 간 각도적 유사성은 레이어 간 중복성을 직관적으로 이해하고 분석하기 쉬우며, 안정적으로 프루닝 결정을 내릴 수 있도록 도와줍니다.

종합적으로 Angular Distance 방식이 프루닝 과정에서 효과적인 이유는 다음과 같습니다:

- 레이어 출력의 방향적 유사성(중복성)을 직관적이고 명확하게 나타냄
- 간단하고 효율적인 계산 방식으로, 계산 비용 절감
- 높은 계산 효율성에도 불구하고 다른 복잡한 방식(Block Influence 등)과 유사한 프루닝 품질 제공

따라서, Angular Distance를 활용하는 접근법은 프루닝 대상 레이어를 정확하고 효율적으로 선정하여 결과적으로 프루닝 후의 모델 성능을 더 안정적이고 효과적으로 유지하게 합니다.

# Q :  이 논문의 접근법(Self-Data Distillation)이 실제 모델 배포 시(inference 단계) speculative decoding 등과 결합될 때 성능 및 효율성 측면에서 어떤 추가적인 장점을 제공하는가?

# A  :

이 논문의 Self-Data Distillation 방법이 실제 모델 배포 시 speculative decoding과 결합되었을 때 성능 및 효율성 측면에서 제공하는 추가적 장점은 다음과 같습니다.

첫째, Self-Data Distillation이 speculative decoding 과정에서 토큰 수락률(Token Acceptance Rate)을 크게 향상시킵니다. Speculative decoding은 상대적으로 작은 모델(draft model)이 초기에 토큰을 예측한 뒤, 큰 모델(target model)이 이 예측된 토큰을 검증하는 구조를 가집니다. 이 과정에서 draft 모델이 target 모델과 매우 유사한 예측 분포를 가져야 높은 토큰 수락률을 얻을 수 있는데, Self-Data Distillation로 미세조정된 모델은 원본 모델(target)의 데이터 분포와 추론 방식을 그대로 유지하기 때문에 높은 수준의 분포적 정렬(distributional alignment)을 제공합니다. 이는 결과적으로 speculative decoding 과정의 예측 정확성을 높이고, 추가적인 교정 토큰(correction token)의 생성을 최소화하여 성능을 크게 향상시킵니다.

둘째, 효율성(추론 속도) 측면에서 큰 장점이 있습니다. Self-Data Distillation로 미세조정된 모델은 speculative decoding을 위한 draft 모델로 사용될 때 target 모델과의 높은 정렬성으로 인해 speculative 단계에서 생성된 토큰이 거의 변경되지 않고 그대로 사용됩니다. 이는 target 모델이 별도의 추가 계산을 거의 하지 않아도 되도록 만들어, 전반적인 추론 속도를 획기적으로 개선하게 됩니다. 즉, 추론의 효율성을 높이고 계산 자원을 절약하여 배포 환경에서 경제적으로 큰 이점을 제공합니다.

셋째, Speculative Decoding 과정에서 Self-Data Distillation로 프루닝된 모델을 draft 모델로 사용할 경우 원본 모델의 강력한 다단계 추론 능력을 그대로 유지할 수 있습니다. 논문에서 확인된 바와 같이 수학적 또는 논리적 복잡성이 높은 task(GSM8k 등)의 경우에도, 원본 모델과 매우 유사한 추론 과정을 유지함으로써 target 모델과 draft 모델 간의 예측 결과가 일관되게 유지되어 speculative decoding에서 추가 교정이 적게 이루어지고, 이는 추론의 신뢰성을 높이는 데 기여합니다.

정리하자면, Self-Data Distillation은 speculative decoding과의 결합 시 다음의 추가적인 장점을 제공합니다:

- Draft 모델과 Target 모델 간의 높은 분포 정렬성으로 토큰 수락률(Token Acceptance Rate)을 크게 높임.
- 수락률 증가로 인해 추가적인 재생성 단계가 줄어들어 추론 속도와 효율성이 크게 향상됨.
- 복잡한 다단계 추론 작업에서도 높은 예측 신뢰성 및 일관성을 유지하여 실제 배포 환경에서 모델의 전반적 성능 및 안정성을 크게 개선함.

이러한 장점들은 실제 서비스 배포 환경에서 대형 언어 모델의 효율적인 추론 성능을 극대화하고 비용을 절감하는 데 매우 유리한 특성을 제공하게 됩니다.