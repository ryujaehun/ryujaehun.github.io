---
title: Transformers are Multi-State RNNs
categories:
 - paper-review
 - with-gpt
tags:
---


논문 : https://arxiv.org/abs/2401.06104


아래글은 Scholar GPT와 대화를 나눈 내용입니다.

# Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점

이 논문은 **Transformers를 Multi-State RNN(MSRNN)으로 재해석하고, TOVA(Token Omission Via Attention)**라는 새로운 KV 캐시 압축 정책을 제안합니다. 이를 통해, Transformer 모델의 메모리 사용을 최적화하면서도 성능을 유지할 수 있습니다.

### 논문의 강점과 독창성
1. **Transformers와 RNN의 연계성**: 논문은 Transformer가 사실상 다중 상태를 갖는 RNN(MSRNN)으로 간주될 수 있음을 보여줍니다. 이는 Transformer와 RNN의 개념적 격차를 줄이는 혁신적인 접근입니다.
   
2. **TOVA 압축 정책**: TOVA는 어텐션 점수가 낮은 토큰을 선택적으로 삭제하여 캐시 용량을 줄이는 정책으로, **모델의 성능을 거의 유지하면서 최대 88%의 캐시 절감**을 가능하게 합니다. 또한 추가 학습 없이 기존 모델에 바로 적용할 수 있어 실용성이 높습니다.

3. **고성능 유지 및 효율성 증가**: 실험 결과, TOVA는 1/8의 캐시 용량으로도 거의 원래 성능을 유지하며, 최대 4.8배의 처리량 증가를 달성합니다. 이는 긴 시퀀스를 효율적으로 처리할 수 있게 합니다.

### 핵심 알고리즘 (TOVA)과 예시 설명
TOVA는 어텐션 점수가 낮은 토큰을 주기적으로 캐시에서 삭제하는 방식입니다. 예를 들어, TOVA는 100개의 토큰을 가진 문장에서 50개의 토큰만 캐시에 남겨야 할 때, 각 토큰의 어텐션 점수를 계산하고, 점수가 낮은 50개의 토큰을 삭제합니다.

- **초기 상태**: 모델은 시퀀스를 처리하며 각 토큰의 어텐션 점수를 평가하여 캐시에 저장할지 결정합니다.
- **압축 과정**: 새로운 토큰이 추가되면서 캐시 용량이 초과될 때마다, TOVA는 가장 어텐션 점수가 낮은 토큰을 삭제해 캐시를 유지합니다.

예를 들어, "고양이가 나무 위에 올라갔다."라는 문장에서 "고양이"와 "올라갔다" 같은 중요한 토큰은 어텐션 점수가 높아 캐시에 남기고, 중요도가 낮은 "나무"와 같은 단어는 삭제합니다.

### 논문의 한계점
1. **장기 문맥 유지의 한계**: TOVA는 메모리 절감을 위해 일부 정보를 삭제하므로, 긴 문맥 유지가 필요한 작업에는 한계가 있을 수 있습니다.
2. **언어적 다양성 제한**: 본 연구는 주로 영어 데이터로 평가되었으므로, 문법 구조가 다른 언어에 적용할 때 성능이 다를 수 있습니다.

이 논문은 TOVA를 통해 Transformer 모델의 캐시 용량을 최적화하여, 메모리 효율을 높이면서도 성능을 유지하는 방안을 제시하는 중요한 기여를 하고 있습니다.

# Q : Transformers와 RNN의 연계성에 대해서 자세하게 설명해 줄 수 있겠니?


# A : 

네, **Transformers와 RNN의 연계성**에 대해 자세히 설명드리겠습니다.

### Transformer와 RNN의 기본 차이점
일반적으로, **RNN(순환 신경망)**과 **Transformer**는 다음과 같은 구조적 차이점이 있습니다.

- **RNN**은 시퀀스 데이터의 **연속적인 토큰**을 순차적으로 처리합니다. RNN은 이전 시점의 **히든 상태(hidden state)**를 현재 시점의 입력과 함께 사용해 현재의 출력을 생성합니다. 즉, RNN은 각 시점마다 하나의 히든 상태 벡터를 유지하며, 입력을 순차적으로 처리하여 문맥을 형성합니다.
- **Transformer**는 시퀀스를 한꺼번에 처리하며, **모든 토큰 간의 어텐션(attention)**을 계산하여 정보를 통합합니다. Transformer는 모든 토큰이 서로 직접적으로 어텐션을 통해 연결되어 있어, 이전의 히든 상태를 사용하지 않고도 문맥 정보를 얻을 수 있습니다.

이러한 차이 때문에 RNN은 **시간순서에 따른 종속성**을 잘 유지하지만, 긴 시퀀스를 처리할 때는 **장기 의존성 문제**가 발생할 수 있습니다. 반면 Transformer는 어텐션 메커니즘 덕분에 긴 시퀀스에서도 효율적으로 정보를 처리할 수 있습니다.

### 논문에서 제시한 **Multi-State RNN (MSRNN)으로서의 Transformer**
이 논문에서는 Transformer의 작동 방식을 **Multi-State RNN(MSRNN)**의 개념으로 설명합니다. MSRNN은 **단일 히든 상태가 아닌 다중 히든 상태를 유지하는 RNN**으로, 각 상태가 시퀀스의 특정 토큰에 해당합니다. Transformer의 경우, 각 토큰마다 고유의 키(Key)와 값(Value) 벡터를 유지하여 이를 히든 상태로 간주할 수 있습니다.

#### Transformer의 작동 방식이 RNN과 유사한 이유
1. **상태 유지**: Transformer는 시퀀스가 길어짐에 따라 **매 시점마다 새로운 키-값 벡터를 추가**하여 모든 이전 토큰을 참조할 수 있습니다. 이는 RNN이 이전 시점의 히든 상태를 계속해서 유지하는 방식과 유사합니다. 다만, Transformer는 단일 히든 상태가 아닌 **여러 개의 상태(키-값 쌍)**를 동시에 유지한다는 점이 다릅니다.

2. **Unbounded Multi-State RNN으로서의 Transformer**: Transformer의 경우, 시퀀스 길이에 제한 없이 각 토큰에 대해 새로운 상태를 추가할 수 있어 **무제한 상태를 가지는 RNN**으로 볼 수 있습니다. 각 키-값 벡터는 특정 토큰의 정보와 연결된 상태로서 기능하며, Transformer는 이 벡터들을 이용해 필요한 정보를 선택적으로 참조합니다. 이는 RNN이 히든 상태 벡터를 사용해 이전 입력 정보를 저장하고 다음 입력을 예측하는 방식과 비슷합니다.

3. **Autoregressive 성질**: Transformer 모델, 특히 **디코더(Decoder) 모델**은 RNN처럼 순차적인 예측을 수행합니다. 즉, 현재 토큰을 생성하기 위해 이전 시점의 정보를 필요로 하며, 이를 통해 다음 토큰을 예측합니다. 이 과정은 Transformer가 시퀀스 정보를 누적하는 방식에서 RNN과 유사한 연속성을 제공합니다.

### **Multi-State RNN을 Bounded(유한)하게 만드는 Transformer 변환**
논문에서는 Transformer가 사실상 **무제한 상태를 가지는 Multi-State RNN**으로 동작한다는 점을 설명하면서, 이를 **유한한 상태를 가지는 Bounded MSRNN**으로 변환하는 방법을 제시합니다. 이 과정에서 **TOVA(Token Omission Via Attention)**라는 정책이 도입되는데, 이는 어텐션 점수가 낮은 상태를 제거하여 일정한 크기의 상태만 유지하도록 하는 방식입니다. 이 방식은 Transformer의 모든 상태를 유지하지 않고, 중요한 정보만을 선택적으로 유지하는 방법으로, RNN의 제한된 메모리와 유사한 환경을 조성합니다.

### Transformer와 RNN의 연계성의 중요성
1. **메모리 효율성 향상**: Transformer를 Multi-State RNN으로 간주하고 유한 상태로 변환함으로써 메모리 사용량을 줄일 수 있습니다. 이는 특히 긴 시퀀스를 처리하는 LLM에서 메모리 사용을 최적화하는 데 도움을 줍니다.
   
2. **Transformer의 RNN적 속성 이해**: Transformer의 어텐션 메커니즘이 단순히 모든 토큰 간 상호작용을 다루는 것이 아니라, 각 토큰이 특정 상태를 형성하여 시퀀스를 누적적으로 처리하는 RNN적 속성을 가지고 있음을 이해할 수 있습니다. 이는 Transformer가 사실상 RNN과 유사하게 시퀀스 의존성을 처리할 수 있음을 보여줍니다.

3. **다양한 모델에 적용 가능성**: MSRNN 개념을 통해 Transformer를 보다 효율적으로 사용할 수 있어, 이를 활용한 다양한 NLP 및 시계열 예측 작업에 적용할 수 있는 가능성이 열립니다.

결론적으로, 이 논문은 Transformer와 RNN 간의 개념적 연결성을 통해 **Transformer의 메모리 관리 효율성을 개선**하고, **RNN의 장점을 결합한 새로운 시각**을 제시합니다.