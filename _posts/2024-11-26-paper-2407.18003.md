---
title: "Keep the Cost Down: A Review on Methods to Optimize LLM’s KV Cache Consumption."
date: 2024-11-26
categories:
 - paper-review
 - with-gpt
---

[논문 링크](https://arxiv.org/abs/2407.18003)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



The uploaded paper is a comprehensive review titled "Keep the Cost Down: A Review on Methods to Optimize LLM’s KV Cache Consumption." Here's a detailed breakdown:

### 강점과 독창적인 지점
1. **철저한 조사와 구조화**:
   - KV Cache(Transformer의 키-값 캐시) 최적화에 대한 최신 기술과 연구를 포괄적으로 다룹니다.
   - 훈련, 배포, 사후 최적화 단계별로 구조화하여 다양한 기술을 논리적으로 비교합니다.

2. **다양한 최적화 방법 분석**:
   - 훈련 단계에서의 구조적 변화(GQA, MQA)부터 배포 및 사후 최적화(캐시 제거, 압축, 정량화)까지 다양한 접근법을 상세히 기술합니다.

3. **효율성 중심 평가**:
   - GPU 메모리 사용량, 처리량, 대기 시간, 언어 모델의 성능 변화(Perplexity 등)를 분석하여 효율성과 성능 간의 균형을 고려합니다.

4. **독창성**:
   - GQA(Grouped Query Attention)와 MQA(Multi-Query Attention)의 효율성을 구체적으로 비교하여 특정 시나리오에 맞는 최적 전략을 제시합니다.
   - 배포 단계에서는 Paged Attention과 분산 KV Cache의 적용을 통해 메모리 병목 문제를 해결하려는 혁신적인 방안을 제공합니다.

### 핵심 알고리즘 및 예시
#### KV Cache 최적화 과정 예시:
1. **기본 개념**:
   - Transformer 모델은 Key와 Value를 사용해 이전의 입력 토큰 정보를 저장하고 이를 재사용하여 효율성을 높입니다.
   - 이 과정에서 KV Cache 크기는 시퀀스 길이에 따라 선형적으로 증가하며 GPU 메모리를 과도하게 소모합니다.

2. **Grouped Query Attention(GQA)**:
   - Query 헤드를 그룹으로 묶고, 각 그룹이 동일한 Key 헤드와 Value 헤드를 공유합니다.
   - 이를 통해 메모리 사용량을 nh/ng로 줄이며, ng는 그룹 수를 나타냅니다.

3. **예시 입력**:
   - 시퀀스: [Token1, Token2, Token3, Token4]
   - GQA 적용:
     - 기존 Multi-Head Attention에서는 4개의 Query, Key, Value를 유지.
     - GQA에서는 Query를 2개의 그룹으로 묶고 각 그룹이 하나의 Key와 Value를 공유.
   - 메모리 절감:
     - 기존: 4 Queries × 4 Keys × 4 Values
     - GQA: 2 Groups × 2 Keys × 2 Values

4. **결과**:
   - GPU 메모리 절감과 계산 속도 향상.

### 한계점
1. **성능과 효율성의 트레이드오프**:
   - MQA 및 GQA 같은 방법은 메모리를 줄이지만, Query 및 Value의 압축은 복잡한 문맥에서 모델 성능 저하 가능.

2. **구현 복잡성**:
   - Paged Attention이나 분산 KV Cache와 같은 최적화는 GPU 및 분산 시스템에 대한 추가적인 설계가 필요하며, 사용자 정의 CUDA 커널을 요구함.

3. **테스트 세트와의 격차**:
   - 긴 문맥 처리 성능을 평가하기 위한 데이터셋이 제한적이며, 모델의 실제 사용 사례를 완벽히 반영하지 못할 가능성.

4. **학습 기반 최적화 한계**:
   - 일부 압축 기법(예: GQA, MLA)은 모델 구조를 수정해야 하므로, 사후 적용이 불가능하거나 제한적.

이 논문은 최신 LLM 최적화 기법을 이해하고 활용하는 데 매우 유용하며, 특히 메모리 효율성 문제가 주요한 경우 효과적인 방향성을 제공합니다. 그러나 실질적인 적용에는 추가적인 검토와 튜닝이 필요합니다.


# Q : 이 논문에서 설명하는 방법들을 아주 자세하게 설명을 추가해서 정리해줘

 

이 논문은 **KV Cache 최적화**를 중심으로, **대형 언어 모델(LLM)**의 효율성을 높이기 위해 다양한 단계에서 사용할 수 있는 방법들을 정리하고 비교합니다. 아래에 논문에서 언급된 최적화 방법을 세부적으로 정리하겠습니다.

---

## 1. **훈련 단계 최적화 (Training Phase Optimization)**
훈련 단계에서 모델의 구조를 변경하거나 키-값(Key-Value) 데이터를 더 효율적으로 저장하는 방법입니다.

### (1) Multi-Query Attention (MQA)
- **개념**: 기존의 Multi-Head Attention(MHA)은 여러 Query, Key, Value 헤드를 사용하지만, MQA는 하나의 Key와 Value 헤드만을 사용해 메모리 요구사항을 대폭 줄입니다.
- **장점**:
  - 메모리 사용량을 nh(헤드 개수)의 1/nh로 줄임.
  - Key와 Value 헤드가 하나로 통합되면서 계산량 감소.
- **한계**:
  - 헤드 수 감소로 인해 특정 컨텍스트에서 성능 저하 가능.
  
### (2) Grouped Query Attention (GQA)
- **개념**: Query 헤드를 그룹으로 묶어, 그룹마다 동일한 Key와 Value 헤드를 공유합니다.
  - ng(그룹 수)로 그룹화하여 메모리 사용량을 nh/ng로 줄임.
  - ng=1이면 MQA, ng=nh이면 MHA와 동일.
- **장점**:
  - MQA보다 성능 손실을 줄이면서 메모리를 효율적으로 관리.
  - ng 값 조정으로 성능과 효율성을 세밀하게 조정 가능.
- **활용 예**:
  - GPT-3와 같은 대형 모델의 긴 문맥 처리 시, GQA로 메모리 효율성을 극대화.

### (3) Cross-Layer Attention (CLA)
- **개념**: 여러 층(layer)에서 동일한 Key와 Value 캐시를 재사용합니다.
  - 예: 한 층에서 생성한 KV Cache를 다음 층에서도 사용.
- **장점**:
  - 메모리 사용량 감소.
  - 훈련된 모델의 재사용이 가능.
- **한계**:
  - 레이어 간 의존성이 높아지고, 데이터 접근 병목 현상이 발생할 수 있음.

---

## 2. **배포 단계 최적화 (Deployment Phase Optimization)**
모델을 배포할 때 시스템 효율성을 높이는 방법입니다.

### (1) Paged Attention
- **개념**: CPU 메모리 관리에서 사용하는 페이지 매핑 기법을 GPU 메모리에 도입.
  - KV Cache를 연속적으로 저장하지 않고, 비연속적인 메모리 블록으로 분할하여 저장.
  - 필요한 KV Cache만 매핑 테이블로 참조.
- **장점**:
  - 메모리 단편화 문제를 줄이고 GPU 메모리 효율성 증가.
  - 대규모 KV Cache 처리에 적합.

### (2) DistAttention (Distributed Attention)
- **개념**: KV Cache를 여러 서버에 분산 저장하여, 대규모 클라우드 환경에서 모델을 최적화.
- **장점**:
  - 분산 저장을 통해 대형 LLM을 처리.
  - GPU 메모리 병목 문제를 완화.

### (3) Chunk Attention
- **개념**: 대화 히스토리를 사전 처리하여, 동일한 컨텍스트를 가진 토큰들의 KV Cache를 재사용.
  - **Dictionary Tree**를 생성해 가장 긴 공통 접두사를 찾아 재사용.
- **장점**:
  - 반복 작업 방지로 처리 속도 증가.
  - GPU 메모리 사용량 감소.

### (4) Speculative Offloading
- **개념**: KV Cache 데이터를 GPU에서 CPU로 오프로드(저장)하고, 필요한 경우 GPU로 다시 로드.
  - CPU에 저장할 때는 일부 중요한 데이터만 남겨둠.
- **장점**:
  - GPU 메모리 절약.
  - 성능 손실 없이 메모리 사용 최적화.

---

## 3. **사후 최적화 (Post-Training Optimization)**
훈련된 모델을 수정하거나 압축하여 최적화.

### (1) Eviction (캐시 제거)
- **정적 정책**:
  - 슬라이딩 윈도우(sliding window)처럼 최신 토큰만 유지.
  - 처음 생성된 토큰도 유지(초기 토큰이 중요하다는 연구 기반).
- **동적 정책**:
  - Attention Weight 기반: 이전 단계에서 높은 Attention을 받은 토큰만 유지.
  - H2 알고리즘: 누적된 Attention 점수를 기준으로 중요 토큰 유지.

### (2) Merging (캐시 병합)
- **개념**: 여러 Key와 Value를 병합하여 저장.
  - Gaussian Weighting, Attention Score 등을 사용해 병합.
- **장점**:
  - 하드 삭제가 아닌 소프트 압축으로 정보 손실을 최소화.

### (3) Quantization (양자화)
- **Full Quantization**:
  - 모델 전체를 4비트 또는 8비트로 압축.
- **KV Cache-Only Quantization**:
  - KV Cache 데이터만 양자화.
  - 예: KVQuant는 Key와 Value를 채널별로 다르게 압축.
- **장점**:
  - 메모리 효율성을 극대화.
  - 기존 모델 성능을 크게 손상시키지 않음.

---

## 4. **평가 기준**
최적화 기술의 성능을 평가하기 위한 주요 기준:
1. **GPU 메모리 사용량**: 토큰당 메모리 사용량 측정.
2. **처리량과 대기 시간**: 초당 생성 토큰 수(token/s)와 토큰당 생성 시간(ms).
3. **Perplexity(PPL)**: 모델의 언어 예측 정확도를 평가.

---

## 결론
이 논문은 **KV Cache 최적화**를 위해 다양한 기술과 그 장단점을 포괄적으로 설명하며, 실질적인 배포 및 훈련 시 적용 가능한 구체적인 전략을 제공합니다. 특히, 긴 문맥 처리가 필요한 대형 언어 모델에서 GPU 메모리 병목을 해결하고, 효율성을 높이는 데 기여합니다. 

이 모든 방법은 모델 구조, 데이터 압축, 그리고 시스템 아키텍처를 포함하여 다양한 단계에서 활용 가능하며, 효율성과 성능 간의 균형을 맞추는 데 중요한 통찰을 제공합니다.