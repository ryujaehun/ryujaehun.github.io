---
title: "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
date: 2024-12-09
categories:
 - paper-review
 - with-gpt
 - LLM-Inference
---

[논문 링크](https://arxiv.org/abs/2402.02750)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘

# A :

### 강점과 독창성

1. **효율적이고 튜닝이 필요 없는 2비트 양자화 알고리즘**:
   - KIVI는 LLM의 키-값 캐시(KV Cache)를 튜닝 없이 2비트로 양자화하여 최대 2.6배 메모리 감소와 최대 3.47배 처리량 증가를 달성합니다. 
   - 기존 연구와 달리 키 캐시는 채널별로, 값 캐시는 토큰별로 양자화하는 새로운 방식을 제안하여 성능 손실을 최소화했습니다.

2. **하드웨어 친화적 구현**:
   - 이 알고리즘은 하드웨어 효율성을 고려한 구현을 통해 GPU의 성능을 극대화하며, 매트릭스 곱셈 과정과 양자화 해제를 통합한 Q_MatMul을 제공합니다.

3. **다양한 모델 및 작업에서 성능 검증**:
   - Llama, Mistral, Falcon 모델 등 다양한 LLM에서 테스트되어, 정밀도 감소 없이 메모리 사용량과 처리량을 최적화하는 데 성공했습니다.

4. **슬라이딩 윈도우 전략 도입**:
   - 부분적으로만 고정밀도를 유지하는 슬라이딩 윈도우 방식으로 어려운 작업에서도 높은 성능을 유지합니다.

---

### 핵심 알고리즘 과정 (예시 입력과 함께)

1. **입력 처리**:
   - 입력 텐서 \(X \in \mathbb{R}^{b \times l_{prompt} \times d}\) (배치 크기 \(b\), 프롬프트 길이 \(l_{prompt}\), 히든 크기 \(d\)).
   - 키 (\(X_K = XW_K\))와 값 (\(X_V = XW_V\)) 생성 후 캐시에 저장.

2. **양자화**:
   - 키 캐시는 채널별로 그룹화 후 양자화 (그룹 크기 \(G\), 잔여 길이 \(R\)).
   - 값 캐시는 토큰별로 양자화하여 새로운 토큰이 추가될 때마다 쉽게 병합.

3. **디코딩 과정**:
   - 각 새로운 입력 토큰 (\(t\))에 대해, 키 및 값을 업데이트:
     - \(t_K = tW_K\), \(t_V = tW_V\).
     - 기존 캐시와 결합: \(X_K \leftarrow \text{Concat}(X_K, t_K)\), \(X_V \leftarrow \text{Concat}(X_V, t_V)\).
   - 관심 출력 계산:
     \[
     t_Q = tW_Q, \quad A = \text{Softmax}(t_Q X_K^T), \quad t_O = AX_V.
     \]

4. **출력**:
   - 디코딩된 토큰 \(t_O\)를 생성하고 업데이트된 캐시를 반환.

---

### 한계점

1. **특정 작업에서의 정밀도 저하**:
   - Falcon-7B 모델과 같이 이미 압축된 구조를 가진 모델에서는 2비트 양자화가 정밀도에 부정적인 영향을 미칠 수 있음.

2. **하드웨어 종속성**:
   - GPU 기반 환경에서 최적화된 알고리즘이므로 CPU 등 다른 하드웨어에서는 성능이 저하될 가능성이 있음.

3. **복잡한 하이퍼파라미터 설정**:
   - 그룹 크기 \(G\)와 잔여 길이 \(R\)의 설정이 정확도와 효율성에 미치는 영향이 크므로, 사용자가 실험적으로 조정해야 하는 부담이 있음.

---

논문은 LLM의 메모리 및 처리량 병목현상을 해결하는 데 기여하며, 특히 채널별 및 토큰별 양자화 전략을 통해 기존 접근법의 한계를 극복합니다. 다만, 특정 모델 구조나 환경에서의 적용 가능성을 면밀히 검토할 필요가 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘

# A :

논문에서 제안한 **KIVI 알고리즘**은 LLM(Key-Value Cache)의 메모리 사용량을 줄이고, 처리량을 높이기 위해 키 캐시와 값 캐시를 각각 **채널별(Per-Channel)** 및 **토큰별(Per-Token)**로 양자화하는 방법을 사용합니다. 이를 예시를 통해 자세히 설명해 보겠습니다.

---

## **KIVI 알고리즘의 핵심 단계와 예시**

### **1. 프리필(Prefill) 단계**

#### **설명**
- 초기 입력 프롬프트 \( X \in \mathbb{R}^{l_{prompt} \times d} \)에서 키와 값 텐서를 계산하고, 이를 캐시에 저장합니다.
- 키 캐시는 **채널별**로, 값 캐시는 **토큰별**로 양자화합니다.

#### **예시**
- **입력 프롬프트 텐서 \( X \):**
  - 크기: \( l_{prompt} = 4 \), \( d = 3 \) (4개의 토큰, 3개의 채널)
  - 값: 
    \[
    X = 
    \begin{bmatrix}
    0.1 & 0.2 & 0.3 \\
    0.4 & 0.5 & 0.6 \\
    0.7 & 0.8 & 0.9 \\
    1.0 & 1.1 & 1.2
    \end{bmatrix}
    \]

- **키 계산 \( X_K \):**
  - 키 가중치 \( W_K \) 적용: \( X_K = XW_K \)
  - 결과:
    \[
    X_K = 
    \begin{bmatrix}
    0.3 & 0.5 & 0.7 \\
    0.6 & 0.8 & 1.0 \\
    0.9 & 1.1 & 1.3 \\
    1.2 & 1.4 & 1.6
    \end{bmatrix}
    \]

- **값 계산 \( X_V \):**
  - 값 가중치 \( W_V \) 적용: \( X_V = XW_V \)
  - 결과:
    \[
    X_V = 
    \begin{bmatrix}
    0.2 & 0.4 & 0.6 \\
    0.3 & 0.5 & 0.7 \\
    0.8 & 1.0 & 1.2 \\
    0.9 & 1.1 & 1.3
    \end{bmatrix}
    \]

- **양자화:**
  - **키 \( X_K \) (채널별):**
    - 각 채널의 최소값/최대값 계산 후 2비트 양자화:
      - 채널별 최소값: \([0.3, 0.5, 0.7]\)
      - 채널별 최대값: \([1.2, 1.4, 1.6]\)
    - 2비트 양자화 결과:
      \[
      Q(X_K) = 
      \begin{bmatrix}
      0 & 1 & 1 \\
      1 & 2 & 2 \\
      2 & 3 & 3 \\
      3 & 3 & 3
      \end{bmatrix}
      \]

  - **값 \( X_V \) (토큰별):**
    - 각 토큰의 최소값/최대값 계산 후 2비트 양자화:
      - 첫 번째 토큰 최소값/최대값: \(0.2, 0.6\)
      - 양자화 결과:
        \[
        Q(X_V) = 
        \begin{bmatrix}
        0 & 2 & 3 \\
        0 & 1 & 3 \\
        1 & 3 & 3 \\
        1 & 3 & 3
        \end{bmatrix}
        \]

---

### **2. 디코딩(Decoding) 단계**

#### **설명**
- 새로운 입력 토큰 \( t \)이 추가될 때마다 키와 값 캐시를 업데이트하고, 관심(attention) 출력 \( t_O \)를 계산합니다.

#### **예시**
- **새로운 입력 토큰 \( t \):**
  - 크기: \( 1 \times 3 \) (토큰 하나, 채널 3)
  - 값: \( t = [0.5, 0.7, 0.9] \)

- **업데이트:**
  - 키 텐서 \( t_K = tW_K \): \( t_K = [0.6, 0.8, 1.0] \)
  - 값 텐서 \( t_V = tW_V \): \( t_V = [0.4, 0.6, 0.8] \)
  - 기존 캐시에 추가:
    - \( X_K \leftarrow \text{Concat}(X_K, t_K) \)
    - \( X_V \leftarrow \text{Concat}(X_V, t_V) \)

- **양자화:**
  - 키 캐시는 채널별로 그룹화하여 업데이트된 부분만 재양자화.
  - 값 캐시는 토큰별로 즉시 양자화.

- **출력 계산:**
  - 쿼리 벡터 \( t_Q = tW_Q \): \( t_Q = [0.3, 0.5, 0.7] \)
  - 관심 스코어 \( A = \text{Softmax}(t_Q X_K^T) \)
  - 최종 출력 \( t_O = AX_V \).

---

### **KIVI 알고리즘의 핵심 장점**

1. **효율성**: 
   - 키 캐시와 값 캐시의 양자화 방식이 서로 다른 요구사항에 최적화되어 있음.
   - GPU에서 하드웨어 친화적인 계산이 가능.

2. **메모리 절약**:
   - 2비트 양자화를 통해 최대 2.6배의 메모리 절감.

3. **정확도 유지**:
   - 슬라이딩 윈도우로 최신 데이터를 고정밀도로 유지하여 성능 손실 최소화.

---

### **정리**

- **KIVI는 키 캐시를 채널별, 값 캐시를 토큰별로 양자화하여 메모리 사용을 최소화하고 처리량을 극대화하는 알고리즘입니다.**
- **양자화는 하드웨어 친화적으로 설계되었으며, 슬라이딩 윈도우를 활용하여 정밀도를 유지하면서도 어려운 작업에서도 우수한 성능을 보입니다.**


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘

# A :

### **논문에서 제시한 주요 결과**

1. **메모리 효율성**:
   - KIVI는 Llama, Mistral, Falcon 모델에서 키-값 캐시(KV Cache)를 2비트로 양자화하여 최대 **2.6배 메모리 절감**을 달성했습니다.
   - 이로 인해 **4배 더 큰 배치 크기**를 지원할 수 있으며, 이는 실질적으로 **2.35~3.47배 처리량 증가**를 가져왔습니다.

2. **정확도 유지**:
   - Llama-2-7B 모델에서는 정확도의 감소 없이 CoQA, TruthfulQA, GSM8K와 같은 벤치마크에서 2비트 양자화를 통해 16비트 풀 정밀도와 거의 동일한 성능을 보였습니다.
   - 특정 어려운 작업(GSM8K)에서도 **슬라이딩 윈도우** 기법을 통해 정확도 감소를 최소화했습니다.

3. **실제 사용 사례**:
   - KIVI는 ShareGPT와 같은 실제 워크로드에 대해 테스트되었으며, GPU 메모리 병목 현상을 줄이는 데 효과적임을 입증했습니다.

---

### **다른 방법론과의 비교**

#### **다른 방법론의 한계**
1. **기존 4비트 양자화**:
   - 기존의 FlexGen 등에서 사용된 4비트 양자화는 메모리를 줄이는 데 효과적이나, 더 낮은 정밀도(2비트)로 줄이려면 정확도가 크게 감소.
   - 주로 값 캐시와 키 캐시 모두 토큰별 양자화를 사용하여 키 캐시의 고유 특성을 반영하지 못함.

2. **토큰 제거 기반 접근법**:
   - Scissorhands와 H2O는 중요하지 않은 토큰을 제거하여 메모리를 절감하지만, 이는 모델이 장기적 의존성(long-term dependencies)을 처리하는 능력을 손상시킬 위험이 있음.

3. **시스템 기반 접근법**:
   - vLLM과 S3는 시스템 수준에서 GPU 메모리 관리를 최적화하지만, 이는 하드웨어에 강하게 의존하고 추가 구현 비용이 높음.

#### **KIVI의 특출난 점**
1. **키와 값 캐시의 독립적인 양자화 전략**:
   - 키 캐시는 채널별(Per-Channel), 값 캐시는 토큰별(Per-Token)로 양자화하여 각 캐시의 데이터 분포 특성을 반영.
   - 키 캐시의 일부 채널이 큰 값(outlier)을 가지는 특성을 활용하여, 채널별 양자화로 **오류를 국소화**.

2. **슬라이딩 윈도우 전략**:
   - 최근 \( R \)개의 키와 값 캐시를 고정밀도로 유지하여 정확도를 유지하면서도 메모리를 절약.
   - 이는 수학적 추론 작업(GSM8K)과 같이 높은 정밀도가 필요한 작업에서도 성능 손실을 줄임.

3. **하드웨어 친화적 설계**:
   - 양자화 및 역양자화 과정을 매트릭스 곱셈에 통합(Q_MatMul)하여 GPU 상에서 높은 효율성 제공.

4. **튜닝 필요 없음**:
   - KIVI는 사전 튜닝 없이도 플러그앤플레이 방식으로 작동 가능.

---

### **KIVI가 결과를 도출한 이유**

논문에서 제시하는 이유는 다음과 같습니다:
1. **키와 값 캐시의 데이터 분포 차이**:
   - 키 캐시는 일부 채널에 큰 값이 집중(outlier pattern)되어 있으므로 채널별 양자화가 더 적합.
   - 값 캐시는 비교적 균등한 분포를 가지므로 토큰별 양자화가 적합.
   
2. **양자화 오류의 국소화**:
   - 채널별 양자화는 키 캐시에서의 오류를 개별 채널에 국한시킴.
   - 토큰별 양자화는 특정 토큰의 양자화 오류가 다른 토큰에 영향을 미치지 않도록 함.

3. **슬라이딩 윈도우 전략**:
   - 최신 데이터에서 고정밀도를 유지함으로써, 장기적 문맥 의존성과 복잡한 추론 작업에서 높은 성능을 유지.

4. **하드웨어 최적화**:
   - KIVI는 실시간 스트리밍 데이터 처리 방식과 양자화 방식을 조합하여 GPU 처리 속도를 극대화.

---

### **내 생각**

1. **KIVI의 핵심 강점**:
   - 다른 접근법과 달리, 키와 값 캐시의 데이터 분포를 세부적으로 분석하여 **맞춤형 양자화 전략**을 개발한 점이 돋보입니다.
   - 특히, 슬라이딩 윈도우 기법은 정확도와 효율성 사이의 균형을 잡는 데 매우 효과적이라고 생각합니다.

2. **독창성과 실용성**:
   - KIVI는 시스템-수준 솔루션(vLLM)과 모델 내부 최적화(FlexGen, Scissorhands)의 강점을 결합한 방식으로, 매우 실용적입니다.
   - 실제 워크로드에서 처리량을 대폭 향상시킨 점은 KIVI의 실질적 가치를 증명합니다.

3. **추가 개선 가능성**:
   - 하드웨어 독립적인 최적화 방안(CPU 환경에서의 성능 향상)이나 다양한 작업(예: 멀티모달 작업)에서의 테스트가 이루어진다면, 더 많은 응용 가능성이 보일 것입니다.

---

### **결론**
KIVI는 LLM의 키-값 캐시에 대한 맞춤형 양자화 전략(채널별 및 토큰별 양자화)을 통해, 다른 방법론에서 해결하지 못한 메모리 효율성과 정확도 문제를 동시에 해결합니다. 이 논문의 가장 독창적인 부분은 데이터 분포에 대한 면밀한 분석과, 이를 기반으로 한 양자화 접근 방식의 설계라고 생각됩니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘

# A :

### **KIVI 논문의 입력 데이터와 추론 과정**

KIVI의 입력 데이터와 추론 과정을 이해하기 위해, LLM이 일반적으로 사용하는 키-값 캐시(Key-Value Cache)의 역할을 설명하고, 이를 기반으로 논문의 양자화 기법을 예시와 함께 자세히 설명하겠습니다.

---

#### **입력 데이터**
- **입력 텐서 \(X\):**
  - 입력은 \(X \in \mathbb{R}^{b \times l_{prompt} \times d}\) 형태로 주어지며, 여기서:
    - \(b\): 배치 크기
    - \(l_{prompt}\): 입력 프롬프트의 길이(토큰 수)
    - \(d\): 모델의 히든 크기(임베딩 차원)
  - 예시:
    - \(b = 2\), \(l_{prompt} = 3\), \(d = 4\)일 때:
      \[
      X = 
      \begin{bmatrix}
      [0.1, 0.2, 0.3, 0.4] \\
      [0.5, 0.6, 0.7, 0.8] \\
      [0.9, 1.0, 1.1, 1.2]
      \end{bmatrix},
      \begin{bmatrix}
      [0.2, 0.3, 0.4, 0.5] \\
      [0.6, 0.7, 0.8, 0.9] \\
      [1.0, 1.1, 1.2, 1.3]
      \end{bmatrix}
      \]

---

#### **추론 과정**

1. **프리필(Prefill) 단계**:
   - **목적**: 입력 프롬프트 \(X\)로부터 키 텐서 \(X_K\)와 값 텐서 \(X_V\)를 생성하고 캐시에 저장.
   - **계산 과정**:
     - 키 텐서: \(X_K = XW_K\), 값 텐서: \(X_V = XW_V\)
     - \(W_K, W_V\): 키와 값 계산을 위한 가중치 행렬.
     - 예시:
       \[
       W_K = 
       \begin{bmatrix}
       0.1 & 0.2 & 0.3 & 0.4 \\
       0.5 & 0.6 & 0.7 & 0.8 \\
       0.9 & 1.0 & 1.1 & 1.2 \\
       1.3 & 1.4 & 1.5 & 1.6
       \end{bmatrix}, 
       \quad
       W_V = 
       \begin{bmatrix}
       0.2 & 0.3 & 0.4 & 0.5 \\
       0.6 & 0.7 & 0.8 & 0.9 \\
       1.0 & 1.1 & 1.2 & 1.3 \\
       1.4 & 1.5 & 1.6 & 1.7
       \end{bmatrix}
       \]
       결과적으로 \(X_K\), \(X_V\)는 각각 \(b \times l_{prompt} \times d\) 크기를 가지며, 이후 양자화를 통해 캐시 메모리 절약.
   - **양자화**:
     - 키 텐서: 채널별(Group-wise) 양자화.
     - 값 텐서: 토큰별(Token-wise) 양자화.

2. **디코딩(Decoding) 단계**:
   - **목적**: 새롭게 생성된 토큰 입력 \(t \in \mathbb{R}^{1 \times d}\)에 대해 키-값 캐시를 업데이트하고, 관심(attention) 출력을 계산.
   - **계산 과정**:
     - 키 업데이트: \(X_K \leftarrow \text{Concat}(X_K, tK)\), 값 업데이트: \(X_V \leftarrow \text{Concat}(X_V, tV)\).
     - 관심 스코어 계산:
       \[
       A = \text{Softmax}(t_Q X_K^T), \quad t_O = AX_V
       \]
       여기서 \(t_Q = tW_Q\), \(W_Q\)는 쿼리 벡터를 계산하기 위한 가중치 행렬.
     - 최종 출력 \(t_O\)는 생성된 토큰의 확률 분포.

---

### **모델 아키텍처 구성**

1. **트랜스포머 기반 구조**:
   - 멀티헤드 셀프 어텐션(Multi-Head Self-Attention)
   - 각 층은 키, 값, 쿼리 벡터의 계산으로 구성:
     \[
     \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
     \]
   - \(Q, K, V\)는 쿼리, 키, 값 행렬로 입력 데이터에서 가중치 행렬(\(W_Q, W_K, W_V\))을 곱해 계산.

2. **키-값 캐시(KV Cache)**:
   - 디코딩 과정에서 계산 효율성을 높이기 위해 생성된 키와 값을 캐시에 저장.
   - **저장 크기**: \(b \times (l_{prompt} + l_{gen}) \times d\)
     - \(l_{gen}\): 생성된 토큰의 길이.

---

### **연산 요구량 및 메모리 요구량**

#### **1. 연산 요구량**
- **어텐션 계산**:
  - \(O(b \times l_{gen} \times l_{prompt} \times d)\)
  - \(QK^T\): \(O(d \times l_{prompt})\), 곱해지는 토큰 수에 따라 선형적으로 증가.
- **양자화 추가 연산**:
  - \(O(\text{group size})\): 채널별 및 토큰별 양자화에 사용되는 연산은 비교적 작음.

#### **2. 메모리 요구량**
- **16비트 캐시**:
  - \(b \times (l_{prompt} + l_{gen}) \times d \times 16\) 비트.
- **KIVI (2비트 양자화)**:
  - 키와 값 캐시를 양자화하여 메모리 사용량 8배 감소.
  - 예: \(b = 512, l_{prompt} = 512, d = 1024\)인 경우,
    - 16비트: 약 2TB.
    - 2비트 KIVI: 약 256GB.

#### **3. 시스템 최적화**
- **GPU 메모리 병목 최소화**:
  - Q_MatMul로 양자화 해제 및 매트릭스 곱셈을 통합.
  - 디코딩 과정에서 캐시 업데이트와 계산을 병렬 처리.

---

### **정리**

- **입력과 추론 과정**: 입력 프롬프트와 생성된 토큰을 기반으로 키-값 캐시를 지속적으로 업데이트하며, 어텐션 메커니즘으로 다음 토큰을 예측.
- **모델 아키텍처**: 트랜스포머 기반 멀티헤드 어텐션 구조를 사용하며, 키-값 캐시는 메모리 절약 및 처리 속도 향상을 위해 양자화.
- **연산 및 메모리 요구량**: KIVI는 기존 16비트 저장 방식에 비해 메모리 사용량을 대폭 줄이며, GPU 하드웨어를 효율적으로 활용하여 연산 속도를 증가시킴.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘

# A :

### **KIVI 논문의 한계**

1. **낮은 정밀도의 한계**:
   - 2비트 양자화는 메모리 사용량을 크게 줄이지만, 일부 작업(예: Falcon-7B와 같은 구조)에서는 정확도가 크게 떨어질 수 있음.

2. **슬라이딩 윈도우 전략의 제한**:
   - 슬라이딩 윈도우에서 유지되는 \(R\)개의 고정밀도 토큰은 양자화의 효과를 약화시킬 수 있음. \(R\) 값을 증가시키면 메모리 절약 효과가 감소하고, \(R\)을 줄이면 정확도가 손상될 위험이 있음.

3. **하드웨어 종속성**:
   - GPU에서 최적화된 알고리즘으로 설계되었기 때문에 CPU 기반 시스템이나 하드웨어 가속기(예: FPGA, TPU)에서의 효율성이 충분히 검증되지 않음.

4. **특정 작업에서 성능 저하**:
   - KIVI는 주로 생성 작업(Generation Tasks)에 초점을 맞췄으며, 다른 유형의 작업(예: 분류, MMLU와 같은 멀티태스크 평가)에서는 검증이 부족.

---

### **한계를 극복하기 위한 연구 흐름**

#### **1. 적응형 양자화(Adaptive Quantization)**
- **아이디어**:
  - 입력 데이터의 분포와 작업의 복잡성에 따라 동적으로 양자화 수준을 조정.
- **가능한 방법**:
  - 중요도가 낮은 토큰이나 채널에 더 낮은 비트 수를 할당하고, 중요도가 높은 데이터에 더 높은 비트를 할당하는 비균일 양자화(Non-uniform Quantization).
  - 예: 토큰별 중요도(weighted importance)를 평가하여 양자화 설정을 조정.
- **장점**:
  - 성능 손실을 줄이면서도 메모리 효율성을 유지할 수 있음.
- **관련 연구**:
  - ATOM (Zhao et al., 2024): 비균일 양자화를 통해 정확도와 효율성의 균형을 맞춤.
  - SqueezeLLM (Kim et al., 2023): 중요도 기반 가중치를 활용한 모델 압축.

---

#### **2. 토큰 중요도 기반 접근법(Token Importance Sparsification)**
- **아이디어**:
  - KIVI는 모든 토큰을 저장하되 양자화하는 방식을 사용했지만, 중요하지 않은 토큰을 제거하거나 압축하는 방법을 결합.
- **가능한 방법**:
  - 중요도가 낮은 토큰을 제거하거나 축소하는 Sparsification 기술을 도입.
  - 기존의 중요한 토큰의 양자화 수준을 높이는 방식과 병합.
- **장점**:
  - 메모리 사용량을 추가로 줄이고 처리 속도를 증가.
- **관련 연구**:
  - **H2O (Zhang et al., 2023)**: 주목(attention) 점수가 낮은 토큰을 삭제하여 메모리 효율성 향상.
  - **Scissorhands (Liu et al., 2024)**: 중요도 유지 가설(Persistence of Importance Hypothesis)을 활용하여 중요한 토큰만 저장.

---

#### **3. 시스템-레벨 최적화(System-Level Optimization)**
- **아이디어**:
  - KIVI의 GPU 중심 알고리즘을 CPU, TPU, 기타 하드웨어 환경에서 효과적으로 구현.
- **가능한 방법**:
  - 메모리 관리 최적화를 통해 캐시 활용도를 높이는 시스템 솔루션 도입.
  - 예: GPU와 메인 메모리 간의 데이터 전송 병목을 줄이기 위한 **PagedAttention**(vLLM) 기술.
- **장점**:
  - 다양한 하드웨어 환경에서 알고리즘의 유연성을 높임.
- **관련 연구**:
  - **vLLM (Kwon et al., 2023)**: 메모리 관리와 페이징 기술을 결합하여 GPU 메모리 병목 문제 해결.
  - **S3 (Jin et al., 2023)**: 스트리밍 데이터와 동적 메모리 관리를 결합하여 처리 속도 향상.

---

#### **4. 멀티모달 데이터에 대한 확장**
- **아이디어**:
  - KIVI는 텍스트 기반 LLM에 초점을 맞췄으나, 이미지, 오디오, 비디오 등 멀티모달 데이터를 처리하는 데에도 적용 가능성을 검토.
- **가능한 방법**:
  - 각 모달리티(텍스트, 이미지, 오디오 등)의 데이터 분포를 분석하여 맞춤형 양자화 전략 설계.
  - 예: 텍스트 데이터는 KIVI, 이미지 데이터는 벡터 양자화(Vector Quantization) 방식 적용.
- **장점**:
  - 멀티모달 LLM에서 메모리 요구량과 처리 속도 병목을 해결.
- **관련 연구**:
  - **Gemini (Reid et al., 2024)**: 멀티모달 데이터를 위한 확장 가능한 컨텍스트 모델.
  - **Perceiver IO**: 텍스트와 비텍스트 데이터를 통합적으로 처리.

---

#### **5. 메모리-연산 균형(Memory-Compute Tradeoff)**
- **아이디어**:
  - 메모리와 연산량 간의 균형을 동적으로 조정하여 하드웨어의 성능을 최적화.
- **가능한 방법**:
  - 양자화 레벨과 슬라이딩 윈도우 크기를 조정하여 연산량을 제어.
  - 연산량이 많아지는 작업에서는 더 낮은 양자화 수준 적용.
- **장점**:
  - 계산 복잡성이 높은 작업에서 처리 효율성을 높일 수 있음.
- **관련 연구**:
  - **SmoothQuant (Xiao et al., 2023)**: 가중치와 활성화의 균형 조정을 통해 양자화를 용이하게 함.
  - **Landmark Attention (Mohtashami and Jaggi, 2023)**: 관심의 중심을 특정 영역에 집중하여 연산량을 감소.

---

### **내 의견**
1. **적응형 양자화**는 다양한 데이터와 작업에 적응할 수 있는 유연성을 제공하므로 KIVI의 한계를 극복하는 가장 중요한 방향이 될 것으로 보입니다.
2. **시스템 레벨 최적화**와의 결합은 KIVI를 하드웨어에 구애받지 않고 다양한 플랫폼에서 활용 가능하게 만들 것입니다.
3. 멀티모달 환경에서의 확장 가능성을 염두에 둔다면 KIVI의 응용 범위를 넓혀 더 많은 실질적 영향을 줄 수 있을 것으로 보입니다.

---

### **결론**
KIVI의 한계를 극복하기 위해서는 적응형 양자화, 토큰 중요도 기반 접근법, 시스템-레벨 최적화 등 다양한 연구 흐름이 필요합니다. 특히, 데이터 특성에 맞춘 유연한 양자화 전략과 하드웨어 독립적인 최적화가 핵심 연구 방향이 될 것입니다.
