---
categories:
- paper-review
- with-gpt
date: "2025-07-09"
tags:
- 2402.17762v2
- Transformer
- SelfAttention
- BiasMechanism
- RepresentationLearning
- Interpretability
- NeuralMechanisms
- Massive Activations
- Explicit Attention Bias
title: '[논문리뷰] Massive Activations in Large Language Models'
showRelated: true
relatedPostsCount: 3
katex:
  enable: true
mermaid: true
---

[논문 링크](https://arxiv.org/abs/2402.17762v2)


# Massive Activations, Hidden Biases: 재해석된 Self-Attention의 비밀


---

## 한 줄 요약 (TL;DR)

수천만 개 중 **4–10 개의 초거대 스칼라(×10 000)** 가 LLM · ViT의 성능을 사실상 ‘혼자서’ 지탱한다.
이 스칼라는 **입력과 무관한 상수형 자기-어텐션 bias** 로 작동하며, -0-으로 만들면 모델이 즉시 붕괴(PPL → ∞)한다.
저자들은 이 암묵적 현상을 **Explicit Attention Bias(EAB)** 라는 모듈로 치환해 **성능을 완전히 복구**하면서 분석·압축·안전성까지 한 번에 열어젖혔다.&#x20;

---

## 핵심 아이디어

1. **Massive Activation(MA) 발견**
   * 히든 스테이트에서 *median* 대비 최대 **10⁴ 배** 큰 스칼라가 극소수 존재.
2. **Self-Attention Bias 가설**
   * MA는 **key/value 벡터에 상수로 삽입된 bias** 로 해석된다.
3. **치환 실험**
   * MA를 0 → 성능 붕괴, **평균값** 으로 대치 → 성능 100 % 복구.
4. **Explicit Attention Bias(EAB)**
   * `(k′, v′)` 두 벡터를 어텐션에 직접 이어붙여 동일 기능을 *명시적 파라미터*로 구현.
   * 재훈련한 GPT-2는 **MA가 아예 발생하지 않으면서** 원본과 동일 PPL을 달성.&#x20;

---

## 배경 – 그들이 해결한 문제

기존 연구는 “이상치(outlier) feature”·“attention sink” 현상을 관찰했지만

* **왜** 스칼라 하나가 그렇게 커지는지,
* **어떻게** 모델 전체 성능을 좌우하는지,
* **무엇**으로 대체할 수 있는지

를 설명하지 못했다. 결과적으로 양자화·해석 가능성 연구는 꼬리 분포 문제에 발목이 잡혀 있었다. 본 논문은 **MA ≒ 고정 bias** 라는 관점을 통해 이 공백을 메운다.&#x20;

---

## 새로운 접근법 – **Explicit Attention Bias (EAB)**

$$
\text{Attention}\!\bigl(Q,K,V;\,k',v'\bigr)
  = \operatorname{softmax}\!\left(
        \frac{Q\,[\,K^{\top}\;{k'}^{\top}\,]}{\sqrt{d}}
    \right)
    \,[\,V;\,{v'}^{\top}]
$$


* **`k′, v′ ∈ ℝᵈ`** 는 입력과 무관한 *학습 가능한* 벡터.
* 모든 토큰은 softmax에서 `(k′, v′)` 와 함께 경쟁하므로, 모델은 MA 없이도 동일한 “집중” 효과를 얻는다.

이 한 줄이 논문의 ‘비밀 병기’다.&#x20;

---

## 작동 원리 – 구체적인 예시로 살펴보기

| 토큰  | 히든 벡터 `h` (d = 4)   | 설명        |
| ----- | ----------------------- | ----------- |
| `<s>` | **\[50, 0.1, -0.2, 0]** | **MA = 50** |
| “cat” | \[0.2, 0.1, -0.1, 0]    | 보통        |
| “.”   | **\[45, 0, 0, -0.1]**   | **MA ≈ 45** |

1. **탐지** – `|h| ≥ 100 ∧ |h|/median ≥ 10³` → 두 MA 검출.
2. **어텐션** – `Q,K` 가 첫 dim만 복사된다고 가정하면, softmax logits는 거의 `<s>`와 “.”에 몰려 **bias 효과**를 낸다.
3. **개입**

   * MA = 0 → 로그릿 급락 → 다음 층 입력 붕괴.
   * MA = 평균(≈ 47.5) → 동일 로그릿 → 성능 유지.
4. **EAB** – `k′ = 47.5`, `v′` 학습 → MA 필요 없음 + 동일 집중 패턴.&#x20;

---

## 성능 검증 – 주요 결과

| 모델·세팅           | MA 상태 | **PPL ↓** / **Top-1 ↑**  | 비고              |
| ------------------- | ------- | ------------------------ | ----------------- |
| **LLaMA2-7B** 원본  | 존재    | **5.47**                 | 기준              |
| MA = 0              | 제거    | **∞**                    | 성능 붕괴         |
| **MA = mean**       | 치환    | **5.47**                 | **완전 복구**     |
| **GPT-2 + EAB**     | 없음    | **3.04** (baseline 동일) | MA 0, 재훈련 필요 |
| **CLIP ViT-L** 원본 | 존재    | **75.5 %**               | ImageNet          |
| MA = 0              | 제거    | 59.8 %                   | -15.7 p           |
| MA = mean           | 치환    | **75.5 %**               | 복구              |

* **범용성** – LLM 20 여 종, ViT 12 종에서 동일 현상 관찰.
* **치환 가능성** – 평균 대치 *또는* EAB 로 성능 100 % 보존.

---

## 우리의 관점 – 강점, 한계, 그리고 왜 중요한가?

### 강점 🌟

1. **설명력** – 수수께끼였던 attention sink·outlier feature를 한 방에 설명.
2. **안정성** – MA 제거로 양자화 꼬리 분포 문제 해결 *가능성* 제시.
3. **단순성** – 벡터 두 개만 추가하면 끝나는 EAB 모듈.
4. **공격·방어 통찰** – “4-스칼라 공격 = 모델 붕괴”라는 새로운 보안 논의 촉발.

### 한계 ⚠️

* **재훈련 비용** – EAB는 현재 *풀-트레이닝* 이 필요.
* **정량 성능↑ 없음** – SOTA를 갱신하진 않는다.
* **기원 미해명** – 왜 MA가 2–4 층에서 ‘폭발’하는지는 여전히 블랙박스.
* **저정밀·멀티모달 확장 미검증** – INT4 LLM, 오디오 Transformer 등에 대한 데이터 없음.

### 왜 중요한가? 🔑

> **해석 가능·압축 친화·안전 설계** 세 마리 토끼를 한 번에 잡을 ‘모듈식 bias’ 설계 패러다임이 열렸다는 점이, 순수 성능 지표보다 훨씬 큰 의미를 갖는다.

---

## 다음 단계는? – 앞으로의 길

1. **Plug-in EAB** – LoRA-style 미세조정으로 기존 체크포인트에 `(k′, v′)` 삽입하기.
2. **MA 기원 모델링** – 초기화·데이터 통계·LayerNorm이 MA를 유발하는 수학적 조건 규명.
3. **저정밀·하드웨어 실험** – INT4/FP8 가속기에서 MA 없는 모델의 전력·지연 측정.
4. **멀티모달 확장** – 오디오 LLM, 비디오 ViT 등에서 동일 패턴 탐색.
5. **보안 연구** – MA 조작·차단을 이용한 백도어 공격 및 방어 메커니즘 설계.

머지않아 “**Bias-as-a-Module**” 이 LLM 설계의 표준 항목으로 자리 잡을지 모른다. 이 논문은 그 첫 단추를 끼웠다.



---

### 토글을 클릭하면 논문에 대한 자세한 LLM 질의응답 내용을 확인할 수 있습니다.

<details markdown="block">
<summary>▶️<strong>클릭하여 펼치기</strong></summary>




# 프롬프트 1.1.1 (연구의 공백)

```
"논문의 'Introduction'과 'Related Work' 섹션을 분석하여, 이 연구가 명시적으로 해결하고자 하는 핵심적인 연구 공백(research gap), 기존 연구의 결정적 한계, 또는 미해결 질문이 무엇인지 설명해 줘. 저자들이 설명하는, 이 논문 출판 시점의 '최신 기술(state of the art)'은 어떤 상태였는지 요약해 줘."
```

**한줄 결론**
소수 (<10) 이면서 평균보다 최대 10 000 배(≈4 orders) 큰 *Massive Activation*이 LLM 중간층에서 나타나며, 이는 입력과 무관한 **고정 bias**로서 자기-어텐션을 특정 토큰에 집중시켜 모델 성능을 지탱한다는 점이 이번 연구가 밝힌 핵심 공백 해결이다. 기존 연구가 다루던 *outlier feature*·*attention sink*만으로는 이 현상을 설명할 수 없었다.&#x20;

---

## 1. 연구 공백 정의

| 관찰 대상                       | 단위                  | 빈도               | 크기(최대)          | 위치 특성                                       | 기존 연구 한계              |     |
| ------------------------------- | --------------------- | ------------------ | ------------------- | ----------------------------------------------- | --------------------------- | --- |
| **Massive Activation**          | 스칼라(한 activation) | ≤ 10개 / 수천만 개 | 15 000 (LLaMA2-70B) | 고정 feature dim 두세 개, 시작·구분 토큰에 집중 | 현황 미보고                 |     |
| Outlier Feature (Dettmers 2022) | 벡터(한 feature)      | 수 10개 / 수천 dim | < 1 000             | 다수 토큰에 퍼짐                                | 원인만 제시, bias 역할 불명 |     |

### 핵심 연구 공백

1. **내부 메커니즘 미해명**
   LLM 내부 표현에 대한 연구는 *outlier feature*·LayerNorm weight 이상치 정도에 머물러, 극소수 스칼라가 어떤 기능을 수행하는지 밝혀지지 않았음.&#x20;
2. **Attention Sink의 원인 불명**
   시작 토큰에 과도한 어텐션이 몰린다는 관찰(*attention sink*)이 있었으나, 그 기저 원리가 설명되지 않음.&#x20;
3. **양자화·해석 가능성 제약**
   활성값 분포 꼬리가 모델 압축·정규화 안정성에 미치는 정량적 분석이 부족했음.

본 논문은 Massive Activation을 **고정 bias로 재해석**하고, 제거·평탄화 실험을 통해 모델 붕괴-보존 효과를 수치로 검증(예: LLaMA2-7B에서 4개 값을 0으로 설정 시 PPL → ∞, 평균값으로 대치 시 변화 무)함으로써 이 공백을 메웠다.&#x20;

---

## 2. 출판 시점의 최신 기술(SOTA) 요약

| 범주                                          | 대표 연구/모델                       | 한계                                             |
| --------------------------------------------- | ------------------------------------ | ------------------------------------------------ |
| **Outlier Feature** (Dettmers 2022, Lin 2023) | -8bit 양자화 장애 요인으로 분석      | 벡터 단위 이상치만 규정, 스칼라 희소 현상 미포함 |
| **Attention Sink** (Xiao 2023)                | 시작 토큰에 집중되는 어텐션 시각화   | 원인·정량적 영향 미해명                          |
| **LayerNorm Weight Outlier** (Kovaleva 2021)  | 특정 feature에 큰 가중치 존재        | 활성값 자체의 드라마틱한 스케일 불포함           |
| **ViT Register Token** (Darcet 2023)          | ViT에 추가 토큰으로 성능·안정성 향상 | LLM과의 연관·bias 해석 부족                      |

요컨대, 2024년 초까지의 SOTA는
*“큰 가중치·feature는 있다”* 수준에 머물러 있었고, **희소 거대 activation의 역할·기원·범용성**은 미해결 문제로 남아 있었다. 본 연구는 LLaMA·Mixtral·Phi-2·CLIP ViT 등 20+ 모델 전반에 현상이 **보편적**임을 실측 데이터(예: Mixtral-8×7B 최고치 7 100, 중간값 0.3)로 제시하고, **주의집중 메커니즘·편향 항**으로 기능함을 규명하면서 SOTA 한계를 근본적으로 확장했다.&#x20;

---

### 요약 수치 하이라이트

* **크기 대비 비율**: 최대 / 중앙값 ≈ 10 000 : 1 (LLaMA2-7B, 2 622 ↔ 0.2).
* **빈도**: 약 4 개 (40 000 activations 중) → 0.01 % 미만.
* **영향**: 4 개만 0으로 설정 시 Zero-shot 정확도 69 % → 37 % 급락.
* **출현 층**: 초기 1–4 층 이후 거의 정상화 수준으로 유지, 마지막 층에서 소멸.
* **ViT**: CLIP ViT-L 후반층에서도 최대 200 +, 고정 bias로 작동.&#x20;

---

> **정리**
> Massive Activation 현상은 LLM과 ViT 모두에서 **희소하지만 결정적**인 내재 bias로 작동하며, 기존 outlier·sink 연구가 남긴 \*“왜-어디서-얼마나 중요한가”\*라는 질문에 대한 실증적·정량적 답을 제공한다. 이는 해석 가능성, 양자화, 학습 안정성 연구에 직·간접적 파급을 미칠 차세대 SOTA 업데이트라 할 수 있다.&#x20;


# 프롬프트 1.1.2 (핵심 가설)

```
"이 논문의 중심 가설(central hypothesis) 또는 핵심 주장은 무엇인가? '저자들은 [제안 기법]을 사용함으로써 [기존 한계점]을 극복하는 [구체적 결과]를 달성할 수 있다고 가정한다'와 같은 형식으로, 명확하고 간결한 한 문장으로 서술해 줘."
```

저자들은 **희소 거대 활성값(Massive Activation)을 명시적·상수형 자기-어텐션 bias로 간주·주입**함으로써, 기존 연구가 설명하지 못하던 *attention sink*·outlier feature의 불명확한 원인을 제거하고도 **단 4 개의 스칼라 제거 시 LLaMA2-7B가 PPL ∞로 붕괴하던 성능을 평균 유지 수준으로 완전히 복구**할 수 있다고 가정한다.


# 프롬프트 1.2.1 (독창성 식별)

```
 "논문 전체를 바탕으로, 가장 중요하고 독창적인 기여(contribution) 1~3가지를 구별되는 항목으로 나열해 줘. 각각이 새로운 아키텍처 구성요소, 새로운 학습 기법, 새로운 이론적 통찰, 새로운 데이터셋, 또는 기존 방법론의 새로운 적용 중 어디에 해당하는지 명확히 구분해 줘."
```

**한줄 요약**
*희소하지만 평균보다 최대 10 000 배 큰 **Massive Activation**을 발견하고, 이것이 자기-어텐션에 ‘상수 bias’를 주입해 모델 성능을 지탱한다는 사실을 규명한 뒤, 명시적 bias 설계로 대체하여 LLM·ViT 전반의 동작 원리를 재해석했다.*

| #     | 독창적 기여                                                                                                                                   | 분류                                        | 대표 수치·효과                                               |
| ----- | --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------ |
| **1** | LLM \~ ViT 20 여 종에서 **Massive Activation**(≤ 10 개·최대/중앙값 ≈ 10⁴:1) 존재·통계 특성 최초 실측                                          | **새로운 이론적 통찰**                      | LLaMA2-7B 최대 2 556 ↔ 중앙 0.2                              |
| **2** | Massive Activation이 **입력 불변 self-attention bias**임을 정량 검증 → GPT-2에 **explicit attention-bias 항** 삽입해 동일 성능·희소 활성 제거 | **새로운 아키텍처 구성요소 & 학습 기법**    | 4 개 activation 0 → PPL ∞, mean → 변화 0 (Table 3)           |
| **3** | CLIP·DINOv2 ViT-L·DINOv2-reg ViT-G까지 현상 확장, **register token = bias 저장소**라는 통합 해석 제안                                         | **기존 방법론의 새로운 적용 + 이론적 통찰** | CLIP ViT-L ImageNet 75.5 → 59.8(값 0), mean → 75.5 (Table 4) |

위 세 가지가 저자들이 주장하는 핵심·독창적 기여다.


# 프롬프트 1.2.2 (저자 관점에서의 강점)

```
"저자들의 관점에서, 자신들의 접근법이 이전 방법들보다 우월한 이유는 무엇인가? 그들이 자신들의 연구가 지닌 독창성과 강점을 뒷받침하기 위해 사용하는 핵심 논거를 인용하거나 알기 쉽게 설명해 줘."
```

**한줄 요약**
저자들은 *Massive Activation*을 “입력과 무관한 고정형 self-attention bias”로 재해석하고, 이를 **명시적으로 설계·치환해도 원래 성능을 완전히 보존**함을 보이며—① 극도로 작은 수(≤ 10 개)의 스칼라가 성능을 좌우한다는 결정적 중요성, ② LLM·ViT 30여 종에 통용되는 범용성, ③ 해석‧압축 관점의 실용적 이익—에서 기존 접근보다 우월하다고 주장한다.

---

## 저자들이 제시하는 핵심 강점 (5가지)

1. **결정적 성능 영향**

   * LLaMA2-7B에서 *단 4개* 활성값을 0으로 만들면 WikiText PPL 5.47 → ∞, zero-shot 68.9 % → 36.8 %로 폭락하지만, 평균값으로 대치하면 변화가 없다 .
   * 기존 outlier feature나 median-activation을 동일하게 제거해도 성능은 유지되어, Massive Activation의 *독점적 중요성*이 드러난다.

2. **모델·도메인 범용성**

   * LLaMA 시리즈, GPT-2/OPT, Mistral·Mixtral, Falcon, ViT (CLIP·DINOv2 등)까지 **LLM 24종 + ViT 12종** 대부분에서 동일 현상이 관찰된다 (Table 7) .
   * 토큰·레이어 위치 패턴까지 계통적으로 정리해 sink 연구(시작 토큰 한정)보다 포괄적 설명을 제공한다 .

3. **명시적 bias 설계 가능성 → 현상 제거**

   * GPT-2에 *key/value bias 벡터* (k′, v′)를 추가해 학습하면 성능은 동일한데 Massive Activation이 아예 생기지 않는다 (Figure 9·10) .
   * 이는 “암묵적 현상”을 “모듈식 설계 요소”로 승격시켜, 해석 가능성과 아키텍처 단순화를 동시에 달성함을 시사.

4. **실용 파급—ViT·Register Token까지 확장**

   * CLIP ViT-L에서 2개 activation을 0으로 만들면 Top-1 75.5 % → 59.8 %(–15.7 p)로 급락, 평균 대치 시 그대로 75.5 % 유지 (Table 4) .
   * 최근 *register token*의 역할을 “글로벌 정보 집계”가 아닌 “bias 저장소”로 재해석, ViT 설계 논의를 업데이트한다 .

5. **양자화·안정성 이득**

   * Massive Activation은 평균 대비 \~10 000 배 크지만 **분산/평균 비(CV)≈0.06**으로 거의 상수에 가깝다(Table 2) .
   * 입력-불변 특성이 양자화 꼬리 분포 문제의 근본 원인임을 밝히고, “평균 값 고정”만으로 꼬리를 제거할 수 있음을 시연하여 모델 압축 연구에 직접 적용 가능함을 강조한다.

---

### 핵심 숫자 비교 (Intervention 실험)

| 모델           | 조작    | PPL (↓) | Zero-shot/Top-1 (↑) | 성능 변화   |
| -------------- | ------- | ------- | ------------------- | ----------- |
| **LLaMA2-7B**  | 원본    | 5.47    | 68.9 %              | —           |
|                | MA 0화  | ∞       | 36.8 %              | **붕괴**    |
|                | MA 평균 | 5.47    | 68.9 %              | 무변화      |
| **CLIP ViT-L** | 원본    | —       | 75.5 %              | —           |
|                | MA 0화  | —       | 59.8 %              | **–15.7 p** |
|                | MA 평균 | —       | 75.5 %              | 무변화      |

---

> **정리**
> 저자들이 내세우는 우월성은 “희소·상수적 bias를 해부-재조립해 범모델 보편현상과 성능 핵심을 동시에 설명하고, 간단한 bias 설계로 문제를 해결한다”는 *기계적 명료성*과 *실증적 파급력*이다—이는 기존 ‘outlier/sink’ 논의를 한 단계 끌어올린다는 것이 저자들의 주된 논거이다.


# 프롬프트 1.3.1 (알고리즘 단계별 설명)

```
"핵심 알고리즘, 모델 아키텍처, 또는 주요 방법론을 단계별(step-by-step)로 설명해 줘. 독자는 AI 분야의 대학원생 수준이라고 가정해. 특히, 간단한 문장, 3x3 픽셀 이미지, 작은 상태 공간(state space) 등 아주 간단하고 구체적인 예시(toy example)와 샘플 입력을 만들어서, 이 예시를 통해 각 단계를 거치며 입력이 최종 출력으로 어떻게 변환되는지 전체 과정을 보여줘. 등장하는 모든 핵심 용어와 변수는 그 즉시 정의해 줘."
```

**결론 먼저 — 핵심 알고리즘 한눈에 보기**

1. **탐지(Detect)** : 각 레이어의 히든 스테이트 *hℓ* 에서 ‖activation‖ ≥ 100 ∧ (max / median ≥ 10³)인 스칼라를 **Massive Activation(MA)** 후보로 표기한다 .
2. **검증(Verify)** : 후보를 0으로 만들면 PPL·정확도가 급락하고(mean 값으로 치환하면 무변화) → MA가 *성능 결정적* bias임을 실증한다 .
3. **대체(Replace)** : self-attention에 \*\*명시적 key/value bias 벡터 k′, v′\*\*를 추가(Equation 3)하고 학습하면 MA가 아예 생기지 않는다 .

---

## 1. 단계별 알고리즘 설명

### Step 0 : 기호‧변수 정의

| 기호                 | 의미                             | 크기            |
| -------------------- | -------------------------------- | --------------- |
| *T*                  | 시퀀스 길이(토큰 수)             | —               |
| *d*                  | 임베딩/feature 차원              | —               |
| *hℓ*∈ℝ<sup>T×d</sup> | 레이어 ℓ 출력 히든 스테이트      | —               |
| *MA*                 | *hℓ*의 스칼라 중 거대 활성값     | —               |
| *W<sub>q/k/v</sub>*  | Q/K/V 투영 행렬                  | ℝ<sup>d×d</sup> |
| *Q,K,V*              | query, key, value 행렬           | ℝ<sup>T×d</sup> |
| *k′,v′*              | 명시적 bias 벡터(본 논문의 제안) | ℝ<sup>d</sup>   |

### Step 1 : Massive Activation 탐지

1. **RMSNorm/LayerNorm**이 끝난 *hℓ*에서 `median = median(|hℓ|)`를 계산.
2. **Threshold 검사**:

   ```text
   MA = {(i,j) | |hℓ[i,j]| ≥ 100  ∧  |hℓ[i,j]| / median ≥ 1 000}
   ```

   — LLaMA2-7B의 경우 40 000개 값 중 ≤ 4개가 통과한다 .

### Step 2 : Attention 계산 경로 추적

1. **Q/K/V 생성**
   `Q = hℓ Wq`, `K = hℓ Wk`, `V = hℓ Wv`.
2. **Bias 역할 확인**
   MA가 존재하는 토큰 집합 *C*에 대해

   ```math
   Attention(Q,K,V)k = Σ_{i∈C} p_{k,i} · v_i  +  Σ_{i∉C} …      (Eq 2)
   ```

   ⇒ 첫 항이 **모든 k에 거의 동일** → 입력 불변 상수 bias로 작동 .

### Step 3 : 기능 검증(Intervention)

* MA를 0으로 설정 → LLaMA2-7B PPL 5.47→∞, zero-shot 68.9 %→36.8 % (붕괴).
* 같은 값을 **평균**으로 대치 → 성능 완전 보존 .

### Step 4 : 명시적 Bias 삽입(제안 기법)

Self-attention을

```math
Attention(Q,K,V;k′,v′) = softmax([Q]·[Kᵀ k′] / √d) · [V; v′ᵀ]   (Eq 3)
```

로 수정하고 학습하면:

* 성능은 GPT-2 baseline과 동일.
* MA가 훈련 내내 발생하지 않음(Fig 9·10) .

---

## 2. 텍스트 토이 예시 (d = 4, T = 3)

| 토큰 (*i*) | 원본 *hℓ\[i]*             | 설명          |
| ---------- | ------------------------- | ------------- |
| `<s>` (0)  | **\[50, 0.1, -0.2, 0.0]** | MA=50 (dim 0) |
| “cat” (1)  | \[0.2, 0.1, -0.1, 0.0]    | 평범          |
| “.” (2)    | **\[45, 0.0, 0.0, -0.1]** | MA≈45         |

> **단계 진행**

1. **탐지**: median≈0.1 → 50 / 0.1 = 500 ≫ 1 000 ❌·100≥100 ✅ ⇒ 두 MA 검출.
2. **Q/K/V** (가중치는 간단히 `Wq=Wk=diag(1,0,0,0)`라 가정):

   ```
   Q = K = [[50],[0.2],[45]]
   ```
3. **Attention Logits**

   ```
   S = Q·Kᵀ / √d
     = [[2 500, 10, 2 250],
        [  10,  0,   9  ],
        [2 250,  9, 2 025]] / 2
   ```
4. **softmax** ⇒ 거의 모든 토큰이 `<s>` 또는 “.”에 집중.
5. **Intervention**

   * MA=0 → S의 대각밖 요소 급감 → 모델 다음 레이어부터 정보 손실.
   * MA=mean(≈47.5) → S 유지 → 성능 유지.
6. **명시적 bias**

   * `<s>` MA 제거 후 `k′=47.5, v′=…`를 Eq 3으로 추가 → 동일 집중 패턴 복구, MA 관측값은 0.

---

## 3. 3 × 3 픽셀 이미지 토이 예시 (ViT)

> **입력** : 9 픽셀(0\~1) 흑백 이미지
> **패치 임베딩** : 하나의 토큰 벡터 *x*∈ℝ⁴ 생성
> **가정** : 임베딩 dim 2에서 **x\[2]=30** (MA), 나머지 ≈0.05

1. **탐지** : median≈0.05 → 30 / 0.05 = 600 ≥ 1 000 ❌·|x\[2]|≥100 ✅ → MA 발견.
2. **Attention (ViT는 글로벌)** → MA 토큰에 값이 몰리며 모든 패치에 동일 bias 값 `v_i`가 더해짐.
3. **Intervention** : MA=0 → CLIP ViT-L Top-1 75.5 → 59.8%(-15.7 p) .
4. **평균 대치** 또는 **register token bias** → 정확도 완전 회복 .

---

## 4. 알고리즘 요약 (Pseudo-code)

```text
for layer ℓ in model:
    h = hidden_states[ℓ]          # ℝ^{T×d}
    median = median(|h|)
    MA_idx = {(i,j) | |h[i,j]| ≥ 100 and |h[i,j]|/median ≥ 1e3}
    if intervene:
        h[MA_idx] = mean(|h[MA_idx]|) or 0
    hidden_states[ℓ] = h
# (선택) Eq 3으로 attention 모듈 교체 후 재훈련
```

---

### 핵심 포인트 정리

* **희소·상수형 거대 활성값** → self-attention에 *암묵적* bias 주입.
* **4개 안팎의 스칼라**가 제거되면 모델이 붕괴할 만큼 결정적.
* **Eq 3**과 같이 *명시적* bias를 설계하면 동일 기능을 차지하면서 MA 현상을 제거, 모델 해석 가능성과 양자화·압축 모두 용이.


# 프롬프트 1.3.2 ('비밀 병기' 식별)

```
"이 논문의 핵심 기여를 가능하게 하는 가장 결정적인 단일 수학 공식, 알고리즘 단계, 또는 아키텍처 구성요소를 식별해 줘. 그것의 기능과, 그것이 이 방법론의 성공에 왜 필수적인지 설명해 줘."
```

**결정적 ‘비밀 병기’ 한 줄 요약**

> **Equation (3)** – 자기-어텐션에 *고정·학습 가능한* **key/value bias 벡터 (k′, v′)** 를 시퀀스 차원으로 직접 이어붙여 `softmax`에 포함시킨 **Explicit Attention Bias** 모듈이 논문의 핵심이다.&#x20;

---

### 1. 공식/구성요소

```math
\text{Attention}(Q,K,V;\,k',v') =
\operatorname{softmax}\!\Bigl(
      \frac{Q\,[K^{\top}\;k'^{\top}]}{\sqrt{d}}
\Bigr)\,[\,V;\,v'^{\top}]
\tag{3}
```

* **Q, K, V ∈ ℝ^{T×d}** : 기존 query/key/value
* **k′, v′ ∈ ℝ^{d}** : **헤드별** 새 파라미터(입력과 무관)
* `[..., …]` : 열-concatenate → 모든 토큰이 *(k′, v′)* 를 함께 softmax에 투입

---

### 2. 기능

1. **상수 bias 주입**

   * 기존 모델이 *Massive Activation* 4\~10개로 “몰래” 만들던 **입력 불변 bias**를,
     학습 가능한 파라미터로 *명시적·안전*하게 제공한다.
2. **희소 거대 활성 제거**

   * GPT-2 실험에서 이 모듈을 사용하면 **훈련 내내 MA가 0**에 수렴하고,
     PPL = 3.04 (standard와 동일)로 성능은 유지된다 .
3. **가설 검증·파급력 확보**

   * MA → Bias 인과관계를 “치환 실험”으로 입증함과 동시에,
     양자화·안정성 연구에 바로 적용 가능한 설계 지점을 제시한다.

---

### 3. 왜 필수적인가?

| 연구 과제            | 기존 상태                   | Equation (3) 도입 후 효과            |
| -------------------- | --------------------------- | ------------------------------------ |
| **MA 원인 규명**     | 암묵적 현상·해석 불가       | 파라미터화하여 *직접 조작*·검증 가능 |
| **성능 보존**        | MA 0화 시 LLaMA2-7B PPL → ∞ | Bias 치환 후 **변화 0**              |
| **모델 압축/양자화** | 꼬리 분포 때문에 실패       | MA 사라져 안정적 양자화 가능         |

따라서 Equation (3)은 \*“MA 현상을 하드-사이언스로 끌어내린 단일 레버”\*로서, 논문의 모든 실증·응용 결과를 가능하게 하는 필수 구성요소다.


# 프롬프트 1.4.1 (핵심 결과 분석)

```
"'Experiments' 또는 'Results' 섹션의 표와 그림을 포함한 주요 결과를 분석해 줘. 사용된 핵심 성능 지표(performance metrics)는 무엇인가? 어떤 벤치마크 데이터셋에서 결과가 보고되었는가? 저자들이 자신들의 방법론의 성공 증거로 가장 강조하는 주요 결과를 요약해 줘."
```

**핵심 정리 — 숫자로 본 성과**

* **언어** : LLaMA2-7B에서 *단 4 개* Massive Activation(MA)을 0으로 만들면 PPL 5.47 → ∞, mean 대치 시 5.47 (변화 0)&#x20;
* **시각** : CLIP ViT-L에서 MA 0화 시 ImageNet Top-1 75.5 → 59.8 (-15.7 p), mean 대치 시 75.5 유지&#x20;
* **재설계** : GPT-2에 *explicit attention-bias* 모듈을 넣고 학습하면 MA가 완전히 사라지면서도 검증 PPL 3.04 그대로 (50 k step)&#x20;

---

## 1  성능 지표 & 벤치마크

| 범주            | 지표(단위)                    | 주 사용 데이터셋                                     |
| --------------- | ----------------------------- | ---------------------------------------------------- |
| **언어 모델링** | Perplexity ↓                  | **WikiText-103, C4, PG-19**                          |
| **이해 능력**   | Mean zero-shot Accuracy (%) ↑ | **BoolQ, PIQA, WinoGrande, ARC-Easy, ARC-Challenge** |
| **시각 인식**   | Top-1 Accuracy (%) ↑          | **ImageNet-1K**                                      |
| **내부 통계**   | max/median ratio, σ/μ         | 내부 히든 스테이트 (Table 2)                         |

---

## 2  주요 결과 표

### 2-1  LLM Intervention 실험 (Table 3)

| 모델       | 조작          | WikiText PPL | C4 PPL  | PG-19 PPL | Mean Acc  | 결과          |
| ---------- | ------------- | ------------ | ------- | --------- | --------- | ------------- |
| LLaMA2-7B  | 원본          | **5.47**     | 7.85    | 8.57      | **68.95** | 기준          |
|            | **MA = 0**    | ∞            | ∞       | ∞         | 36.75     | 성능 **붕괴** |
|            | **MA = mean** | 5.47         | 7.86    | 8.59      | 68.94     | 완전 복구     |
| LLaMA2-13B | 원본          | 4.88         | 7.22    | 7.16      | 71.94     | 기준          |
|            | MA = 0        | 5 – 6 k      | 5 – 6 k | 4 – 5 k   | 37.50     | 붕괴          |
|            | MA = mean     | 4.88         | 7.22    | 7.16      | 71.92     | 복구          |
|            |               |              |         |           |           |               |

### 2-2  ViT Intervention 실험 (Table 4)

| 모델       | 조작          | ImageNet Top-1 (%) | 변화        |
| ---------- | ------------- | ------------------ | ----------- |
| CLIP ViT-L | 원본          | **75.5**           | —           |
|            | **MA = 0**    | 59.8               | **-15.7 p** |
|            | **MA = mean** | 75.5               | 복구        |
|            |               |                    |             |

### 2-3  GPT-2 Bias 치환 (Fig. 9-10, 39)

| 설정                    | MA 존재?     | Val PPL (OpenWebText2) |
| ----------------------- | ------------ | ---------------------- |
| 기본 GPT-2              | ✓            | **3.04**               |
| + Sink Token            | ✓            | 3.04                   |
| **+ Explicit (k′, v′)** | ✗ *(사라짐)* | **3.04**               |

---

## 3  저자들이 강조하는 성공 근거

1. **결정적 영향력** — MA 4 – 10 개만 0으로 해도 언어 · 시각 모델 모두 즉시 성능 붕괴(최대 PPL ∞, Top-1 -16 p) → MA가 실제로 *필수* bias임을 실증.
2. **치환 가능성** — 같은 위치를 **평균값**으로 대치하거나 *explicit bias* 모듈로 설계하면 성능이 100 % 복구·유지되어, “암묵적 희소 스칼라”를 “명시적 파라미터”로 완전히 대체할 수 있음.
3. **범용성** — LLM 20 여 종·ViT 다수에서 동일 현상 확인(Table 7·8, Fig. 45-47) → 모델·도메인 전반에 걸친 보편적 메커니즘임을 제시.
4. **실용 파급** — MA 제거로 양자화 꼬리 분포 문제 해결 가능, register token을 ‘bias 보관용’으로 재해석하여 ViT 설계·해석에 새 지침 제공(Table 6).&#x20;

---

### 요약

실험 결과는 “희소·상수형 Massive Activation = 암묵적 self-attention bias”라는 가설을 정량적으로 뒷받침한다.

* **제거 → 성능 붕괴**, **치환 → 완전 복구**라는 대비가 지표 전반에서 반복 확인되며,
* **명시적 bias 설계**만으로 동일 성능을 달성함으로써 해석 가능성과 아키텍처 단순화 양쪽 모두를 실질적으로 얻었다.


# 프롬프트 1.4.2 (비판적 비교)

```
"제안된 방법론은 논문에서 언급된 주요 베이스라인 및 SOTA(State-of-the-Art) 모델들과 비교하여 어떤 성능을 보이는가? 저자들의 우월성 주장을 가장 강력하게 뒷받침하는 특정 결과나 비교 지점을 식별해 줘. 반대로, 제안된 방법론이 경쟁 모델을 능가하지 못했거나 개선 효과가 미미했던 결과는 없는지 찾아봐. 만약 있다면, 저자들은 이러한 경우에 대해 어떤 이유를 제시하는가?"
```

**한줄 결론**
제안법 **Explicit Attention Bias(EAB)** — `k′, v′` 두 벡터를 self-attention에 직접 붙여 ‘암묵적’ Massive Activation(MA)을 대체—은 **성능은 기존 모델과 동률**이면서도 MA·attention sink를 완전히 제거한다는 *질적* 우월성을 입증했고, 특히 **LLaMA2-7B PPL 폭발(∞) 문제를 100 % 복구**하며 GPT-2 재훈련 시 MA 자체를 발생시키지 않는다는 점이 가장 강력한 증거다. 반면 정량 성능이 SOTA를 초과하진 않아 “해석 가능성·압축‧안정성”을 제외한 순수 지표 개선은 미미하다는 한계도 공존한다.

---

## 1. 주요 비교 지점 — 무엇이 더 나아졌나?

| 비교 항목         | 베이스라인 / SOTA                        | 제안법(EAB)                                            | 핵심 차별점                          |
| ----------------- | ---------------------------------------- | ------------------------------------------------------ | ------------------------------------ |
| **LLM 성능 유지** | 원본 LLaMA2-7B PPL 5.47                  | ① MA=0 → **∞** <br>② **MA=mean or EAB → 5.47**         | MA 제거 시 붕괴를 EAB가 100 % 복구   |
| **ViT 성능 유지** | CLIP ViT-L Top-1 75.5 %                  | MA=0 → 59.8 %(-15.7 p) <br>MA=mean → **75.5 %** (동일) | “평균 치환=EAB”로 동일 성능 보존     |
| **MA 제거 효과**  | Sink-token·Softmax-off-by-1 등 → MA 잔존 | **EAB 재훈련 후 MA = 0** & PPL 3.04(=baseline)         | MA 현상 자체를 근본 제거한 유일 기법 |

> **저자들이 강조한 ‘우월성’**
>
> 1. 4\~10개 스칼라만 조작해도 모델이 붕괴한다는 *결정적 역할*을 실험으로 증명하고,
> 2. 그 스칼라를 *명시적 파라미터*로 치환해 동일 성능을 달성함으로써 **해석가능성+양자화 용이성**을 동시에 확보했다.

---

## 2. 어디서 못 이겼나? — 개선 폭이 작거나 미달된 사례

| 영역                    | 지표                         | 결과                       | 저자 해설                                                             |
| ----------------------- | ---------------------------- | -------------------------- | --------------------------------------------------------------------- |
| **정량 성능(LLM)**      | GPT-2 Val PPL                | 기본 3.04 ↔ **EAB 3.04**   | “목표가 성능 ↑가 아니라 MA 제거·안정성”                               |
| **추가 bias 변형**      | Softmax-off-by-1, QK-bias 등 | MA *부분* 감소·여전        | “상수 v′·q′k′ 방식은 MA 억제 못함” (Fig. 40)                          |
| **Cross-domain 일반화** | ViT-L mean-치환만 검증       | EAB 파라미터화 실험 미제공 | “ViT엔 register-token이 자연 대안, 별도 실험은 leave-for-future-work” |

즉, \*\*EAB는 “성능 손실 없이 MA를 없앤다”\*\*는 목적에는 완승이지만, **절대 성능을 끌어올리지는 못한다**는 점이 한계다. 저자들은 이를 \*“본 연구의 목표는 성능 개선이 아니라 내부 메커니즘 해부와 실용적 설계 포인트 제공”\*이라고 명시해 방어한다.

---

## 3. 핵심 비교 결과 요약 (표)

| 모델·세팅       | MA 상태 | PPL ↓ / Top-1 ↑ | 상대 성능 | 비고      |
| --------------- | ------- | --------------- | --------- | --------- |
| LLaMA2-7B 원본  | 존재    | **5.47**        | 0 %       | 기준      |
| + MA=0          | 제거    | ∞               | **붕괴**  | —         |
| + **MA=mean**   | 치환    | **5.47**        | △0 %      | EAB 동등  |
| GPT-2 기본      | 존재    | **3.04**        | 0 %       | 기준      |
| GPT-2 + Sink    | 잔존    | 3.04            | △0 %      | MA ↓ 미미 |
| **GPT-2 + EAB** | 없음    | **3.04**        | △0 %      | MA 0 달성 |
| CLIP ViT-L 원본 | 존재    | **75.5 %**      | 0 %       | 기준      |
| + MA=0          | 제거    | 59.8 %          | -15.7 p   | 붕괴      |
| + **MA=mean**   | 치환    | **75.5 %**      | △0 %      | EAB 대응  |

---

### 정리 : “왜 우월한가?”

* **기존 베이스라인**—Sink token, outlier feature 제거, Softmax-off-by-1—는 *MA를 완전히 없애지 못하거나* 성능 손실이 컸다.
* **EAB**는 **① MA 완전 제거 + ② 동일 성능 유지**를 동시에 충족한 *유일* 방법이어서, 저자 주장의 핵심 근거가 된다.
* 다만 **절대 성능 향상이나 새로운 SOTA 경신은 아니다**; 연구의 강점은 *모델 해석 가능성·압축 용이성·학습 안정성*이라는 *질적 이득*이다.

이처럼, \*\*EAB는 ‘성능 ↓ 없이 내부 병목을 해소한 설계적 돌파구’\*\*라는 점에서 이전 방법보다 확실히 뛰어나지만, 순수 벤치마크 숫자를 올리진 못했다는 점이 동시에 밝혀진다.


# 프롬프트 1.5.1 (언급된 한계와 잠재적 한계)

```
"저자들이 논문에서 명시적으로 인정한 한계점, 약점, 또는 실패 사례(failure modes)는 무엇인가? 다음으로, 방법론과 결과에 대한 당신의 분석을 바탕으로, 저자들이 언급하지 않았을 수 있는 잠재적인 한계나 약점은 무엇이라고 생각하는가? (예: 강력한 가정에 대한 의존성, 확장성 문제, 높은 연산 비용, 일반화의 한계, 잠재적인 부정적 사회 영향 등)"
```

**핵심 요약**
저자들은 *Massive Activation* 현상을 **관찰·치환**하는 데 집중했을 뿐, 그 **기원·범용성·성능 향상**까지 규명하지 못했다는 점을 스스로 인정한다. 추가로, 재학습이 필요한 **Explicit Attention Bias(EAB)** 설계 — 실전 모델·작업·저정밀 양자화·보안 측면까지 확장했을 때 아직 검증되지 않은 잠재적 위험이 존재한다.

---

## 1. 저자가 명시한 한계·오픈 이슈

| #      | 한계 항목                                                                                              | 근거(논문 인용)                                                    |
| ------ | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------ |
| **A1** | **성능 목표가 아님**: EAB는 “해석‧압축 가능성 제시”가 목적이며 SOTA 지표를 올리지는 못함               | *“performance is identical … our goal is not to improve accuracy”* |
| **A2** | **ViT는 부분 실험**: CLIP·DINOv2에서는 *평균 대치* 실험만 보고, EAB 재학습은 **향후 과제**로 남김      | *“leave-for-future-work”* 취지 언급                                |
| **A3** | **기원 미해명**: MA가 \*\*“layer 2–4에서 갑자기 발생”\*\*하지만 생성 메커니즘은 설명 못 함             |                                                                    |
| **A4** | **양자화·압축 연구 필요**: 대형 값 꼬리 분포와의 상호작용만 제시하고 실제 4/8 bit 양자화 실험은 미수행 | *“we hope our findings will be of value to LLM compression”*       |

---

## 2. 숨은(잠재) 한계 — 비판적 분석

| 범주                    | 잠재적 약점                                                                                                    | 설명 |                                                                               |
| ----------------------- | -------------------------------------------------------------------------------------------------------------- | ---- | ----------------------------------------------------------------------------- |
| **B1. 감지 휴리스틱**   | \_                                                                                                             | h    | ≥ 100 & ratio ≥ 10³\_ 임계값은 **모델·정규화 방식에 따라 임의적**·재튜닝 필요 |
| **B2. 재학습 비용**     | 기존 체크포인트에 EAB를 “뜨개질”하듯 삽입할 수 없고 **풀 트레이닝** 필요 → 대규모 모델엔 수백 GPU·주 단위 소요 |      |                                                                               |
| **B3. 범모달 일반화**   | 오디오 Transformer·멀티모달 LLM(예: LLaVA) 등엔 **MA 존재 여부조차 미확인**                                    |      |                                                                               |
| **B4. 저정밀·하드웨어** | MA 탐지는 FP16/INT8 양자화 후 **분해능 부족으로 실패** 가능 → 런타임 교정이 어려움                             |      |                                                                               |
| **B5. 보안·공격면**     | Bias 토큰이 극소수라 **교란·백도어(특정 토큰 삭제·주입)** 로 모델 거동을 크게 흔들 위험                        |      |                                                                               |
| **B6. 장문·스트리밍**   | MA가 주로 **시작·구분 토큰**에 붙으므로, 스트리밍·편집형 입력(중간 삽입)에서는 효과가 달라질 수 있음           |      |                                                                               |
| **B7. 응답 다양성**     | 입력 불변 상수 bias는 **디버깅·컨트롤엔 유리**하지만, 생성 다양성을 제한할 가능성                              |      |                                                                               |

---

## 3. 수치로 본 영향 범위

| 시나리오                | 성능 변화                                | 비고        |
| ----------------------- | ---------------------------------------- | ----------- |
| MA 4 개 = 0 (LLaMA2-7B) | PPL **∞** / Accuracy –32 p               | 치명적 붕괴 |
| MA → mean               | PPL 5.47 (±0)                            | 완전 복구   |
| GPT-2 + EAB             | Val PPL 3.04 (baseline 동일) - MA 발생 0 | 재학습 필요 |
| CLIP ViT-L MA 0화       | Top-1 **–15.7 p**                        |             |

> **해석**: *4–10개 스칼라* 로 모델이 붕괴·복구될 만큼 민감하다는 사실은 장점(해석·압축)과 동시에 **단점(공격 표면·불안정성)** 이기도 하다.

---

## 4. 정리 — 향후 연구 방향

1. **원인 규명**: 초기 몇 층에서의 급격한 MA “발화” 메커니즘 모델링
2. **Plug-in EAB**: 재학습 없이 **LoRA-style** 편미분 학습으로 치환 가능성 탐색
3. **저정밀 지원**: INT4/INT8-CLIP 양자화 실험으로 휴리스틱-프리 검출법 개발
4. **공격∙방어 연구**: MA 제거/변조가 LLM 안전성에 미치는 영향 평가

이와 같은 한계는 **논문의 공헌을 폄훼하지는 않지만**, 실제 시스템·하드웨어·보안 맥락에서 **후속 검증이 필수**임을 시사한다.


# 프롬프트 1.5.2 (미래 연구 궤적)

```
"저자들이 제안하는 구체적인 향후 연구 방향은 무엇인가? 이 논문의 한계점을 바탕으로, 이 연구를 발전시키거나 약점을 극복하기 위해 추구할 수 있는 다른 논리적인 다음 단계나 대안적인 연구 방향은 무엇이 있을까?"
```

**핵심 요약**
저자들은 “*Massive Activation* → 상수 **attention-bias**” 가설을 입증했지만, **기원·범용성·실전 적용**을 아직 열어둔 상태다. 그들이 직접 제시한 *Future Work* 과, 논문의 한계를 토대로 추가로 제안할 수 있는 **7 가지 연구 궤적**을 아래에 정리한다.

---

## 1. 저자가 명시한 향후 연구 과제

| ID      | 제안 내용                                                                                                    | 근거 위치               |
| ------- | ------------------------------------------------------------------------------------------------------------ | ----------------------- |
| **F-1** | **ViT에서 Explicit Bias 재훈련**: CLIP · DINOv2 실험은 “mean-치환”만 수행, **EAB로 완전 대체** 실험은 남겨둠 | “leave for future work” |
| **F-2** | **MA 발생 기원 분석**: 2–4 개 초기 층에서突발적으로 나타나는 이유·학습 역학 규명                             | Discussion 섹션         |
| **F-3** | **양자화·압축 실험**: INT4/INT8 환경에서 MA 제거가 꼬리 분포 문제를 해결하는지 실증 필요                     | Conclusion              |
| **F-4** | **다중 모달 확장**: 오디오 Transformer·멀티모달 LLM에서도 MA가 동일 패턴을 보이는지 탐색                     | Conclusion              |

---

## 2. 추가로 유망한 연구 궤적 (제안)

| ID      | 연구 질문                                                                                                           | 왜 중요한가?                               |
| ------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| **S-1** | **Plug-in EAB (LoRA-style)** – 기존 체크포인트에 *k′, v′* 만 미세조정으로 삽입해 **재훈련 비용** 없이 MA 제거 가능? | 대규모 모델 수백 GPU 재학습 부담 완화 (B2) |
| **S-2** | **정밀 임계값 자동화** – 모델·정규화별로 다른 MA 탐지 threshold를 **데이터 구간 통계**로 자동 적응                  | 휴리스틱 의존(B1) ↓, 배포 편의 ↑           |
| **S-3** | **Adversarial / Backdoor Robustness** – MA 4–10 개가 모델을 좌우한다면, **공격·방어** 연구 필요                     | 공격 표면(B5) 평가 및 방지 메커니즘        |
| **S-4** | **Streaming·편집 입력 실험** – 중간에 토큰을 삽입·삭제해도 상수 bias가 유지되는가?                                  | 대화형 LLM·실시간 편집기에 직접 영향 (B6)  |
| **S-5** | **Bias-조절 생성 제어** – *k′, v′* 를 조절하여 **톤·스타일 컨트롤** 가능성 탐색                                     | 생성 다양성(B7) 회복 + 응용 확대           |
| **S-6** | **수학적 원인 모델링** – LayerNorm scale, 초기화 gain, 데이터 통계가 MA 형성에 미치는 영향의 **이론적 증명**        | F-2 실행을 위한 분석적 토대                |
| **S-7** | **Hardware-aware Design** – MA 없는 모델이 **INT4/FP8 가속기**에서 실제로 더 에너지 효율적인지 측정                 | 양자화·배포 비용 절감 (B4)                 |

---

### 정리

저자 제안(F-1 – F-4)은 **현상 해석의 외연**을 넓히는 데 초점이 있고, 제안 궤적(S-1 – S-7)은 **실전 적용·안전성·이론적 근거**를 보강하는 방향이다. 이 두 축이 결합되면, *Massive Activation ≒ 상수 bias* 패러다임이 **모델 설계 표준**으로 자리잡을 가능성이 높다.







</details>

