---
categories:
- paper-review
- with-gpt
date: "2024-10-31"
tags: null
title: Keyformer KV Cache Reduction through Key Tokens Selection for Efficient Generative
  Inference
---

논문 : https://arxiv.org/abs/2403.09054

개인의견 : 예전부터 KV cache중에 핵심이 되는 내용만 잘 선택하면 오버헤드를 줄일수 있지 않을까라고 생각했는데 핵심 토큰만 가지고 이러한 방법을 구현한 논문입니다.
이러한 논문을 볼때 마다 항상 과연 정확도에 얼마나 영향을 줄까라는 의심이 생길수 밖에 없지만 이렇게 알고리즘쪽에서 경량화를 많이 해주면 좋겠네요

아래글은 Scholar GPT와 대화를 나눈 내용입니다.

# Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점

# Answer : 
### 논문의 주요 내용, 강점, 독창성, 알고리즘 및 한계점

논문 **"Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference"**는 대규모 언어 모델의 생성 추론 효율성을 개선하기 위해 **핵심 토큰을 선택하여 KV Cache(키-값 캐시)를 줄이는 Keyformer 기법**을 제안하고 있습니다.

#### 강점 및 독창성

1. **KV Cache 최적화**: Keyformer는 KV Cache의 크기를 줄여 메모리 사용량을 최적화합니다. 대부분의 생성 추론 단계에서 **90% 이상의 주의(attention) 가중치가 일부 특정 토큰(핵심 토큰)에 집중**됨을 이용하여 불필요한 토큰을 캐시에서 제외하고 중요한 토큰만 남깁니다.
   
2. **효율성 증가**: 이를 통해 KV Cache의 크기와 메모리 대역폭을 줄여도 정확도를 유지하면서 **추론 속도를 2.1배, 토큰 생성 처리량을 2.4배 향상**시킬 수 있습니다. 이는 특히 긴 문맥의 요약이나 대화형 생성 작업에서 효과적입니다.

3. **정확도 유지**: Keyformer는 기존 방법들에 비해 성능 손실을 최소화하면서도 효율을 높이는 독창적인 방식으로, 다른 모델들과 비교해 정확도 저하가 거의 없이 성능을 유지합니다.

#### 핵심 알고리즘 설명 (예시 포함)

Keyformer는 다음과 같은 과정을 통해 핵심 토큰을 식별하고 KV Cache 크기를 줄입니다.

1. **주의 가중치 계산 및 핵심 토큰 선택**: 모델이 입력된 텍스트에서 주의 가중치가 높은 핵심 토큰을 선택합니다. 예를 들어, 문장이 "The quick brown fox jumps over the lazy dog"이라면 'fox', 'jumps', 'dog' 같은 중요한 키워드가 핵심 토큰으로 선택될 수 있습니다.

2. **Gumbel 분포를 활용한 로짓 정규화**: 선택된 핵심 토큰이 특정 관심 영역에 집중되도록 Gumbel 분포를 사용하여 주의 가중치를 조정합니다. 이는 특정 토큰들이 다른 토큰들보다 중요하게 다뤄지도록 도와줍니다.

3. **온도 매개변수 사용**: 온도 매개변수 τ를 활용해 분포의 균일성을 제어하고, 각 토큰의 중요성을 조절하여 캐시 공간에서 중요한 토큰들이 유지될 확률을 높입니다.

4. **감소된 KV Cache 적용**: 핵심 토큰과 최신 토큰(예: 최근 5개의 단어)을 KV Cache에 유지하고, 불필요한 토큰은 캐시에서 제거하여 메모리 사용을 줄입니다. 예를 들어, 길이 2048의 입력에서 50%의 KV Cache를 줄일 경우, 핵심 토큰과 최신 토큰만 유지하여 나머지는 캐시에서 제외합니다.

#### 한계점

1. **주의 가중치 편향**: 핵심 토큰을 선택하는 데 있어 주의 가중치의 편향이 발생할 수 있습니다. 특히 긴 문맥에서 초기 토큰에 과도한 중요성이 부여될 가능성이 있어 모든 문맥을 완전히 이해하는 데 한계가 있을 수 있습니다.

2. **정규화로 인한 계산 복잡도 증가**: Gumbel 분포를 활용한 로짓 정규화 과정은 추가적인 계산을 요구하며, 이로 인해 일부 성능 저하가 발생할 수 있습니다.

3. **일부 문맥에서 정확도 손실 가능성**: KV Cache 크기를 줄임으로써 일부 경우에서는 문맥 정보가 손실될 수 있어, 모든 문장 구조나 복잡한 맥락에서 높은 정확도를 유지하지 못할 가능성도 존재합니다.

Keyformer는 대규모 언어 모델의 효율적인 생성 추론을 위해 주의 가중치에 기반한 중요한 토큰을 선택해 캐시를 줄이는 혁신적인 접근법을 제공합니다. 이로 인해 기존의 방법에 비해 높은 효율성과 성능을 유지하면서도 메모리 사용량을 절감하는 성과를 보여줍니다

# Q : Figure 6. The overview of Keyformer 부분을 조금 더 자세하게 설명해줘 그리고 전체적으로는 attention을 attention하는 구조라고 생각되는데 잘 이해한게 맞을까?

# Answer : 

### Figure 6: Keyformer 개요 - 상세 설명

Figure 6은 Keyformer 알고리즘의 전체적인 구조를 보여주는 그림으로, 각 단계별로 Keyformer가 **KV Cache를 효율적으로 관리하여 메모리 사용량을 줄이면서 성능을 높이는 방식**을 설명합니다. 이를 단계별로 자세히 설명하겠습니다:

1. **초기 디코딩 단계 (Decoding Step 1)**
   - Keyformer의 첫 번째 디코딩 단계에서는 **n개의 모든 토큰**을 대상으로 Key 및 Value를 계산하여 KV Cache에 저장합니다. 이 단계는 텍스트 생성의 첫 번째 토큰을 예측하기 위한 준비 단계입니다.
   - 이때, Keyformer는 KV Cache에서 **최근 윈도우(recent window)**와 **핵심 토큰(key tokens)**만 유지하고 나머지 토큰은 제거하여 KV Cache의 크기를 줄이기 위한 구조를 준비합니다.

2. **Gumbel Noise 추가와 핵심 토큰 선택**
   - Keyformer는 특정 토큰이 중요할 가능성을 높이기 위해 Gumbel 분포를 활용하여 노이즈(분포값)를 추가합니다. 이렇게 함으로써, **특정 중요 토큰들이 다른 토큰들보다 선택될 확률을 높이게 됩니다**.
   - Keyformer의 **Score Function (fθ)**는 이 Gumbel 기반의 정규화된 값을 사용하여 토큰의 중요도를 평가하고, **상위 (k-w) 개의 핵심 토큰**을 선정합니다. 이 과정은 선택된 최근 윈도우의 \(w\)개의 토큰과 결합하여 다음 단계에서 사용될 **축소된 KV Cache**를 구성합니다.

3. **토큰 생성 단계에서의 축소된 KV Cache 활용 (Decoding Step 2)**
   - 다음 디코딩 단계에서는 첫 번째 단계에서 만들어진 **축소된 KV Cache**를 사용합니다. 첫 번째 생성된 토큰은 KV Cache에 있는 k개의 토큰에만 주의를 집중하여 효율성을 높입니다.
   - 각 디코딩 단계에서 최근 윈도우는 한 칸씩 오른쪽으로 이동하며, 그 과정에서 Score Function (fθ)가 업데이트되어 새로운 핵심 토큰을 식별하고, **필요한 경우 최근 윈도우와 핵심 토큰의 조합으로 KV Cache를 업데이트**합니다.
   - 이 단계에서는 온도 파라미터 \(τ\)가 한 단계씩 증가하면서 점차적으로 확률 분포를 부드럽게 조정합니다. 이는 텍스트 생성이 진행됨에 따라 캐시에 남아있는 토큰들이 보다 **일관성 있는 확률 분포를 가지도록 하기 위함**입니다.

4. **Keyformer의 지속적 Score Function 축적 및 토큰 유지**
   - Keyformer는 각 디코딩 단계의 Score Function을 축적하여, **이전에 생성된 토큰과의 일관성을 바탕으로 핵심 토큰을 식별**합니다. 이렇게 함으로써, 단순히 현재 토큰과 이전 토큰 간의 관계뿐만 아니라, **생성된 모든 토큰 간의 일관성**을 반영하여 캐시 내에서 중요한 토큰을 유지할 수 있습니다.

### "Attention on Attention" 구조 - 이해가 맞는지

맞습니다. Keyformer는 기존 Attention Mechanism의 구조를 개선하여, 각 토큰이 가지는 중요도에 따라 선택적으로 주의를 집중하는 방식으로 작동합니다. 이를 **"attention을 attention하는 구조"**로 볼 수 있습니다. 특히, 모든 토큰이 아닌 중요한 토큰에만 집중함으로써 **주의 메커니즘 자체의 효율성을 높이는 역할**을 합니다. 

Keyformer는 각 토큰이 얼마나 많은 attention을 받는지를 기반으로 중요도를 계산하고, **이 중요한 토큰들에 대한 캐싱을 최적화**함으로써 **필요 없는 메모리 접근을 줄여 추론 효율성을 극대화**하는 접근 방식입니다.


# Q : section 3 KEYFORMER: INTUITION AND DESIGN 의 전체적인 흐름과 Gumbel Logit Adjustment같은 방법을 사용하는 이유를 예시를 추가해서 자세하게 설명해줘

### Section 3: Keyformer의 직관과 설계 - 개요와 주요 기법 설명

논문의 3장은 Keyformer의 설계 원리와 이를 효과적으로 구현하기 위한 기법을 설명하고 있습니다. 특히, **Keyformer가 왜 특정 토큰만 선택하여 캐싱하고, 이를 어떻게 효율적으로 관리하는지**에 대해 설명하고 있으며, 이 과정에서 Gumbel Logit Adjustment를 포함한 여러 기법들이 사용됩니다.

### 1. Keyformer 설계의 전체적인 흐름

Keyformer는 **긴 문맥을 처리하는 언어 모델의 효율성을 높이기 위해 등장**했습니다. 기본 아이디어는 LLM에서 생성 추론 과정 중 모든 토큰이 동일한 중요도를 가지지 않는다는 점을 활용하는 것입니다. 예를 들어, 긴 문장에서 문맥을 이해하는 데 있어 중요한 정보만 담고 있는 특정 토큰에 주의(attention)가 집중되는 경향이 있습니다. 이를 **핵심 토큰(Key Tokens)**이라고 부르며, Keyformer는 이러한 토큰에만 집중적으로 주의를 두고 KV Cache에 저장하여 효율성을 극대화하려 합니다.

이를 위해, Keyformer는 **이전 단계에서 중요도가 높다고 판단된 핵심 토큰과 최근 생성된 토큰만을 KV Cache에 저장**하고, 나머지 덜 중요한 토큰은 제외하여 메모리 사용을 줄입니다. 이때, 어떤 토큰이 중요한지 결정하는 과정에서 **Gumbel Logit Adjustment** 같은 정규화 기법을 사용해 각 토큰의 중요도를 평가합니다.

### 2. Gumbel Logit Adjustment의 사용 이유와 효과

#### 왜 Gumbel Logit Adjustment를 사용하는가?

1. **주의 가중치 편향 해소**: LLM은 초기 토큰에 대해 높은 주의 가중치를 주는 경향이 있습니다. 예를 들어, **문장의 시작 부분에 있는 단어**가 더 중요한 정보로 평가될 수 있습니다. 그러나 이러한 편향은 후반부의 중요한 정보가 반영되지 않는 문제를 야기할 수 있습니다.
   
2. **최대값을 찾기 위한 분포 특성**: Gumbel 분포는 확률론에서 **최대값을 찾기 위한 분포로 적합**합니다. Keyformer의 목적은 전체 토큰 중 가장 중요한 몇 가지 토큰을 선별하는 것이므로, Gumbel 분포를 활용하면 초기 값에 편향되지 않고 중요한 정보를 가진 토큰들을 선택할 수 있습니다.

#### 예시를 통한 Gumbel Logit Adjustment 설명

다음은 Gumbel Logit Adjustment의 동작 원리를 간단한 예를 통해 설명합니다.

- 예를 들어, "The quick brown fox jumps over the lazy dog"이라는 문장이 있다고 가정합시다.
- 이 문장에서 중요한 정보는 **"fox"**, **"jumps"**, **"dog"**일 수 있습니다. Keyformer는 이 토큰들에 대한 주의 가중치를 높이기 위해 **Gumbel 분포를 사용하여 각 토큰의 중요도를 정규화**합니다.
- 각 토큰이 가지는 주의 가중치를 기반으로 로짓 값을 계산할 때, Gumbel 분포를 사용해 노이즈를 추가하여, 중요도가 높은 토큰들이 최종적으로 높은 점수를 갖도록 합니다.
  
  이를 수식으로 나타내면 다음과 같습니다:
  
  \[
  y_i = x_i + \zeta_i
  \]

  여기서 \( y_i \)는 정규화된 로짓 값이고, \( x_i \)는 각 토큰의 원래 로짓 값이며, \( \zeta_i \)는 Gumbel 분포에서 나온 노이즈 값입니다. 이 방식으로 중요도가 높은 토큰은 더 큰 값을 갖게 되고, Keyformer는 이 정보를 통해 핵심 토큰을 선택합니다.

#### Gumbel Softmax의 추가적인 장점

- **불균일한 확률 분포 보정**: 단순히 모든 토큰에 동일한 확률을 주는 것이 아닌, 중요도가 높은 토큰에게 더 높은 확률을 부여할 수 있도록 **Gumbel Softmax**를 사용합니다.
- **유연한 온도 매개변수 조정**: 온도 파라미터 \( \tau \)를 통해 확률 분포의 경직성을 조정할 수 있습니다. 예를 들어, 낮은 온도일수록 특정 토큰에 주의가 집중되고, 높은 온도일수록 확률 분포가 고르게 퍼져 모든 토큰에 비슷한 주의를 줄 수 있습니다.

### 3. Gumbel Logit Adjustment를 통한 핵심 토큰 선택 과정

1. **점수 함수 계산**: Keyformer는 각 토큰의 **주의 가중치에 Gumbel 분포 기반의 노이즈를 추가**하여, 중요도가 높은 토큰에 더 큰 값이 주어지도록 합니다.
   
2. **온도 파라미터 조정**: 이후 단계마다 온도 파라미터를 점진적으로 증가시키면서, 현재 단계에서는 중요한 토큰에 더 높은 주의를 주고, 이후 단계에서는 보다 많은 토큰에 고르게 주의를 두도록 확률을 조정합니다.
   
3. **최종 핵심 토큰 선별**: 최종적으로, 계산된 점수 함수에 따라 가장 높은 점수를 가진 토큰들이 KV Cache에 남아 중요한 정보로서의 역할을 하게 됩니다.

### 요약
Keyformer는 **주의 메커니즘 내에서 중요한 정보를 갖는 핵심 토큰에 집중**하여 효율적으로 메모리를 사용할 수 있도록 설계되었습니다. Gumbel Logit Adjustment는 **중요한 토큰을 효과적으로 선택**하는 데 필요한 확률 분포를 제공하며, 이를 통해 전체적인 추론 성능을 높이는 데 기여합니다.



# Q: 알고리즘 1 섹션을 자세하게 코드와 예시를 추가해서 설명해주겠니?

알고리즘 1은 Keyformer가 **KV Cache의 크기를 줄이면서도 중요한 토큰을 유지하는 방법**을 구현한 단계별 프로세스를 나타냅니다. 이를 통해 Keyformer는 주어진 문장 내에서 중요도가 높은 토큰을 선택하고, 메모리 사용량을 줄이면서도 정확도를 유지합니다. 아래에서는 이 알고리즘을 코드와 예시를 통해 설명하겠습니다.

### 알고리즘 1: Keyformer의 단계별 프로세스

#### Pseudo-Code 설명

알고리즘의 주요 변수:
- \( k \): KV Cache의 최대 크기 (즉, 저장할 토큰의 수)
- \( w \): 최근 토큰 윈도우의 크기 (즉, 최신 토큰들 중에서 유지할 토큰의 수)
- \( T \): 생성할 텍스트의 길이 (즉, 총 생성할 토큰 수)
- \( \tau_{init} \)과 \( \tau_{end} \): 온도 파라미터의 시작 및 종료 값
- \( S_n \): 프롬프트 시퀀스의 초기 길이

#### 코드 기반 알고리즘 설명

```python
# 알고리즘 1 Keyformer

# 입력 변수 설정
k = KV_cache_size               # KV Cache 크기
w = recent_window               # 최신 윈도우 크기
T = text_generation_length      # 생성할 텍스트 길이
tau_init, tau_end = init_temp, end_temp # 온도 초기값과 종료값
S_n = prompt_sequence_length    ## 프롬프트 시퀀스 길이

# 초기화
f_theta = 0                     # Score Function 초기화
tau = tau_init                  # 온도 매개변수 초기화
delta_tau = (tau_end - tau_init) / T  # 온도의 단계적 증가량 설정
zeta = gumbel_distribution()    # Gumbel 노이즈 초기화

for t in range(T):               # 각 디코딩 단계에서
    tau = tau_init + t * delta_tau  # 온도 매개변수 업데이트
    
    if phase == 'prompt':         ## 프롬프트 처리 단계인 경우
        x_i = compute_logits(Q_i, K_all, S_n)  # 모든 토큰에 대해 로짓 계산
        m = n                      # 전체 시퀀스 길이로 설정
    else:                          # 생성 단계인 경우
        x_i = compute_logits(Q_i, K_selected, S_k)  # KV Cache 내 선택된 토큰들에 대해 로짓 계산
        m = k                      # 선택된 KV Cache 길이로 설정
    
    # Score Function 업데이트
    f_theta[i] += exp((x_i + zeta_i) / tau) / sum(exp((x_j + zeta_j) / tau) for j in range(m))

    # 최근 w개의 토큰 선택
    Sw = select_recent_tokens(w)
    
    # 핵심 토큰 선택
    Skey = select_top_k_tokens(f_theta, k - w)
    
    # 최종 KV Cache 업데이트
    Sk = Skey + Sw                # 핵심 토큰과 최신 토큰을 합쳐 KV Cache 생성
```

### 예시와 함께 단계별로 설명

**문장 예시**: "The quick brown fox jumps over the lazy dog"이라는 문장을 프롬프트로 사용하여 Keyformer가 동작하는 과정을 단계별로 설명하겠습니다.

#### 단계 1: 초기 설정

1. **입력 설정**:
   - KV Cache 크기 \( k = 5 \): 이 예제에서는 최대 5개의 토큰을 캐시에 저장할 수 있다고 가정합니다.
   - 최근 윈도우 크기 \( w = 2 \): 최신 토큰 2개를 무조건 KV Cache에 저장하도록 설정합니다.
   - 텍스트 생성 길이 \( T = 10 \): 10개의 토큰을 생성할 계획입니다.
   - 온도 매개변수 \( \tau_{init} = 1 \), \( \tau_{end} = 2 \): 온도가 1에서 시작하여 점차 증가하도록 합니다.

2. **초기화**:
   - \( f_{\theta} \)는 Score Function의 초기 값으로, 각 토큰의 중요도를 평가하기 위해 사용됩니다.
   - \( \tau \)는 초기 온도 값으로 시작하여 점진적으로 증가합니다.

#### 단계 2: 첫 번째 디코딩 루프 (디코딩 단계 t=0)

1. **프롬프트 처리 단계**:
   - 모든 토큰에 대해 \( x_i = \text{compute_logits}(Q_i, K_{\text{all}}, S_n) \)를 계산하여 로짓 값을 얻습니다. 예를 들어 "The", "quick", "brown" 등에 대한 로짓이 계산됩니다.

2. **Score Function 업데이트**:
   - 각 토큰에 대해 Gumbel 분포를 사용하여 노이즈를 추가한 다음 \( f_{\theta} \)를 업데이트합니다.
   - 예를 들어 "fox"와 "dog"가 높은 점수를 받아 핵심 토큰으로 선정될 수 있습니다.

3. **최근 토큰 및 핵심 토큰 선택**:
   - 최신 토큰 2개 ("lazy", "dog")는 **무조건 KV Cache에 추가**됩니다.
   - 핵심 토큰 후보 중 상위 3개를 선택하여 (예: "fox", "jumps", "quick") KV Cache에 포함합니다.

4. **KV Cache 갱신**:
   - 최종 KV Cache는 **"fox", "jumps", "quick", "lazy", "dog"**으로 갱신됩니다.

#### 단계 3: 생성 단계 (디코딩 단계 t=1부터)

1. **로짓 계산 및 온도 조정**:
   - 디코딩 단계가 진행될수록 \( \tau \)가 증가하여, 새로 추가되는 토큰의 중요도가 더 고르게 분포되도록 유도합니다.
   
2. **Score Function 축적**:
   - Score Function \( f_{\theta} \)는 이전 단계에서 얻은 중요도를 반영하여 각 토큰의 일관성을 평가합니다.

3. **KV Cache 유지 및 업데이트**:
   - 각 생성 단계에서 최신 토큰과 핵심 토큰이 계속해서 업데이트됩니다. 이를 통해 **최적의 메모리 사용량을 유지하면서 중요한 정보를 잃지 않도록 보장**합니다.

### 요약

Keyformer 알고리즘은 Gumbel 분포를 사용하여 각 토큰의 중요도를 평가하고, 가장 중요한 토큰과 최신 토큰을 유지하는 방식으로 **KV Cache의 크기를 줄이는 동시에 모델의 성능을 유지**합니다.


# Q : keyformer의 압축율은 어떻게 결정될 수 있을까? 그리고 압축으로 인하여 정확도가 논문에서 설명한것과 달리 떨어지는 모델 구조 및 벤치마크가 있을까? 

# Answer 

### Keyformer의 압축률 결정 요인

Keyformer의 압축률은 **KV Cache에서 유지할 핵심 토큰과 최신 토큰의 수**에 의해 결정됩니다. 이를 더 구체적으로 설명하자면:

1. **핵심 토큰의 비율 (Key Token Ratio)**:
   - Keyformer는 문맥 이해에 중요한 핵심 토큰을 선정하여 KV Cache에 유지합니다. 예를 들어, 전체 토큰 수가 100이라면 핵심 토큰을 30%로 설정하여 **최대 30개의 토큰만 캐시에 유지**할 수 있습니다. 이는 필요에 따라 10%, 20% 등으로 조정할 수 있으며, 압축률은 선택된 비율에 따라 결정됩니다.
   - 따라서 압축률은 **핵심 토큰의 비율**에 반비례하게 결정됩니다. 즉, 핵심 토큰 비율이 낮을수록 압축률은 높아집니다.

2. **최근 윈도우 크기 (Recent Window Size)**:
   - Keyformer는 최신 정보의 중요성을 반영하기 위해 **최근에 생성된 토큰을 일정 수 유지**합니다. 이 또한 압축률에 영향을 미치는데, 최근 윈도우 크기를 크게 설정하면 KV Cache의 크기가 증가하므로 압축률이 떨어지고, 작게 설정하면 압축률이 높아집니다.

3. **Gumbel Logit Adjustment의 온도 파라미터 (Temperature Parameter)**:
   - 온도 파라미터 \(\tau\)는 토큰 선택의 고르게 분포될 정도를 조절합니다. 이 값이 높아지면 덜 중요한 토큰도 일정 확률로 선택되므로 핵심 토큰의 수가 늘어나게 되고, 결과적으로 **압축률이 낮아질 수 있습니다**.

### 모델 구조 및 벤치마크에 따른 압축의 영향

Keyformer의 KV Cache 압축은 여러 모델과 벤치마크에서 테스트되었으나, 일부 구조에서는 예상보다 정확도가 떨어질 수 있습니다. 압축으로 인한 정확도 저하가 발생할 가능성이 있는 모델 구조와 벤치마크는 다음과 같습니다:

1. **대화형 모델과 추론이 복잡한 벤치마크**:
   - 대화형 언어 모델은 **긴 문맥을 이해하고 다양한 대화 흐름을 추적**해야 하기 때문에, 많은 양의 문맥 정보를 잃을 경우 정확도가 크게 떨어질 수 있습니다. 예를 들어, MPT-7B와 같은 긴 대화 및 추론이 필요한 벤치마크에서 압축률이 지나치게 높으면 대화의 일관성이나 답변의 질이 저하될 수 있습니다.

2. **고도의 시퀀스 의존성을 요구하는 모델 (예: GPT-J, T5 계열)**:
   - 이들 모델은 입력 시퀀스의 모든 토큰에 대한 강한 의존성을 갖고 있어, 일부 중요한 토큰이 사라지면 모델의 성능이 더 크게 저하될 수 있습니다. 특히 요약(summarization)과 같은 **정확한 문맥 파악이 중요한 작업**에서는, 과도한 압축이 정보를 잃게 만들어 성능이 낮아질 수 있습니다.

3. **ROUGE 점수를 중심으로 한 벤치마크 (예: CNN/DailyMail)**:
   - 요약 작업을 다루는 ROUGE 점수 기반 벤치마크에서, Keyformer가 지나치게 많은 토큰을 제거할 경우 정확도가 떨어질 가능성이 높습니다. CNN/DailyMail과 같은 데이터셋은 텍스트 간의 유사성과 일관성이 중요하기 때문에, **50% 이상의 KV Cache 감소**가 정확도 저하로 이어질 수 있습니다.

4. **MLPerf와 같은 높은 정확도 요구 벤치마크**:
   - MLPerf는 정확도 기준을 99% 이상으로 유지하도록 요구하는데, 이처럼 **높은 정확도를 요구하는 환경에서는 지나친 압축으로 인한 정보 손실**이 성능 저하를 초래할 수 있습니다. 예를 들어, 전체 90%의 압축률을 목표로 할 때, Keyformer가 99% 정확도를 달성하는 데 어려움을 겪을 수 있습니다.

결론적으로, Keyformer는 높은 효율성과 적정 수준의 정확도를 유지할 수 있지만, 일부 모델 구조와 벤치마크에서는 압축률에 따라 성능 저하가 발생할 가능성이 있으며, 이러한 경우 압축률을 적절히 조절해야 합니다.