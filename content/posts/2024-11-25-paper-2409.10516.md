---
categories:
- paper-review
- with-gpt
date: "2024-11-25"
title: 'RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval'
---

[논문 링크](https://arxiv.org/abs/2409.10516)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문의 주요 내용 및 강점

**논문의 목적**  
RetrievalAttention은 대규모 언어 모델(LLM)의 긴 문맥 처리에서 발생하는 연산 복잡성과 GPU 메모리 사용량 문제를 해결하기 위해 제안된 방법입니다. 이 방법은 주의 메커니즘의 동적 희소성을 활용하여, CPU 메모리에 있는 키-값(KV) 벡터를 근사 최근접 이웃 검색(ANNS) 방식으로 검색하여 주목할 만한 KV 벡터만 GPU로 가져와 연산합니다.

---

**논문의 강점**  
1. **효율성**:
   - RetrievalAttention은 1%~3%의 키 벡터만 검색하면서도 정확도를 유지합니다.
   - NVIDIA RTX4090 GPU를 사용하여 128K 토큰을 처리할 때 0.188초라는 빠른 토큰 생성 속도를 달성했습니다.
   
2. **저렴한 자원 소모**:
   - 대부분의 KV 벡터를 CPU 메모리에 저장하고 필요할 때만 GPU 메모리로 전송하여 메모리 사용량을 획기적으로 줄였습니다.
   - RTX4090(24GB 메모리)에서 8B 모델을 효율적으로 실행 가능.

3. **독창성**:
   - 기존 ANNS 기법의 한계인 분포 불일치 문제(Out-of-Distribution, OOD)를 해결하기 위해 쿼리와 키 벡터 간의 관계를 모델링하는 주의 메커니즘 특화 벡터 검색 알고리즘을 제안.

---

### 핵심 알고리즘 및 과정 설명 (예시 포함)

**1. 주요 알고리즘 단계**:
- **쿼리 생성**: 주어진 입력에 대해 쿼리 벡터가 생성됩니다.
- **주의 희소성 활용**: 기존 모든 키 벡터와 상호작용하는 대신, 주요 키 벡터를 선택.
- **OOD 문제 해결**:
  - 쿼리-키 벡터 관계를 KNN(K-Nearest Neighbor)을 기반으로 매핑.
  - 쿼리와 키 벡터의 분포 불일치를 벡터 공간 내 투영 기법으로 보정.
- **CPU-GPU 협력 실행**:
  - CPU에서 동적으로 검색된 벡터와 GPU에서 저장된 고정 벡터를 통합해 최종 주의 점수 계산.

---

**2. 예시를 통한 과정 설명**:
- **입력 예시**: 길이 64K 토큰의 텍스트가 주어진다고 가정.
  - 쿼리 벡터 \( q_t \): \( 1 \times 1024 \) 차원의 벡터.
  - 키 벡터 \( K \): 64K × 1024 크기의 데이터.

**처리 과정**:
1. 쿼리 \( q_t \) 생성.
2. CPU 메모리에 저장된 키 벡터 중, ANNS로 상위 1%의 관련 벡터를 검색.
3. GPU에 저장된 고정 키 벡터(예: 최근 512개 토큰)와 CPU에서 검색된 동적 키 벡터를 결합.
4. GPU와 CPU에서 각각 부분적인 주의 점수 계산 후 통합:
   \[
   o_t = \gamma_1 \cdot o_W + \gamma_2 \cdot o_\Omega
   \]
   여기서 \( \gamma_1 \)와 \( \gamma_2 \)는 각 세트의 점수를 재조정하는 계수.

---

### 한계점 및 개선 가능성

**1. 한계점**:
- **분포 불일치 보정의 일반화**:
  - OOD 문제를 해결하기 위한 접근법이 특정 시나리오나 모델에 한정될 가능성 존재.
- **대규모 CPU 메모리 의존**:
  - CPU 메모리 사용량이 증가하며, 이는 메모리 자원이 제한된 환경에서는 문제로 작용할 수 있음.
- **초기 단계의 추가 비용**:
  - 벡터 인덱스를 구축하는 초기 단계에서 추가 시간이 필요.

**2. 개선 가능성**:
- **압축 기법 도입**: CPU 메모리에 저장된 벡터를 양자화(quantization)하여 저장 공간 최적화.
- **분산 시스템 활용**: CPU 메모리 병목을 줄이기 위해 분산형 메모리 시스템과 통합 가능.

---

이 논문은 긴 문맥 처리의 병목 현상을 효과적으로 해결하며, 특히 자원이 제한된 GPU 환경에서 LLM을 실행하기 위한 실용적이고 독창적인 솔루션을 제공합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### 논문의 핵심 알고리즘: **RetrievalAttention**

RetrievalAttention의 핵심은 **긴 문맥에서 중요한 키-값(Key-Value) 벡터만 효율적으로 검색하고 계산하여, 대규모 언어 모델(LLM)의 연산 비용을 줄이는 것**입니다. 아래에 알고리즘의 과정을 예시와 함께 상세히 설명하겠습니다.

---

### 알고리즘의 주요 단계
1. **쿼리 생성**
2. **주의 희소성 활용**
3. **OOD(Out-of-Distribution) 문제 해결**
4. **CPU-GPU 협력 실행**
5. **최종 결합**

---

### 예시를 통해 단계별로 설명

#### 가정된 상황
- **모델**: 8B 매개변수의 LLM
- **입력 길이**: 64K 토큰 (64,000개의 단어 또는 기호)
- **쿼리 벡터 크기**: \( q_t \in \mathbb{R}^{1 \times 1024} \) (1024차원 벡터)
- **키 벡터 크기**: \( K \in \mathbb{R}^{64000 \times 1024} \) (64,000개의 1024차원 벡터)
- **값 벡터 크기**: \( V \in \mathbb{R}^{64000 \times 1024} \)

#### **1. 쿼리 생성**
현재 입력 시점 \( t \)에서 쿼리 벡터 \( q_t \)가 생성됩니다.  
- 쿼리 벡터는 입력 텍스트의 현재 문맥을 바탕으로 만들어지며, 이후 주의 메커니즘에서 다른 키 벡터와 연관성을 계산하는 데 사용됩니다.
- 예시: \( q_t = [0.12, 0.56, \dots, -0.34] \)

---

#### **2. 주의 희소성 활용**
LLM의 주의 메커니즘은 대부분의 쿼리가 특정한 일부 키-값 벡터에만 높은 상호작용을 가집니다. 이를 "희소성"이라고 합니다.
- 관찰: 쿼리 \( q_t \)와 관련된 키 벡터는 전체 키 벡터의 1~3%에 불과합니다.
- 해결책: 전체 키 벡터를 사용하지 않고, **상위 1%~3%의 중요 벡터만 선택**.

예시:
- \( K = [K_1, K_2, \dots, K_{64000}] \) 중에서 중요한 벡터 640개(1%)만 선택.
- 선택된 벡터: \( K_{\text{selected}} = [K_{10}, K_{200}, \dots, K_{63990}] \)

---

#### **3. OOD 문제 해결**
일반적인 ANNS(근사 최근접 이웃 검색) 알고리즘은 쿼리 벡터와 키 벡터가 같은 분포를 따른다고 가정하지만, 주의 메커니즘에서는 그렇지 않습니다.  
**RetrievalAttention**은 쿼리와 키 벡터 간의 분포 차이를 보정합니다.

- **해결 방법**:
  1. **쿼리-키 관계 학습**: 이전 단계에서 생성된 \( q_t \)와 각 \( K \) 간의 상관성을 학습하여 분포 차이를 줄입니다.
  2. **KNN 기반 매핑**: 쿼리 벡터가 가장 중요한 \( K \) 벡터를 정확히 찾을 수 있도록 매핑.

예시:
- \( q_t \)가 가장 중요한 키 \( K_{10}, K_{200} \)와 연결되었다고 학습된 경우, 이 관계를 통해 검색 속도를 향상시킴.

---

#### **4. CPU-GPU 협력 실행**
RetrievalAttention은 CPU와 GPU 메모리를 분리하여 계산 효율성을 극대화합니다.

- **GPU**:
  - 최근 사용된 512개 키 벡터와 값 벡터를 저장.
  - GPU 메모리에서 고정적인 주의 계산 수행.
  - 예: \( K_{\text{GPU}} = [K_{1}, K_{2}, \dots, K_{512}] \)

- **CPU**:
  - 대규모 키-값 벡터(나머지)를 저장하고 필요할 때 검색.
  - CPU에서 검색된 동적 벡터를 GPU로 전송해 계산.
  - 예: \( K_{\text{CPU}} = [K_{513}, K_{514}, \dots, K_{64000}] \)

---

#### **5. 최종 결합**
CPU와 GPU에서 각각 계산된 주의 결과를 통합하여 최종 출력 벡터 \( o_t \)를 생성합니다.

- GPU에서 계산된 결과: \( o_W = \text{Softmax}(q_t \cdot K_{\text{GPU}}^T) \cdot V_{\text{GPU}} \)
- CPU에서 계산된 결과: \( o_\Omega = \text{Softmax}(q_t \cdot K_{\text{CPU}}^T) \cdot V_{\text{CPU}} \)
- 최종 출력 결합:
  \[
  o_t = \gamma_1 \cdot o_W + \gamma_2 \cdot o_\Omega
  \]
  여기서 \( \gamma_1 \), \( \gamma_2 \)는 각 부분 결과의 중요도를 재조정하는 계수.

---

### 알고리즘 요약
1. 쿼리 벡터 \( q_t \)를 생성.
2. 전체 키 벡터에서 중요한 벡터를 ANNS로 검색.
3. 쿼리-키 벡터 간 분포 불일치를 보정하여 검색 정확도를 향상.
4. GPU에서 고정된 키-값 벡터를, CPU에서 동적으로 검색된 벡터를 사용해 주의 계산.
5. CPU와 GPU에서 얻은 결과를 결합하여 최종 출력 \( o_t \) 생성.

---

### 핵심 성과 및 결과
- **시간 효율성**: 기존 전체 키 벡터를 사용하는 방식 대비, 약 4.9배 빠른 토큰 생성 속도.
- **자원 절약**: GPU 메모리 사용량을 10배 이상 줄이면서도 정확도 손실 없음.
- **적용 가능성**: RTX4090(24GB GPU)에서 8B 모델로 128K 토큰 처리 가능.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### 논문에서 제시한 결과
**RetrievalAttention**은 긴 문맥 처리에서 효율성을 크게 개선하면서도 모델 정확도를 유지했습니다. 아래는 주요 결과와 비교 분석입니다.

---

### **1. 주요 성능 결과**

#### **1.1 정확도 유지**
- RetrievalAttention은 **전체 주의(full attention)**와 거의 동일한 정확도를 달성했습니다.
- **∞-Bench 및 RULER 벤치마크**에서 RetrievalAttention은 아래의 결과를 보였습니다:
  - **∞-Bench**:
    - 예를 들어, KV Retrieval(복잡한 검색 작업)에서 RetrievalAttention은 128K 문맥 길이에서도 Full Attention에 근접한 정확도를 보임.
    - RetrievalAttention이 정확히 검색한 키 벡터가 결과 정확도 유지에 기여함.
  - **RULER**:
    - RetrievalAttention은 긴 문맥에서도 정확도가 유지되었으며, Full Attention에 비해 1-2% 미만의 정확도 감소.

#### **1.2 시간 효율성**
- **토큰 생성 속도**:
  - 128K 문맥 길이에서 RetrievalAttention은 Full Attention 대비 **4.9배 속도 향상**.
  - 기존 ANNS 기반 접근법인 IVF 대비 **1.98배 빠름**.
- **GPU 메모리 사용**:
  - Full Attention의 GPU 메모리 사용량을 대폭 줄여 RTX 4090(24GB 메모리)에서도 실행 가능.
  - 예를 들어, Full Attention이 500GB 메모리를 필요로 하는 경우, RetrievalAttention은 CPU 오프로드와 효율적인 검색으로 이를 해결.

#### **1.3 비교**  
다른 방법론과의 비교에서 RetrievalAttention의 특출난 성능은 다음과 같습니다:
- **StreamingLLM**:
  - 고정된 패턴(예: 최근 512개 토큰)을 사용하므로, 중요한 토큰이 동적으로 변할 때 정확도 손실이 큼.
  - RetrievalAttention은 동적 희소성을 활용해 이러한 한계를 극복.
- **SnapKV**:
  - GPU 메모리에 제한된 키-값 저장, 검색을 수행하나, RetrievalAttention만큼 세밀한 검색은 불가.
- **Flat 및 IVF (전통적 ANNS)**:
  - Flat: 정확하지만, 모든 데이터를 스캔하므로 느림.
  - IVF: 일부 데이터만 스캔하지만, 쿼리-키 간 분포 불일치를 해결하지 않아 정확도 저하.

---

### **2. 특출난 점**

RetrievalAttention의 강점은 정확도와 효율성의 균형에 있습니다.

#### **2.1 동적 희소성 활용**
- 이 방법은 쿼리가 모든 키 벡터와 상호작용하지 않고, **중요한 소수의 키 벡터만 선택**합니다.
- 키 벡터의 상호작용이 동적으로 변하더라도 ANNS를 통해 적합한 벡터를 실시간으로 검색.

#### **2.2 OOD 문제 해결**
- 기존 ANNS 방법론에서는 쿼리 벡터와 키 벡터가 다른 분포를 가질 때 검색 성능이 크게 저하.
- RetrievalAttention은 **쿼리-키 간 분포 차이를 매핑 및 보정**하여 검색 정확도를 유지.

#### **2.3 CPU-GPU 협력**
- GPU는 최근 사용된 고정 패턴 벡터를 처리하고, CPU는 동적 검색 벡터를 처리.
- 이 구조는 GPU 메모리 제한을 극복하면서도 계산 병렬성을 높임.

---

### **3. 논문에서 제시하는 성과의 원인**

#### **3.1 주요 기여**
1. **OOD 문제를 해결한 Attention-Aware Vector Search**:
   - 쿼리 벡터의 분포와 키 벡터의 분포를 매핑하는 KNN 기반 기법을 적용.
   - 쿼리-키 간 불일치를 줄임으로써 정확도 손실을 최소화.
2. **CPU-GPU Co-Execution**:
   - GPU에서 고정된 주요 키 벡터를 처리하고, CPU에서 검색된 동적 키 벡터를 병렬로 처리.
3. **희소성 활용**:
   - 1~3%의 키 벡터만 검색하고 계산해도 전체 주의 메커니즘과 유사한 정확도를 유지.

---

### **4. 내 생각**

#### **4.1 RetrievalAttention의 설계 강점**
- RetrievalAttention은 이론적으로 타당하며 실제 적용에서 큰 성과를 보였습니다. 특히, OOD 문제를 해결하는 방식은 기존 ANNS 방식의 근본적 한계를 극복한 점에서 독창적입니다.
- CPU와 GPU를 활용한 병렬 처리 설계는 제한된 GPU 메모리 환경에서도 대규모 LLM을 효율적으로 실행할 수 있게 합니다.

#### **4.2 한계와 개선 방향**
1. **초기 인덱스 구축 시간**:
   - RetrievalAttention은 인덱스를 구축하는 초기 단계에서 시간과 자원이 많이 소모됩니다. 이를 해결하기 위해 인덱스 업데이트 방식을 동적으로 만들거나, 더욱 효율적인 초기화를 도입할 필요가 있습니다.
2. **CPU 메모리 의존성**:
   - CPU 메모리를 다수 사용하므로, 대규모 환경에서는 비용 효율성이 떨어질 가능성이 있습니다. 양자화(Quantization) 또는 분산 메모리 시스템을 활용하면 이를 개선할 수 있습니다.

#### **4.3 미래 적용 가능성**
- RetrievalAttention은 자원이 제한된 환경에서 대규모 언어 모델을 활용하는 데 매우 유용합니다. 클라우드 기반 서비스, 모바일 디바이스 환경 등에서 적용 가능성이 크며, OOD 문제 해결 기법은 다른 분야에서도 응용될 수 있습니다.

---

### **최종 정리**
RetrievalAttention의 강력한 성능은 동적 희소성 활용, OOD 문제 해결, CPU-GPU 협력이라는 혁신적 방법론에서 비롯되었습니다. 이는 긴 문맥 처리에서 기존 방법론 대비 정확도와 효율성 모두를 크게 개선한 점에서 학술적, 실용적 가치가 높습니다.

# Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니? 



RetrievalAttention을 기존 방법에 통합하려면 추가 구현 및 자원 요구 사항을 이해하고 이를 계산해야 합니다. 아래는 **필요한 구현 단계**와 **각 단계에서 소모되는 공수 및 자원 요구량**을 분석한 내용입니다.

---

## **필요한 구현 추가 사항**

### **1. Attention-Aware Vector Search (주의 인식 벡터 검색)**
**필요한 작업**:
- 키 벡터(K)와 쿼리 벡터(Q)를 기반으로 **근사 최근접 이웃 검색(ANNS) 인덱스 구축**.
- OOD(Out-of-Distribution) 문제를 해결하기 위해 KNN 기반 관계 매핑 및 벡터 투영 기법 구현.
- GPU와 CPU 간 데이터 분리 및 적응형 로드 구현.

**필요한 자원 및 연산량**:
- **인덱스 구축**:
  - 벡터 데이터 크기 \( N \times d \) (예: \( N = 64,000 \), \( d = 1024 \))를 기반으로 초기화.
  - GPU를 사용해 KNN 계산: \( O(N \cdot d \cdot k) \) 복잡도 (\( k \)는 상위 키 벡터 개수).
  - 결과: 초기 설정 시 GPU 연산 집중, 이후 CPU로 전환.

- **인덱스 검색**:
  - 매 쿼리마다 전체 키 벡터의 1~3%만 검색: \( O((0.01 \cdot N) \cdot d) \) 복잡도.

---

### **2. CPU-GPU Co-Execution (CPU-GPU 협력 실행)**

**필요한 작업**:
- GPU에 고정된 상위 \( W \)개의 키-값 벡터를 유지.
- CPU에서 나머지 동적 벡터를 저장하고, 검색된 벡터를 GPU로 전송.
- CPU와 GPU의 계산 결과를 결합.

**필요한 자원 및 연산량**:
- **GPU 메모리**:
  - 고정된 \( W \)개의 벡터 저장 (\( W = 512 \) 기준): \( W \cdot d \cdot 2 \cdot \text{data precision} \).
  - 예: FP16 기준 \( 512 \cdot 1024 \cdot 2 \cdot 2 = 2MB \).

- **CPU 메모리**:
  - 나머지 \( N - W \)개의 키-값 벡터 저장.
  - 예: \( (64000 - 512) \cdot 1024 \cdot 2 \cdot 2 = 약 500MB \).

- **연산량**:
  - GPU: 고정된 \( W \)개 벡터와 쿼리 \( q_t \)의 주의 계산: \( O(W \cdot d) \).
  - CPU: 검색된 \( \Omega \) 벡터와의 계산 (\( \Omega = 0.01 \cdot N \) 기준): \( O(\Omega \cdot d) \).

---

### **3. Attention Fusion (결합 단계)**

**필요한 작업**:
- CPU와 GPU에서 얻은 주의 결과를 결합 (Rescaling Factor \( \gamma_1, \gamma_2 \) 계산 포함).
- FlashAttention과 유사한 결합 알고리즘 구현.

**필요한 자원 및 연산량**:
- GPU에서의 결합 연산: \( O(d) \) 복잡도 (\( \text{쿼리 수} \times d \)).
- CPU에서 결합 결과 전송: 메모리 전송 대역폭 의존.

---

### **공수 및 자원 요구량 계산**

#### **1. 구현 공수**
| 구현 작업                | 작업 난이도 | 소요 공수 (개발/테스트)            |
| ------------------------ | ----------- | ---------------------------------- |
| 벡터 인덱스 구축         | 중          | 2~4주 (KNN, OOD 처리)              |
| CPU-GPU 협력 실행        | 높음        | 3~6주 (데이터 분리 및 전송 최적화) |
| 결합 단계 구현 및 최적화 | 중          | 2~3주 (FlashAttention 기반)        |
| 테스트 및 통합           | 중          | 2주                                |

#### **2. 컴퓨팅 자원 요구**
- **GPU**:
  - 초기 인덱스 구축 시 연산 집중.
  - **RTX 4090 기준**: 1-2시간 내 인덱스 구축 가능.
- **CPU**:
  - 검색 및 벡터 인덱스 유지: 500MB~1GB 메모리 소모.
  - 10코어 이상의 병렬 연산 활용 시 검색 시간 \( < 5ms \).
- **PCIe 전송 대역폭**:
  - 검색된 벡터를 GPU로 전송: 약 \( 2MB/s \) 전송 대역폭 필요 (FP16 기준).

---

### **최종 분석**
**추가 구현의 주요 요구 사항**:
1. **KNN 기반 벡터 검색 시스템 구현**.
2. CPU와 GPU 간 데이터 전송 최적화.
3. FlashAttention 기반 결합 연산 수정.

**자원 소모 및 성능 전망**:
- GPU 메모리와 CPU 메모리 요구량은 기존 방법보다 효율적.
- 구현은 기술적 난이도가 있지만, LLM을 긴 문맥에서 더 효율적으로 실행하려는 경우 필요한 투자.

**결론**:
RetrievalAttention 구현은 초기 구축 비용이 있지만, 긴 문맥 처리에서 **메모리 절약과 속도 향상**이라는 장기적 이점을 제공합니다. 특히, OOD 해결과 CPU-GPU 협력 메커니즘은 기존 방법에 비해 독창적이고 실용적입니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### 논문의 입력 데이터와 추론 과정

RetrievalAttention의 **입력 데이터**와 **추론 과정**을 단계별로 설명하고, 모델 아키텍처 및 연산 요구 사항에 대해 자세히 분석하겠습니다.

---

### **1. 입력 데이터**

#### 입력 데이터의 구조:
1. **텍스트 데이터**:
   - 긴 문맥을 포함한 텍스트 (예: 128K 토큰, 한 문장은 평균 20~30개의 토큰).
   - 각 토큰은 임베딩 벡터로 변환됨 (\( \mathbb{R}^d \), \( d = 1024 \)).

2. **모델의 초기 상태**:
   - 키-값(Key-Value) 벡터:
     - \( K \): \( \mathbb{R}^{N \times d} \), 키 벡터.
     - \( V \): \( \mathbb{R}^{N \times d} \), 값 벡터.
   - 쿼리 벡터:
     - \( Q_t \): 현재 시점 \( t \)의 쿼리 벡터 (\( \mathbb{R}^d \)).

#### 예시:
- **텍스트**: "In a village of La Mancha, the name of which I have no desire to call to mind..."
- **변환된 데이터**:
  - 텍스트를 64K 토큰으로 분할.
  - 각 토큰은 임베딩 벡터 (\( d = 1024 \))로 매핑:
    \[
    \text{예: "In"} \rightarrow [0.21, -0.34, 0.78, \dots, 0.11] \in \mathbb{R}^{1024}
    \]

---

### **2. 추론 과정**

#### 단계별 설명:
1. **쿼리 벡터 생성**:
   - 현재 토큰(예: "In")을 기반으로 쿼리 벡터 \( Q_t \) 생성.

2. **주의 점수 계산**:
   - 모든 키 벡터 \( K_i \)에 대해 쿼리 벡터와의 내적 계산:
     \[
     z_i = Q_t \cdot K_i^T
     \]
   - 쿼리와 키 간의 상호작용 강도를 나타냄.

3. **희소성 적용**:
   - \( z_i \) 값 중 상위 1~3%에 해당하는 \( K \) 및 \( V \) 벡터 선택.
   - 선택된 벡터:
     \[
     K_{\text{selected}}, V_{\text{selected}}
     \]

4. **CPU-GPU 협력 계산**:
   - **GPU**:
     - 최근 512개 키-값 벡터의 부분 계산 수행.
   - **CPU**:
     - 동적으로 검색된 키-값 벡터로 계산 수행.
   - 결과 결합:
     \[
     O_t = \gamma_1 \cdot O_W + \gamma_2 \cdot O_\Omega
     \]

5. **출력 생성**:
   - 최종 벡터 \( O_t \)는 다음 토큰 예측 확률 분포로 변환:
     \[
     P(\text{토큰} | \text{이전 토큰들}) = \text{Softmax}(O_t \cdot W_{\text{out}})
     \]

---

### **3. 모델 아키텍처**

#### **모델 구성**
RetrievalAttention이 적용된 모델은 Transformer 기반 LLM의 확장입니다.

- **구성 요소**:
  - 입력 임베딩 (\( \mathbb{R}^{N \times d} \)).
  - 다중 헤드 주의(Multi-Head Attention, MHA).
  - 피드포워드 네트워크 (FFN).
  - CPU-GPU 데이터 관리 및 병렬 계산.

- **예시 모델 아키텍처**:
  - **Transformer 레이어 수**:
    - \( L = 32 \)~\( 48 \) (예: Yi-6B, Yi-9B 모델).
  - **쿼리/키 헤드**:
    - \( H = 32 \) 쿼리 헤드.
    - \( K = 4 \) 키 헤드.

---

### **4. 연산 및 메모리 요구 사항**

#### **연산 복잡도**
1. **주의 계산 (Attention)**:
   - 기존: \( O(N^2 \cdot d) \) (모든 키-값 계산).
   - RetrievalAttention:
     \[
     O((W + \Omega) \cdot d), \quad W = 512, \quad \Omega = 0.01 \cdot N
     \]
     - \( W \): GPU의 고정 키-값 벡터 수.
     - \( \Omega \): CPU에서 검색된 벡터 수.

#### **메모리 요구량**
1. **GPU 메모리**:
   - 고정된 \( W \)개 벡터 저장:
     \[
     W \cdot d \cdot \text{데이터 정밀도} \quad (\text{FP16 사용 시: } 2 \text{바이트})
     \]
     - 예: \( 512 \times 1024 \times 2 = 1MB \).

2. **CPU 메모리**:
   - \( (N - W) \) 키-값 벡터 저장:
     \[
     (N - W) \cdot d \cdot \text{데이터 정밀도}
     \]
     - 예: \( (64000 - 512) \times 1024 \times 2 = 127MB \).

3. **추론 단계 메모리 요구량**:
   - CPU에서 검색된 벡터 전송: \( \Omega \cdot d \cdot \text{데이터 정밀도} \).
   - 예: \( \Omega = 640 \), \( d = 1024 \), FP16 사용 시 1.28MB 전송.

---

### **5. 컴퓨팅 요구량**

#### **초기화 단계**
- **벡터 인덱스 구축**:
  - 연산량: \( O(N \cdot d \cdot k) \) (\( k \)는 KNN 크기).
  - 자원: GPU 중심.

#### **추론 단계**
1. **CPU 연산량**:
   - 검색된 벡터에 대해 계산 (\( \Omega = 0.01 \cdot N \)):
     \[
     O(\Omega \cdot d)
     \]

2. **GPU 연산량**:
   - 고정된 \( W \) 벡터에 대해 계산:
     \[
     O(W \cdot d)
     \]

3. **결합 연산**:
   - CPU와 GPU의 결과 결합: \( O(d) \).

---

### **최종 분석**

RetrievalAttention은 다음을 요구합니다:
1. **연산 최적화**: 기존 \( O(N^2 \cdot d) \) 복잡도를 \( O((W + \Omega) \cdot d) \)로 줄임.
2. **메모리 효율성**: GPU 메모리 요구량을 \( W \) 벡터로 제한.
3. **컴퓨팅 요구량**: 초기화 단계는 다소 비용이 들지만, 추론 단계는 기존 방식보다 대폭 감소.

결론적으로, RetrievalAttention은 **긴 문맥에서도 메모리와 연산 효율성을 크게 향상**시키는 실용적인 기법으로 평가됩니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **RetrievalAttention의 한계와 이를 극복하기 위한 연구 흐름**

논문이 제시하는 **RetrievalAttention**은 대규모 언어 모델의 긴 문맥 처리 효율성을 크게 향상시켰지만, 여전히 몇 가지 한계를 가지고 있습니다. 이를 해결하기 위한 연구 흐름과 발전 가능성을 살펴보겠습니다.

---

### **1. RetrievalAttention의 한계**

#### **1.1 인덱스 구축 비용**
- **문제**:
  - 초기화 단계에서 키-값 벡터의 인덱스 구축(KNN 매핑)이 비용이 크며, 대규모 데이터셋에서는 시간이 오래 걸림.
  - 예: 64K 벡터에 대한 KNN 계산이 필요하며, GPU 연산 집중 발생.

- **영향**:
  - 모델을 실시간 애플리케이션(예: 실시간 챗봇)에서 초기화하거나 자주 업데이트해야 하는 경우 제한적.

---

#### **1.2 CPU 메모리 의존성**
- **문제**:
  - CPU 메모리에 대량의 키-값 벡터 저장(예: 500MB 이상).
  - CPU의 데이터 전송 속도가 GPU의 처리 속도를 따라가지 못할 가능성.

- **영향**:
  - 고성능 DRAM이나 대규모 CPU 메모리가 없는 환경에서 성능 저하.
  - 모바일 및 임베디드 디바이스에서는 비효율적.

---

#### **1.3 긴 문맥 확장성**
- **문제**:
  - RetrievalAttention은 GPU에서 고정된 \( W \)개의 벡터와 CPU에서 검색된 벡터만 처리하므로, 문맥이 1M 토큰 이상으로 확장되면 추가 성능 저하 가능.
  - CPU에서 검색된 벡터 수가 증가할수록 전송 병목 발생.

- **영향**:
  - 초장문(1M~10M 토큰)을 처리해야 하는 도메인(예: 법률 텍스트, 생물학 데이터)에서는 제한적.

---

### **2. 한계를 극복하기 위한 연구 흐름**

#### **2.1 인덱스 구축 최적화**
##### **동적 인덱스 업데이트**
- **아이디어**:
  - 기존의 정적 KNN 매핑 대신, 새로운 입력이 들어올 때마다 필요한 키 벡터만 점진적으로 업데이트.
  - 예: 중요도에 따라 자주 사용하는 벡터를 우선적으로 유지.

- **장점**:
  - 초기화 비용 감소.
  - 실시간 처리에 적합.

- **관련 연구**:
  - **Adaptive ANNS**: 벡터의 지역적 업데이트를 지원하는 동적 인덱싱 기법 (예: HNSW 기반 업데이트).

##### **온디맨드 인덱싱(On-Demand Indexing)**
- **아이디어**:
  - 모든 키 벡터에 대한 인덱스를 사전 구축하지 않고, 특정 문맥에서 자주 참조되는 벡터만 인덱싱.
  - CPU에서만 인덱스를 구축하고 GPU는 기존 데이터를 재사용.

- **장점**:
  - 드문 데이터에 대한 불필요한 계산 감소.
  - 메모리와 연산 비용 최적화.

---

#### **2.2 메모리 요구량 최적화**
##### **키-값 벡터 압축**
- **아이디어**:
  - CPU에 저장되는 키-값 벡터를 양자화(Quantization)하여 메모리 사용량을 줄임.
  - 예: 16비트 대신 8비트 또는 4비트로 압축.

- **장점**:
  - 메모리 사용량 감소.
  - 전송 속도 향상.

- **관련 연구**:
  - **PQCache**: 키-값 벡터의 Product Quantization(제품 양자화) 적용.

##### **모바일 환경에서의 경량화**
- **아이디어**:
  - CPU 메모리가 부족한 환경에서는 중요도가 높은 벡터만 유지하고 나머지는 폐기.
  - GPU 메모리 내 캐싱 비율 증가.

- **장점**:
  - 메모리 제한이 심한 디바이스에서도 사용 가능.

---

#### **2.3 긴 문맥 확장**
##### **계층적 주의 메커니즘(Hierarchical Attention)**
- **아이디어**:
  - 긴 문맥을 여러 계층으로 나누고, 상위 계층에서만 요약 정보를 처리.
  - 예: 1M 토큰을 10K 단위로 나누고, 각 블록의 대표 키-값 벡터만 활용.

- **장점**:
  - 연산량을 블록 단위로 축소.
  - GPU 메모리와 CPU 검색량 감소.

- **관련 연구**:
  - **BigBird**: 블록 기반 희소성 주의 기법.
  - **Longformer**: 국소적(Local) 및 전역적(Global) 주의 혼합.

##### **임시적 캐싱(Temporal Caching)**
- **아이디어**:
  - 이전에 계산된 주의 결과를 임시 캐싱하여 반복된 계산을 줄임.
  - 최근 문맥(예: 마지막 1K 토큰)에 대한 결과를 GPU에 저장.

- **장점**:
  - GPU 메모리 활용 증가.
  - 반복 계산 감소로 속도 향상.

- **관련 연구**:
  - **FlashAttention**: GPU 메모리 IO 최적화로 임시 데이터 캐싱.

---

### **3. 내 생각 및 결론**

#### **3.1 유망한 연구 방향**
1. **온디맨드 인덱싱**과 **동적 업데이트**:
   - 실시간 애플리케이션을 목표로 하는 경우, 초기화 비용을 줄이는 동적 인덱스 구축이 필수적.
2. **벡터 압축**:
   - 메모리와 전송 병목을 동시에 해결할 수 있는 양자화 및 압축 연구는 매우 실용적.
3. **계층적 처리**:
   - 긴 문맥 확장을 위한 블록 기반 접근법은 RetrievalAttention의 설계를 보완할 수 있음.

#### **3.2 결론**
RetrievalAttention의 한계를 극복하기 위한 연구는 **효율성과 확장성**을 동시에 추구하는 방향으로 나아가고 있습니다. 특히, **메모리 압축**, **동적 업데이트**, **블록 기반 확장**은 현재와 미래의 긴 문맥 처리 요구를 만족시키기 위한 핵심 연구 흐름으로 자리잡을 것입니다.