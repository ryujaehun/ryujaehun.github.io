---
categories:
- paper-review
- with-gpt
date: "2024-12-17T00:00:00Z"
title: 'FlightLLM: Efficient Large Language Model Inference with a Complete Mapping
  Flow on FPGAs'
---

[논문 링크](https://arxiv.org/abs/2401.03868)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문 요약 및 분석  
**제목:** *FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs*  
**주제:** FPGA 기반 대형 언어 모델(LLM) 추론을 위한 효율적 아키텍처  

---

### **핵심 결론 및 강점**  
1. **결론 요약**  
   - FlightLLM은 FPGA의 특화된 리소스를 활용하여 LLM의 추론 효율성을 대폭 개선.  
   - **에너지 효율성**: 상용 GPU 대비 **6.0배 향상** (NVIDIA V100S 기준).  
   - **비용 효율성**: GPU 대비 **1.8배** 더 나은 효율성.  
   - 최신 FPGA (Versal VHK158)로 NVIDIA A100 GPU 대비 **1.2배 높은 처리량** 달성.  

2. **강점**  
   - **구성 가능한 Sparse DSP 체인**: 다양한 희소성 패턴을 효율적으로 지원해 **1.6배 계산 효율** 향상.  
   - **Always-On-Chip Decode**: 반복적 외부 메모리 접근을 줄여 **메모리 대역폭 활용도 65.9%**로 개선.  
   - **길이 적응형 컴파일**: 컴파일 오버헤드를 500배 줄여 LLM 실시간 배포 가능.  
   - **성능 최적화**: 다양한 스패셜리티를 가진 FPGA의 DSP48 리소스를 최대한 활용.

---

### **핵심 알고리즘 설명**  
#### **1. 핵심 아키텍처 구성**  
   - **MPE (Matrix Processing Engine):**  
     - Sparse 및 Dense 행렬 연산 모두를 효율적으로 수행.  
     - 구성 가능한 Sparse DSP 체인을 통해 희소 패턴에 최적화된 계산 수행.  

   - **Always-On-Chip Decode (Decode 최적화):**  
     - 작은 크기의 Activation 데이터를 FPGA의 온칩 메모리에 유지하며 외부 메모리 접근 최소화.  
     - Operator Fusion 기법으로 Softmax 및 Activation 연산을 MPE와 결합.  

   - **길이 적응형 컴파일:**  
     - 입력 및 출력 길이가 동적으로 변하는 LLM 추론에 대응.  
     - 짧은 길이 토큰에서는 동일한 명령어를 재사용하여 명령어 크기를 크게 감소.  

#### **2. 예시 입력과 과정**  
   - **입력:** 512개 토큰으로 구성된 입력 시퀀스 (`prefill`) → 1개 토큰 출력 (`decode`).  
   - **과정:**  
     1. **MPE**에서 입력 토큰을 Sparse Matrix와 연산하여 Attention Weight 계산.  
     2. Sparse 패턴에 맞게 가용 DSP48 코어가 동적으로 구성됨.  
     3. **Always-On-Chip Decode**를 통해 이전 토큰의 결과를 온칩 메모리에 유지.  
     4. **Mixed-Precision**을 통해 데이터가 INT8로 변환되어 효율적 계산 수행.  
     5. 최종적으로 Softmax 및 LayerNorm 연산이 SFU (Special Function Unit)에서 수행됨.  

---

### **독창적인 지점**  
| **기술/기법**             | **주요 개선점**                              | **결과**                        |
| ------------------------- | -------------------------------------------- | ------------------------------- |
| **Sparse DSP 체인**       | N:M 희소 패턴을 효율적으로 계산하도록 재설계 | 계산 효율 **1.6배** 향상        |
| **Always-On-Chip Decode** | 온칩 메모리 유지 및 Operator Fusion          | 메모리 대역폭 활용도 **65.9%**  |
| **길이 적응형 컴파일**    | 동적 입력 길이에서 명령어 크기 최소화        | 명령어 저장 공간 **500배 감소** |
| **Mixed-Precision 지원**  | INT8 기반 Mixed Precision 연산 최적화        | 메모리 접근 비용 감소           |

---

### **한계점**  
1. **FPGA 기반의 리소스 제약:**  
   - FPGA의 DSP48 코어와 메모리 용량은 GPU보다 작아 **대규모 배치(batch)** 수행 시 성능이 하락함.  
   - 배치 사이즈가 클수록 GPU의 고성능 메모리 대역폭이 유리.  

2. **모델 호환성:**  
   - 일부 최신 GPU 최적화 코드 (예: gpt-fast)와 비교 시 성능 격차가 존재.  
   - 기존 GPU 생태계와의 완전한 통합에는 한계가 있음.  

3. **컴파일 및 설계 복잡도:**  
   - 길이 적응형 컴파일 및 Sparse DSP 체인의 설정이 복잡하고 하드웨어에 최적화된 설계가 필수.  

---

### **결론**  
FlightLLM은 FPGA를 활용한 혁신적인 LLM 추론 최적화 솔루션으로, 에너지 및 비용 효율성을 극대화하며 GPU를 뛰어넘는 성능을 보였다. 특히 Sparse DSP 체인과 Always-On-Chip Decode가 결합되어 메모리 병목을 해결한 점이 독창적이다. 하지만 대규모 배치 처리 및 GPU 생태계와의 비교에서는 개선 여지가 남아 있다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### **논문의 핵심 알고리즘 설명 및 예시**  
논문 **"FlightLLM"**은 FPGA를 활용한 대형 언어 모델(LLM) 추론 최적화의 핵심 알고리즘을 제시합니다. 여기서는 **구성 가능한 Sparse DSP 체인**과 **Always-On-Chip Decode**를 중심으로 상세한 예시와 함께 설명합니다.

---

## **1. 구성 가능한 Sparse DSP 체인**  
**목표:** LLM에서 희소 행렬 연산(Sparse Matrix)을 효율적으로 수행하여 계산 자원을 최대한 활용합니다.  

### **핵심 원리**  
1. **Sparse 데이터 특성:**  
   - 희소 행렬은 대부분의 요소가 0으로, 연산 시 0값은 무시되어야 성능이 향상됩니다.  
   - LLM의 주요 연산인 **Matrix-Vector Multiplication (MV)** 및 **Matrix-Matrix Multiplication (MM)**에서 Sparse 패턴을 효율적으로 처리합니다.

2. **N:M 희소 패턴**  
   - **N:M 희소 패턴**: N개의 요소 중 M개만 0이 아닌 값으로 구성됩니다.  
   - 예시: **4:2 희소 패턴** → 4개 중 2개가 비-제로.  

3. **Sparse DSP 체인 구성**  
   - FPGA의 DSP48을 활용해 **Cascaded DSP Chain**을 유연하게 구성합니다.  
   - 고정된 DSP 체인을 나누어 **Sparse MUX**와 **Reduction Node**를 추가하여 Sparse 데이터를 효율적으로 계산합니다.  

---

### **예시 입력 및 연산 과정**  
**문제**:  
두 행렬 \( A \)와 \( B \)가 주어졌을 때 \( A \times B \)를 Sparse DSP 체인을 통해 계산합니다.  
- \( A \): **4x4** 희소 행렬 (4:2 희소 패턴)  
- \( B \): **4x1** 벡터  

**입력 데이터 예시**:  
- \( A = \begin{bmatrix} 1 & 0 & 2 & 0 \\ 0 & 3 & 0 & 4 \\ 5 & 0 & 6 & 0 \\ 0 & 7 & 0 & 8 \end{bmatrix} \)  
- \( B = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} \)  

**단계별 과정**:

1. **Sparse MUX 처리**  
   - Sparse MUX는 \( A \) 행렬에서 **비-제로 요소**만 선택합니다.  
   - 4:2 희소 패턴에 따라 각 행에서 **2개의 비-제로 요소**만을 DSP로 전달:  
     \[
     \text{Row 1: } (1, 2), \, \text{Row 2: } (3, 4), \, \text{Row 3: } (5, 6), \, \text{Row 4: } (7, 8)
     \]  

2. **DSP 체인 연산**  
   - **비-제로 요소**와 벡터 \( B \)의 대응 값을 곱합니다:  
     - Row 1: \( (1 \times 1) + (2 \times 3) = 1 + 6 = 7 \)  
     - Row 2: \( (3 \times 2) + (4 \times 4) = 6 + 16 = 22 \)  
     - Row 3: \( (5 \times 1) + (6 \times 3) = 5 + 18 = 23 \)  
     - Row 4: \( (7 \times 2) + (8 \times 4) = 14 + 32 = 46 \)  

3. **Reduction Node**  
   - Sparse 연산의 결과를 Reduction Node를 통해 집계하여 최종 출력을 생성합니다.

**출력 결과**:  
\[
A \times B = \begin{bmatrix} 7 \\ 22 \\ 23 \\ 46 \end{bmatrix}
\]

---

## **2. Always-On-Chip Decode**  
**목표:** LLM의 디코딩 단계에서 반복적으로 접근하는 작은 크기의 활성화 벡터(Activation)를 **온칩 메모리**에 유지하여 외부 메모리 접근을 최소화합니다.  

---

### **핵심 원리**  
1. **Decode 단계 특성:**  
   - LLM 디코딩은 한 번에 하나의 토큰을 생성하며, 이전 출력 결과를 다시 입력으로 사용합니다.  
   - 이는 **Matrix-Vector 연산(MV)**만 필요하므로 메모리 접근이 빈번합니다.  

2. **온칩 메모리 활용**  
   - 모든 **활성화 벡터**를 FPGA의 온칩 메모리에 유지합니다.  
   - **계산 결과를 바로 다음 연산에 재사용**하여 외부 메모리 접근을 줄입니다.  

3. **Operator Fusion**  
   - MV 연산과 후속 Softmax, LayerNorm 같은 연산을 **연속적으로** 온칩에서 수행합니다.  

---

### **예시 입력 및 연산 과정**  
**입력:**  
- 초기 입력 벡터: \( X = [1, 2, 3, 4] \)  
- 가중치 \( W \): **4x4** 행렬  

**단계별 과정:**  
1. **Matrix-Vector 연산 (MV):**  
   - \( X \times W \)를 계산:  
     \( W = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 4 \end{bmatrix} \)  
     \[
     Y = X \times W = \begin{bmatrix} 1 \times 1 \\ 2 \times 2 \\ 3 \times 3 \\ 4 \times 4 \end{bmatrix} = \begin{bmatrix} 1 \\ 4 \\ 9 \\ 16 \end{bmatrix}
     \]

2. **Softmax 연산:**  
   - \( Y = [1, 4, 9, 16] \)에 대해 Softmax 적용:  
     \[
     \text{Softmax}(Y) = \frac{e^{Y_i}}{\sum e^{Y_j}} \quad \text{for each } i
     \]  

3. **온칩 유지:**  
   - \( Y \) 벡터는 온칩 메모리에 저장되며, 다음 연산에 직접 사용됩니다.

---

## **알고리즘 정리**  

| **단계**                  | **설명**                                                                                          |
| ------------------------- | ------------------------------------------------------------------------------------------------- |
| **Sparse DSP 체인**       | FPGA의 DSP48을 Cascaded 방식으로 유연하게 구성해 Sparse 행렬 연산 수행. **N:M 희소 패턴**을 활용. |
| **Sparse MUX**            | 희소 행렬에서 비-제로 요소만 선택하여 DSP에 전달.                                                 |
| **Reduction Node**        | Sparse 연산의 결과를 집계하여 최종 출력 생성.                                                     |
| **Always-On-Chip Decode** | 디코딩 단계에서 활성화 벡터를 온칩 메모리에 유지해 외부 메모리 접근 최소화.                       |
| **Operator Fusion**       | MV 연산 후 Softmax, LayerNorm 등의 후속 연산을 온칩에서 연속 수행.                                |

---

### **결론**  
FlightLLM의 핵심 알고리즘은 FPGA의 고유한 리소스를 활용하여 **Sparse 계산 최적화**와 **메모리 접근 최소화**를 실현합니다. Sparse DSP 체인을 통해 계산 자원을 최대로 활용하고, 온칩 메모리 및 Operator Fusion을 통해 디코딩 단계의 성능을 극대화합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### **논문 결과 및 특출난 점 분석**  

---

## **1. 논문에서 제시한 결과 요약**  

**FlightLLM**은 FPGA를 활용해 대형 언어 모델(LLM) 추론 효율성을 극대화한 결과를 보여줍니다. 주요 성능 및 효율 개선 사항은 아래와 같습니다:  

| **비교 대상**               | **성능 개선**                        | **비교 지표**       |
| --------------------------- | ------------------------------------ | ------------------- |
| **NVIDIA V100S GPU**        | **6.0x** 에너지 효율 향상            | Token/Joule         |
|                             | **1.8x** 비용 효율 개선              | Token/s/Cost($)     |
| **NVIDIA A100 GPU**         | **4.2x** 에너지 효율 향상            | Token/Joule         |
|                             | **1.4x** 비용 효율 개선              | Token/s/Cost($)     |
| **최신 Versal VHK158 FPGA** | **1.2x** 더 높은 처리량 (Throughput) | Token/s (LLaMA2-7B) |

### **핵심 요인:**  
- **Sparse DSP 체인**: 다양한 희소 패턴을 효율적으로 처리해 계산 자원을 최대한 활용.  
- **Always-On-Chip Decode**: 디코딩 단계에서 활성화 벡터를 온칩 메모리에 유지해 **메모리 병목** 해소.  
- **Mixed-Precision 지원**: 데이터의 비트 폭을 동적으로 조정해 **메모리 대역폭** 최적화.  
- **길이 적응형 컴파일**: 동적 입력 길이에 맞춰 **컴파일 오버헤드**를 최소화.  

---

## **2. 다른 방법론과의 비교: 특출난 점**  

| **비교 대상**                           | **차별화된 점**                 | **설명**                                                                    |
| --------------------------------------- | ------------------------------- | --------------------------------------------------------------------------- |
| **GPU 기반 최적화 (vLLM, SmoothQuant)** | **Sparse 패턴 완벽 지원**       | GPU는 고정된 구조적 희소성만 지원 (예: 2:4 패턴), FPGA는 N:M 패턴까지 지원. |
| **기존 FPGA 가속기 (DFX 등)**           | **Always-On-Chip Decode** 구현  | 기존 방법은 디코딩 단계에서 반복적 외부 메모리 접근 병목이 존재.            |
|                                         | **Mixed-Precision 비트 최적화** | GPU의 SIMD 구조는 가변 비트 연산에 비효율적이나, FPGA는 유연한 처리 가능.   |
| **최신 GPU 코드(gpt-fast)**             | **하이브리드 메모리 활용**      | HBM+DDR 메모리를 병렬로 활용하여 소규모 데이터 접근 최적화.                 |

### **특출난 점 정리:**  
- **Sparse 패턴 최적화**: GPU는 고정된 희소 패턴(2:4 구조)만을 지원하는 반면, FPGA는 N:M 패턴까지 유연하게 지원해 **연산 효율**을 극대화합니다.  
- **Always-On-Chip Decode**: 반복적으로 발생하는 작은 활성화 데이터를 온칩 메모리에 유지해 외부 메모리 접근 병목을 해결합니다.  
- **길이 적응형 컴파일**: 입력 길이에 따라 컴파일 시간을 줄이고 명령어 크기를 최소화하여 **실시간 배포**를 가능하게 합니다.  
- **Mixed-Precision 지원**: 비트 폭을 동적으로 조정해 **메모리 접근 비용**을 줄이면서도 정확도를 유지합니다.  

---

## **3. FlightLLM의 결과를 도출하게 된 핵심 방법**  

1. **Sparse DSP 체인**:  
   - FPGA의 **DSP48** 유닛을 **구성 가능한 체인**으로 설계해 Sparse 행렬 연산의 효율을 높였습니다.  
   - 기존 GPU는 SIMD 구조에 맞춰 **고정된 구조적 희소성**(2:4 패턴)만 처리 가능하지만, FlightLLM은 N:M 패턴까지 지원해 계산 자원의 활용도를 극대화합니다.

2. **Always-On-Chip Decode**:  
   - **온칩 메모리**를 활용해 작은 크기의 활성화 데이터를 유지하며, 반복적인 외부 메모리 접근을 최소화했습니다.  
   - Operator Fusion을 통해 MV 연산과 후속 Softmax 및 Activation 연산을 연속으로 수행해 성능을 극대화합니다.  

3. **길이 적응형 컴파일**:  
   - 입력 및 출력 토큰 길이를 **그룹화**해 동일한 명령어 세트를 재사용하여 명령어 저장 공간을 크게 줄였습니다.  
   - FPGA의 **N:M Sparse 패턴** 및 **Mixed-Precision**을 고려한 하드웨어/소프트웨어 최적화를 수행합니다.  

4. **Mixed-Precision 지원**:  
   - FPGA 특화된 하드웨어 유닛을 통해 **가변 비트** 데이터를 **INT8** 형식으로 변환하여 연산을 수행합니다.  
   - 이 과정에서 **LUT**(Look-Up Table) 오버헤드를 최소화하고 성능을 유지합니다.

---

## **4. FlightLLM이 이러한 결과를 도출하게 된 이유**  

1. **FPGA의 유연한 구조**:  
   - GPU의 고정된 SIMD 구조와 달리 FPGA는 **유연하게 하드웨어 자원(DSP48, 메모리)**를 설계할 수 있어 최적의 Sparse 계산과 메모리 활용이 가능했습니다.

2. **디코딩 단계 최적화**:  
   - 디코딩 단계에서 외부 메모리 접근이 병목으로 작용하는 점을 지적하고, 온칩 메모리를 활용해 **Always-On-Chip Decode**를 구현했습니다.  
   - 이로 인해 메모리 대역폭 활용률이 65.9%까지 향상되었습니다.

3. **하드웨어/소프트웨어 협력 설계**:  
   - 길이 적응형 컴파일과 같은 소프트웨어 최적화 기법을 통해 FPGA의 하드웨어 자원을 최대한 활용했습니다.

---

## **5. 나의 생각**  

**FlightLLM**은 FPGA의 강점인 **유연한 하드웨어 자원 구성**을 극대화한 설계로, Sparse 연산과 메모리 병목 문제를 해결했다는 점에서 매우 인상적입니다.  

1. **Sparse DSP 체인**과 **Always-On-Chip Decode**는 기존 GPU나 다른 FPGA 가속기에서 해결하지 못한 문제를 혁신적으로 풀어냈습니다.  
   - GPU 대비 전력 소모가 적으면서도 성능을 유지하거나 오히려 더 나아졌다는 점이 실용적입니다.  

2. 다만, FPGA의 DSP48 자원과 메모리 용량의 한계로 **대규모 배치(batch)** 환경에서 GPU의 성능을 따라잡기 어려운 점은 명확한 한계입니다.  
   - 향후 FPGA가 더 큰 용량의 메모리와 높은 대역폭을 갖추게 된다면, FlightLLM과 같은 방법론이 더욱 확장될 수 있을 것으로 보입니다.

3. **기술적 접근 방식**은 매우 탄탄하지만, FPGA 설계 및 컴파일 과정이 GPU에 비해 복잡하고 시간이 많이 소요될 수 있어 실시간 배포 측면에서는 개선이 필요합니다.

결론적으로, **FlightLLM**은 **Sparse 계산 최적화**와 **온칩 메모리 활용**을 통해 GPU 대비 **에너지 효율**과 **비용 효율**에서 큰 강점을 보여준 혁신적인 연구입니다. FPGA의 잠재력을 최대한 활용했다는 점에서 실용적이며, 향후 하드웨어의 발전에 따라 더 큰 가능성이 열릴 것으로 보입니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### **논문의 입력 데이터, 추론 과정, 모델 아키텍처 구성 및 연산 분석**

---

## **1. 입력 데이터와 추론 과정 예시**  

### **1.1 입력 데이터의 특성**  
FlightLLM에서 입력 데이터는 **LLM 추론** 시 사용되는 **텍스트 토큰 시퀀스**입니다.  
- 입력: 토큰화된 **문장(prompt)**  
- 출력: 모델이 생성한 다음 **단어(토큰)**  

예를 들어, GPT 계열 모델처럼 LLM 추론은 **Prefill 단계**와 **Decode 단계**로 나뉩니다.

---

### **1.2 추론 단계와 예시**  

#### **1단계: Prefill 단계**  
- **정의:** 주어진 입력 텍스트를 LLM이 이해하고 **첫 번째 출력 토큰**을 생성하는 과정.  
- **계산:** 입력 토큰 시퀀스 \( X \)를 기반으로 **Matrix-Matrix 연산**을 수행.  
- **예시 입력:**  
  - 입력 문장: **"Who won the match?"**  
  - 토큰화된 입력: \( X = [w_1, w_2, w_3, w_4] \)  
    - \( w_1 = \text{Who}, \, w_2 = \text{won}, \, w_3 = \text{the}, \, w_4 = \text{match} \)  

- **계산 과정:**  
   \( X \)는 LLM의 **Embedding 레이어**를 통해 벡터로 변환되어 연산됩니다.  
   - Query (\( Q \)), Key (\( K \)), Value (\( V \)) 계산:  
     \[
     Q = X W_Q, \, K = X W_K, \, V = X W_V
     \]  
     - \( X \): 입력 토큰 벡터 (Batch Size = 1, Token 길이 = 4)  
     - \( W_Q, W_K, W_V \): 각 가중치 행렬  

   - Multi-Head Attention 연산:  
     \[
     \text{Attention}(Q, K, V) = \text{Softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
     \]  

   - 결과: 모델은 다음 토큰 **"Alex"**를 생성합니다.  

---

#### **2단계: Decode 단계**  
- **정의:** 첫 번째 출력 토큰부터 **한 번에 하나씩** 토큰을 생성하는 과정.  
- **특징:** 입력 토큰의 길이가 1로 고정되므로 **Matrix-Vector 연산**만 필요합니다.  

- **예시 입력:**  
  - 입력: 이전 단계에서 생성된 **토큰 "Alex"**  
    - \( X_{\text{new}} = [\text{Alex}] \)  

- **계산 과정:**  
   - Query, Key, Value 계산:  
     \[
     Q_{\text{new}} = X_{\text{new}} W_Q, \, K_{\text{new}} = X_{\text{new}} W_K, \, V_{\text{new}} = X_{\text{new}} W_V
     \]  
     - 가중치 행렬 \( W_Q, W_K, W_V \)는 이전 단계와 동일합니다.  

   - Attention 결과로 다음 토큰 **"won"**을 예측합니다.  

- **반복:** LLM은 이 단계를 반복하며, 입력에 새로 생성된 토큰을 추가해 출력 토큰을 생성합니다.  

---

## **2. 모델 아키텍처 구성**  

FlightLLM은 LLM 추론에 특화된 FPGA 기반 아키텍처로 **세 가지 주요 컴포넌트**로 구성됩니다.  

### **2.1 주요 구성 요소**  
1. **Matrix Processing Engine (MPE):**  
   - 핵심 역할: **Matrix-Matrix 연산**(Prefill 단계)과 **Matrix-Vector 연산**(Decode 단계)을 수행합니다.  
   - **구성:**  
     - Sparse DSP 체인: Sparse 패턴에 따라 유연하게 계산 리소스를 활용.  
     - Sparse MUX: 비-제로 요소만 선택해 DSP 체인에 전달.  
     - Reduction Node: 연산 결과를 집계하여 최종 출력.  

2. **Special Function Unit (SFU):**  
   - Softmax, LayerNorm과 같은 **비선형 연산**을 담당합니다.  
   - Fine-grained 병렬 처리를 지원해 연산 지연을 최소화합니다.  

3. **Memory Management Unit (MMU):**  
   - **온칩 메모리**와 **외부 메모리(HBM/DDR)**를 효율적으로 관리합니다.  
   - Always-On-Chip Decode 방식을 통해 디코딩 단계에서 메모리 병목을 해결합니다.  

---

### **2.2 모델 연산 요구량**  

1. **Prefill 단계 (Matrix-Matrix 연산):**  
   - 입력 길이 \( N \), Embedding 차원 \( d \), 가중치 \( W \)가 주어질 때, 연산량은 다음과 같습니다:  
     \[
     \text{연산량} \, = \, O(N \cdot d^2)
     \]  
     - \( N \): 입력 토큰 개수, \( d \): 차원 크기 (예: 4096)  

2. **Decode 단계 (Matrix-Vector 연산):**  
   - 입력 길이 \( N = 1 \)로 고정되므로, 연산량은:  
     \[
     \text{연산량} \, = \, O(d^2)
     \]  

3. **Memory 요구량:**  
   - LLaMA2-7B 모델 기준 메모리 요구량:  
     - 가중치 저장: **7B 파라미터 ≈ 14GB** (FP16 기준)  
     - Activation: **수십 MB** (디코딩 단계에서 1개 토큰 기준)  

---

## **3. 모델 메모리 및 컴퓨팅 요구량**  

### **3.1 메모리 요구량**  
1. **Prefill 단계:**  
   - 입력 토큰 수에 비례해 큰 메모리 대역폭이 필요합니다.  
   - 주요 메모리:  
     - **가중치 메모리** (Weights): HBM에 저장.  
     - **Activation 메모리**: 온칩 버퍼에 저장.  

2. **Decode 단계:**  
   - 입력 길이가 1이므로 메모리 요구량이 상대적으로 적습니다.  
   - **Always-On-Chip Decode**를 통해 Activation을 온칩 메모리에 유지해 메모리 병목을 제거합니다.

---

### **3.2 컴퓨팅 요구량**  
1. **Dense 연산:**  
   - Prefill 단계에서 **O(N \cdot d^2)** 만큼의 FLOPs가 필요합니다.  
2. **Sparse 연산:**  
   - N:M Sparse 패턴을 활용하면 연산량이 **50% 이상 감소**합니다.  
     - 예: **4:2 Sparse** → 50% 연산량 감소.  

---

## **정리: FlightLLM 추론 과정**  

| **단계**         | **주요 연산**           | **메모리 요구량**             | **연산량**               |
| ---------------- | ----------------------- | ----------------------------- | ------------------------ |
| **Prefill 단계** | Matrix-Matrix 연산 (MM) | 가중치 + 활성화 저장 (HBM)    | \( O(N \cdot d^2) \)     |
| **Decode 단계**  | Matrix-Vector 연산 (MV) | 온칩 메모리 유지 (Activation) | \( O(d^2) \)             |
| **모델 최적화**  | N:M Sparse DSP 체인     | Mixed-Precision 가중치 저장   | 연산량 최대 **50% 감소** |

---

### **결론**  
FlightLLM은 **Prefill**과 **Decode** 단계의 서로 다른 특성을 최적화하기 위해 Sparse DSP 체인과 온칩 메모리 전략을 사용합니다. 이러한 설계로 메모리 대역폭 병목과 컴퓨팅 리소스의 비효율성을 해결하며, GPU 대비 뛰어난 **에너지 효율성**과 **비용 효율성**을 달성했습니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **이 논문의 한계와 이를 극복하기 위한 연구 흐름**  

---

## **1. 논문의 한계점 요약**  

1. **FPGA의 리소스 제약**  
   - **문제:** FPGA는 GPU에 비해 계산 자원(DSP48 수)과 메모리 대역폭이 부족합니다. 특히 대규모 배치(batch)에서 GPU의 성능을 따라잡기 어렵습니다.  
   - **영향:**  
     - 대규모 토큰 입력(예: 긴 문장)을 처리할 때 **Prefill 단계**의 연산량이 FPGA 리소스 한계를 초과.  
     - **멀티 배치(batch size > 1)** 상황에서는 GPU 대비 성능 저하 발생.

2. **FPGA 프로그래밍 복잡도**  
   - **문제:** FPGA 최적화는 하드웨어 설계와 소프트웨어 간의 협업이 필수적입니다.  
   - **영향:**  
     - 소프트웨어 개발자가 FPGA에 LLM을 최적화하기 위해서는 **RTL 설계** 및 하드웨어 지식이 필요.  
     - 프로그래밍 복잡도가 높아 배포와 유지보수가 어려움.

3. **메모리 리소스와 대역폭**  
   - **문제:** FPGA의 메모리 용량과 대역폭은 GPU (특히 최신 NVIDIA A100/A800)보다 낮습니다.  
   - **영향:**  
     - 대규모 모델(예: GPT-3, LLaMA2-70B 등)의 추론을 위한 파라미터 전체를 FPGA에 저장하기 어려움.  
     - **HBM 대역폭**이 GPU의 **HBM2e**나 **HBM3**에 비해 열세.

4. **Sparse 최적화의 한계**  
   - **문제:** Sparse 연산은 계산량을 줄여주지만, **비-제로 요소의 불균형**이나 **동적 희소성**이 존재하면 효율이 저하될 수 있음.  
   - **영향:**  
     - Sparse 패턴이 정적이지 않고 동적일 경우, Sparse DSP 체인의 활용도가 낮아질 수 있음.  

---

## **2. 한계를 극복하기 위한 연구 흐름**  

### **2.1 FPGA의 리소스 확장 및 하드웨어 발전**  

#### **1. 차세대 FPGA 플랫폼 도입**  
- 최신 FPGA는 더 많은 DSP, 메모리 용량 및 높은 대역폭을 제공합니다.  
   - **AMD/Xilinx Versal 시리즈:**  
     - **Versal VHK158 FPGA**는 **819 GB/s 대역폭**과 **32GB HBM** 제공.  
     - 향후 출시될 **Versal HBM3**는 GPU의 최신 메모리 수준을 따라잡을 가능성이 높음.  

#### **2. Multi-FPGA 클러스터화**  
- 여러 FPGA를 **병렬 연결**하여 대규모 LLM 추론을 수행하는 방법.  
   - **기술:**  
     - FPGA 간 **고속 상호 연결 네트워크**(예: PCIe Gen5, UPI)를 활용.  
     - LLM의 **모델 병렬화**와 **파이프라인 병렬화**를 적용해 연산을 분산 처리.  
   - **관련 연구:**  
     - **DFX**는 Multi-FPGA 가속기를 사용해 트랜스포머 기반 모델의 디코딩을 병렬화.  

#### **3. FPGA-GPU 하이브리드 시스템**  
- FPGA와 GPU를 결합한 **하이브리드 컴퓨팅 시스템**.  
   - **FPGA 역할:** Sparse 연산과 디코딩 단계를 담당해 GPU의 병목 해소.  
   - **GPU 역할:** Dense 연산 및 Prefill 단계의 대규모 행렬 계산 수행.  
   - **관련 연구 흐름:**  
     - **Heterogeneous Computing Frameworks**: OpenCL, SYCL 등으로 FPGA와 GPU를 통합적으로 제어.  

---

### **2.2 하드웨어/소프트웨어 협력 최적화**  

#### **1. 고급 컴파일러 및 자동화 도구 개발**  
- FPGA의 **프로그래밍 복잡도**를 낮추기 위해 고수준 합성(HLS)과 **자동 최적화** 툴이 필요합니다.  
   - **방향:**  
     - 모델 구조와 Sparse 패턴을 자동 분석하여 최적화된 RTL 코드 생성.  
     - 사용자 정의 모델에 맞춘 **자동 길이 적응형 컴파일러** 개선.  
   - **기술 사례:**  
     - **HLS 2.0**과 같은 AI 모델 최적화 도구가 연구되고 있음.  
     - NVIDIA의 **TensorRT**와 같은 FPGA용 모델 최적화 툴 개발 필요.

#### **2. 동적 Sparse 최적화**  
- Sparse 패턴이 입력 데이터에 따라 동적으로 변하는 문제를 해결하기 위해:  
   - **런타임 스케줄러**를 도입하여 FPGA에서 동적으로 Sparse 패턴을 재구성.  
   - Sparse 데이터의 불균형을 보완하는 **로드 밸런싱** 기법 적용.  
   - **관련 연구:**  
     - **Dynamic Sparsity Scheduling (DSS)**: 비-제로 요소의 위치를 런타임에서 최적화.  

---

### **2.3 메모리 병목 해소를 위한 연구**  

#### **1. 메모리 압축 및 분할**  
- 대규모 모델의 파라미터를 압축하거나 분할해 FPGA의 메모리 용량 한계를 극복합니다.  
   - **압축 기법:**  
     - **저비트 양자화(Mixed Precision):** INT8 이하의 저비트 가중치 사용.  
     - **파라미터 공유:** 모델의 비슷한 가중치를 그룹화하여 저장.  

#### **2. 하이브리드 메모리 시스템 최적화**  
- FPGA의 HBM과 DDR 메모리를 **계층적**으로 활용해 데이터 접근을 최적화합니다.  
   - 자주 사용하는 데이터 (예: Activation, KV Cache)는 **HBM**에 저장.  
   - 덜 자주 접근하는 가중치는 **DDR**에 분산 저장.  

---

### **2.4 대규모 모델의 효율적 분산 추론**  

#### **1. 파이프라인 모델 병렬화**  
- LLM을 **Layer-wise 병렬화**하여 여러 FPGA/하드웨어 장치에서 동시에 연산합니다.  
   - **기술:**  
     - Transformer 모델의 각 층을 여러 FPGA에 분산하여 계산.  

#### **2. Efficient Token Parallelism**  
- 생성된 토큰을 **동시에 병렬로 처리**하는 기법 연구.  
   - GPU가 **Prefill** 단계에서 빠른 처리 속도를 보이는 것처럼, FPGA에서도 토큰 병렬화 구현 필요.  

---

## **정리: 연구 흐름 요약**  

| **한계점**              | **극복 방법**                         | **연구 흐름**                             |
| ----------------------- | ------------------------------------- | ----------------------------------------- |
| FPGA 리소스 제약        | 차세대 FPGA, Multi-FPGA 클러스터화    | Versal FPGA, Multi-FPGA 병렬화 연구       |
| 프로그래밍 복잡도       | 자동화된 컴파일러 및 최적화 도구 개발 | HLS 2.0, 런타임 최적화 프레임워크         |
| 메모리 용량 및 대역폭   | 압축, 하이브리드 메모리 관리          | Mixed-Precision, 데이터 분할 및 압축 기법 |
| 동적 Sparse 패턴 최적화 | 런타임 Sparse 스케줄링 및 로드 밸런싱 | DSS (Dynamic Sparsity Scheduling)         |
| 대규모 배치 병목        | 파이프라인 모델 병렬화, 토큰 병렬화   | Token Parallelism, Layer-wise 병렬화 연구 |

---

### **결론**  
논문에서 제기된 한계를 극복하기 위해서는 차세대 FPGA 플랫폼, **자동화된 소프트웨어 도구**, **동적 Sparse 최적화**, **메모리 효율화** 등이 필요합니다. 또한 FPGA와 GPU를 결합한 **하이브리드 시스템** 연구와 Multi-FPGA 클러스터화를 통해 LLM 추론 성능을 극대화하는 방향이 중요합니다. 이를 통해 FlightLLM의 잠재력을 더욱 확장하고 대규모 배치 및 모델에서도 GPU와 경쟁 가능한 솔루션으로 발전할 수 있을 것입니다.

# Q: FlightLLM이 Sparse DSP 체인을 통해 다양한 N:M 희소 패턴을 지원한다고 했는데, 실제로 이러한 패턴이 기존 GPU의 구조적 희소성(예: 2:4 패턴)과 비교해 계산 효율성을 어떻게 개선하는지 설명할 수 있는가?  

# A: 

### **FlightLLM의 N:M 희소 패턴과 기존 GPU 구조적 희소성 비교**  

---

## **1. 기존 GPU의 구조적 희소성 (2:4 패턴)**  

**GPU의 특징:**  
- GPU는 **고정된 구조적 희소성**만 지원합니다. 대표적으로 **2:4 희소 패턴**이 있습니다.  
- **2:4 패턴**: 4개의 요소 중 정확히 2개만 비제로 값을 가지도록 강제합니다.  

**장점:**  
- 고정된 패턴이므로 **SIMD 구조**와 함께 하드웨어 최적화가 용이합니다.  
- 정해진 패턴에 맞게 연산을 병렬화하여 효율적으로 처리할 수 있습니다.  

**한계:**  
- **유연성 부족**: 다양한 희소 패턴(N:M)이나 불규칙한 희소성을 지원하지 못합니다.  
- 고정된 2:4 패턴을 맞추기 위해 모델의 성능이 손실될 수 있습니다.  
  - 예시: **75% 희소성**을 원해도 2:4 패턴(50%)만 적용 가능해 더 많은 연산이 필요합니다.  
- Sparse 패턴이 제한적이어서 LLM 모델의 구조와 잘 맞지 않으면 **DSP나 계산 리소스 활용도가 낮아집니다.**

---

## **2. FlightLLM의 N:M 희소 패턴**  

**N:M 패턴의 특징:**  
- **N:M 희소 패턴**: N개의 요소 중 M개가 비제로인 패턴으로, N과 M이 더 자유롭게 설정될 수 있습니다.  
  - 예: 16:4, 16:8, 4:2 등 다양한 비율의 희소성을 지원.  

**구현 방법:**  
- FlightLLM은 **Sparse DSP 체인**을 도입하여 FPGA의 **DSP48 유닛**을 유연하게 구성합니다.  
- **Sparse MUX**를 사용해 비-제로 요소만 DSP에 전달하고, **Reduction Node**로 결과를 집계합니다.  
- N:M 패턴을 기반으로 DSP 체인을 **동적으로 그룹화**하여 더 많은 연산을 병렬로 처리합니다.  

---

## **3. 계산 효율성 비교: GPU vs FlightLLM**  

| **비교 항목**       | **기존 GPU (2:4)**                            | **FlightLLM (N:M)**                                    |
| ------------------- | --------------------------------------------- | ------------------------------------------------------ |
| **희소 패턴**       | 고정된 2:4 패턴만 지원                        | 다양한 N:M 패턴 지원 (예: 4:2, 8:4, 16:4 등)           |
| **유연성**          | 희소 패턴이 모델에 맞지 않으면 성능 저하      | 희소 패턴을 모델 특성에 맞게 동적으로 구성             |
| **연산 효율**       | 일부 DSP나 Tensor Core가 비효율적으로 동작    | 비-제로 요소만 선택해 DSP 자원 활용률 극대화           |
| **모델 성능 손실**  | 고정 패턴에 맞추기 위해 모델 정확도 손실 발생 | 더 적은 비-제로 값으로도 성능을 유지하면서 연산량 절감 |
| **하드웨어 활용도** | Sparse 패턴이 불규칙하면 자원이 낭비됨        | Sparse MUX와 Reduction Node로 자원 낭비 방지           |

---

## **4. 개선된 계산 효율의 원인**  

1. **유연한 Sparse 패턴 지원:**  
   - 모델의 특성에 따라 **가변적인 희소성**을 적용할 수 있어 연산량을 줄이면서 정확도를 유지합니다.  
   - 예를 들어, 모델의 특정 층에서 **75% 희소성**이 필요하면 16:4 패턴을 적용해 DSP를 효율적으로 사용합니다.  

2. **Sparse DSP 체인의 효율적 활용:**  
   - Sparse MUX를 사용하여 비-제로 요소만 선택 → DSP 체인이 **비어있는 시간**을 최소화합니다.  
   - Reduction Node를 통해 여러 결과를 동시에 집계 → **중복 연산 방지**.  

3. **비-제로 데이터만 처리:**  
   - GPU는 고정 패턴이지만, FlightLLM은 Sparse 패턴의 유연성 덕분에 **불필요한 0값 연산을 완전히 제거**합니다.  

4. **더 높은 하드웨어 활용률:**  
   - GPU는 고정 패턴으로 인해 일부 계산 유닛이 비어있을 수 있으나, FlightLLM은 **N:M 패턴을 동적으로 최적화**해 모든 DSP 자원을 효과적으로 사용합니다.  

---

## **5. 결론: 계산 효율성의 개선**  
FlightLLM이 지원하는 **N:M 희소 패턴**은 GPU의 고정된 2:4 구조적 희소성과 달리 더 높은 **유연성**과 **효율성**을 제공합니다. Sparse DSP 체인을 통해 FPGA의 계산 자원을 최대한 활용하며, 비-제로 요소만 선택적으로 처리함으로써 **연산량을 절감**하고 **하드웨어 활용률**을 극대화합니다.  
따라서, 동일한 희소성을 적용할 때 GPU 대비 더 적은 자원으로도 **모델 성능을 유지하면서** 높은 계산 효율을 달성합니다.

# Q: Always-On-Chip Decode 방식이 디코딩 단계에서 메모리 병목을 해소한다고 했는데, 이 접근법이 대규모 배치나 멀티 토큰 생성 상황에서도 동일한 성능 개선을 보일 수 있는가?  

# A: 

### **Always-On-Chip Decode 방식의 한계와 대규모 배치 및 멀티 토큰 생성 시 성능 개선 여부**  

---

## **1. Always-On-Chip Decode 방식 개요**  

**목표:**  
디코딩 단계에서 **활성화 벡터(Activation)**를 **온칩 메모리**에 유지해 외부 메모리(HBM/DDR) 접근을 최소화하고, **메모리 병목**을 해소합니다.  

**핵심 원리:**  
- 디코딩 단계에서는 **입력 길이 = 1**이므로, 연산량이 줄어드는 대신 **메모리 접근 빈도**가 증가합니다.  
- FlightLLM은 **온칩 메모리**에 활성화 데이터를 유지하고, 계산 결과를 바로 다음 연산에 사용하도록 **Operator Fusion** 기법을 적용합니다.  
- 결과적으로 외부 메모리(HBM)로의 접근을 줄여 **메모리 대역폭 활용률**을 크게 향상시킵니다 (35.6% → 65.9%).

---

## **2. 대규모 배치 및 멀티 토큰 생성 상황의 특성**  

### **1. 대규모 배치 (Batch Size > 1)**  
- **특징:** 여러 문장 또는 입력 시퀀스를 동시에 처리하는 상황입니다.  
- **문제점:** 각 배치의 활성화 데이터를 모두 **온칩 메모리**에 유지하기 어렵습니다.  
  - FPGA의 **온칩 메모리 용량** (URAM, BRAM)은 GPU의 **L2 캐시**나 **HBM**에 비해 작기 때문입니다.  
- **영향:**  
  - 배치 크기가 커질수록 온칩 메모리에 저장할 수 있는 활성화 데이터의 양이 제한됩니다.  
  - 일부 활성화 데이터는 결국 **HBM**이나 **DDR**로 이동하게 되어 메모리 병목이 다시 발생할 수 있습니다.  

---

### **2. 멀티 토큰 생성 (Multi-token Generation)**  
- **특징:** LLM이 **한 번에 여러 토큰**을 생성하는 상황입니다.  
  - 예: 디코딩 단계에서 1개의 입력 토큰이 아니라 **n개의 토큰**을 동시에 처리합니다.  
- **문제점:**  
  - 멀티 토큰을 생성하면 각 토큰에 대해 별도의 **활성화 벡터**와 **가중치 연산**이 필요합니다.  
  - 온칩 메모리에는 1개의 토큰에 대한 활성화 데이터만 유지하기 때문에 **메모리 관리**가 더 복잡해집니다.  
- **영향:**  
  - 멀티 토큰 상황에서는 각 토큰의 활성화 벡터가 온칩 메모리에 저장되지 못하고 **외부 메모리**로 이동하게 됩니다.  
  - 결국 Always-On-Chip Decode 방식의 효율이 낮아질 수 있습니다.

---

## **3. 성능 개선 여부 분석**  

### **1. 대규모 배치 상황**  
- **온칩 메모리 한계:**  
  - FPGA의 온칩 메모리는 **단일 토큰**을 처리할 때 효율적이지만, 대규모 배치의 모든 활성화 데이터를 저장하기에는 용량이 부족합니다.  
- **메모리 접근 빈도 증가:**  
  - 일부 데이터는 온칩 메모리에 유지될 수 있지만, 나머지는 **HBM** 또는 **DDR**에 저장되므로 **메모리 병목**이 다시 발생합니다.  
- **결론:**  
  Always-On-Chip Decode 방식은 **Batch Size = 1**일 때 가장 효과적입니다. 대규모 배치에서는 성능 개선이 제한적일 수 있습니다.  

---

### **2. 멀티 토큰 생성 상황**  
- **Operator Fusion의 한계:**  
  - Fusion 방식은 하나의 토큰에 대한 **연속적인 연산**을 온칩 메모리에서 수행하기 때문에 효율적입니다.  
  - 멀티 토큰 생성 시, 각 토큰의 연산을 **동시에 처리**해야 하므로 온칩 메모리의 **캐시 히트율**이 낮아집니다.  
- **메모리 대역폭 병목:**  
  - 여러 활성화 벡터를 외부 메모리로 이동하면서 병목이 다시 발생할 수 있습니다.  
- **결론:**  
  Always-On-Chip Decode 방식은 **단일 토큰 생성**에 최적화되어 있으며, 멀티 토큰 생성 시 성능 개선이 크게 감소합니다.

---

## **4. 해결 방안과 연구 방향**  

### **1. 온칩 메모리 확장**  
- FPGA에서 URAM/BRAM을 더 효율적으로 사용하여 **온칩 메모리 용량**을 확장하는 연구가 필요합니다.  
- **방법:**  
  - **가중치 공유 기법**: 동일한 레이어의 가중치를 공유해 메모리 사용량을 줄입니다.  
  - **데이터 압축**: 활성화 벡터를 저비트 양자화(INT4/INT8)로 저장하여 온칩 메모리 활용률을 높입니다.  

### **2. 계층적 메모리 관리**  
- 온칩 메모리와 외부 메모리(HBM/DDR)를 **계층적으로 관리**하는 접근법이 필요합니다.  
  - 자주 접근하는 데이터(예: 활성화 벡터)는 온칩 메모리에 유지하고, 나머지는 외부 메모리에 저장.  
- **기술 사례:**  
  - GPU의 **L1/L2 캐시 관리 기법**을 FPGA에 적용하는 연구.  

### **3. 병렬 처리 최적화**  
- 대규모 배치 및 멀티 토큰 상황에서도 Always-On-Chip Decode 방식을 유지하려면 **데이터 병렬화**가 필요합니다.  
  - **방법:**  
    - 여러 토큰을 동시에 처리하되, 각 토큰의 연산을 병렬로 분할하여 온칩 메모리에 맞게 조정.  
    - **Layer-wise 병렬화**: 모델의 각 층을 여러 FPGA 코어에 분산 처리.

---

## **5. 결론**  
Always-On-Chip Decode 방식은 **단일 배치(batch size = 1)**와 **단일 토큰** 추론에 최적화된 접근법입니다. 하지만 **대규모 배치**나 **멀티 토큰 생성** 상황에서는 온칩 메모리의 용량 한계와 데이터 병렬 처리의 어려움으로 인해 성능 개선이 제한적일 수 있습니다. 이를 극복하기 위해 **온칩 메모리 확장**, **계층적 메모리 관리**, **병렬 처리 최적화**와 같은 추가 연구가 필요합니다.  
이러한 연구를 통해 FPGA의 메모리 병목을 더욱 완화하고, 대규모 추론 환경에서도 Always-On-Chip Decode의 효율을 유지할 수 있을 것입니다.  

# Q: 길이 적응형 컴파일이 FPGA의 메모리 제약을 극복하는 데 중요한 역할을 하지만, 입력 길이의 동적 변화가 매우 클 경우 명령어 재사용에 따른 성능 저하가 발생하지는 않는가? 이에 대한 해결 방안은 무엇인가?  

# A: 

### **길이 적응형 컴파일의 한계와 성능 저하 문제**  

---

## **1. 길이 적응형 컴파일 개요**  

**목적:**  
- FPGA의 **제한된 메모리 용량**을 고려해 입력 및 출력 토큰 길이에 따라 **명령어(Instruction)를 재사용**하여 저장 공간을 줄이는 방식입니다.  
- 명령어 파일의 크기를 크게 줄여 FPGA의 **DDR 메모리**에 저장 가능한 수준(1.67TB → 3.25GB)으로 축소합니다.  

**핵심 원리:**  
- 입력 토큰 길이를 **그룹화**하여 비슷한 길이의 입력은 동일한 명령어를 재사용하도록 설정합니다.  
- 예를 들어, 입력 길이 **1~16 토큰**은 동일한 명령어를 사용하고, 17~32 토큰은 또 다른 명령어를 재사용하는 방식입니다.  

---

## **2. 입력 길이의 동적 변화에 따른 문제점**  

1. **동적 입력 길이로 인한 성능 저하**  
   - 입력 길이가 매우 다양하고 **동적**으로 변화하면 그룹화된 명령어가 실제 입력에 최적화되지 않을 수 있습니다.  
   - 특히 **짧은 입력**에 최적화된 명령어를 **긴 입력**에 적용하거나 반대로 긴 입력 명령어를 짧은 입력에 사용할 경우 **불필요한 연산**이 발생합니다.  

2. **메모리 낭비**  
   - 입력 토큰 길이의 편차가 클수록, 명령어 그룹화에 따른 **오버헤드**가 커져 성능이 저하될 수 있습니다.  
   - 예를 들어, 33토큰 입력이 48토큰 명령어에 맞춰 처리될 경우 불필요한 명령어 실행이 늘어나게 됩니다.  

3. **리소스 활용률 저하**  
   - FPGA의 **DSP 리소스와 메모리 대역폭**이 입력 길이의 동적 변화에 맞게 최적화되지 못해 활용률이 떨어질 수 있습니다.  
   - 결과적으로 계산 지연과 메모리 병목이 발생할 수 있습니다.  

---

## **3. 해결 방안**  

### **1. 동적 명령어 생성 및 최적화**  
- **해결 원리:** 입력 길이에 맞춰 명령어를 **런타임에서 동적으로 생성**하고 최적화합니다.  
- **방법:**  
   - **명령어 템플릿**을 사용해 기본 구조를 미리 정의하고, **동적 파라미터**(예: 토큰 길이)를 런타임에 삽입.  
   - 입력 길이에 따라 필요한 부분만 명령어를 생성하여 실행합니다.  
- **장점:**  
   - 불필요한 명령어 실행을 줄이고, 입력 길이에 맞춘 **최적화된 명령어**를 사용하여 성능을 유지.  
- **한계:**  
   - 동적 명령어 생성에는 추가적인 제어 로직이 필요하며, FPGA의 **컨트롤 오버헤드**가 발생할 수 있습니다.

---

### **2. 하이브리드 명령어 재사용 전략**  
- **해결 원리:** 명령어를 **세분화된 그룹**과 **유연한 재사용** 방식으로 결합합니다.  
- **방법:**  
   - 명령어 그룹을 더 **세분화**하여 입력 길이에 맞춘 재사용을 유도합니다.  
   - 예를 들어, 입력 길이를 **1~4, 5~8, 9~16, 17~32**와 같이 더 세밀하게 나눕니다.  
   - 동적으로 길이 변화가 큰 경우, 하위 명령어를 **부분적으로 실행**하여 필요한 연산만 수행하도록 구성.  
- **장점:**  
   - 입력 길이 편차가 클 경우에도 성능 저하를 최소화할 수 있습니다.  
   - 세분화된 명령어 그룹을 사용하면 **불필요한 연산**이 줄어들어 효율이 향상됩니다.  
- **한계:**  
   - 세분화가 지나치면 명령어 파일의 크기가 다시 커질 수 있어 **메모리 공간**을 더 많이 사용할 수 있습니다.

---

### **3. 입력 길이 예측 기반 스케줄링**  
- **해결 원리:** 입력 길이를 **사전에 예측**하고 이에 맞춰 명령어를 할당하는 스케줄링 기법입니다.  
- **방법:**  
   - **히스토리 기반 예측**: 이전 입력 길이 패턴을 학습해 다음 입력 길이를 예측.  
   - **동적 조정**: 런타임에서 입력 길이가 예상보다 길거나 짧을 경우, **가장 가까운 명령어 그룹**을 동적으로 할당.  
- **장점:**  
   - 입력 길이의 변화에 선제적으로 대응할 수 있어 명령어 실행의 비효율성을 줄입니다.  
- **한계:**  
   - 입력 길이의 변화가 매우 불규칙하면 예측 정확도가 떨어질 수 있습니다.

---

### **4. 압축 및 계층적 명령어 관리**  
- **해결 원리:** 명령어 파일을 **압축**하여 저장하고, 필요할 때만 **해제**하는 방식입니다.  
- **방법:**  
   - **명령어 압축 알고리즘**을 사용해 재사용 명령어의 크기를 줄입니다.  
   - 자주 사용하는 명령어는 **온칩 메모리**에 유지하고, 나머지는 **외부 메모리**에서 필요 시 로드.  
- **장점:**  
   - FPGA의 **온칩 메모리** 사용량을 줄이고, 메모리 병목을 완화합니다.  
- **한계:**  
   - 압축/해제 과정에서 추가 연산이 필요하며, 시간 지연이 발생할 수 있습니다.  

---

## **4. 결론**  

길이 적응형 컴파일은 입력 길이를 **그룹화**하여 메모리 사용량을 줄이는 데 효과적이지만, **동적 입력 길이의 변화가 클 경우** 성능 저하가 발생할 수 있습니다. 이를 극복하기 위한 해결 방안은 다음과 같습니다:  

1. **동적 명령어 생성 및 최적화**: 런타임에서 입력 길이에 맞춘 명령어를 동적으로 생성.  
2. **하이브리드 명령어 재사용 전략**: 세분화된 명령어 그룹과 부분 실행을 결합해 불필요한 연산 최소화.  
3. **입력 길이 예측 기반 스케줄링**: 입력 길이를 예측하고 최적의 명령어 그룹을 동적으로 할당.  
4. **압축 및 계층적 명령어 관리**: 명령어를 압축 저장하고, 필요할 때만 로드하여 메모리 사용 최적화.  

이러한 접근법을 통해 **명령어 재사용의 비효율성**을 줄이면서 FPGA의 메모리 제약을 극복하고 **성능 저하를 최소화**할 수 있습니다.