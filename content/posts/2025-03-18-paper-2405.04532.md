---
categories:
- paper-review
- with-gpt
date: "2025-03-18"
title: 'QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving'
---

[논문 링크](https://arxiv.org/abs/2405.04532)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### **논문의 강점과 독창적인 지점**

#### **1. 강점**
- **저비트 정량화(Quantization)의 실용적 개선:** 기존 INT4 정량화 기법들이 클라우드 기반 LLM 서빙에서 성능 개선을 보이지 못하는 문제를 해결.
- **QoQ(W4A8KV4) 알고리즘 제안:** 
  - **4비트 가중치(W4), 8비트 활성화(A8), 4비트 KV 캐시(KV4)**를 적용하여 정량화에 따른 정확도 손실을 최소화.
  - **진행형 그룹 정량화(Progressive Group Quantization):** 8비트 중간 표현을 활용하여 INT8 텐서 코어에서 연산을 수행, 기존 INT4 방식보다 높은 성능 제공.
  - **SmoothAttention 기법:** 4비트 KV 정량화에 따른 정확도 저하를 완화하는 메커니즘.
- **QServe 시스템과 알고리즘 공동 설계(System-Algorithm Co-design):**
  - **GPU 서빙 성능 극대화:** CUDA 코어에서 수행되는 비효율적인 연산을 줄이고, 텐서 코어 활용도를 극대화.
  - **레지스터 수준 병렬성(Register-Level Parallelism) 활용:** INT4→INT8 변환 시 감산 후 곱셈(Subtraction after Multiplication) 방식을 적용하여 연산량 감소.
  - **연산 중심 가중치 재배열(Compute-aware Weight Reordering):** CUDA 코어에서의 포인터 연산량을 줄여 L1 캐시 활용 최적화.

#### **2. 독창적인 지점**
| 기존 기법                                  | QoQ (논문 기법)                       |
| ------------------------------------------ | ------------------------------------- |
| W4A4의 낮은 정확도 문제                    | W4A8로 INT8 텐서 코어 활용 가능       |
| W4A16의 높은 메모리 사용량                 | KV4 도입으로 메모리 효율 개선         |
| INT4 GEMM에서 발생하는 CUDA Core 연산 병목 | Register-Level Parallelism으로 해결   |
| 기존 KV 캐시 정량화의 정확도 저하          | SmoothAttention으로 키(Key) 값 정규화 |

---
### **핵심 알고리즘 과정 설명**
#### **1. QoQ 정량화 알고리즘**
QoQ는 두 단계의 정량화로 이루어짐:
1. **8비트 중간 정량화(Per-Channel INT8 Quantization)**
   - 가중치 \(W\)를 먼저 8비트로 정량화:
     \[
     Ŵ = QW_8 \cdot S_8
     \]
     여기서 \(S_8\)은 채널별 FP16 스케일 값.

2. **4비트 그룹 정량화(Per-Group INT4 Quantization)**
   - 8비트 정량화된 값을 다시 4비트로 변환:
     \[
     QW_8 = (QW_4 - Z_4) \cdot S_4
     \]
     여기서 \(Z_4\)는 그룹 단위의 4비트 제로포인트.

3. **보호 정량화 범위(Protective Quantization Range)**
   - INT4 → INT8 변환 시 오버플로 방지.
   - 8비트 값의 최대 범위를 \([-119, 119]\)로 제한.

---
#### **2. W4A8 GEMM 실행 과정 예시**
##### **입력 예시**
가중치(W), 활성화(X), KV 캐시를 아래처럼 가정:
```plaintext
W = [[-2, 1], [3, -4]]
X = [[5, -3], [2, 6]]
KV = [[-1, 3], [4, -2]]
```
##### **과정**
1. **정량화**
   - 8비트 정량화 후:
     ```plaintext
     QW_8 = [[-2, 1], [3, -4]]
     ```
   - 4비트 정량화 후:
     ```plaintext
     QW_4 = [[-1, 0], [2, -2]]
     ```
2. **GEMM 연산**
   - INT4 → INT8 변환 후 계산:
     ```plaintext
     Output = [[-2 * 5 + 1 * (-3)], [3 * 2 + (-4) * 6]]
            = [[-10 -3], [6 -24]]
            = [[-13], [-18]]
     ```
3. **부동소수점 복원**
   - \(Output\) 값을 다시 FP16으로 변환하여 최종 결과 생성.

---
### **논문의 한계점**
1. **정확도 손실 문제**
   - W4A8KV4에서도 FP16 대비 정확도가 다소 낮음.
   - 특정 과제(예: Winogrande)에서는 최대 4.82% 정확도 손실 발생.

2. **GPU 종속적인 최적화**
   - A100, L40S GPU에 최적화되어 있어 타 하드웨어에서 성능 보장이 어려움.

3. **복잡한 시스템 요구**
   - Progressive Quantization, Compute-aware Weight Reordering 등의 기법이 구현 복잡도를 증가시킴.

---
### **결론**
QoQ 알고리즘과 QServe 시스템을 결합하여 **LLM 서빙에서 기존 TensorRT-LLM 대비 최대 3.5배 속도 향상**을 이루었음. 그러나 여전히 정확도 손실과 GPU 종속성 등의 문제가 남아 있어 추가적인 최적화 연구가 필요함.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



## **QoQ(W4A8KV4) 알고리즘 상세 설명 및 예제**

### **1. QoQ 알고리즘 개요**
QoQ(Quattuor-Octō-Quattuor)는 **4비트 가중치(W4), 8비트 활성화(A8), 4비트 KV 캐시(KV4)**를 활용하는 정량화 방법으로, 기존 INT4 기반의 정량화 기법이 가지는 **GPU 성능 저하 문제를 해결**하고 **높은 처리량을 유지**할 수 있도록 설계됨.

QoQ는 다음의 두 가지 핵심 정량화 전략을 적용함:
1. **진행형 그룹 정량화(Progressive Group Quantization)**  
   - 기존 정량화가 가중치를 한 번에 4비트로 변환하는 것과 달리, **8비트로 먼저 정량화 후 다시 4비트로 변환**하는 2단계 과정 사용.
   - 이렇게 하면 **INT8 텐서 코어에서 연산을 수행할 수 있어 속도가 빨라짐**.
  
2. **SmoothAttention**  
   - 4비트 KV 캐시 정량화가 정확도 저하를 유발하는 문제를 해결.
   - **쿼리(Query) 벡터는 정량화하지 않고, 키(Key) 벡터의 이상값을 조정**하여 정밀도를 유지.

---

### **2. QoQ 알고리즘의 전체 과정**
#### **1) 초기 가중치 및 활성화 데이터**
일반적인 신경망의 가중치 행렬(W)과 입력 활성화 행렬(X), KV 캐시는 다음과 같은 형태:

```plaintext
W = [[-2.4, 1.1], [3.2, -4.8]]
X = [[5.3, -3.7], [2.9, 6.1]]
KV = [[-1.2, 3.4], [4.5, -2.7]]
```

---

### **3. QoQ 진행형 그룹 정량화 (Progressive Group Quantization)**
**기존 방법(W4A4)**는 가중치를 바로 4비트로 변환하여 정확도가 낮고 CUDA 코어에서 많은 연산을 필요로 함.  
QoQ는 **2단계로 정량화하여** INT8 텐서 코어에서 연산이 가능하도록 최적화.

#### **1단계: 8비트 중간 정량화**
- 우선 가중치를 **채널별 8비트로 변환(Per-Channel INT8 Quantization)**:
  \[
  Ŵ = QW_8 \cdot S_8
  \]
  여기서:
  - \(QW_8\) : 8비트로 변환된 가중치
  - \(S_8\) : 채널별 부동소수점 스케일 값 (FP16)

##### **예제: 8비트 변환**
```plaintext
원본 가중치(W) = [[-2.4, 1.1], [3.2, -4.8]]
최대 절댓값 = 4.8
스케일 (S8) = 4.8 / 127 ≈ 0.0378
정량화된 가중치 (QW8) = [[-64, 29], [85, -127]]
```

---

#### **2단계: 4비트 그룹 정량화**
- 8비트로 변환된 \(QW_8\)을 다시 **4비트로 변환(Per-Group INT4 Quantization)**:
  \[
  QW_8 = (QW_4 - Z_4) \cdot S_4
  \]
  여기서:
  - \(Z_4\) : 그룹별 4비트 제로포인트(Zero Point)
  - \(S_4\) : 그룹별 4비트 스케일 값

##### **예제: 4비트 변환**
```plaintext
QW8 = [[-64, 29], [85, -127]]
최대 절댓값 = 127
스케일 (S4) = 127 / 15 ≈ 8.47
제로포인트 (Z4) = 7 (가운데 값)
정량화된 가중치 (QW4) = [[-8, 4], [10, -15]]
```

---

#### **3단계: 보호 정량화 범위 적용 (Protective Quantization Range)**
- INT4 → INT8 변환 시 **값이 INT8 범위를 넘어서지 않도록** 제한.
- 8비트 범위를 \([-119, 119]\)로 줄여서 오버플로 방지.

##### **예제: 보호 범위 적용**
```plaintext
변환 전 값: 127 → 128 (INT8 범위를 초과)
보호 범위 적용 후: 127 → 119 (최대값 제한)
```

---

### **4. QoQ의 빠른 역정량화 (Efficient Dequantization)**
QoQ는 **CUDA 코어에서 실행되는 느린 연산을 줄이고, 텐서 코어를 최대한 활용**하기 위해 다음 기법을 적용.

#### **1) 감산 후 곱셈(Subtraction after Multiplication)**
- 기존 방법은 \(X \times W\) 연산 후, **FP32 변환 → 정량화된 값 변환** 순서로 실행됨.
- QoQ는 **FP16 변환을 마지막에 수행하여 연산량 감소**.

##### **기존 방식**
```plaintext
1. INT4 → FP16 변환
2. FP16 연산 수행
3. 결과를 다시 FP16으로 변환 (느림)
```
##### **QoQ 방식**
```plaintext
1. INT4 → INT8 변환 후 텐서 코어 연산 (빠름)
2. 마지막에 FP16 변환
```

---

#### **2) 가중치 재배열 (Compute-Aware Weight Reordering)**
- GPU의 L1 캐시에서 가중치를 로드할 때, **연속된 128비트 단위로 로드하도록 재배열**.
- 기존 방식은 **비연속적인 메모리 액세스로 인한 지연**이 발생하지만, **QoQ는 연속된 데이터 로딩으로 캐시 효율 증가**.

##### **예제**
```plaintext
기존 방식: W = [[-8, 4], [10, -15]]
QoQ 방식: W = [[-8, 10], [4, -15]] (메모리 연속성 개선)
```

---

### **5. SmoothAttention을 통한 KV4 정량화 보정**
QoQ는 4비트 KV 캐시 정량화로 인한 정확도 손실을 **SmoothAttention 기법**으로 보완.

#### **1) 문제점: 키(Key) 값의 이상값**
- 활성화 벡터(Key)에는 이상값이 많아 4비트 정량화 후 큰 오류가 발생.

##### **해결책: SmoothAttention 적용**
1. **쿼리(Query) 벡터는 정량화하지 않음** (더 높은 정밀도 유지).
2. **키(Key) 벡터의 이상값을 줄이기 위해 스무딩 계수(λ)를 적용**:
   \[
   K' = K \times λ
   \]
   여기서 \(λ = \max(|K|)^{0.5}\)

##### **예제**
```plaintext
원본 Key 벡터: [-1.2, 3.4, 4.5, -2.7]
최대값: 4.5
SmoothAttention 적용 후: [-0.9, 2.8, 3.7, -2.2] (이상값이 줄어듦)
```

---

### **6. QoQ의 전체 정리**
| 단계                     | 설명                              | 예제                          |
| ------------------------ | --------------------------------- | ----------------------------- |
| **1. 8비트 정량화**      | 가중치를 INT8로 변환              | QW8 = [[-64, 29], [85, -127]] |
| **2. 4비트 그룹 정량화** | 8비트 → 4비트 변환                | QW4 = [[-8, 4], [10, -15]]    |
| **3. 보호 정량화 범위**  | INT4 → INT8 변환 시 오버플로 방지 | 최대값 127 → 119로 제한       |
| **4. 빠른 역정량화**     | 감산 후 곱셈으로 연산량 감소      | INT4 → INT8 변환 후 연산 수행 |
| **5. SmoothAttention**   | KV 캐시 정량화로 인한 오류 보정   | Key 값 이상값 조정            |

QoQ는 기존 W4A4 대비 **2.4배 빠른 처리 속도**와 **더 높은 정확도**를 제공하는 강력한 정량화 기법.

# Q : 이 논문에서 사용하는 방법을 학습하기 위하여 어떤 데이터셋이 필요할까? 그리고 어떻게 학습을 진행하면 될지 예시를 들어서 아주 자세하게 설명해줘



## **QoQ(W4A8KV4) 학습을 위한 데이터셋 및 학습 방법 상세 설명**

### **1. 필요한 데이터셋**
QoQ 알고리즘을 적용하여 LLM을 학습하려면 다음과 같은 데이터셋이 필요함.

| 데이터셋 종류                            | 설명                                                                       | 예시                                      |
| ---------------------------------------- | -------------------------------------------------------------------------- | ----------------------------------------- |
| **언어 모델 학습 데이터**                | LLM이 일반적인 언어 이해 및 생성을 학습하는 데 필요한 대규모 텍스트 데이터 | **C4, The Pile, Wikipedia, OpenWebText**  |
| **정량화 검증 데이터**                   | 정량화된 모델의 성능을 평가하기 위한 데이터                                | **WikiText-2, C4 Validation Set**         |
| **Zero-shot/ Few-shot 평가 데이터**      | 모델이 실제 문제 해결 능력을 평가하기 위한 데이터                          | **ARC, PIQA, HellaSwag, WinoGrande**      |
| **정량화 보정 데이터 (Calibration Set)** | 정량화 시 정밀도를 보정하는 데 사용                                        | **The Pile Validation, Wikipedia Subset** |

QoQ 알고리즘을 적용하려면 먼저 **Pretraining → Fine-tuning → Quantization → Evaluation** 단계를 수행해야 함.

---

## **2. 학습 및 정량화 진행 방법**
QoQ를 적용한 모델을 학습하려면 다음과 같은 5단계를 따름.

1. **Pretraining (사전 학습)**
2. **Fine-tuning (미세 조정)**
3. **Calibration (정량화 보정)**
4. **Quantization (QoQ 정량화 적용)**
5. **Evaluation (성능 평가)**

각 단계별로 필요한 데이터셋과 절차를 **예제를 포함하여** 상세히 설명하겠음.

---

### **Step 1: Pretraining (사전 학습)**
QoQ를 적용할 모델(예: Llama, Mistral, Yi 모델 등)은 기본적으로 **대규모 언어 모델**이므로, 먼저 사전 학습이 필요함.

#### **✅ 데이터셋 선택**
- **C4 (Colossal Clean Crawled Corpus)**
- **The Pile (800GB 이상의 텍스트 데이터)**
- **Wikipedia, OpenWebText**
- **BooksCorpus, Common Crawl**

#### **✅ 학습 과정**
1. 모델의 입력 토큰을 토큰화함 (예: SentencePiece 사용).
2. Transformer 기반 LLM을 학습시킴.
3. 손실 함수: **Cross Entropy Loss** 사용.
4. GPU 또는 TPU에서 **mixed precision training (FP16 / BF16)**으로 학습 가속.

##### **📌 예제 코드 (PyTorch)**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
import datasets

# 데이터 로드
dataset = datasets.load_dataset("c4", split="train[:1%]")  # 1% 샘플 사용

# 토큰화
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7B-hf")
dataset = dataset.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length"), batched=True)

# 모델 로드
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7B-hf")

# 학습 설정
training_args = TrainingArguments(
    output_dir="./llama2_pretrain",
    per_device_train_batch_size=4,
    num_train_epochs=2,
    save_steps=500,
    fp16=True
)

trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()
```

---

### **Step 2: Fine-tuning (미세 조정)**
사전 학습된 모델을 특정 도메인(예: 법률, 의학) 또는 특정 태스크(예: 질의응답)에 맞게 미세 조정.

#### **✅ 데이터셋 선택**
- **OpenAI API 문서, StackOverflow** (코딩 태스크용)
- **MMLU (Massive Multitask Language Understanding)** (상식 기반 평가)
- **Alpaca, Dolly, FLAN-T5 데이터** (Instruction Tuning)

#### **✅ 학습 과정**
- **손실 함수:** Cross Entropy Loss 사용.
- **학습 방법:** LoRA, QLoRA와 함께 적용 가능 (메모리 효율적 학습).

##### **📌 예제 코드 (Fine-tuning with LoRA)**
```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM

# LoRA 설정
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1
)

# 모델 로드 후 LoRA 적용
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7B-hf")
model = get_peft_model(model, lora_config)

# Fine-tuning 수행
trainer.train()
```

---

### **Step 3: Calibration (정량화 보정)**
정량화를 적용하기 전에, **모델의 정량화 감도를 측정**하고 이를 보정하는 단계.

#### **✅ 데이터셋 선택**
- **The Pile Validation**
- **WikiText-2**

#### **✅ 보정 과정**
1. 모델이 특정 계층(layer)에서 **어느 부분에서 정확도 손실이 발생할 가능성이 있는지 분석**.
2. 가중치의 분포를 측정하고, FP16에서 INT4로 변환할 때 **Outlier(극단적인 값)가 많은 채널을 식별**.
3. SmoothAttention 및 Activation-aware Channel Reordering을 적용.

##### **📌 예제 코드 (Calibration with SmoothQuant)**
```python
from smoothquant import smooth_llm

# 사전 학습된 모델 로드
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7B-hf")

# SmoothQuant 적용
model = smooth_llm(model)
```

---

### **Step 4: QoQ Quantization (정량화 적용)**
이제 QoQ를 적용하여 모델을 **W4A8KV4 형식으로 변환**.

#### **✅ 정량화 과정**
1. 가중치(W) → **8비트 정량화** 후 **4비트 그룹 정량화** 적용.
2. 활성화(A) → **8비트 정량화** 적용.
3. KV 캐시 → **4비트 정량화** 후 SmoothAttention 적용.

##### **📌 예제 코드 (QoQ 적용)**
```python
from qserve import apply_qoq_quantization

# QoQ 적용 (W4A8KV4)
model = apply_qoq_quantization(model, weight_bit=4, act_bit=8, kv_bit=4)
```

---

### **Step 5: Evaluation (정량화 모델 성능 평가)**
QoQ 적용 후 모델 성능을 **Zero-shot/Few-shot 평가 데이터셋**으로 측정.

#### **✅ 평가 데이터셋**
- **PIQA (물리적 상식 평가)**
- **ARC (AI2 Reasoning Challenge)**
- **HellaSwag (문맥 추론)**
- **WinoGrande (문맥적 관계 추론)**

##### **📌 예제 코드 (Zero-shot 평가)**
```python
from evaluate import load
from transformers import pipeline

# 모델 로드
model.eval()
nlp = pipeline("text-generation", model=model)

# 평가 실행
results = load("piqa").compute(predictions=nlp("A ball is rolling down a hill. What happens next?"))
print(results)
```

---

## **결론: QoQ 학습 과정 요약**
| 단계             | 데이터셋     | 주요 작업          |
| ---------------- | ------------ | ------------------ |
| **Pretraining**  | The Pile, C4 | LLM 기본 학습      |
| **Fine-tuning**  | Alpaca, MMLU | 도메인별 미세 조정 |
| **Calibration**  | WikiText-2   | 정량화 보정        |
| **Quantization** | -            | W4A8KV4 변환       |
| **Evaluation**   | PIQA, ARC    | 모델 평가          |

QoQ를 적용하면 **기존 모델 대비 2.4~3.5배 성능 향상**이 가능하면서도, **정확도 손실을 최소화**할 수 있음. 🚀

# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘


## **논문의 실험 결과 분석 및 기존 방법과의 비교**

### **1. 논문에서 제시한 주요 결과**
논문은 **QoQ (W4A8KV4) + QServe 시스템**이 기존 방법론 대비 **최대 3.5배의 속도 향상**을 달성했다고 보고함. 특히, **TensorRT-LLM 대비 2.4~3.5배 성능 향상**이 가장 두드러짐.

#### **(1) WikiText-2 Perplexity (언어 모델 정확도)**
| 모델        | FP16     | W8A8 | W4A16 | W4A4 (QuaRot) | W4A4 (Atom) | W4A8KV4 (QoQ) |
| ----------- | -------- | ---- | ----- | ------------- | ----------- | ------------- |
| Llama-2-7B  | **5.47** | 5.54 | 5.60  | 6.10          | 6.03        | **5.75**      |
| Llama-2-13B | **4.88** | 4.95 | 4.97  | 5.40          | 5.27        | **5.12**      |
| Llama-2-70B | **3.32** | 3.36 | 3.41  | 3.79          | 3.69        | **3.52**      |

✅ **특출난 점**
- QoQ(W4A8KV4)는 **기존 4비트 모델(W4A4)보다 높은 정확도 유지** (QuaRot 대비 최대 0.49 perplexity 개선).
- W8A8, W4A16보다 약간 낮은 정확도지만, **2.4~3.5배 높은 속도 제공**.
- Atom(W4A4) 대비도 높은 정확도를 기록.

#### **(2) Zero-shot Task 성능 비교**
| 모델             | FP16       | W4A4 (QuaRot)   | W4A4 (Atom)     | W4A8KV4 (QoQ)       |
| ---------------- | ---------- | --------------- | --------------- | ------------------- |
| Llama-2-7B 평균  | **68.98%** | 64.69% (-4.29%) | 59.73% (-9.25%) | **67.22% (-1.76%)** |
| Llama-2-13B 평균 | **71.72%** | 69.01% (-2.71%) | 63.51% (-8.21%) | **70.56% (-1.16%)** |
| Llama-2-70B 평균 | **76.57%** | 75.43% (-1.14%) | 67.52% (-9.05%) | **75.91% (-0.66%)** |

✅ **특출난 점**
- W4A4(QuaRot, Atom) 대비 정확도 손실이 훨씬 적음.
- 특히 **Llama-2-70B에서 FP16과 비교해도 거의 차이가 없음**.

#### **(3) GPU 서빙 속도 (A100, L40S)**
| 모델               | TensorRT (W8A8) | QoQ (W4A8KV4) | 속도 향상 |
| ------------------ | --------------- | ------------- | --------- |
| Llama-2-7B (A100)  | 2334 tokens/s   | 2908 tokens/s | **1.25×** |
| Llama-2-13B (A100) | 1277 tokens/s   | 1741 tokens/s | **1.36×** |
| Llama-2-70B (A100) | 234 tokens/s    | 419 tokens/s  | **1.79×** |
| Llama-2-7B (L40S)  | 1271 tokens/s   | 2394 tokens/s | **1.88×** |
| Llama-2-13B (L40S) | 440 tokens/s    | 1327 tokens/s | **3.02×** |
| Qwen1.5-72B (L40S) | 53 tokens/s     | 340 tokens/s  | **6.42×** |

✅ **특출난 점**
- **L40S에서 특히 강력한 속도 향상** (W8A8 대비 3~6배 성능 개선).
- **A100에서도 여전히 1.25~1.79배의 성능 향상**.
- **Qwen1.5-72B와 같은 초대형 모델에서 속도 향상이 가장 크다 (6.42×).**

---

### **2. QoQ(W4A8KV4) vs 기존 방법 비교**
| 기법                     | 정확도 손실 | 속도 향상    | 주요 문제점                     |
| ------------------------ | ----------- | ------------ | ------------------------------- |
| **FP16**                 | 0%          | 1.0×         | 속도가 느리고, 메모리 사용량 큼 |
| **W8A8 (TensorRT-LLM)**  | -0.1%       | 1.0×         | 메모리 사용량이 여전히 높음     |
| **W4A16 (TensorRT-LLM)** | -0.2%       | 1.1×         | 16비트 활성화로 연산량이 많음   |
| **W4A4 (QuaRot)**        | -3~4%       | 0.57×        | CUDA 코어 연산 병목 발생        |
| **W4A4 (Atom)**          | -6~9%       | 0.38×        | 정량화된 부분에서 오버플로 발생 |
| **W4A8KV4 (QoQ)**        | -0.6~1.7%   | **2.4~3.5×** | **최상의 속도-정확도 균형**     |

---

### **3. QoQ가 높은 성능을 낼 수 있었던 이유**
논문에서는 **QoQ의 구조적 개선**이 이러한 결과를 만들어낸다고 주장함.

#### **(1) Progressive Group Quantization (진행형 그룹 정량화)**
- 기존 방법(W4A4)은 **INT4 → FP32 변환**이 필요하여 CUDA 코어에서 병목 발생.
- QoQ는 **INT4 → INT8** 변환 후 INT8 텐서 코어에서 연산을 수행하여 **속도 2배 향상**.

#### **(2) SmoothAttention (스무딩 기법)**
- 기존 4비트 KV 캐시는 **키(Key) 벡터 이상값으로 인해 정확도 손실이 컸음**.
- SmoothAttention은 **쿼리(Query) 벡터는 유지하고 키(Key) 벡터만 정규화**하여, **정확도 저하를 방지**.

#### **(3) Compute-aware Weight Reordering (가중치 재배열)**
- 기존 방법(W4A4)은 **메모리 불연속성 문제**로 인해 캐시 활용도가 낮음.
- QoQ는 **가중치 로딩 순서를 재구성**하여 **메모리 대역폭 활용 최적화**.

#### **(4) Efficient Dequantization (효율적인 역정량화)**
- 기존 방법은 **역정량화(Dequantization)** 과정에서 CUDA 코어 연산이 많았음.
- QoQ는 **레지스터 수준 병렬화(Register-Level Parallelism, RLP)** 적용하여 연산량 절반 감소.

---

### **4. 내 생각**
논문의 결과를 보면 QoQ는 **정확도와 속도를 모두 잡은 정량화 기법**으로 보임.  
특히 다음과 같은 점에서 기존 방법론보다 우수함:

1. **QoQ는 기존의 W4A4 방식보다 훨씬 빠르면서도 정확도가 높다.**
   - QuaRot, Atom 같은 기존 4비트 정량화 기법들은 **CUDA 코어 병목 현상**이 심했음.
   - QoQ는 **INT8 텐서 코어를 활용하여 속도를 개선**했고, **SmoothAttention으로 정확도 저하를 막음**.

2. **A100, L40S 같은 GPU에서 일관되게 높은 성능을 보여준다.**
   - 기존 W4A4 기반 Atom, QuaRot 방식은 A100에서조차 성능이 TensorRT-LLM보다 낮았음.
   - QoQ는 A100에서도 **최대 1.8배 향상**, L40S에서는 **3배 이상의 성능 향상**을 보임.

3. **Qwen1.5-72B 같은 초대형 모델에서 6.42배 성능 향상은 놀라운 결과다.**
   - 이는 **QoQ의 메모리 효율성이 크기 때문**이라고 보임.
   - W4A8KV4 구조 덕분에 **기존보다 훨씬 큰 모델을 더 빠르게 서빙할 수 있음**.

### **결론**
QoQ는 단순히 4비트 정량화를 적용한 것이 아니라, **진행형 정량화(Progressive Quantization), 스무딩 기법(SmoothAttention), 가중치 재배열(Compute-aware Weight Reordering)** 등의 혁신적인 기법을 도입하여 **속도와 정확도를 모두 향상**시킨 방법론임. 🚀

# Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니? 



## **QoQ(W4A8KV4) + QServe 적용 시 필요한 추가 구현 요소 및 리소스 분석**

논문의 방법론(QoQ + QServe)을 기존 LLM 서빙 환경(TensorRT-LLM 등)에 적용하려면 **추가적인 구현 작업 및 컴퓨팅 자원**이 필요함.  

이를 위해, 먼저 **기존 방법과의 차이점을 기반으로 추가 구현이 필요한 부분을 분석**하고,  
그에 따른 **연산량 증가 및 GPU 리소스 소요량을 계산**해 보겠음.

---

## **1. QoQ(W4A8KV4) 적용을 위한 추가 구현 작업**
QoQ는 기존 TensorRT-LLM 및 W8A8 방식과 비교하여 다음과 같은 **추가 구현이 필요**함.

### **(1) 추가적으로 필요한 구현 요소**
| 구현 요소                                                           | 설명                                    | 추가 연산량                |
| ------------------------------------------------------------------- | --------------------------------------- | -------------------------- |
| **진행형 그룹 정량화 (Progressive Group Quantization)**             | 8비트 정량화 후 4비트 변환 적용 (2단계) | INT8 연산량 2배 증가       |
| **SmoothAttention (KV4 정량화 보정)**                               | Key 벡터 스무딩 (λ 값 계산 및 적용)     | 추가 행렬 연산 (O(N))      |
| **Compute-aware Weight Reordering (가중치 재배열)**                 | CUDA 메모리 접근 방식 최적화            | GPU 메모리 정렬 추가 비용  |
| **Efficient Dequantization (레지스터 병렬화 및 감산 후 곱셈 적용)** | INT4 → INT8 변환 과정 개선              | INT4 → INT8 변환 연산 추가 |

✅ **핵심 포인트**  
- **QoQ를 적용하려면 기존 모델에 정량화 과정이 추가됨 (8비트 → 4비트 2단계 변환).**
- **SmoothAttention은 키(Key) 벡터 스무딩을 수행해야 하므로 추가 연산이 필요.**
- **Compute-aware Weight Reordering은 메모리 접근 방식을 변경해야 하므로 가중치 저장 방식이 수정됨.**

---

## **2. 연산량 비교 (QoQ vs 기존 방식)**
이제, **각 연산의 FLOPs (Floating Point Operations per Second) 증가량**을 계산하여, QoQ 적용 시 추가되는 컴퓨팅 비용을 분석.

### **(1) GEMM 연산량 비교**
GEMM (General Matrix Multiplication) 연산량은 **기존 W8A8 대비 QoQ에서 2배 증가**.

| 방법                    | 연산량 (FLOPs) | 추가 연산 비율 |
| ----------------------- | -------------- | -------------- |
| **W8A8 (기존 방식)**    | \(O(N^3)\)     | 1.0×           |
| **W4A8 (QoQ)**          | \(O(2N^3)\)    | **2.0×**       |
| **W4A4 (QuaRot, Atom)** | \(O(1.5N^3)\)  | 1.5×           |

**➡ QoQ는 TensorRT-LLM(W8A8) 대비 GEMM 연산량이 약 2배 증가**  

---

### **(2) 추가 연산량: SmoothAttention (Key Smoothing)**
SmoothAttention은 **키(Key) 벡터의 이상값을 보정**하는 역할을 함.  
- 키(Key) 벡터에서 최대값을 찾고, 이를 기반으로 정규화 (\(\lambda\))를 수행.  
- 키(Key) 크기가 \(N \times D\)일 때, 연산량은 \(O(ND)\).  

#### **연산량 비교**
| 방법                           | 추가 연산량 (FLOPs) | 영향     |
| ------------------------------ | ------------------- | -------- |
| **W8A8 (기존)**                | \(O(ND)\)           | 없음     |
| **QoQ (SmoothAttention 적용)** | \(O(2ND)\)          | 2배 증가 |

**➡ KV 캐시 정량화에 따른 연산량이 증가하지만, 키(Key) 벡터만 조정하므로 영향은 적음.**  

---

### **(3) 메모리 사용량 비교**
QoQ는 **INT4 (4비트) 정량화를 적용**하므로,  
**메모리 사용량은 W8A8 대비 절반 수준**으로 줄어듦.

#### **KV 캐시 메모리 사용량**
| 방법              | 메모리 사용량 (MB) |
| ----------------- | ------------------ |
| **W8A8**          | \(O(8N)\)          |
| **W4A8KV4 (QoQ)** | \(O(4N)\)          |

**➡ 메모리는 절반으로 감소하지만, 레지스터 사용량 증가로 인해 일부 추가 비용이 발생.**  

---

## **3. QoQ 적용 시 필요 GPU 자원 계산**
이제 실제 QoQ 적용 시 필요한 **GPU FLOPs, 메모리 대역폭, 연산 속도**를 계산해 보겠음.

### **(1) FLOPs 계산 (Llama-2-13B 기준)**
- **Llama-2-13B의 기본 FLOPs:**  
  \[
  1.27 \times 10^{15} \text{ FLOPs}
  \]
- **QoQ 적용 후 FLOPs:**  
  \[
  2.54 \times 10^{15} \text{ FLOPs} \quad (\text{2배 증가})
  \]

**➡ A100 GPU에서 QoQ를 실행하려면 약 312 TFLOPs 이상의 성능 필요**.  

### **(2) 메모리 대역폭 계산 (A100 기준)**
- **W8A8 (기존 TensorRT-LLM)**
  \[
  2 \times 700 \text{ GB/s} = 1400 \text{ GB/s}
  \]
- **QoQ (W4A8KV4)**
  \[
  2 \times 350 \text{ GB/s} = 700 \text{ GB/s}
  \]

**➡ 메모리 사용량이 50% 감소하여, 메모리 대역폭 부담이 크게 줄어듦.**  

### **(3) 실질적인 GPU 요구 사항**
| 방법                    | GPU 요구 FLOPs (TFLOPs) | 메모리 대역폭 (GB/s) |
| ----------------------- | ----------------------- | -------------------- |
| **FP16 (기존)**         | 1270 TFLOPs             | 1400 GB/s            |
| **W8A8 (TensorRT-LLM)** | 640 TFLOPs              | 1400 GB/s            |
| **W4A8KV4 (QoQ)**       | **1280 TFLOPs**         | **700 GB/s**         |

**➡ GPU 연산량은 2배 증가하지만, 메모리 대역폭 부담이 절반으로 줄어듦.**  
**➡ 따라서, A100보다는 H100 같은 최신 GPU에서 성능이 더 좋을 가능성이 높음.**  

---

## **4. QoQ 적용 시 공수 및 비용 분석**
### **(1) 개발 공수**
| 작업 항목                               | 소요 시간 | 난이도 |
| --------------------------------------- | --------- | ------ |
| **Progressive Group Quantization 구현** | 3주       | ★★★★☆  |
| **SmoothAttention 적용**                | 2주       | ★★★☆☆  |
| **Compute-aware Weight Reordering**     | 2주       | ★★★★☆  |
| **Efficient Dequantization 최적화**     | 3주       | ★★★★★  |

✅ **총 예상 개발 공수: 10주 (약 2~3개월 소요)**  

### **(2) 클라우드 비용 (A100 vs H100)**
| GPU             | 가격      | QoQ 실행 속도 |
| --------------- | --------- | ------------- |
| **A100 (80GB)** | 3.5$/시간 | 1.25×         |
| **H100 (80GB)** | 4.5$/시간 | 1.8×          |

✅ **H100에서 실행하면 비용 대비 성능 효율이 더 높아짐.**

---

## **5. 결론**
- **QoQ는 기존 W8A8 대비 2배 연산량이 증가하지만, 메모리 사용량이 절반으로 감소하여 실질적인 GPU 부담은 줄어듦.**
- **추가적인 정량화 및 최적화 작업이 필요하지만, TensorRT-LLM과 통합하면 실무 적용 가능.**
- **개발 공수는 약 10주, GPU 요구 사양은 A100 이상이 필요하며, H100에서 더 효과적.**

🚀 **QoQ를 적용하면 서빙 속도를 크게 개선하면서도 비용 효율적으로 운영 가능함!**

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



## **QoQ(W4A8KV4) 기반 LLM의 입력 데이터 및 추론 과정 상세 분석**

QoQ(W4A8KV4) 기반 LLM의 **입력 데이터와 추론 과정**을 예시를 들어가며 자세히 설명하고,  
모델의 **아키텍처 구성**, 그리고 **각 연산이 필요로 하는 메모리, 연산량 및 컴퓨팅 리소스**까지 분석하겠음.

---

## **1. QoQ 기반 모델의 입력 데이터**
### **(1) 입력 데이터 형식**
QoQ를 적용하는 모델은 **대형 언어 모델(LLM)**이므로, 입력 데이터는 **토큰(token) 단위**로 변환됨.

#### **✅ 예제 입력 문장**
```plaintext
"QoQ is an efficient quantization method for LLM inference."
```

#### **✅ 토큰화(Tokenization) 과정**
입력 문장은 **WordPiece 또는 SentencePiece 기반 토큰화**를 수행하여 모델이 이해할 수 있는 정수 시퀀스로 변환됨.

```plaintext
["QoQ", "is", "an", "efficient", "quantization", "method", "for", "LLM", "inference", "."]
```

각 토큰을 **고유 ID**로 변환:
```plaintext
[10234, 2003, 2019, 4631, 25000, 4113, 2005, 21124, 10544, 1012]
```

#### **✅ 입력 데이터 포맷**
LLM 추론 과정에서는 입력을 **배치(batch), 시퀀스 길이(sequence length), 임베딩 차원(embedding dimension)** 형태의 **텐서(Tensor)**로 변환.

- **입력 텐서 크기:** \(\text{Batch Size} \times \text{Sequence Length} \times \text{Embedding Dimension}\)
- 예제:
  ```plaintext
  [1, 10, 4096]  # 배치 1개, 10개 토큰, 4096차원 임베딩
  ```

---

## **2. QoQ 기반 LLM의 추론 과정**
QoQ 적용 모델의 추론 과정은 기존 FP16 모델과 비교하여 몇 가지 차이가 있음.

### **(1) 전처리 (Preprocessing)**
- **토큰화(Tokenization)**: 입력을 정수형 인덱스 시퀀스로 변환.
- **입력 패딩(Padding)**: 고정된 길이로 맞춤 (예: 시퀀스 길이 128).
- **정량화된 임베딩(Quantized Embedding) 조회**: INT8 또는 INT4 정량화된 임베딩을 사용.

**📌 예제**
```plaintext
입력: "QoQ is an efficient quantization method for LLM inference."
토큰: [10234, 2003, 2019, 4631, 25000, 4113, 2005, 21124, 10544, 1012]
정량화된 임베딩: [[-12, 34, 56, ...], [23, -45, 67, ...], ...]
```

---

### **(2) 모델의 Forward Pass (추론 연산)**
입력 데이터가 모델을 거쳐 최종 출력이 생성되는 과정을 단계별로 분석.

#### **✅ 1단계: 임베딩 레이어 (Embedding Layer)**
- 각 토큰을 **고정된 차원의 벡터(예: 4096차원)로 변환**.
- QoQ에서는 **W4A8 정량화된 임베딩 테이블**을 사용하여 연산량 절감.

#### **✅ 2단계: Transformer Layer (Multi-Head Attention + FFN)**
Transformer Layer를 거치면서 문맥(Context)를 학습.

1. **Attention 연산 (QoQ 적용)**
   - 기존 FP16 모델: FP16 행렬 곱셈
   - QoQ 적용 모델: **W4A8 (INT4 가중치, INT8 활성화)**
   - **연산량 감소**: INT4 → INT8 → INT8 연산을 활용하여 **속도 향상**

2. **MLP (FFN, Feed-Forward Network) 연산**
   - 기존 FP16 모델: FP16 연산
   - QoQ 적용 모델: **W4A8 연산** (INT4 → INT8 변환 후 연산)

3. **Layer Normalization**
   - 정규화 적용, QoQ에서는 FP16 유지.

#### **✅ 3단계: KV 캐시 활용 (QoQ 적용)**
- 기존 방법(W8A8)은 **8비트 정량화된 KV 캐시**를 저장.
- QoQ(W4A8KV4)는 **4비트로 KV 캐시를 저장**하여 **메모리 절감**.

#### **✅ 4단계: 출력 생성 (Token Decoding)**
- 최종 출력 벡터를 소프트맥스(Softmax) 연산 후 **가장 확률이 높은 토큰을 선택**하여 다음 토큰을 생성.

---

## **3. QoQ 기반 모델 아키텍처**
### **(1) Transformer Layer 구조**
QoQ 모델은 기존 Transformer 구조를 유지하면서, **정량화 기법**을 추가 적용.

```plaintext
[Input Tokens] → [Embedding Layer (W4A8)] → [Multi-Head Attention (W4A8)] 
→ [FFN (W4A8)] → [Output Probabilities]
```

**QoQ 적용 연산**
1. **Embedding Layer**: W4A8 적용
2. **Multi-Head Attention**: INT4 가중치, INT8 활성화 (W4A8)
3. **FFN (MLP)**: W4A8 방식 사용
4. **KV 캐시**: 기존 FP16 → INT4로 저장 (KV4)

---

## **4. 모델 연산 및 메모리 요구량 분석**
QoQ를 적용한 경우와 기존 FP16 모델의 **연산량(FLOPs), 메모리 요구량**을 비교.

### **(1) 연산량(FLOPs) 분석**
| 모델                    | 연산량 (FLOPs)           |
| ----------------------- | ------------------------ |
| **FP16 모델 (기존)**    | \(O(N^3)\)               |
| **W8A8 (TensorRT-LLM)** | \(O(N^3)\)               |
| **W4A8 (QoQ)**          | \(O(0.5N^3)\) (50% 감소) |

**➡ QoQ는 TensorRT-LLM 대비 연산량이 50% 감소**  
(4비트 정량화를 적용하면서도 INT8 텐서 코어를 활용하여 최적화)

---

### **(2) 메모리 사용량 분석**
QoQ는 기존 FP16 대비 **메모리 사용량이 4배 감소**함.

| 모델                    | 메모리 사용량 (GB) |
| ----------------------- | ------------------ |
| **FP16 (기존 방식)**    | \(O(16N)\)         |
| **W8A8 (TensorRT-LLM)** | \(O(8N)\)          |
| **W4A8KV4 (QoQ)**       | \(O(4N)\)          |

**➡ KV 캐시를 INT4로 저장하는 것이 가장 큰 메모리 절감 효과를 가져옴.**

---

### **(3) GPU 요구량 분석**
QoQ를 적용하면 연산량이 줄어들지만, **정량화 과정에서 추가적인 연산이 필요**하여 최적의 성능을 내기 위해 **A100 이상급 GPU**가 필요.

| GPU             | 연산량 (TFLOPs) | 필요 메모리 대역폭 (GB/s) |
| --------------- | --------------- | ------------------------- |
| **A100 (80GB)** | 1248 TFLOPs     | 2.0 TB/s                  |
| **H100 (80GB)** | 1984 TFLOPs     | 3.35 TB/s                 |

**➡ QoQ는 A100에서 실행 가능하지만, H100에서 최적의 성능 발휘.**

---

## **5. 결론**
QoQ 적용 시:
1. **입력 데이터는 기존 FP16 모델과 동일하지만, W4A8 정량화된 형태로 변환됨.**
2. **추론 과정에서 INT4 → INT8 변환 후 연산을 수행하여 성능 최적화.**
3. **KV 캐시를 4비트로 저장하여 메모리 사용량이 4배 감소.**
4. **연산량은 기존 FP16 대비 50% 감소하면서도 GPU 연산량은 증가할 수 있음.**
5. **A100에서도 실행 가능하지만, H100에서 더 높은 성능을 낼 수 있음.**

**➡ QoQ는 기존 방법보다 메모리를 절감하면서도, 연산 최적화를 통해 속도를 크게 향상시킨 방법론이다!** 🚀

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



## **QoQ(W4A8KV4) 논문의 한계점 및 극복을 위한 연구 흐름 정리**

논문의 QoQ(W4A8KV4) 방법은 기존 LLM 정량화(Quantization) 방법 대비 속도와 정확도를 향상시켰지만, 여전히 **몇 가지 한계점**이 존재함. 이를 해결하기 위한 연구 방향을 최신 연구 흐름과 함께 설명하겠음.

---

## **1. QoQ(W4A8KV4)의 한계점**
QoQ 방법론의 주요 한계는 크게 **정확도 손실, 하드웨어 의존성, 최적화 복잡성, 추가적인 연산량 발생**의 4가지로 정리됨.

### **(1) 정량화로 인한 정확도 손실**
- QoQ는 기존의 W8A8 대비 정확도를 유지하려고 하지만, **FP16 대비 여전히 0.6~1.7%의 정확도 손실**이 존재.
- 특히 **Winogrande, HellaSwag 같은 상식 추론 데이터셋에서 성능이 낮아짐.**
- **SmoothAttention이 정확도를 보완하지만 완벽하지 않음**.

✅ **해결 방안 연구 흐름**
- **혼합 정량화(Mixed Precision Quantization)**: 중요 가중치는 FP16/INT8 유지, 나머지는 INT4로 정량화.
- **동적 정량화(Dynamic Quantization)**: 실시간 입력 데이터 특성에 따라 정량화 스케일을 조정하는 방법.

---

### **(2) 특정 하드웨어(GPU) 최적화에 종속적**
- QoQ는 **NVIDIA A100, L40S 같은 최신 데이터센터급 GPU에 최적화**되어 있음.
- 그러나 **CPU나 모바일 장치에서는 성능 향상이 보장되지 않음**.
- INT8 텐서 코어를 활용하는 방식이므로, **Tensor Cores가 없는 GPU에서는 성능이 낮을 가능성**이 있음.

✅ **해결 방안 연구 흐름**
- **LLM용 저전력 정량화 기법 연구**  
  - **EdgeBERT** (Tambe et al., 2021): 모바일 및 엣지 디바이스에서 2비트 정량화를 활용하여 전력 소모 감소.
  - **Adaptive Precision Scaling**: CPU에서도 실행 가능하도록 FP16, INT8, INT4를 동적으로 조정.

- **TPU, FPGA 및 ASIC에서의 최적화 연구**  
  - Google의 TPU에서는 INT4보다 **bfloat16이 더 적합**할 수도 있음.
  - FPGA에서는 메모리 접근 최적화가 필요 (DOTA, 2022).

---

### **(3) 추가적인 정량화 연산으로 인한 오버헤드**
- 기존의 W8A8 모델과 비교했을 때, **QoQ는 8비트 → 4비트 변환 과정을 추가적으로 수행해야 함**.
- Progressive Group Quantization으로 인해 **추가적인 정량화 오버헤드 발생**.
- 특히 **INT4 → INT8 변환 과정에서 연산량이 증가**하여, 오히려 **작은 배치(batch size 1~4)에서는 성능이 낮아질 수 있음**.

✅ **해결 방안 연구 흐름**
- **Zero-cost Quantization (ZQ)**  
  - 사전 정량화 없이, 실행 중에 **가중치 범위를 실시간으로 조정**하여 성능 저하를 방지하는 연구.
  - 대표 연구: **OmniQuant (Shao et al., 2023)** → 4비트 정량화를 실시간으로 조정.

- **Quantization-aware Training (QAT) 적용**  
  - 현재 QoQ는 **Post-Training Quantization (PTQ) 방식**이므로, **학습 중 정량화(QAT)**를 적용하면 성능 향상 가능.
  - 연구 흐름: **SmoothQuant(2023) + QAT 적용** → QoQ보다 높은 정확도를 달성할 가능성.

---

### **(4) KV 캐시 정량화로 인한 Attention Bottleneck**
- QoQ는 KV 캐시를 **4비트(KV4)로 정량화**하여 메모리를 절감함.
- 하지만 **INT4에서 Softmax 연산 시 정보 손실이 발생할 가능성이 높음**.
- 결과적으로, **Attention 연산에서 병목이 발생할 수 있음**.

✅ **해결 방안 연구 흐름**
- **Sparse Attention 적용 (Sparsity-based KV Cache Quantization)**  
  - 기존의 **ReLU를 활용한 Activation Sparsity 기법**을 활용하여, 중요한 KV만 FP16으로 유지하고 나머지는 INT4로 변환.
  - **SpAtten (Wang et al., 2021)** → 희소성(Sparsity)을 적용한 Attention 최적화 연구.

- **Rank-adaptive KV Quantization**  
  - 특정 토큰(중요한 단어)에 대해서만 **고정점 정밀도(FP8, INT8)로 유지**.
  - 최근 연구: **QuaRot(2024)** → 정량화 오차를 최소화하는 **선형 변환 기반 정량화**.

---

## **2. 최신 연구 흐름 및 극복 방안**
### **(1) 혼합 정량화 (Mixed Precision Quantization)**
**➡ 일부 중요한 가중치는 FP16 유지, 나머지는 INT4로 변환하여 정확도 손실 감소**
- **대표 연구:**  
  - **AWQ (Activation-aware Weight Quantization, 2023)**: 활성화 분포를 분석하여 일부 중요한 가중치는 FP16 유지.

✅ **QoQ 개선 방향**
- **특정 레이어에서는 FP8/INT8을 유지**하고 나머지만 INT4로 변환.

---

### **(2) Zero-cost Quantization (ZQ)**
**➡ 실행 중 정량화 스케일을 동적으로 조정하여 오버헤드를 제거**
- **대표 연구:**  
  - **OmniQuant (2023)** → 정량화 오버헤드를 최소화하는 기술.
  - **GPTQ (Frantar et al., 2023)** → 실행 중 정량화 최적화.

✅ **QoQ 개선 방향**
- **KV 캐시 정량화(KV4) 대신 동적 정량화 적용하여 병목 해결 가능.**

---

### **(3) Attention 최적화**
**➡ KV 캐시 정량화(KV4)로 인한 병목을 개선**
- **대표 연구:**  
  - **SpAtten (Wang et al., 2021)**: Sparse Attention 적용하여 메모리 사용량 감소.
  - **Rank-adaptive KV Quantization (2024)**: **일부 KV만 FP8/INT8 유지**.

✅ **QoQ 개선 방향**
- **Softmax 연산 이전에 희소성(Sparsity) 적용하여, 연산량을 줄일 수 있음.**

---

## **3. 결론: QoQ 개선을 위한 연구 방향 요약**
QoQ(W4A8KV4)의 한계를 극복하기 위해 **다음 연구 흐름을 적용할 수 있음**.

| 한계점               | 연구 흐름                                | 대표 연구                      |
| -------------------- | ---------------------------------------- | ------------------------------ |
| **정확도 손실**      | 혼합 정량화 (Mixed Precision)            | AWQ (2023), SmoothQuant (2023) |
| **GPU 종속성**       | LLM 최적화 FPGA/ASIC 연구                | EdgeBERT (2021), TPU 최적화    |
| **추가 연산량 증가** | Zero-cost Quantization (ZQ)              | OmniQuant (2023), GPTQ (2023)  |
| **KV 캐시 병목**     | Sparse Attention, Rank-adaptive KV Quant | SpAtten (2021), QuaRot (2024)  |

QoQ는 **기존 4비트 정량화보다 높은 속도와 정확도를 보장하지만, 여전히 일부 한계가 존재함**.  
이를 극복하기 위해, **혼합 정량화, Zero-cost Quantization, Sparse Attention 기법을 활용하는 방향으로 발전할 가능성이 큼**. 🚀


# Q :  QoQ(W4A8KV4) 정량화 기법에서 Progressive Group Quantization이 기존 W4A4 정량화 대비 정확도를 향상시키는 핵심적인 이유는 무엇인가?

 

Progressive Group Quantization이 기존 W4A4 방법과 비교해 우수한 정확도를 달성하는 핵심 이유는, 4비트로 한번에 바로 정량화하지 않고, 가중치를 먼저 중간 단계인 8비트로 정량화한 뒤 추가적으로 4비트 그룹 정량화를 적용하기 때문임.

기존 W4A4 방법의 경우, 가중치를 직접 4비트로 정량화하면 곧바로 4비트 값에서 FP32로의 복원(dequantization)이 연산 중 발생하여 GPU의 저속 CUDA 코어에서 병목이 발생하게 됨. 이로 인해 GPU의 이론적 성능을 충분히 활용하지 못하고 높은 연산 지연(overhead)이 발생함.

반면 QoQ의 진행형 그룹 정량화는 가중치를 먼저 FP16 → INT8로 변환한 후, 다시 INT8 → INT4로 변환하여, 최종 연산은 INT8 텐서 코어에서 수행하도록 설계함. 이렇게 하면 GPU에서 CUDA core가 아닌 INT8 tensor core를 활용하여 매우 빠른 속도로 연산을 처리할 수 있음. 또한 INT4 → INT8 역정량화 과정에서 발생할 수 있는 값의 범위를 미리 제한하는 보호 정량화 범위(protective range)를 설정함으로써, INT8 범위를 벗어나는 값에 대한 계산 오류(overflow)를 방지할 수 있어 추가적인 정확도 손실을 최소화함.

결과적으로 QoQ의 진행형 그룹 정량화는 CUDA core 연산의 병목 문제를 없애고 INT8 tensor core를 최대한 활용할 수 있도록 하여, 기존 W4A4 방식보다 현저히 높은 정확도와 속도를 동시에 얻을 수 있었음.

# Q : SmoothAttention 기법이 4비트 KV 캐시 정량화에서 발생하는 정확도 손실을 어떻게 보완하며, 기존 방법들과의 차이점은 무엇인가?

 

SmoothAttention 기법은 4비트로 KV 캐시를 정량화할 때 발생하는 정확도 손실을 키(Key) 값의 극단적(outlier) 채널을 평탄화(smoothing)하여 완화한다. 일반적으로 Key 값에는 이상치(outlier)가 존재해 KV를 4비트와 같이 낮은 비트수로 정량화할 경우 심각한 정확도 손실을 유발하게 된다. 예를 들어, 특정 채널이 나머지보다 10배 이상 큰 활성화를 가진다면, 4비트로 정량화할 때 해당 채널은 매우 큰 오차로 표현된다.

SmoothAttention은 이러한 키(Key)의 이상값 문제를 해결하기 위해 **쿼리(Query)는 정량화하지 않고 원본(FP16)을 유지**한 상태에서, 오직 키(Key)에 대해서만 활성화 값의 채널별 최대값을 기준으로 한 스무딩 계수를 적용하여 이상값을 미리 완화한다. 즉, 채널별로 활성화 값의 최대 절댓값을 계산한 뒤, 다음과 같은 스케일링을 적용한다.

Z = (QΛ)(KΛ⁻¹)ᵀ, Λ = diag(λ), λᵢ = max(|Kᵢ|)^α

여기서 λ는 채널별로 계산된 스케일 값이며, 논문에서는 α=0.5를 사용하여 간단히 정의하였다. 키 값이 클수록 더 큰 폭으로 줄이고 작은 키는 상대적으로 덜 변형하여 전체적으로 균일한 크기를 가지게 하는 것이 특징이다.

이렇게 키의 활성화가 균일해지면, 4비트로 정량화를 해도 채널 간 큰 오차가 발생하지 않아 정밀도가 증가한다. 이 기법의 핵심 아이디어는 다음과 같다:

기존 방법(기존의 4비트 정량화 방식):

- 키(Key)와 쿼리(Query)를 모두 동일한 비트(4비트 또는 8비트)로 정량화
- 이상값을 정량화 과정에서 다루지 않아 채널별 오차가 크게 발생
- 결국 정량화 시 정확도 손실 발생

SmoothAttention(QoQ)의 차별점:

- 쿼리(Query)는 아예 정량화하지 않고 FP16으로 유지함으로써 정밀도를 완벽히 유지
- 키(Key)는 per-channel 스케일링을 통해 미리 이상값을 완화한 뒤 정량화
- RoPE(rotary positional embedding)를 사용하는 모델에서 스케일링 계수를 미리 선형 계층에 흡수할 수 있도록 하여 추가적 연산 오버헤드를 제거함

| 구분             | 기존 방법 (예: TensorRT-LLM, QuaRot 등) | SmoothAttention(QoQ)                            |
| ---------------- | --------------------------------------- | ----------------------------------------------- |
| 정량화 방식      | Key와 Query 모두 같은 비트로 정량화     | Query는 FP16 유지, Key만 INT4로 정량화          |
| 정확도 손실      | Key 이상값에 의해 높은 정확도 손실      | Key를 스무딩하여 정확도 손실 최소화             |
| 연산량 추가 여부 | 추가 연산 없음                          | 채널별 최대값 계산 및 Scaling(연산량 소폭 추가) |
| 특징             | 쉬운 구현이나 정확도 손실 큼            | 추가 계산이 필요하나 정확도 손실 매우 적음      |

요약하면, SmoothAttention 기법은 낮은 비트 수를 활용한 KV 캐시 정량화에서 나타나는 키의 이상값 문제를 해결함으로써, 낮은 비트(4비트) 정량화에서도 기존 기법 대비 월등히 높은 정확도를 유지하는 데 성공한 독창적인 방법론이다.

# Q : QServe 시스템이 TensorRT-LLM 대비 높은 추론 속도를 달성할 수 있었던 주요 시스템 최적화 기법 3가지는 무엇이며, 각각이 성능 향상에 기여하는 방식은 무엇인가?

 

QServe가 기존의 TensorRT-LLM과 비교하여 큰 폭의 성능 향상을 이룰 수 있었던 핵심적인 시스템 최적화 기법은 크게 세 가지로 요약할 수 있다.

1. Compute-aware Weight Reordering (가중치 연산 인지 재배열)
2. Subtraction-after-Multiplication Dequantization (감산 후 곱셈 방식의 효율적 역정량화)
3. Fused Attention의 메모리 제한(Memory-bound) 최적화

각각의 기법이 성능 향상에 기여하는 방식은 다음과 같다.

1. Compute-aware Weight Reordering (연산 중심의 가중치 재배열)
- 기존 방법에서는 GPU에서 데이터를 로드할 때 비연속적인 메모리 접근으로 인해 높은 포인터 산술 연산 오버헤드 발생함.
- QServe는 가중치를 연산이 수행되는 순서에 맞춰 미리 메모리에 배치하여, 메모리 접근의 연속성을 극대화함.
- 이로 인해 CUDA 코어에서의 포인터 산술 연산 횟수가 현저히 감소하며, 메모리 접근 속도가 빨라져 **최대 1.5배** GEMM 연산 성능 향상에 기여함.

2. Efficient Dequantization with Register-Level Parallelism (레지스터 병렬화를 이용한 효율적인 역정량화)
- 기존 INT4 기반 방법들은 INT4 → FP32 변환 과정에서 CUDA 코어의 연산 병목이 심각하게 발생함.
- QServe는 감산을 곱셈 이후에 수행하는 방식을 채택하여 INT4 → INT8 변환을 GPU의 텐서코어에서 병렬로 수행할 수 있도록 설계함.
- 이를 통해 INT4 → INT8 변환 시 병렬 레지스터 연산(vadd4)을 적극 활용하여 CUDA 코어의 연산 오버헤드를 최소화하고, 텐서 코어의 높은 성능을 최대한 활용하여 **최대 2배**의 속도 개선을 달성함.

3. 메모리 제한(Memory-bound) 기반 Fused Attention 최적화
- 기존의 Fused Attention 커널은 4비트 KV 캐시를 단순히 적용했을 때, CUDA 코어에서 수행되는 추가적인 INT4→FP16 변환 연산으로 인해 계산이 메모리 제한(memory-bound)을 넘어 계산 제한(compute-bound) 영역으로 넘어가는 문제가 발생함.
- QServe에서는 FP32 연산을 FP16으로 변경하여 GPU의 연산 성능 한계를 높이고, 비트 연산 최적화(bit-trick) 기법을 도입하여 INT4→FP16 변환 시 필요한 연산량을 대폭 감소시킴.
- 결과적으로 Attention 연산이 다시 메모리 제한 영역으로 이동하여 INT4 KV 캐시가 실제로 2배의 속도 향상을 얻을 수 있도록 만들어, 전체 Attention 연산의 **최대 1.7배** 성능 향상에 기여함.

각 기법의 성능 향상 요약

| 최적화 기법                                           | 해결한 병목 현상                                       | 성능 향상 효과                                 |
| ----------------------------------------------------- | ------------------------------------------------------ | ---------------------------------------------- |
| Compute-aware Weight Reordering                       | CUDA 코어의 메모리 접근 속도 향상 (포인터 연산 최소화) | GEMM 연산 성능 최대 1.5배 향상                 |
| Efficient Dequantization (Register-Level Parallelism) | INT4 → INT8 변환 연산 속도 대폭 개선                   | CUDA 코어 연산 오버헤드 50% 감소 (성능 약 2배) |
| 메모리 제한 기반 KV4 Attention 최적화                 | Attention 연산의 계산 제한 → 메모리 제한 전환          | Attention 연산 속도 최대 1.7배 향상            |

즉, QServe의 뛰어난 성능 향상은 위 3가지 최적화 기법이 상호작용하여 GPU의 하드웨어 특성(메모리 및 연산 유닛)을 최대한 활용할 수 있게끔 시스템적으로 설계되었기 때문이라고 볼 수 있음.