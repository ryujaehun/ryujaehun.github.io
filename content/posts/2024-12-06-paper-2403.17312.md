---
categories:
- paper-review
- with-gpt
- LLM-Inference
date: "2024-12-06T00:00:00Z"
title: 'ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching'
---

[논문 링크](https://arxiv.org/abs/2403.17312)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문 요약: **ALISA: 대규모 언어 모델 추론 가속화를 위한 희소성 기반 KV 캐싱**

이 논문은 대규모 언어 모델(LLM)의 추론 속도를 개선하기 위해 **ALISA**라는 새로운 알고리즘-시스템 공동 설계 방안을 제시합니다. 특히, GPU 메모리가 제한된 환경에서 LLM 추론 효율을 극대화하기 위한 다양한 방법론을 다룹니다.

---

### 논문의 주요 기여점

1. **희소 윈도우 주의(Sparse Window Attention, SWA):**
   - Transformer의 주의(attention) 가중치에서 높은 희소성을 활용하여 불필요한 계산을 줄이는 새로운 알고리즘을 제안합니다.
   - 글로벌 동적 패턴과 로컬 정적 패턴을 결합하여 중요한 토큰만 캐싱하고, 메모리 사용량을 줄이면서도 정확도를 유지합니다.

2. **3단계 동적 스케줄링(Dynamic Scheduling):**
   - 메모리 캐싱과 재계산 간의 균형을 동적으로 조정하여 시스템 성능을 최적화합니다.
     - **1단계**: GPU 메모리가 충분할 때 모든 KV 텐서를 GPU에 저장.
     - **2단계**: 메모리가 부족해질 경우 일부 KV 텐서를 CPU로 오프로드.
     - **3단계**: CPU 접근 속도가 느려지면 일부 텐서를 재계산(recompute)하여 성능 최적화.

3. **KV 압축(KV Compression):**
   - INT8 양자화(quantization)를 활용하여 메모리 점유를 줄이고, 성능과 정확도 간의 균형을 유지합니다.

4. **성능 개선:**
   - ALISA는 기존 FlexGen 대비 최대 **3배** 더 높은 처리량(throughput)을 제공하며, vLLM 대비 **1.9배** 더 효율적입니다.

---

### ALISA의 강점 및 독창성

1. **효율적인 희소성 활용:**
   - Transformer의 주의 가중치 희소성을 적극 활용하여 메모리 사용량을 대폭 줄이면서도 정확도를 유지합니다.

2. **정교한 토큰 수준 관리:**
   - 기존 시스템이 블록 단위나 헤드 단위로 캐싱을 관리한 반면, ALISA는 토큰 단위로 캐싱을 관리하여 더욱 정밀한 메모리 제어를 가능하게 합니다.

3. **알고리즘-시스템 공동 설계:**
   - SWA 알고리즘과 동적 스케줄링을 결합하여 전체 시스템 성능을 극대화합니다.

4. **실제 환경에 적합:**
   - GPU 메모리 한계나 CPU-GPU 간 I/O 병목 현상 등 현실적인 제약을 고려한 설계로, 실제 활용 가능성을 높였습니다.

5. **다양한 모델과 작업에서 확장성 검증:**
   - OPT, LLaMA, Pythia와 같은 다양한 LLM에서 안정적인 성능 향상을 입증했습니다.

---

### 핵심 알고리즘(SWA)의 작동 과정: 예시로 설명

#### 입력 예시:
문장이 `["The", "quick", "brown", "fox", "jumps"]`일 때, 각 토큰의 중요도를 계산해 중요한 토큰만 선택합니다.

1. **주의 가중치 계산:**
   - 각 토큰 간의 주의 가중치(attention weights)를 계산합니다.
   - 예를 들어, "jumps"를 생성할 때 가장 큰 영향을 미치는 토큰은 `["brown", "fox"]`일 수 있습니다.

2. **중요 토큰 선택:**
   - **로컬 정적 패턴**: 최근 토큰(예: "brown", "fox")을 항상 유지합니다.
   - **글로벌 동적 패턴**: 주의 가중치 합이 높은 토큰(예: "jumps")을 선택합니다.

3. **희소 KV 텐서 생성:**
   - 선택된 중요한 토큰들만 포함한 희소 KV 텐서를 생성하여 메모리 사용량을 줄입니다.

4. **주의 점수 계산:**
   - 희소 텐서를 사용해 주의 점수를 계산합니다. 이 과정은 계산량과 메모리 사용량을 크게 줄이면서도 정확도를 유지합니다.

---

### 논문의 한계점

1. **희소성에 대한 의존성:**
   - SWA의 성능은 주의 가중치의 희소성에 크게 의존하므로, 특정 작업에서는 성능 이점이 줄어들 가능성이 있습니다.

2. **복잡한 스케줄링 구현:**
   - 동적 스케줄링은 구현 난이도가 높고, 특히 캐싱과 재계산 간의 균형을 맞추는 과정에서 최적화를 필요로 합니다.

3. **높은 희소성에서 정확도 저하:**
   - 80% 이상의 희소성에서 일부 정확도 저하가 발생할 수 있으며, 이는 민감한 작업에서 문제가 될 수 있습니다.

4. **싱글 GPU-CPU 환경에 국한:**
   - 멀티 GPU 또는 분산 환경에서는 추가적인 설계가 필요할 수 있습니다.

5. **양자화의 영향:**
   - INT8 양자화는 메모리를 절약하지만, 미세한 정확도 손실 가능성이 있습니다.

---

### 결론

ALISA는 대규모 언어 모델의 추론 속도를 향상시키기 위한 혁신적인 방법으로, 특히 자원이 제한된 환경에서 뛰어난 성능을 발휘합니다. 희소성을 효과적으로 활용한 SWA 알고리즘과 동적 스케줄링의 결합은 메모리와 연산 효율성을 동시에 극대화하는 독창적인 접근 방식으로 평가됩니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



논문 **"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"**에서 제안된 핵심 알고리즘인 **Sparse Window Attention (SWA)**은, 대규모 언어 모델(LLM)의 효율성을 높이기 위해 **스파스(희소) 패턴을 활용**하여 메모리 사용량을 줄이고 계산 효율성을 높이는 방법입니다.

---

### **핵심 알고리즘: Sparse Window Attention (SWA)**

SWA는 기존의 **Self-Attention** 연산을 최적화하여, 모든 토큰(Key-Value 쌍)을 계산에 사용하는 대신 **중요 토큰**만 선택해 활용합니다. 이로써 **메모리 사용량을 크게 줄이고도 정확도 손실을 최소화**합니다.

---

### **SWA 작동 과정: 예제와 함께 설명**

#### **입력 데이터:**
입력 문장은 다음과 같습니다:
> "The quick brown fox jumps over the lazy dog."

#### **기본 Attention에서의 문제점:**
- Self-Attention은 모든 단어를 서로 비교하여 중요도를 계산합니다.
- 입력 길이가 길수록(예: 1000개 토큰 이상) 연산량이 **O(n²)**로 증가하여 메모리와 계산 비용이 기하급수적으로 늘어납니다.

#### **Sparse Window Attention의 해결 방식:**
1. **Attention Weight 계산:**
   각 토큰(단어)의 중요도를 나타내는 Attention Weight를 계산합니다.
   - 예를 들어, `Attention Weight`가 다음과 같이 나왔다고 가정합니다:
     ```
     ["The": 0.1, "quick": 0.3, "brown": 0.2, "fox": 0.4, "jumps": 0.6, "over": 0.05, "lazy": 0.1, "dog": 0.35]
     ```

2. **스파스 패턴 생성:**
   - **Locally Static Tokens** (최근 몇 개의 토큰 유지): 문장 맥락을 유지하기 위해 항상 마지막 두 개의 단어("lazy", "dog")를 선택합니다.
   - **Globally Dynamic Tokens** (가장 중요한 토큰 선택): Attention Weight가 높은 토큰("jumps", "fox")을 추가로 선택합니다.
   - 최종 선택된 토큰:
     ```
     ["fox", "jumps", "lazy", "dog"]
     ```

3. **Sparse KV Tensor 생성:**
   - 선택된 토큰만으로 Key-Value (KV) 쌍을 구성합니다.
   - 기존의 Dense Tensor (전체 토큰) 대신 선택된 희소 토큰으로 연산을 수행하여 메모리 사용량을 감소시킵니다.

4. **Attention Score 계산:**
   - 선택된 KV Tensor를 사용하여 다음 토큰 생성을 위한 Attention Score를 계산합니다.
   - 예를 들어, 다음과 같은 연산을 수행:
     ```
     Attention(Q, K_selected, V_selected) = Softmax(Q · K_selected^T) · V_selected
     ```
   - 여기서 `Q`는 Query 벡터입니다.

5. **반복:**
   - 새로운 토큰이 생성되면 다시 SWA를 적용하여 중요 토큰을 선택하고 다음 단계의 계산에 사용합니다.

---

### **SWA를 통해 얻는 이점**

1. **메모리 효율성:**
   - 전체 토큰 대신 일부 선택된 토큰만 저장 및 계산에 사용하여 GPU 메모리 사용량을 줄임.
   - 긴 시퀀스를 처리하는 데 필요한 메모리 제한 문제를 완화.

2. **계산 속도 증가:**
   - Sparse KV Tensor를 사용함으로써 Self-Attention 연산의 계산량을 크게 감소.
   - 기존의 Dense Attention 대비 1.5배~3배 빠른 속도를 보임.

3. **정확도 유지:**
   - 중요한 토큰을 선택하는 방식(Local + Global)으로 인해 기존 Dense Attention과 비교해도 정확도가 거의 동일함.
   - 최대 80%의 KV 희소성을 유지하며 정확도 손실이 5% 미만으로 유지됨.

---

### **알고리즘 정리**

1. **Sparse Token Selection:**
   - 최근의 중요한 토큰(Local)과 글로벌 중요 토큰(Global)을 결합한 스파스 패턴 생성.

2. **Sparse Matrix 연산:**
   - 희소 형태의 Key-Value Tensor를 사용하여 Attention 연산 수행.

3. **적응형 스케줄링:**
   - GPU 메모리 상태에 따라 토큰 재계산(Recompute) 및 캐싱(Caching)을 동적으로 조정.

---

Sparse Window Attention은 기존 Transformer 구조의 단점을 보완하여, 대규모 언어 모델의 추론 속도와 메모리 효율성을 획기적으로 개선하는 혁신적인 알고리즘입니다. 


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



이 논문은 **대규모 언어 모델(LLM)**의 추론 속도를 향상시키는 데 초점을 맞춘 **Sparse Window Attention (SWA)**과 **동적 스케줄링**을 통해 기존 방법론에 비해 다음과 같은 특출난 결과를 도출했습니다.

---

### **논문에서 제시한 주요 결과**

1. **Throughput 개선:**
   - 단일 GPU-CPU 환경에서 기존 방법론보다 **최대 3배(FlexGen 대비)**, **1.9배(vLLM 대비)** 빠른 추론 속도를 보여줌.

2. **메모리 효율성:**
   - Sparse Window Attention을 통해 KV Tensors의 메모리 사용량을 줄여 긴 시퀀스를 처리하는 성능 향상을 달성.
   - 메모리 부족 문제를 완화하고, GPU 메모리 및 GPU-CPU 데이터 이동의 병목현상을 줄임.

3. **정확도 유지:**
   - Sparse Attention을 사용해도 최대 80% 희소성에서 기존 Dense Attention과 비교해 **정확도 손실이 거의 없음(5% 미만)**.

4. **다양한 모델에서의 확장성:**
   - 다양한 대규모 언어 모델(OPT, LLaMA, Pythia)과 작업(Task)에서 일관된 성능 개선을 입증.

---

### **다른 방법론과의 비교에서 특출난 점**

| **방법론**                            | **특징**                                                | **ALISA와 비교**                                                |
| ------------------------------------- | ------------------------------------------------------- | --------------------------------------------------------------- |
| **FlexGen**                           | 정적 스케줄링 방식으로 KV Tensors를 GPU와 CPU 간에 이동 | ALISA는 동적 스케줄링을 통해 메모리 및 속도 효율성을 극대화함.  |
| **vLLM**                              | 블록 단위 KV 관리 및 메모리 최적화                      | ALISA는 **토큰 단위**로 관리하며 더 세밀하고 효율적인 접근.     |
| **Dense Attention**                   | 모든 토큰을 고려해 정확도가 높으나 계산량이 큼          | ALISA는 비슷한 정확도를 유지하면서 계산량과 메모리 사용을 감소. |
| **Local Attention/Strided Attention** | 단순 스파스 패턴 적용, 정확도 손실이 큼                 | SWA는 동적/정적 스파스 패턴 결합으로 정확도 유지.               |

---

### **결과를 도출한 주요 기법**

#### **1. Sparse Window Attention (SWA)**
- **논문에서 제시한 이유:**
  - Attention Weight가 희소한 특징(대부분의 값이 중요하지 않음)을 활용하여 **중요 토큰만 선택**해 계산량과 메모리 사용을 줄임.
  - SWA는 중요 토큰을 **Local(가까운 토큰)**과 **Global(전체에서 중요도 높은 토큰)**로 나눠 선택하여, 언어 모델의 맥락 이해를 유지하면서 효율성을 극대화.

- **내 생각:**
  - 기존의 Sparse Attention(예: Local, Strided)은 중요 토큰을 충분히 포착하지 못해 정확도에 악영향을 끼쳤음.
  - SWA는 **정적(Local) 패턴**과 **동적(Global) 패턴**을 결합해 정확도 유지와 메모리 절감을 동시에 달성한 점이 혁신적임.
  - 특히, 동적 패턴은 Attention Weight 변화에 적응할 수 있어 LLM의 다양성에 적합함.

#### **2. 동적 스케줄링 (Dynamic Scheduling)**
- **논문에서 제시한 이유:**
  - GPU 메모리가 부족할 때 일부 KV Tensors를 CPU에 저장하거나 필요 시 재계산(Recompute)하는 방식을 통해 GPU-CPU 데이터 이동 병목을 해결.
  - 세 단계로 구분된 스케줄링:
    1. **Phase I:** GPU에 KV Tensors를 전부 캐싱.
    2. **Phase II:** GPU-CPU에 KV를 나눠 저장.
    3. **Phase III:** GPU 메모리가 가득 차면 덜 중요한 KV는 삭제하고 필요 시 재계산.

- **내 생각:**
  - 정적 스케줄링(FlexGen) 방식은 미리 설정된 메모리 분배 전략으로 인해 긴 시퀀스에서는 비효율적임.
  - 반면, ALISA의 동적 스케줄링은 **토큰 단위 관리**를 통해 메모리와 계산 시간의 균형을 최적화. 특히, 재계산 전략은 CPU-GPU 데이터 전송 비용을 줄이는 데 효과적임.

#### **3. KV Compression**
- **논문에서 제시한 이유:**
  - FP16 포맷의 KV Tensors를 INT8로 압축하여 메모리 사용량을 추가적으로 감소시킴.
  - 채널별 양자화를 적용해 정확도 손실 없이도 높은 압축률을 달성.

- **내 생각:**
  - 기존 모델 가중치 압축 기법을 KV Tensors에 적용한 아이디어가 독창적임.
  - 정확도를 유지하면서 메모리 병목을 완화하는 데 기여.

---

### **ALISA의 혁신적 결과를 뒷받침하는 이유**

1. **희소성 활용의 적절성:**
   - 논문에서 Attention Weight의 희소성을 실험적으로 보여주며, 대부분의 값이 중요하지 않음을 증명.
   - 이를 통해 불필요한 계산을 줄이고 효율성을 높인 점이 결과의 핵심.

2. **토큰 단위 접근 방식:**
   - 기존 블록 또는 헤드 단위 관리 방식보다 더 세밀하게 메모리를 관리함으로써 성능 향상을 달성.

3. **동적 최적화:**
   - 각 추론 단계에서 메모리 상태와 계산 비용을 동적으로 최적화하여 다양한 환경에서 높은 유연성을 발휘.

---

### **결론**
ALISA는 Sparse Window Attention, 동적 스케줄링, KV Compression을 통해 정확도 유지와 속도 및 메모리 효율성을 동시에 달성한 점이 매우 혁신적입니다. 특히, 단순히 이론적인 성능 향상이 아닌 실제 LLM 추론 환경에서의 문제를 실질적으로 해결했다는 점에서 높은 평가를 받을 만합니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



논문 **"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"**에서 사용된 입력 데이터와 추론 과정을 구체적으로 설명하겠습니다. 또한 모델 아키텍처, 구성 연산, 메모리 요구량, 컴퓨팅 요구량도 자세히 다룹니다.

---

### **1. 입력 데이터와 추론 과정**

#### **입력 데이터**
- 입력 데이터는 **자연어 텍스트 시퀀스**로, LLM의 작업에 따라 달라질 수 있습니다.
  - 예시 1: "The quick brown fox jumps over the lazy dog."
  - 예시 2 (질의응답): 질문 - "What is the capital of France?", 컨텍스트 - "France is a country in Europe. Paris is its capital."

#### **추론 과정**
1. **Prefilling Stage**:
   - 전체 입력 시퀀스가 모델에 동시에 처리됨.
   - 예를 들어, 입력 문장이 "The quick brown fox"라면, 이 토큰들이 모델의 Transformer Layer를 통해 모두 처리되어 Key-Value(KV) 텐서를 생성.
   - 결과는 각 입력 토큰에 대한 내부 상태(Intermediate State)로 KV 텐서에 저장됨.

2. **Decoding Stage**:
   - 모델은 하나의 토큰씩 순차적으로 생성.
   - 현재 시퀀스와 이전 단계의 KV 텐서를 사용해 다음 토큰을 예측.
   - 예: "The quick brown fox" → "The quick brown fox jumps".
   - **KV Caching**:
     - 이전 단계에서 생성된 KV 텐서를 재사용해 새 토큰을 계산하는 데 필요한 작업량을 줄임.
     - Sparse Window Attention(SWA)를 사용해 **중요한 KV만 선택**하여 메모리와 계산 효율을 높임.

---

### **2. 모델 아키텍처**

#### **Transformer 아키텍처 개요**
- **구성요소**:
  - 입력 임베딩 레이어
  - 여러 개의 Transformer Decoder Layers
  - 출력 레이어 (Softmax를 포함한 단어 생성)
  
- **Transformer Decoder Layer 구성**:
  - Multi-Head Attention (MHA)
  - Feed-Forward Network (FFN)
  - Layer Normalization (LayerNorm)
  - Residual Connection

#### **Sparse Window Attention 적용**
- **Sparse 패턴 생성**:
  - SWA는 중요 토큰(Local + Global)만 선택하여 KV 텐서의 희소성을 높임.
  - 예: "The quick brown fox"에서 **"fox"와 "brown"**만 중요하다고 판단되면 해당 KV만 남김.

#### **모델 흐름 예시**
1. **입력 텍스트**: "The quick brown fox"
2. **Embedding**:
   - 각 단어가 벡터로 변환 (e.g., 768차원).
3. **MHA**:
   - Attention Weight를 계산해 중요한 KV 텐서를 선택 (SWA 적용).
4. **FFN**:
   - MHA 출력을 입력으로 받아 비선형 변환 수행.
5. **다음 토큰 생성**:
   - 최종 출력은 다음 토큰인 "jumps"로 예측됨.

---

### **3. 연산 및 요구사항 분석**

#### **연산량**
1. **Dense Attention**:
   - 입력 시퀀스 길이가 \(n\), 히든 차원이 \(d\)라면, Attention 연산의 계산 복잡도는 \(O(n^2 \cdot d)\).
   - 이는 \(n\)이 길어질수록 비효율적임.

2. **Sparse Window Attention**:
   - 중요 토큰만 선택해 연산을 수행하므로, 계산 복잡도는 \(O(k \cdot n \cdot d)\)로 감소 (\(k\)는 유지되는 희소 토큰의 개수).

#### **메모리 요구량**
1. **Dense Attention**:
   - KV 텐서는 시퀀스 길이에 따라 \(O(n \cdot d)\)의 메모리를 사용.
   - 예를 들어, \(n=1024\), \(d=2048\)라면 단일 레이어에서 \(2 \times 1024 \times 2048 = 4MB\)의 메모리 필요.

2. **Sparse Window Attention**:
   - 희소 패턴 덕분에 실제 KV 텐서 저장에 필요한 메모리 사용량이 감소.
   - \(k \ll n\)일 경우 메모리 사용량은 \(O(k \cdot d)\)로 줄어듦.

#### **컴퓨팅 요구량**
- 기존 Dense Attention 대비, SWA는 불필요한 연산을 줄여 GPU의 FLOPS 사용량을 감소.
- GPU와 CPU 간 데이터 전송을 줄이기 위해 **동적 스케줄링**과 **KV 텐서 재계산**을 적절히 활용.

---

### **4. 실제 환경에서의 동작 예시**

#### **환경 설정**:
- **모델**: OPT-6.7B
- **입력 데이터**: Alpaca 데이터셋 (자연어 질문과 컨텍스트 포함).
- **시퀀스 길이**: 입력 128, 출력 512
- **하드웨어**: NVIDIA Tesla V100 GPU + Intel Xeon CPU

#### **추론 과정**:
1. 입력 문장 "The quick brown fox jumps over the lazy dog"가 들어옴.
2. 모델은 첫 128개 토큰을 처리하고, 이후 단일 토큰씩 생성.
3. SWA를 사용해 중요 토큰만 선택:
   - Local 토큰: 마지막 두 토큰 ("lazy", "dog").
   - Global 토큰: Attention Weight가 높은 "jumps", "fox".
4. Sparse KV Tensor 생성 및 저장:
   - 메모리 사용량이 \(20\%\) 감소.
5. 동적 스케줄링 적용:
   - 초기 단계는 GPU에만 KV 저장(Phase I).
   - 시퀀스 길이가 길어지면 일부 KV를 CPU로 이동(Phase II).
   - 최종적으로 덜 중요한 KV는 GPU에서 재계산(Phase III).

---

### **5. 결론**

- **Sparse Window Attention (SWA)**와 **동적 스케줄링**을 통해 추론 과정의 메모리 및 컴퓨팅 요구량이 크게 감소.
- 이 방법은 LLM 추론 시 GPU 메모리 병목을 완화하고 속도를 대폭 향상시키며, 특히 긴 시퀀스를 처리하는 응용 프로그램에 적합.
- 정확도를 유지하면서 자원을 효율적으로 활용하는 방법론은 대규모 모델 추론에 중요한 기여를 합니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



논문 **"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"**은 대규모 언어 모델(LLM) 추론의 효율성을 개선하는 중요한 기여를 했지만, 몇 가지 한계가 존재합니다. 이러한 한계를 극복하기 위한 연구 흐름과 방향을 아래에 정리했습니다.

---

### **1. 한계 및 극복 방안**

#### **한계 1: 희소성 기반 방법의 일반화 한계**
- **문제점**:
  - Sparse Window Attention(SWA)은 Attention Weight가 희소하다는 가정을 기반으로 하지만, 특정 데이터셋이나 작업에서 이러한 희소성이 충분히 보장되지 않을 수 있음.
  - 예를 들어, 일부 작업(예: 번역, 시각적 설명 생성 등)에서는 모든 토큰이 중요할 가능성이 높음.

- **극복 방안**:
  1. **적응형 희소성 모델**:
     - 데이터셋이나 작업의 특성을 학습하여 희소성을 동적으로 조정.
     - 예: **Learned Sparsity** 기법을 적용하여 모델이 특정 상황에서 희소성을 자동으로 최적화하도록 설계.
  2. **Task-Specific Sparsity**:
     - 특정 작업(예: 생성 vs 분류)에 맞춰 희소성을 설정하는 방법 연구.

---

#### **한계 2: 동적 스케줄링의 복잡성과 오버헤드**
- **문제점**:
  - 동적 스케줄링(특히 GPU와 CPU 간 데이터 이동)은 실행 환경에서 오버헤드를 유발할 수 있음.
  - 특히, GPU-CPU 전송 비용이 높은 시스템에서는 Phase II와 Phase III에서 효율성이 낮아질 가능성이 있음.

- **극복 방안**:
  1. **메모리 계층 최적화**:
     - GPU, CPU, NVM(비휘발성 메모리) 간 다단계 메모리 계층을 활용하여 데이터 전송 비용을 최소화.
     - 예: **High-Bandwidth Memory (HBM)** 기반 솔루션 도입.
  2. **프리페칭 기법**:
     - 미래 사용될 KV 텐서를 미리 예측하고 전송하여 대기 시간을 줄이는 방식 연구.
     - 예: LSTM이나 Transformer를 사용한 **Prefetching Policy** 설계.

---

#### **한계 3: 메모리 재계산(Recompute) 비용**
- **문제점**:
  - Phase III에서 GPU 메모리가 부족할 경우, 덜 중요한 KV 텐서를 삭제하고 필요 시 재계산.
  - 재계산 비용이 데이터 전송 비용보다 클 경우 성능 향상에 한계가 있음.

- **극복 방안**:
  1. **하이브리드 캐싱-재계산 전략**:
     - 중요도에 따라 일부 텐서는 캐싱, 일부는 재계산.
     - 예: 중요도가 높은 KV 텐서는 GPU에 유지하고, 낮은 중요도 텐서는 재계산.
  2. **압축 기반 계산**:
     - KV 텐서를 삭제하는 대신, 압축하여 메모리에 유지 후 필요 시 디코딩.
     - 예: INT4, INT8 기반 **Quantized KV Tensors** 활용.

---

#### **한계 4: 모델 크기의 증가로 인한 확장성 문제**
- **문제점**:
  - SWA와 동적 스케줄링은 특정 크기(예: OPT-30B 수준)에서는 잘 동작하지만, GPT-4와 같은 초대규모 모델에서는 메모리 관리와 계산 효율성이 낮아질 가능성이 있음.

- **극복 방안**:
  1. **분산 추론 최적화**:
     - 여러 GPU와 CPU 클러스터 간 효율적으로 작업을 분산.
     - 예: **Pipeline Parallelism** 및 **Model Parallelism** 결합.
  2. **가속기 하드웨어 활용**:
     - Sparse Attention 연산을 하드웨어 수준에서 가속화.
     - 예: NVIDIA TensorRT, Google TPU, Graphcore IPU 같은 **LLM 전용 하드웨어** 활용.

---

#### **한계 5: 정확도 손실**
- **문제점**:
  - 희소성 비율이 80% 이상으로 높아질 경우, 일부 작업에서 정확도 손실 발생 가능.
  - 특히, 특정 토큰이 중요하게 작용하는 생성 작업에서는 결과 품질이 낮아질 가능성.

- **극복 방안**:
  1. **정확도-희소성 트레이드오프 연구**:
     - 모델이 희소성을 조정할 수 있도록 동적으로 정확도와 효율성을 트레이드오프.
     - 예: Fine-Tuning을 통해 중요한 작업에서 희소성을 낮추는 전략.
  2. **Adaptive Attention Augmentation**:
     - 희소 토큰에 더 많은 계산 자원을 배정하여 정확도를 유지.
     - 예: **Weighted Sparse Attention** 기법 적용.

---

### **2. 미래 연구 흐름**

1. **모델-하드웨어 공동 설계**:
   - Sparse Attention 연산이 하드웨어 효율성을 극대화하도록 최적화된 설계.
   - 예: Sparse Matrix 연산 전용 가속기 설계.

2. **동적 희소성 학습**:
   - Attention Weight의 희소성 비율을 작업이나 데이터셋에 따라 동적으로 조정.
   - 학습 가능한 희소성 기법(Learned Sparsity).

3. **작업 특화 최적화**:
   - 생성, 분류, 번역 등 특정 작업에 적합한 희소성과 스케줄링 전략 개발.

4. **다단계 메모리 계층 연구**:
   - GPU, CPU, NVM(비휘발성 메모리)을 최적화해 LLM의 초대규모 시퀀스를 처리할 수 있는 인프라 설계.

5. **프롬프트 최적화와 결합**:
   - 희소성을 프롬프트 엔지니어링과 결합하여, 입력 단계에서 불필요한 정보 제거.

---

### **결론**
ALISA의 한계를 극복하려면 희소성의 적응형 활용, 메모리와 연산 비용의 동적 조정, 초대규모 환경 확장성을 고려해야 합니다. 앞으로의 연구는 **작업 특화**, **하드웨어-소프트웨어 공동 설계**, **정확도 보장 희소성**에 초점을 맞추어야 하며, 이를 통해 LLM 최적화의 새로운 가능성을 열 수 있을 것입니다.
