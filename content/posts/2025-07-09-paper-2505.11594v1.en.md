---
categories:
- paper-review
- with-gpt
date: "2025-07-09"
tags:
- 2505.11594v1
- LowPrecision
- FP4
- INT8Training
- EfficientAttention
- SageAttention
- TransformerOptimization
- BlackwellGPU
- Quantization
- InferenceAcceleration
- TrainingEfficiency
- CUDA
- Triton
title: '[paper review] SageAttention3: Microscaling FP4 Attention for Inference and An Exploration
  of 8-bit Training'
cover: https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png
---


## \[Paper Review] SageAttention 3 & SageBwd ‚Äî FP4-Powered Inference and 8-bit Training

**Paper link**: [https://arxiv.org/abs/2505.11594v1](https://arxiv.org/abs/2505.11594v1)

### üìù TL;DR

The **SageAttention‚ÄØ3** (inference) and **SageBwd** (training) systems are designed to fully exploit the FP4 Tensor Core on Blackwell‚Äëgeneration GPUs, offering simultaneously:

* **1,038‚ÄØTOPS inference kernel** ‚Üí 5√ó faster than FlashAttention‚ÄØ2,
* **1.67√ó training speed-up**,
* **‚âà zero quality degradation** ‚Äî marking the first instance of low-bit attention moving from demo to practical deployment.

### üí° Core Idea

1. **1‚ÄØ√ó‚ÄØ16-block FP4 microscaling** ‚Äî block-wise scaling of Q,‚ÄØK,‚ÄØV to handle the ¬±7 FP4 value limitation, followed by direct launch of FP4 MM (1600‚ÄØTOPS).
2. **Two-level scaling** ‚Äî splitting Softmax into (row‚Äënormalized ‚Üí FP4 block quantization) halves data-range error by ‚âà‚ÄØ80%.
3. **Selective FP16 gradient** ‚Äî keep only dO‚ÄØV·µÄ in FP16 among 7 backprop matmuls to reduce gradient noise while achieving INT8 training acceleration.

### üèóÔ∏è Background: The Problems Addressed

| Prior SOTA                   | Limitation                              | Result             |
| ---------------------------- | --------------------------------------- | ------------------ |
| **FlashAttention‚ÄØ2 (FP16)**  | Memory/speed bottleneck; no FP4 support | 212‚ÄØTOPS           |
| **FlashAttention‚ÄØ3 (FP8)**   | Hopper-only; no backward pass           | Speed not measured |
| **SageAttention‚ÄØ1/2 (INT8)** | Inference-only; no FP4 utilization      | ‚âà‚ÄØ470‚ÄØTOPS         |

**Research gap**: there's no FP4‚Äëbased attention kernel nor low-bit attention training support until this work.

### üöÄ Novel Approach: SageAttention‚ÄØ3 (Inference) & SageBwd (Training)

| Component           | Precision         | Function                                      | Key Benefit                         |
| ------------------- | ----------------- | --------------------------------------------- | ----------------------------------- |
| **SageAttention‚ÄØ3** | FP4               | 1√ó16 microscaling + two‚Äëlevel scaling + FP4MM | 1,038‚ÄØTOPS on RTX 5090              |
| **SageBwd**         | INT8 (+ one FP16) | 6/7 matmul in INT8; only dO‚ÄØV·µÄ in FP16        | 1.67√ó training speed-up on RTX 4090 |


### ‚öôÔ∏è How It Works: Explained with a Toy Example

#### Toy Scenario ‚Äî 3 tokens √ó 4 channels

```text
Q = [ 1‚ÄÜ ‚àí2‚ÄÜ  3‚ÄÜ 0 ]        s_Q = 0.5  ‚Üí  QÃÇ = [ 2 ‚àí4 6 0 ]
K = [ 2‚ÄÜ  1  ‚àí1  4 ]  ‚Äì‚Äìœï‚Äì‚Äì‚ñ∂ KÃÇ
1Ô∏è‚É£ FP4MM(QÃÇ, KÃÇ)        ‚Üí  S = 18
2Ô∏è‚É£ Softmax(S)          ‚Üí  PÃÉ = 1
3Ô∏è‚É£ Two-Level Scaling   ‚Üí  PÃÇ = 7, s_P1 ‚âà 3.7e-4
4Ô∏è‚É£ FP4MM(PÃÇ, VÃÇ)        ‚Üí  O_tmp = 20
5Ô∏è‚É£ Restore             ‚Üí  O = O_tmp √ó s_P1 ‚âà 7.4e-3
```

The same pipeline applies directly to 3√ó3 image patches, meaning it also accelerates **vision models**.

---

### üìä Performance Evaluation: Key Results

| Metric                  | Setting      | Sage           | Baseline                   | Gain        |
| ----------------------- | ------------ | -------------- | -------------------------- | ----------- |
| **Kernel Throughput**   | RTX 5090     | **1,038‚ÄØTOPS** | FlashAttention 2: 212‚ÄØTOPS | **‚âà‚ÄØ5√ó**    |
| **End-to-End Latency**  | HunyuanVideo | **164‚ÄØs**      | 489‚ÄØs                      | **3.0√ó ‚Üì**  |
|                         | CogVideoX    | **27‚ÄØs**       | 64‚ÄØs                       | **2.4√ó ‚Üì**  |
| **Training Step**       | Llama 16K    | **5.2‚ÄØs**      | 6.0‚ÄØs                      | **1.15√ó ‚Üì** |
| **Finetuning Accuracy** | GSM8K        | 0.520          | 0.521                      | ‚àí0.1‚ÄØpp     |
|                         | MMLU         | **0.653**      | 0.640                      | +1.3‚ÄØpp     |

KV-cache memory usage is reduced by **75‚ÄØ% vs. FP16**, enabling up to **4√ó batch size** or **32K sequence length** on the same GPU.

---

### üîç Our Take: Strengths, Limitations, and Why It Matters

#### ‚úÖ Strengths

1. **Speed + Memory + Quality** ‚Äî achieves 5√ó speed-up with virtually zero loss in accuracy.
2. **First Practical Low-Bit Training** ‚Äî demonstrates 8-bit attention training without degradation.
3. **Plug-and-Play** ‚Äî simply swap in the kernel to existing model code for instant gain.

#### ‚ùó Limitations

* **Blackwell-Specific** ‚Äî these gains aren‚Äôt applicable to A100/H100.
* **Pretraining Convergence** ‚Äî training from scratch in 8-bit converges more slowly.
* **Efficiency Gap** ‚Äî kernel is still 20‚Äì30‚ÄØ% short of theoretical FLOPS peak; more Triton optimization needed.

#### üí° Why This Work Matters

This is the **first practical demonstration** of low-precision Tensor Core usage on next-gen GPUs. It lifts low-bit attention from ‚Äúdemo-only‚Äù to **production-ready**, supporting both inference and training in real-world pipelines.

---

### üõ£Ô∏è What‚Äôs Next?

1. **Kernel 2.0** ‚Äî Redesign in Triton/CUTLASS to close the gap with theoretical 4√ó acceleration.
2. **Full Low-Bit Stack** ‚Äî Unify MLP, normalization, optimizer in FP4/INT8.
3. **Cross-HW Adaptation** ‚Äî Enable ‚Äúpseudo-FP4‚Äù on Hopper/TPU.
4. **Adaptive Precision Training** ‚Äî Use 8-bit early, 4-bit late with dynamic scheduling.
5. **Responsible Deployment** ‚Äî Develop watermarking that‚Äôs aware of low-bit precision to prevent misuse in deepfake applications.

> **Summary**: SageAttention‚ÄØ3 & SageBwd dismantle the notion that ‚Äúlow-bit = slow or poor quality,‚Äù and offer a new standard for **FP4 inference + 8-bit training**. The next challenge is to make it **universal and responsible**, across hardware, models, and ethical deployment.

<details markdown="block">
<summary>‚ñ∂Ô∏è <strong>Click to expand for full Q&A analysis</strong></summary>


## Prompt 1.1.1 ‚Äî **Research Gap Analysis**

> *"Analyze the 'Introduction' and 'Related Work' sections to identify the central research gaps this paper explicitly addresses. What limitations of prior work do the authors emphasize? What was the state-of-the-art at the time of publication?"*

### üöÄ Key Takeaways

* **Gap 1 ‚Äî No FP4 Attention Kernels**: As of 2025, there were no attention kernels capable of using Blackwell GPUs' **FP4 Tensor Cores**.
* **Gap 2 ‚Äî No Low-Bit Trainable Attention**: All previous low-bit attention methods (‚â§ 8 bits) were **inference-only**, with no support for backpropagation or gradient computation.
* **Prior SOTA** (e.g., FlashAttention 2/3) relied on FP16/FP8, capped at \~212 TOPS on RTX 5090, with some methods being **Hopper-exclusive** or lacking training support.
* **SageAttention 3** is the **first to enable FP4 inference (1038 TOPS)** and **8-bit trainable attention**, addressing both gaps simultaneously.

---

### 1. Explicit Research Gaps & Open Questions

| #   | Description                                                                                                         | Supporting Quote                                              |
| --- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| ‚ë†   | **No FP4 Attention Kernels** ‚Äî No way to exploit the >1 PFLOPS FP4 TC on Blackwell GPUs                             | ‚ÄúWe design the first FP4 attention‚Ä¶‚Äù                          |
| ‚ë°   | **Low-Bit Attention = Inference Only** ‚Äî FlashAttention 3, SageAttn 1/2 all **forward-only**                        | ‚ÄúPrevious low-bit attention works ‚Ä¶ focus only on inference.‚Äù |
| ‚ë¢   | **Quantization Challenges** ‚Äî (C1) FP4 value limits, (C2) narrow FP8 scale range, (C3) gradient quantization errors | ‚ÄúThere are two primary obstacles‚Ä¶‚Äù                            |
| ‚ë£   | **Trainable 8-bit Attention Missing** ‚Äî No prior success with 8-bit gradients in attention backprop                 | ‚ÄúNo prior work has explored low-bit attention for training‚Ä¶‚Äù  |

The authors tackle these challenges using **Microscaling FP4**, **Two-Level Scaling**, and **Selective FP16 Gradients** to build a practical inference and training solution.

---

### 2. State-of-the-Art (SOTA) at Time of Publication

| Method                   | Precision | HW Scope        | Kernel Speed (RTX5090)  | Backward Support | Limitations                           |
| ------------------------ | --------- | --------------- | ----------------------- | ---------------- | ------------------------------------- |
| FlashAttention 2         | FP16      | All GPUs        | ‚âà 212 TOPS              | ‚úÖ                | High precision ‚Üí slow & memory-heavy  |
| FlashAttention 3         | FP8       | **Hopper only** | N/A on RTX5090          | ‚ùå                | Forward-only, low compatibility       |
| xFormers (CUDA)          | FP16      | All GPUs        | 8‚Äì11√ó slower than Sage3 | ‚úÖ                | Not optimized for low-bit performance |
| SageAttention 1/2        | INT8      | All GPUs        | \~470 TOPS              | ‚ùå                | Inference-only, no FP4 TC utilization |
| **SageAttention 3**      | **FP4**   | **Blackwell**   | **1038 TOPS**           | ‚ùå                | First FP4 kernel                      |
| **SageBwd (this paper)** | **INT8**  | RTX4090+        | 1.67√ó training speedup  | ‚úÖ                | Slower convergence during pretraining |

> In short: Existing methods hit speed/memory bottlenecks or lacked trainability. **SageAttention 3 and SageBwd fill all these gaps.**

---

### 3. How This Paper Solves the Gaps

1. **Microscaled FP4 Attention**: Quantizes Q, K, V into 1√ó16 blocks to avoid FP4 value limitations and achieves **1038 TOPS**.
2. **Trainable 8-bit Attention (SageBwd)**: Uses INT8 for 6/7 matmuls in backprop while retaining 1 in FP16 to preserve accuracy.
3. **Practical Acceleration**: Video models like HunyuanVideo show **3√ó lower latency**, validating real-world performance.


## Prompt 1.1.2 ‚Äî **Central Hypothesis**

> *"What is the central hypothesis or core claim of this paper? Express it in a single clear sentence, ideally in the format: 'The authors hypothesize that \[proposed method] can overcome \[prior limitations] to achieve \[specific outcomes].'"*

**The authors hypothesize that** by using **SageAttention 3 with FP4 microscaling** for inference and **SageBwd for 8-bit trainable attention**, they can overcome prior limitations of not leveraging FP4 Tensor Cores and the inference-only nature of low-bit attention, **achieving 5√ó faster inference (1038 TOPS on RTX5090)** and **1.67√ó faster training (on RTX4090)** **without accuracy degradation**.

---

## Prompt 1.2.1 ‚Äî **Main Contributions**

> *"List the 1‚Äì3 most important and original contributions of the paper. For each, specify whether it‚Äôs a new architecture component, training technique, theoretical insight, dataset, or novel application of an existing method."*

### üöÄ Summary of Contributions

* **SageAttention 3 (FP4)**: First-ever FP4 attention kernel, achieving **1038 TOPS** on RTX5090 ‚Äî **5√ó faster than FlashAttention 2**.
* **SageBwd (8-bit trainable attention)**: First to enable low-bit attention with backpropagation, achieving **1.67√ó faster training** without compromising accuracy.
* **Quantization Techniques**: A novel combination of **1√ó16 block microscaling** and **Two-Level Scaling** drastically reduces FP4/INT8 quantization errors ‚Äî CosSim +1.15%, RMSE ‚Äì79%.

---

### üß† Detailed Contribution Table

| #   | Contribution                         | Type                       | Description                                                                                                                                               |
| --- | ------------------------------------ | -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ‚ë†   | **SageAttention 3 (FP4 kernel)**     | New architecture component | Custom kernel design using 1√ó16 microscaling, softmax-quantize fusion, and warp-level scheduling. Achieves 1038 TOPS on RTX5090 ‚Äî **5√ó faster than FA2**. |
| ‚ë°   | **SageBwd (8-bit training)**         | New training technique     | Uses INT8 for 6/7 backward matmuls, retaining only **dO¬∑V·µÄ** in FP16 to reduce gradient error while achieving **1.67√ó faster training**.                  |
| ‚ë¢   | **Microscaling + Two-Level Scaling** | Theoretical insight        | Addresses FP4‚Äôs value/clipping issues and scale range limits, improving CosSim from 98.4% to 99.5%, and RMSE from 0.994 to 0.201.                         |

---

### üåü Why It Matters

* **Hardware shift enabler**: Unlocks the full potential of Blackwell FP4 Tensor Cores ‚Äî **1 PFLOPS-class capability** made practical.
* **Trainable low-bit attention**: Extends low-bit quantization beyond inference for the first time.
* **Accuracy-speed balance**: Resolves the long-standing tradeoff between efficiency and quality in low-bit transformers.

> Together, these contributions define **a new standard for low-precision attention**, paving the way for faster and cheaper LLM inference and training.


## Prompt 1.2.2 ‚Äî **Author's Perspective on Strengths**

> *"From the authors' point of view, why is their approach superior to previous ones? Quote or paraphrase their key arguments supporting the originality and strengths of their work."*

### üöÄ Summary in 3 Points

1. **Superior speed** ‚Äî Achieves 1038 TOPS on RTX5090, **5√ó faster** than FlashAttention 2 by fully utilizing FP4 Tensor Cores.
2. **Preserved quality** ‚Äî Thanks to Microscaling and Two-Level Scaling, **virtually no accuracy loss** is observed in inference, and SageBwd maintains **parity with BF16** in fine-tuning.
3. **Broad applicability** ‚Äî First-ever trainable low-bit attention; avoids "inference-only" and "Hopper-only" limitations in prior work.

---

### üîç Key Superiority Claims and Evidence

| Category                      | Claim                                                                                 | Supporting Evidence                                                      | Why It Outperforms Prior Work                                   |
| ----------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | --------------------------------------------------------------- |
| **‚ë† Speed & Utilization**     | Achieves **1038 TOPS** on RTX5090 ‚Äî 5√ó faster than FA2                                | Fig. 1 shows FlashAttn2 = 212 TOPS vs SageAttn3 = 1038 TOPS              | FA2/FA3 don't use FP4 Tensor Cores                              |
| **‚ë° Quality Preservation**    | ‚ÄúAlmost no end-to-end quality loss across various models‚Äù                             | Evaluation results on CLIPSIM, FID, GSM8K, MMLU show <¬±0.3 pp deviation  | FA3 (FP8) degrades accuracy depending on model/task             |
| **‚ë¢ Backward Support**        | 8-bit **trainable** attention matches BF16 accuracy                                   | <0.3 pp gap across multiple datasets, multiple seeds                     | All previous low-bit attention (FA3, Sage1/2) were forward-only |
| **‚ë£ Quantization Robustness** | Addresses (C1) FP4 range, (C2) scale overflow, (C3) gradient noise                    | Overcomes challenges via Microscaling + 2-Level Scaling + Selective FP16 | Previous per-tensor/channel quantization was lossy              |
| **‚ë§ Plug-and-Play Usability** | Accelerates video/text/image models like HunyuanVideo, CogVideoX with no model change | Table 4 shows latency reduction of **2.4‚Äì3.0√ó**                          | FA3 is Hopper-only and not tested on video/image tasks          |

---

### üîÅ Author‚Äôs Logical Flow

1. **Hardware Potential ‚Üí Speed**: Blackwell's FP4 Tensor Cores offer \~8√ó FLOPS over FP16, yet were unused. SageAttn3 achieves **5√ó faster kernels**.
2. **Precision Worries ‚Üí Microscaling**: FP4's 15-value limit and narrow scale range are solved via token-wise normalization + 1√ó16 block quantization.
3. **Training Limitation ‚Üí Selective FP16**: Only **dO¬∑V·µÄ** matmul is kept in FP16 to avoid gradient drift, enabling 8-bit training with **no accuracy loss**.
4. **Empirical Proof**: Kernel latency, E2E throughput, and accuracy across diverse benchmarks prove the ‚Äúfast yet accurate‚Äù claim.

---

### üìå Core Message

> **In the authors‚Äô view**: ‚ÄúThrough FP4 Microscaling and 8-bit SageBwd, we simultaneously achieve **speed, memory efficiency, and accuracy**, turning low-bit attention from a 'tech demo' into a **practical inference + training tool**.‚Äù

---

## Prompt 1.3.1 ‚Äî **Step-by-Step Algorithm Explanation (With Toy Example)**

> *"Explain the core algorithm, model architecture, or method step-by-step as if to a graduate student. Use toy examples (e.g., 3x3 image or small matrix) to show how the input transforms through each step. Define all variables clearly."*

---

### üöÄ 5-Line Summary

1. **Microscaling FP4 Attention (SageAttn3)** uses `1√ó16` block quantization to run both QK·µÄ and PV in FP4MM ‚Äî achieving **1038 TOPS on RTX5090 (5√ó faster)**.
2. **Two-Level Scaling** splits softmax scaling into row normalization + block quantization, **reducing FP8 scale error by ‚âà 80%**.
3. **SageBwd (8-bit Training)** performs 6 of 7 backward matmuls in INT8, keeping only **dO¬∑V·µÄ in FP16** to accelerate training **1.67√ó** without loss.
4. **Key Idea**: Quantize aggressively, reuse softmax scaling, and exploit warp-level pipelining for optimal speed-memory-accuracy trade-off.
5. **Concrete toy examples** (3-token √ó 4-dim matrix and 3√ó3 image patch) demonstrate how every step transforms the input to the final attention output.


## üîß 1. Terms & Notation (Refer as Needed)

| Symbol      | Definition (FPx = FPx-bit floating point) |
| ----------- | ----------------------------------------- |
| **Q, K, V** | Query, Key, Value matrices (in FP16)      |
| **sX**      | FP8 scale factor matrix for X             |
| **XÃÇ**       | Quantized value of X (FP4 or INT8)        |
| **œï(¬∑)**    | Microscaling quantization:                |

```
                      ‚ÄÉ‚ÄÉ`s = max(|X_block|)/6`, ‚ÄÉ‚ÄÉ`XÃÇ = round(X/s)`                        |
```

\| **FP4MM**               | FP4 matrix multiply: `C = FP4MM(√Ç, s_A, ÃÇB, s_B)`                    |
\| **PÃÉ, PÃÇ**               | Softmax result (PÃÉ), and its quantized FP4 block version (PÃÇ)        |
\| **Two-Level Scaling**   | `(row normalization ‚Üí FP4 block quantization)` applied to softmax PÃÉ |

---

## üìò 2. SageAttention 3 ‚Äì Inference Path Step-by-Step

> **Goal**: Fully utilize FP4 Tensor Cores (1600 TOPS) while maintaining accuracy loss < 0.1 pp.

| Step                     | Operation                                                  | Notes                                    |
| ------------------------ | ---------------------------------------------------------- | ---------------------------------------- |
| **0. Preprocessing**     | Center K: `K ‚Üê K ‚àí mean(K)`                                | Mitigates outliers (also used in Sage 1) |
| **1. œï Quantization**    | `QÃÇ, s_Q = œï(Q)`, `KÃÇ, s_K = œï(K·µÄ)`                          | Apply **1√ó16 block** scaling             |
| **2. FP4 MatMul**        | `S = FP4MM(QÃÇ, s_Q, KÃÇ, s_K)`                                | 8√ó speedup vs. FP16                      |
| **3. Online Softmax**    | `m = rowmax(S)`, `PÃÉ = exp(S ‚àí m)`                          | Uses rowmax reuse for fast softmax       |
| **4. Two-Level Scaling** | `s_P1 = rowmax(PÃÉ)/(448√ó6)`, `PÃÉ ‚Üê PÃÉ/s_P1`, `PÃÇ, s_P2 = œï(PÃÉ)` | Expands FP8 scale range                  |
| **5. FP4 MatMul**        | `O_tmp = FP4MM(PÃÇ, s_P2, VÃÇ, s_V)`                           | Second FP4 matmul                        |
| **6. Restore Output**    | `O = O_tmp √ó s_P1`                                         | Final rescaling                          |

> **Entire procedure** is shown in Algorithm 1 (lines 1‚Äì15)

---

## üîÅ 3. SageBwd ‚Äì Training Path Highlights

* **Forward (Alg. 2)**: QK·µÄ and PV both quantized via œï to INT8; PÃÉ is row-wise quantized (1/127) with FP32 scale.
* **Backward (Alg. 3)**: Out of 7 matmuls, **only dO¬∑V·µÄ** is kept in FP16, the rest are INT8 ‚Üí suppresses gradient error accumulation.
* **Result**: Fine-tuning accuracy = BF16, training speed **1.67√ó faster**.

---

## üé≤ 4. Toy Example ‚ë† ‚Äî Text (3 tokens, d = 4)

> **Input**: ‚ÄúA B C‚Äù tokens ‚Üí FP16 embeddings
> Q = \[1, ‚Äì2, 3, 0], ‚ÄÉ‚ÄÉK = \[2, 1, ‚Äì1, 4]

| Step              | Computation                                           | Sample Value |
| ----------------- | ----------------------------------------------------- | ------------ |
| 1. œï Quantization | `s_Q = 3/6 = 0.5`, ‚ÄÉ‚ÄÉ`QÃÇ = [2, ‚Äì4, 6, 0]`              |              |
| 2. S Matrix       | `S = FP4MM(QÃÇ, s_Q, KÃÇ, s_K)` ‚Üí e.g., 18.0              |              |
| 3. Softmax        | `m = 18`, ‚ÄÉ‚ÄÉ`PÃÉ = exp(0) = 1`                          |              |
| 4. 2-Level Scale  | `s_P1 ‚âà 3.7e-4`, ‚ÄÉ‚ÄÉ`PÃÉ‚ÇÇ = 2700`, ‚ÄÉ‚ÄÉ`PÃÇ = clip to max 7` |              |
| 5. Output MatMul  | `O_tmp = 20`, ‚ÄÉ‚ÄÉ`O = 20 √ó s_P1 ‚âà 7.4e-3`              |              |
| 6. Final Output   | Final attention output for token A = **\~7.4 √ó 10‚Åª¬≥** |              |

*Values are illustrative for understanding only.*

---

## üñºÔ∏è 5. Toy Example ‚ë° ‚Äî 3√ó3 Image Patch

| Pixel Position | R,G,B (0‚Äì1) | Patch Vector P·µ¢        |
| -------------- | ----------- | ---------------------- |
| (0,0)          | 0.1 0.2 0.3 | \[0.1, 0.2, 0.3, 0, 0] |
| ‚Ä¶              | ‚Ä¶           | ‚Ä¶                      |

Same quantization + FP4MM steps are applied ‚Äî per-patch scale ‚âà 0.05, quantized values ‚àà \[‚Äì5, 5].

> Like text tokens, **image tokens** follow the exact same FP4 attention pipeline.

---

## üîÅ 6. Parameter Flow Overview (Diagram)

```
FP16 Q, K, V
   ‚îÇ   œï(¬∑) per block
   ‚îú‚îÄ‚îÄ‚ñ∫ QÃÇ, s_Q ‚îÄ‚îê
   ‚îÇ             ‚îÇ FP4MM (TC)
   ‚îú‚îÄ‚îÄ‚ñ∫ KÃÇ, s_K ‚îÄ‚îò   ‚îÇ
   ‚ñº                 ‚ñº
   S ‚îÄ‚îÄOnlineSoftmax‚îÄ‚îÄ‚ñ∫ PÃÉ ‚îÄ‚îÄTwo-Level‚îÄ‚îÄ‚ñ∫ PÃÇ, s_P2
                                     ‚îÇ
FP4MM (TC) ‚óÑ‚îÄ‚îÄ VÃÇ, s_V ‚óÑ‚îÄ‚îÄ œï(V)
   ‚îÇ
   ‚ñº
 O = O_tmp √ó s_P1  ‚Üí  Final FP16 Output
```

---

## üß† 7. Summary

* **Microscaling œï**: Per-block (1√ó16) scaling resolves FP4's 15-value constraint (C1)
* **Two-Level Scaling**: Solves scale range limitation (C2) of FP8
* **Selective FP16**: Mitigates gradient accumulation error (C3) in training

‚Üí Together, they enable **5√ó faster inference** and **1.67√ó faster training** with **no accuracy loss**.

> **Core Intuition**: *‚ÄúSlice and reuse scale factors.‚Äù* ‚Äî Even under ultra-low precision, SageAttention 3 and SageBwd **maximize Tensor Core usage with minimal information loss**.



## üîç Prompt 1.3.2 (Identifying the ‚ÄúSecret Weapon‚Äù)

> ‚ÄúIdentify the single most critical mathematical formula, algorithm step, or architectural component that enables the key contribution of this paper. Explain its function and why it is essential to the success of the proposed method.‚Äù

### üöÄ One-line Summary

**1√ó16 Block FP4 Microscaling Quantization**

$$
s_{ij} = \frac{\max \left( \lvert X_{ij} \rvert \right)}{6}, \quad
\hat{X}_{ij} = \operatorname{round} \left( \frac{X_{ij}}{s_{ij}} \right)
$$

‚ÄîThis single equation transforms Q, K, V into a format directly compatible with FP4 Tensor Cores, enabling **1600 TOPS matrix multiplication (‚âà8√ó FP16)** on the RTX5090. It also significantly improves precision (CosSim ‚Üë by 1.1 pp, RMSE ‚Üì by 79%).

---

### Why This Formula is the ‚ÄúSecret Weapon‚Äù

| Function                       | Description                                                                                                               | Supporting Evidence from Paper                                       |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| **Dynamic Per-block Scaling**  | Applies 1√ó16 token block-specific scaling, mapping values to the FP4 range (¬±7), isolating outliers within blocks         | ‚ÄúQuantization group size 1√ó16 ‚Ä¶ improving FP4 quantization accuracy‚Äù |
| **Directly Enables FP4MM**     | Removes the need for dequantization (`œï‚Åª¬π`) by producing inputs directly consumable by FP4MM kernels (FP4 ISA)            | ‚ÄúFP4 microscaling MatMul ‚Ä¶ 1600 TOPS vs 200 TOPS‚Äù                    |
| **Preserves Precision**        | Stores scale in FP8 (E4M3) format to minimize overflow/underflow issues ‚Üí CosSim ‚Üë from 98.4 ‚Üí 99.5, RMSE ‚Üì 0.994 ‚Üí 0.201 | See Table 1(a), Figure 12(c)                                         |
| **Foundation for Later Steps** | Same quantization formula reused in softmax output and gradient steps ‚Üí Enables Two-Level Scaling and Selective FP16      | Used in Algorithm 1 & 3                                              |

### Final Insight

Without this **microscaling quantization**, FP4‚Äôs 15-value limit would result in massive quantization error, either crippling accuracy or rendering FP4 Tensor Cores unusable. Therefore, **all of SageAttention 3‚Äôs speed and precision gains rest on this one equation**.

---

## üìà Prompt 1.4.1 (Core Experimental Results)

> ‚ÄúAnalyze the core results presented in the ‚ÄòExperiments‚Äô or ‚ÄòResults‚Äô section, including figures and tables. What are the key performance metrics used? What benchmarks are reported? Summarize the results the authors emphasize most as proof of the method‚Äôs success.‚Äù

### üöÄ TL;DR (3 Key Points)

1. **Inference Speed**: Achieves **1038 TOPS** on RTX5090 ‚Äî \~5√ó faster than FlashAttention 2 (212 TOPS).
2. **Accuracy**: Quality loss is **< 0.3 pp** on tasks like CogVideoX, Stable Diffusion, and HunyuanVideo. SageBwd‚Äôs 8-bit fine-tuning performs on par with BF16.
3. **Training Speed**: SageBwd speeds up training by **1.67√ó** (e.g., 6.0 ‚Üí 5.2 seconds per step on Llama 16K).

---

## 1. Core Performance Metrics

| Category                      | Metric                                                  | Purpose                             |
| ----------------------------- | ------------------------------------------------------- | ----------------------------------- |
| **Kernel/System Efficiency**  | Throughput (TOPS), sec/iter, tokens/sec (TPS)           | Evaluate GPU efficiency & latency   |
| **Vision Generation Quality** | CLIPSIM ‚Üë, CLIP-T ‚Üë, FID ‚Üì, sFID ‚Üì, VQA-a/t ‚Üë, FScore ‚Üë | Evaluate T2I/T2V model quality      |
| **Language Model Accuracy**   | GSM8K Acc ‚Üë, DROP F1 ‚Üë, MMLU Acc ‚Üë, HellaSwag Acc ‚Üë     | Verify fine-tuning fidelity         |
| **Training Stability**        | Pre-training / Fine-tuning loss curves                  | Assess low-bit training reliability |

---

## 2. Benchmarks, Datasets, and Models

* **Text-to-Text**: Qwen 2.5 (1.5B, 3B), Llama 3.2 (1B, 3B)
  ‚ÄÉ‚Üí Datasets: GSM8K, DROP, MMLU, HellaSwag
* **Text-to-Video**: CogVideoX (2B), HunyuanVideo, Mochi
* **Text-to-Image**: Flux, Stable-Diffusion 3.5
* **Pre-training**: FineWeb-Edu corpus (Llama 400M)

---

## 3. Highlighted Results at a Glance

| Category               | Metric / Environment      | SageAttention 3 / SageBwd | Baseline (FlashAttn2/BF16) | Gain       |
| ---------------------- | ------------------------- | ------------------------- | -------------------------- | ---------- |
| **Kernel**             | Throughput, RTX5090       | **1038 TOPS**             | 212 TOPS                   | **‚âà5√ó**    |
| **E2E Latency**        | CogVideoX                 | **27 sec**                | 64 sec                     | **2.4√ó ‚Üì** |
|                        | HunyuanVideo              | **164 sec**               | 489 sec                    | **3.0√ó ‚Üì** |
| **Vision Quality**     | CLIPSIM (Video)           | **0.1881**                | 0.1865                     | +0.0016    |
|                        | FID (Image)               | **162.1**                 | 162.8                      | ‚Äì0.7       |
| **Training Speed**     | Llama 16K, iteration time | **5.2 sec**               | 6.0 sec                    | **1.15√ó**  |
| **Fine-tune Accuracy** | GSM8K (Qwen 1.5B)         | 0.520                     | 0.521                      | ‚Äì0.1 pp    |
|                        | MMLU (Qwen 3B)            | **0.653**                 | 0.640                      | +1.3 pp    |

> Interpretation: **SageAttention 3 boosts speed**, and **SageBwd accelerates training** ‚Äî while preserving or slightly improving model quality.

---

## 4. Authors‚Äô Key Evidence for Success

1. **Maximizes Hardware** ‚Äî Fully utilizes FP4 Tensor Core to surpass existing throughput limits on RTX5090.
2. **Real-World Latency Gains** ‚Äî Achieves 2‚Äì3√ó faster latency in real T2V/T2I models (HunyuanVideo, CogVideoX).
3. **Near-Zero Quality Loss** ‚Äî CLIPSIM, FID, MMLU, GSM8K show deviations **‚â§ 0.3 pp**.
4. **Trainable Low-Bit Attention** ‚Äî First report of 8-bit attention with BF16-equivalent accuracy and 1.67√ó speedup.

---

### üîö Summary

SageAttention 3 + SageBwd achieves **kernel-level acceleration**, **system-wide latency reduction**, and **quality preservation** simultaneously. It provides the first practical demonstration that **ultra-low-bit attention** can be deployed in both **inference and training pipelines** at scale.


## üîç Prompt 1.4.2 (Critical Comparison with SOTA)

> ‚ÄúHow does the proposed method compare to baseline and state-of-the-art models discussed in the paper? Identify the most compelling comparative results that support the authors‚Äô claims. Conversely, are there any cases where the proposed method fails to outperform others or shows minimal improvement? If so, how do the authors explain these?‚Äù

### üöÄ Summary in 3 Lines

1. **Speed** ‚Äì SageAttention 3 achieves **1038 TOPS** on RTX5090, \~**5√ó faster** than FlashAttention 2 (212 TOPS), and reduces real-world latency (e.g., HunyuanVideo) by **3√ó**.
2. **Accuracy** ‚Äì SageBwd‚Äôs 8-bit backward pass maintains performance within **¬±0.3 pp** of BF16 across GSM8K, MMLU, etc., while achieving **1.67√ó training speedup**.
3. **Limitations** ‚Äì (‚Ö∞) Pretraining convergence is slower than BF16, and (‚Ö±) actual throughput falls **20‚Äì30% short of theoretical FP4 TC peak**. The authors attribute this to gradient quantization error and suboptimal Triton kernel tuning.

---

## 1. Quantitative Comparison with Baselines

| Category              | Metric / Setup            | SageAttention 3 / SageBwd | Baseline (FlashAttn2 / BF16) | Improvement   |
| --------------------- | ------------------------- | ------------------------- | ---------------------------- | ------------- |
| **Kernel**            | Throughput (RTX5090)      | **1038 TOPS**             | 212 TOPS                     | **\~4.9√ó ‚Üë**  |
| **E2E Inference**     | Latency (HunyuanVideo)    | **164 sec**               | 489 sec                      | **\~3.0√ó ‚Üì**  |
|                       | Latency (CogVideoX)       | **27 sec**                | 64 sec                       | **\~2.4√ó ‚Üì**  |
| **Training Speed**    | Llama 16K seq / iteration | **5.2 sec**               | 6.0 sec                      | **1.15√ó ‚Üì**   |
|                       | Fwd + Bwd (RTX4090)       | **1.67√ó faster**          | 1.0√ó                         | **+67%**      |
| **Finetune Accuracy** | GSM8K (Qwen 1.5B)         | 0.520                     | 0.521                        | ‚Äì0.1 pp       |
|                       | MMLU (Qwen 3B)            | **0.653**                 | 0.640                        | +1.3 pp       |
| **Image Quality**     | Flux FID                  | **162.1**                 | 162.8                        | ‚Äì0.7 (better) |

> üîë **Most Convincing Comparison**: Kernel-level **1038 TOPS** and **3√ó faster inference latency** strongly validate the authors‚Äô claim of ‚Äúlow-bit attention without sacrificing performance.‚Äù

---

## 2. Where It Falls Short & Author's Explanation

| Observation                        | Detail                                                   | Author‚Äôs Explanation                                                                  |
| ---------------------------------- | -------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| **Slower Pretraining Convergence** | Loss curve during pretraining is slower vs. BF16         | Gradient quantization error from low-bit matmuls accumulates over long training       |
| **Sub-theoretical Speed**          | 1.67√ó vs. expected 4√ó speedup on FP4 TC                  | ‚ÄúDue to suboptimal Triton implementation‚Äù; acknowledges kernel tuning is still needed |
| **Minor Accuracy Dip**             | Llama 1B, HellaSwag Acc = 0.823 vs. BF16 0.828 (‚Äì0.5 pp) | Attributed to statistical variance; not significant according to authors              |

---

## 3. Interpretation

The strongest support for the authors‚Äô superiority claims lies in **absolute throughput and real-model latency gains**. However, for **low-bit training at scale**, challenges like **pretraining convergence** and **non-ideal kernel tuning** remain. These open avenues for continued research.

> ‚úÖ **Bottom Line**: SageAttention 3 / SageBwd outperform most baselines in speed, memory, and quality ‚Äî but **full-stack low-bit training** is not yet fully solved, leaving room for future work.

---

## üöß Prompt 1.5.1 (Limitations ‚Äî Stated and Potential)

> ‚ÄúWhat limitations or failure cases do the authors explicitly acknowledge in the paper? Based on your analysis, what additional limitations or risks do you see that are not directly addressed?‚Äù

### üöÄ Summary

* **Author-Stated Limitations**:

  1. **Slow convergence in pretraining** ‚Äî While finetuning is stable, low-bit gradients cause slower pretraining convergence.
  2. **Throughput below theoretical peak** ‚Äî Current implementation achieves \~70‚Äì80% of FP4 theoretical FLOPS due to Triton kernel limitations.
  3. **Remaining mixed precision** ‚Äî One backward matmul (`dO V·µÄ`) must still run in FP16 to avoid instability.

* **Additional Potential Limitations** (our analysis):

  * Hardware dependency (Blackwell-only)
  * Scale memory overhead (\~6.25%)
  * Potential degradation in ultra-long contexts or cross-domain tasks
  * Ecosystem fragmentation (e.g., PyTorch/CUTLASS-specific integration)
  * Ethical risks (e.g., deepfakes from accelerated T2V)
  * Lack of distributed or multi-node performance testing

---

## 1. Limitations Acknowledged by the Authors

| ID      | Type                    | Description                                                                                       | Impact                                      |
| ------- | ----------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| **E-1** | Pretraining convergence | ‚ÄúSageBwd ‚Ä¶ convergence speed is relatively slow. This limits applicability in pretraining tasks.‚Äù | Slower optimization in large-scale training |
| **E-2** | Throughput gap          | ‚ÄúGap between current speed and theoretical upper bounds ‚Ä¶ due to sub-optimal Triton kernel.‚Äù      | Leaves 20‚Äì30% speed unrealized              |
| **E-3** | Mixed precision needed  | ‚ÄúdO V·µÄ matmul must remain in FP16 to suppress gradient noise.‚Äù                                    | Full 8-bit training not yet achieved        |

---

## 2. Additional Potential Limitations

| Category                   | Issue                                                                                    | Rationale                                     |
| -------------------------- | ---------------------------------------------------------------------------------------- | --------------------------------------------- |
| **Hardware Dependency**    | Requires Blackwell‚Äôs FP4 Tensor Core ‚Üí not compatible with A100/H100 or earlier RTX GPUs | All benchmarks were done on RTX5090           |
| **Scale Memory Overhead**  | Each 1√ó16 block requires FP8 scale metadata ‚Üí \~6.25% memory overhead                    | Not negligible at scale                       |
| **Precision Accumulation** | Long sequences (>32K) or multimodal domains may accumulate rounding errors               | No stress tests shown in these regimes        |
| **Portability**            | CUTLASS/Triton kernels not yet generalized for JAX/XLA or TPU                            | Ecosystem integration effort required         |
| **Ethical Risks**          | T2V inference time drops 3√ó ‚Üí risk of deepfake/abuse increases                           | Authors propose watermarking in future work   |
| **Reproducibility**        | Code not yet released; single-GPU results only                                           | Distributed training compatibility not proven |

---

## 3. Implications

The authors are transparent about their **current bottlenecks**, especially in pretraining convergence and kernel inefficiency. From our side, **hardware exclusivity, ecosystem integration, and social responsibility** emerge as critical dimensions for future development.

> ‚ö†Ô∏è **Conclusion**: SageAttention 3 and SageBwd are fast and accurate ‚Äî but **not yet universally applicable or ethically safe by default**. Continued effort is needed across hardware, software, and deployment dimensions.



## üõ£Ô∏è Prompt 1.5.2 (Future Research Directions)

> ‚ÄúWhat concrete future research directions do the authors suggest? Based on the paper‚Äôs limitations, what logical next steps or alternative directions could further develop this research?‚Äù

### üöÄ Summary in 3 Lines

1. **Author Suggestions** ‚Äî Improve Triton kernel to close the gap between **1.67√ó real speedup** and **4√ó theoretical FP4 TC**; extend **8-bit attention to full pretraining** despite current convergence issues.
2. **Immediate Next Steps** ‚Äî Move toward a **fully low-bit end-to-end stack** (FP4/INT8 for activations, grads, MLPs), and develop **non-Blackwell fallbacks** for H100/TPU environments.
3. **Long-Term Path** ‚Äî Tackle ultra-long context stability, dynamic precision scheduling, and **ethical safeguards** for high-speed low-bit video generation.

---

## 1. Authors‚Äô Official Future Work (as stated)

| Area                    | Details                                                                            | Source |
| ----------------------- | ---------------------------------------------------------------------------------- | ------ |
| **Kernel Tuning**       | Optimize Triton implementation to push current 70‚Äì80% ‚Üí **95%+ FP4 TC FLOPS**      | ‚úî      |
| **Low-bit Pretraining** | Solve convergence issues to make **8-bit attention feasible for full pretraining** | ‚úî      |

---

## 2. Logical Next Steps (based on limitations)

| Limitation                         | Future Direction                                                                      | Expected Benefit                         |
| ---------------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------------- |
| **Blackwell-only support**         | Implement ‚Äúpseudo-FP4‚Äù (e.g., INT4 + shift) or FP6 fallback for H100 / TPU            | Wider hardware compatibility             |
| **Scale memory overhead**          | Use entropy coding or low-rank approximation for FP8 scale metadata                   | Up to **1.05√ó additional VRAM savings**  |
| **Mixed-precision gradient**       | Replace FP16 `dO V·µÄ` with quant-aware optimizers (e.g., loss-scaled AdamW)            | Enables **fully 8-bit training**         |
| **Slow convergence**               | Explore techniques like momentum correction, or KL-divergence pre-warmup              | **20‚Äì30% step reduction**                |
| **Limited context generalization** | Build **stress-test benchmarks** for 256K+ sequence or multimodal (audio/video) input | Robustness & error accumulation tracking |
| **Ethical risk**                   | Develop **precision-aware watermarking** or real-time misuse detectors                | Safer deployment                         |

---

## 3. Suggested Research Roadmap

1. **Kernel 2.0** ‚Äî Rewriting Triton/CUTLASS to maximize utilization; overlap FP4 matmul with tensor parallelism.
2. **Full-Low-Bit Stack** ‚Äî Extend FP4/INT8 beyond attention: MLP, LayerNorm, embeddings, and even optimizers.
3. **Cross-HW Adaptation** ‚Äî PTX-level conditional kernels for Hopper/Blackwell/TPU parity.
4. **Adaptive Precision Scheduler** ‚Äî Train with **dynamic bitwidth**, e.g., 8-bit in early phase, 4-bit later.
5. **Responsible Deployment** ‚Äî Incorporate **low-bit-aware watermarking**, adversarial detectors, and policy-aligned finetuning.

> üìå **Summary**: The authors prioritize kernel tuning and pretraining convergence. Beyond this, a full solution will require: broader HW compatibility, all-layer quantization, long-context verification, and safety/ethics integration ‚Äî paving the way for real-world adoption in both open-source and industry settings.

---

## ‚öôÔ∏è Prompt 1.6.x (Implementation, Hardware, Resources, and Metrics)

> ‚ÄúWhat are the key software dependencies (e.g., CUDA, Triton)? What is the expected memory usage during training and inference? What is the throughput on target hardware? Are compute costs like total FLOPs or Petaflop-days reported?‚Äù

### TL;DR ‚Äî Execution Environment at a Glance

| Category                  | Key Metric                                                  | Source Info        |
| ------------------------- | ----------------------------------------------------------- | ------------------ |
| **Software Stack**        | CUDA 12+, CUTLASS 3.4, Triton 2.2, PyTorch ‚â• 2.3            | ‚úÖ                  |
| **FP4 Kernel Throughput** | **1038 TOPS** (RTX5090)                                     | Table/Figures      |
| **MatMul Speed**          | FP16 = 200 TOPS ‚Üí FP4 = **1600 TOPS** (\~8√ó)                | ‚úÖ                  |
| **Inference Latency**     | CogVideoX: 64s ‚Üí **27s**                                    | Table 4            |
| **Training Latency**      | Llama 16K: 6.0s ‚Üí **5.2s**                                  | Table 5            |
| **Memory Savings**        | KV-cache cut by **75%** (FP4 vs FP16)                       | Equation + Figures |
| **Compute Cost**          | Finetune: \~0.5 PF-day / Pretrain: \~6 PF-days (400M model) | FLOPs estimate     |

---

## 1. Required Software / Hardware Stack

* **CUDA ‚â• 12.0**, with FP4 Tensor Core support (Blackwell generation)
* **CUTLASS 3.4**: Custom GEMM kernels with FP4MM + Softmax fusion
* **Triton 2.2 (OpenAI)**: Used for SageBwd (INT8 backward) kernels
* **PyTorch ‚â• 2.3** integration via FlashAttention APIs
* No mention of multi-node or MPI/NCCL, but compatible in principle

---

## 2. Memory Profile (Theoretical)

Using FP4 (4-bit) and FP8 scale metadata:

| Component         | FP16 Baseline | SageAttention 3 FP4 | Savings |
| ----------------- | ------------- | ------------------- | ------- |
| Q/K/V (KV-cache)  | 100%          | 25%                 | ‚Üì 75%   |
| Attention map (P) | 100%          | 25%                 | ‚Üì 75%   |

üßÆ For example:
Llama-2 7B with batch=32, seq=8K ‚Üí KV-cache shrinks from **13 GB ‚Üí \~3.2 GB**

---

## 3. Throughput / Latency Benchmarks

### Inference (RTX-5090)

| Metric                | FlashAttn2 (FP16) | SageAttention 3 (FP4) | Gain   |
| --------------------- | ----------------- | --------------------- | ------ |
| Kernel-only TOPS      | 200               | **1600**              | **8√ó** |
| Full Attention Kernel | ‚Äî                 | **1038 TOPS**         | \~5√ó   |

End-to-End (E2E) latency:

| Model        | FlashAttn2 | SageAttn3 |
| ------------ | ---------- | --------- |
| CogVideoX 2B | 64s        | **27s**   |
| HunyuanVideo | 489s       | **164s**  |

---

### Training (RTX-4090)

| Metric            | FlashAttn2 | SageBwd (INT8) | Gain        |
| ----------------- | ---------- | -------------- | ----------- |
| Fwd+Bwd TOPS      | 89 ‚Üí 150   | ‚Äî              | **1.67√ó** ‚Üë |
| Iteration latency | 6.0 s      | **5.2 s**      | 1.15√ó       |

---

## 4. Compute Cost (FLOPs / Petaflop-days)

Estimated:

* **Finetuning (1B model, 700 steps, 32√ó8K tokens)** ‚âà 0.48 PF-day
* **Pretraining (400M model, 20K steps, 2M tokens/step)** ‚âà 6.1 PF-days
  ‚Üí With SageBwd acceleration: ‚âà **4.3 PF-days**

---

### üß† Final Takeaway

SageAttention 3 and SageBwd achieve **TOP-tier throughput and memory efficiency** using standard CUDA+Triton pipelines, reducing real-world latency, memory, and FLOPs across both inference and training. Their resource profile makes **small-scale fine-tuning possible on a single GPU in <48h**, while medium-scale pretraining is within reach at \~4 PF-days ‚Äî representing a **30‚Äì40% cost reduction** compared to FP16 pipelines.

</details>