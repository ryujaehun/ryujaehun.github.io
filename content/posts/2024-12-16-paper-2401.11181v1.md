---
categories:
- paper-review
- with-gpt
date: "2024-12-16"
title: 'Inference without Interference: Disaggregate LLM Inference for Mixed Downstream
  Workloads'
---

[논문 링크](https://arxiv.org/abs/2401.11181v1)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 결론: TetriInfer의 주요 기여 및 핵심 아이디어
TetriInfer는 대규모 언어 모델(LLM)의 추론 과정에서 발생하는 **간섭 문제(interference)**를 해결하기 위해 설계된 클라우드 기반 추론 시스템입니다. 주요 강점은 **사전 단계(prefill)**와 **디코드 단계(decode)**의 분리(disaggregation)와, 이를 통해 효율적인 자원 활용을 달성했다는 점입니다. 이를 기반으로 **TTFT(Time-To-First-Token)**를 97%, **작업 완료 시간(JCT)**을 47% 단축하고, **자원 효율성(performance per dollar)**을 최대 2.4배 개선했습니다.

---

### 강점과 독창성
#### 1. **분리된 사전/디코드 아키텍처**
- **기존 방식**: 사전(prefill)과 디코드(decode) 요청이 같은 인스턴스에서 처리되어 성능 간섭을 초래함.
- **TetriInfer**: 이를 분리하여 독립적으로 동작하도록 설계함으로써 성능 병목을 최소화.
- 결과적으로 디코드 요청이 중첩되어도 간섭을 줄이고 작업 시간을 단축.

#### 2. **고정 크기 청킹(Chunked Prefill)**
- **문제**: 긴 입력 프롬프트(prefill)가 자원을 과도하게 사용하여 다른 요청 처리에 지연을 유발.
- **해결책**: 입력을 고정 크기로 나누어 처리하여 하드웨어의 계산 자원 활용도를 최적화.

#### 3. **예측 기반 이중 레벨 스케줄링**
- 디코드 단계에서 발생할 자원 사용량(메모리 및 계산 시간)을 예측하는 모델을 활용.
- 이를 바탕으로 간섭을 최소화하는 두 단계 스케줄링(중앙 스케줄링 및 로컬 스케줄링) 구현.

#### 4. **유연한 인스턴스 전환(Instance Flip)**
- 프롬프트 중심 작업(prefill-heavy) 또는 생성 중심 작업(decode-heavy)에 따라 사전/디코드 인스턴스를 동적으로 재구성.

---

### 핵심 알고리즘의 동작 과정 (예시)
1. **입력**: 긴 프롬프트와 짧은 디코드 요청이 혼재된 작업 요청들이 들어옴.
   - 프롬프트: "Generate a detailed summary of..."
   - 디코드 예상 길이: 50 tokens.

2. **스케줄링 및 청킹**:
   - 프롬프트는 고정 크기(예: 512 tokens)로 청킹되어 처리.
   - **예측 모델**은 디코드 길이를 분류(예: 50~100 tokens)하여 자원 요구량 추정.

3. **사전 처리**:
   - 각 청크가 독립적으로 처리되어 키-값(KV) 캐시를 생성.
   - 생성된 KV 캐시는 디코드 인스턴스로 전송.

4. **디코드 처리**:
   - 디코드 요청은 예상 길이와 현재 인스턴스 상태를 기반으로 분배.
   - 예를 들어, **가벼운 디코드**는 메모리 부담이 적은 인스턴스로 할당.

5. **결과 출력**:
   - 병렬 처리를 통해 모든 작업 완료 후 응답을 반환.

---

### 한계점
1. **프롬프트-디코드 모두가 무거운 작업(HPHD)에 비효율적**
   - 디코드와 사전을 분리하면서 발생하는 전송 및 관리 오버헤드로 인해 큰 이점이 없음.
   - 관련 실험에서 성능 개선이 미미하거나 오히려 자원 소모가 증가.

2. **예측 모델의 제한**
   - 디코드 길이 예측이 정밀하지 않아 일부 스케줄링 정책의 최적 성능 미달.
   - 향후 더 정밀한 예측 모델 필요.

3. **하드웨어 의존성**
   - 테스트는 NVIDIA V100 GPU를 중심으로 수행되었으며, 고급 네트워크 스택(NVLink 등)의 제한적인 구현으로 실제 배포 시 추가 검증이 필요.

---

### 비교 및 요약
| 특징                    | TetriInfer | 기존 vLLM | Sarathi | Splitwise |
| ----------------------- | ---------- | --------- | ------- | --------- |
| **사전/디코드 분리**    | ✅          | ❌         | ❌       | ✅         |
| **고정 크기 청킹**      | ✅          | ❌         | ✅       | ❌         |
| **예측 기반 스케줄링**  | ✅          | ❌         | ❌       | ✅         |
| **자원 효율성(perf/$)** | 2.4배 개선 | 기준      | 미제공  | 일부 개선 |

TetriInfer는 특히 **혼합 워크로드**에서 뛰어난 성능 개선을 보여주며, 간섭 문제를 해결하기 위한 아키텍처 설계의 새로운 기준을 제시했습니다. 하지만 무거운 작업에서는 추가적인 최적화 필요성이 남아 있습니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



논문의 핵심 알고리즘은 **TetriInfer**라는 시스템에서 작동하는 방식으로, 대규모 언어 모델(LLM)의 **추론 요청 처리 과정에서 발생하는 간섭(interference)을 최소화**하고, **성능과 자원 효율성을 극대화**하는 데 초점을 맞추고 있습니다. 이를 구현하기 위해 **사전 단계(prefill)**와 **디코드 단계(decode)**를 분리하고, 이를 지원하기 위한 **고정 크기 청킹(chunked prefill)** 및 **예측 기반 이중 레벨 스케줄링**을 사용합니다. 아래는 각 알고리즘과 과정을 예시를 들어 상세히 설명합니다.

---

## 1. **Prefill 단계: 입력 청킹과 스케줄링**

### 알고리즘 개요:
- **문제**: 긴 프롬프트(prefill 요청)가 하드웨어 자원을 초과적으로 점유해 병목을 초래.
- **해결책**: 프롬프트를 고정 크기 청크(chunk)로 나누어 처리해 하드웨어 자원의 최적 활용.

### 예시:
#### 입력:
- 프롬프트: `"Summarize the following document: [긴 텍스트]"` (1,200 tokens)

#### 처리 과정:
1. **청킹**: 프롬프트를 고정 크기(예: 512 tokens)로 나눔.
   - 청크 1: `Summarize the following document: [앞부분 512 tokens]`
   - 청크 2: `[중간 512 tokens]`
   - 청크 3: `[마지막 176 tokens (패딩 포함 512)]`

2. **스케줄링**:
   - 청크들이 **FCFS(First-Come-First-Serve)** 또는 **Shortest-Job-First(SJF)** 정책에 따라 정렬.
   - 각 청크는 독립적으로 처리되어 **키-값 캐시(KV cache)**를 생성.

3. **출력**:
   - 각 청크에서 생성된 KV cache는 디코드 단계로 전달.

### 효과:
- 하드웨어가 계산이 포화되기 직전까지 효율적으로 사용됨.
- 긴 프롬프트로 인해 발생하는 병목을 완화.

---

## 2. **Decode 단계: 예측 기반 스케줄링**

### 알고리즘 개요:
- **문제**: 디코드 요청의 메모리 사용량 및 계산 요구량의 차이로 인해 간섭 발생.
- **해결책**: 디코드 요청 길이를 예측하여 자원 사용량 기반으로 요청을 스케줄링.

### 예시:
#### 입력:
- 디코드 요청 1: `"Explain the concept of entropy."` (예상 길이: 50 tokens)
- 디코드 요청 2: `"Generate a detailed story outline."` (예상 길이: 300 tokens)

#### 처리 과정:
1. **예측 모델 활용**:
   - OPT-125M과 같은 작은 예측 모델이 디코드 요청 길이를 추정.
   - 디코드 요청 1: 길이 범위 [50–100 tokens] → 가벼운 디코드(light decode)로 분류.
   - 디코드 요청 2: 길이 범위 [300–400 tokens] → 무거운 디코드(heavy decode)로 분류.

2. **스케줄링**:
   - 디코드 요청을 **로컬 스케줄러**가 관리:
     - 가벼운 요청은 메모리 사용량이 적은 인스턴스로 배정.
     - 무거운 요청은 메모리 자원이 충분한 인스턴스로 배정.
   - 두 요청 간의 간섭을 줄이기 위해 동적 배치(batch) 크기를 조정.

3. **병렬 처리**:
   - KV cache 전송 후 각 요청이 개별적으로 처리.
   - 가벼운 요청은 빠르게 완료, 무거운 요청은 병렬로 진행.

### 효과:
- 메모리 충돌(head-of-line blocking)을 줄이고 디코드 간섭 완화.
- 자원 사용량 기반 스케줄링으로 효율성 증가.

---

## 3. **TetriInfer의 통합 워크플로**

### 알고리즘 전체 동작 흐름:
#### 1단계: **입력 스케줄링 및 청킹**
- 긴 프롬프트 입력을 고정 크기로 나누고 최적화된 스케줄링 정책(SJF 등)으로 정렬.

#### 2단계: **사전 처리(prefill)**
- 각 청크가 LLM을 통해 처리되어 KV 캐시 생성.
- 생성된 캐시는 디코드 인스턴스로 전달.

#### 3단계: **디코드 처리(decode)**
- KV 캐시를 기반으로 디코드 요청이 처리.
- 디코드 요청은 예상 길이를 기반으로 배치되어 간섭을 줄임.

---

### 정리: TetriInfer의 강점 요약
| 기능                          | 기존 시스템    | TetriInfer                             |
| ----------------------------- | -------------- | -------------------------------------- |
| **고정 크기 청킹**            | ❌              | ✅ 하드웨어 자원 최적화                 |
| **사전/디코드 분리**          | ❌              | ✅ 단계별 자원 간섭 최소화              |
| **예측 기반 스케줄링**        | ❌              | ✅ 요청 길이 기반 자원 사용 최적화      |
| **효율성(performance per $)** | 기준 대비 낮음 | 2.4배 개선 (특히 디코드 중심 작업에서) |

TetriInfer의 설계는 입력 데이터의 불균형 및 작업 유형의 차이에 적응할 수 있는 유연성과 확장성을 제공합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### 결론: **TetriInfer의 성과와 주요 기여**
TetriInfer는 기존 방식과 비교해 **LLM 추론 효율성을 크게 개선**했으며, 특히 혼합 워크로드에서 **Time-to-First-Token(TTFT)**, **Job Completion Time(JCT)**, 및 **자원 효율성(performance per dollar)** 면에서 탁월한 성능을 보였습니다. 논문의 저자들은 이를 가능하게 한 핵심 요소로 **사전/디코드 분리(disaggregation)**, **고정 크기 청킹(chunked prefill)**, 그리고 **예측 기반 스케줄링(length-aware scheduling)**을 제시했습니다.

---

### 결과 요약 및 비교
#### 실험 결과:
논문에서 제시한 TetriInfer와 기존 접근법의 성능 차이는 아래 표로 정리됩니다:

| 워크로드 유형                    | **TTFT 개선율** | **JCT 개선율** | **자원 효율성(perf/$)** | **주요 원인**                                 |
| -------------------------------- | --------------- | -------------- | ----------------------- | --------------------------------------------- |
| **Light Prefill + Light Decode** | 44%             | 40%            | 1.4배                   | 고정 크기 청킹으로 프롬프트 처리 효율 향상    |
| **Light Prefill + Heavy Decode** | 97%             | 47%            | 2.4배                   | 디코드와 사전 분리, 예측 기반 디코드 스케줄링 |
| **Heavy Prefill + Light Decode** | 9%              | 23%            | **낮아짐** (-43%)       | 디코드 분리 효과 작고, 청킹 오버헤드 발생     |
| **Heavy Prefill + Heavy Decode** | 19%             | 19%            | 1.1배                   | 디코드 분리 효과는 있지만 자원 소모 증가      |
| **혼합 워크로드(Mixed)**         | 85%             | 50%            | 1.9배                   | 분리된 처리와 동적 스케줄링의 조합            |

---

### 다른 방법론과 비교: TetriInfer의 특출난 점
논문에서는 여러 기존 접근법과 TetriInfer를 비교하며 그 독창성을 강조합니다. 관련 내용을 정리한 표는 아래와 같습니다:

| **특징**                      | **TetriInfer** | **vLLM** | **Splitwise** | **Sarathi** |
| ----------------------------- | -------------- | -------- | ------------- | ----------- |
| **사전/디코드 분리**          | ✅              | ❌        | ✅             | ❌           |
| **고정 크기 청킹**            | ✅              | ❌        | ❌             | ✅           |
| **예측 기반 디코드 스케줄링** | ✅              | ❌        | ✅             | ❌           |
| **혼합 워크로드 최적화**      | ✅              | ❌        | 일부 개선     | ❌           |
| **자원 효율성(perf/$)**       | 탁월           | 기준     | 일부 개선     | 미제공      |

---

### TetriInfer의 성능 향상의 이유와 논문에서 제시하는 근거
#### 1. **사전/디코드 분리 (Disaggregation of Prefill and Decode)**
- **문제**: 기존 시스템은 프롬프트 사전(prefill)과 디코드(decode)를 같은 인스턴스에서 처리.
  - 프롬프트는 **계산 중심(compute-heavy)** 작업.
  - 디코드는 **메모리 중심(memory-intensive)** 작업으로 상충 발생.
- **TetriInfer의 해결책**:
  - 사전 작업과 디코드 작업을 **별도의 인스턴스**에서 처리하여 간섭 제거.
  - 디코드 요청은 예측된 길이 기반으로 **메모리 최적화된 배치**를 통해 처리.

#### 2. **고정 크기 청킹 (Chunked Prefill)**
- **문제**: 긴 프롬프트는 하드웨어 자원을 과도하게 점유, 나머지 요청의 지연 유발.
- **TetriInfer의 해결책**:
  - 프롬프트를 고정 크기 청크(예: 512 tokens)로 분할하여 처리.
  - 하드웨어의 계산 자원이 초과되지 않도록 조정.

#### 3. **예측 기반 디코드 스케줄링**
- **문제**: 디코드 요청은 길이에 따라 자원 소모량이 크게 다르며, 혼합 처리 시 간섭이 발생.
- **TetriInfer의 해결책**:
  - 작은 예측 모델(예: OPT-125M)을 활용하여 디코드 요청 길이를 사전 추정.
  - 예측 결과를 바탕으로 디코드 요청을 **메모리 사용량 기반**으로 분배.

#### 논문에서 제시하는 근거:
- **분리된 아키텍처의 효과**: **TTFT**가 최대 97% 개선됨.
- **청킹의 효과**: 프롬프트 처리 대기 시간이 86% 감소.
- **스케줄링 효과**: 혼합 워크로드에서 **perf/$**가 2배 이상 개선.

---

### 개인 의견: TetriInfer의 설계와 결과의 상관성
#### 1. 설계의 합리성
TetriInfer는 **사전과 디코드의 상충(interference)**이라는 근본적인 문제를 해결하기 위해 설계되었습니다. 사전과 디코드의 분리는 독립적 작업 스케줄링을 가능하게 했으며, 고정 크기 청킹과 디코드 요청 예측은 **자원 사용 효율을 극대화**했습니다. 이 설계는 클라우드 환경에서 **혼합 워크로드**에 특히 적합하다고 판단됩니다.

#### 2. 한계와 개선 가능성
- **Heavy Prefill + Heavy Decode 워크로드**에서 자원 사용량이 증가한 점은 청킹과 분리로 인한 오버헤드가 원인으로 보입니다. 이를 개선하기 위해 디코드 캐시 전송을 최적화하거나, 스케줄링 모델의 정밀도를 높일 필요가 있습니다.
- 예측 기반 스케줄링은 **예측 정확도(74.9%)**에 따라 성능이 제한될 수 있습니다. 예측 모델의 정밀도를 높이는 것이 장기적으로 필요합니다.

#### 3. 실용적 의미
이 논문은 **혼합 워크로드 환경에서 LLM 서비스의 성능 최적화**를 위한 새로운 기준을 제시했습니다. TetriInfer의 접근 방식은 **단순한 자원 증가가 아닌 구조적 최적화**를 통해 효율성을 개선했으며, 이는 클라우드 서비스 환경에서 중요한 기여라고 평가할 수 있습니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### 논문의 입력 데이터와 추론 과정: 예시를 통한 상세 설명

---

#### **입력 데이터의 특성**
이 논문에서는 대규모 언어 모델(LLM) 추론에서 발생하는 간섭 문제를 해결하기 위해 **혼합 워크로드**를 대상으로 실험을 수행했습니다. **ShareGPT** 데이터셋을 활용해 프롬프트와 디코드 요청을 구성하며, 다양한 워크로드 유형을 생성합니다.

- **프롬프트 데이터**:
  - 길이가 긴 프롬프트는 주로 요약 요청이나 문서 분석과 같은 작업에서 발생.
  - 예: `"Summarize the following document: [긴 텍스트]"` (512~1,200 tokens)

- **디코드 데이터**:
  - 응답의 길이는 짧거나 긴 요청이 혼합됨.
  - 짧은 디코드: `"What is entropy?"` (50 tokens)
  - 긴 디코드: `"Write a detailed story outline for a novel."` (300~500 tokens)

---

#### **추론 과정 예시**
**목표**: 긴 프롬프트와 짧은 디코드 요청을 처리하면서 간섭을 최소화.

1. **입력 데이터**:
   - **프롬프트**: `"Summarize the following document: [긴 텍스트]"` (1,200 tokens)
   - **디코드 요청**:
     - 요청 1: `"What is entropy?"` (50 tokens 예상)
     - 요청 2: `"Write a detailed story outline."` (300 tokens 예상)

2. **1단계: 사전(prefill) 처리**
   - 프롬프트를 **고정 크기 청크**로 나눔 (예: 512 tokens 단위).
   - 청킹 결과:
     - 청크 1: `[Summarize the following document: ...]` (512 tokens)
     - 청크 2: `[중간 텍스트...]` (512 tokens)
     - 청크 3: `[마지막 텍스트 (176 tokens + 패딩)]`

   - **청킹된 프롬프트**는 LLM으로 전달되어 각 청크에서 **키-값 캐시(KV Cache)**가 생성됨.

3. **2단계: 디코드(decode) 처리**
   - **디코드 요청 1**: "What is entropy?"는 가벼운 디코드 요청으로 분류.
   - **디코드 요청 2**: "Write a detailed story outline."는 무거운 디코드 요청으로 분류.
   - 각 디코드 요청은 **예측 모델(OPT-125M)**로 처리 자원을 추정하여 스케줄링:
     - 요청 1: 짧은 길이 → 메모리와 계산 부담이 적은 인스턴스로 배정.
     - 요청 2: 긴 길이 → 메모리 여유가 많은 인스턴스로 배정.
   - **결과**: 디코드가 병렬로 처리되어 각 요청에 대한 결과를 반환.

---

### 모델 아키텍처: 구성 및 연산 설명

#### **1. 주요 구성 요소**
TetriInfer의 모델 아키텍처는 아래와 같은 구성으로 이루어집니다:
1. **사전(prefill) 인스턴스**:
   - **고정 크기 청킹** 및 프롬프트 처리.
   - 각 청크에서 **키-값 캐시(KV Cache)**를 생성.
   - 예측 모델(OPT-125M)을 통해 디코드 요청 길이를 추정.

2. **디코드(decode) 인스턴스**:
   - 사전 처리된 KV Cache를 받아 디코드 수행.
   - 디코드 요청의 길이와 자원 요구량에 따라 **로컬 스케줄링**.

3. **중앙 제어부(Centralized Control Plane)**:
   - 클러스터 상태 모니터링 및 사전/디코드 요청의 스케줄링 관리.

---

#### **2. 연산 구성**
**Transformer 아키텍처 기반 LLM (OPT-13B)**:
- **Self-Attention 연산**:
  - 입력 토큰 간 상관성을 계산하는 핵심 연산.
  - **O(N²·d)**의 계산량 필요 (N: 입력 토큰 수, d: 모델 차원).
  - 긴 프롬프트(1,200 tokens)는 짧은 디코드(50 tokens)에 비해 20배 이상의 계산량을 소모.

- **Feed-Forward Layer**:
  - 토큰별 비선형 변환.
  - 계산량: **O(N·d²)** (N: 토큰 수, d: 모델 차원).

- **KV Cache 생성 및 활용**:
  - 사전 단계에서 생성된 KV Cache는 디코드 단계에서 매번 재사용.
  - 메모리 요구량은 토큰 길이에 비례: **O(N·d)**.

---

#### **3. 메모리 요구량**
**OPT-13B 모델 기준**:
- **모델 파라미터**: 약 13B (≈26GB, FP16 사용).
- **KV Cache 메모리**:
  - KV Cache 크기: **토큰 길이 × 2 × d** (key와 value 각각 저장).
  - 예: 1,200 tokens × 2 × 13,312-d 차원 ≈ 31MB (FP16 기준).

---

#### **4. 계산 요구량**
**예시 시나리오 계산**:
- 프롬프트: 1,200 tokens, 디코드: 300 tokens
- **Self-Attention 연산량**:
  - 사전 단계: **O(1,200²·13,312) ≈ 19.1B FLOPs**
  - 디코드 단계(300 tokens): **O(300²·13,312) ≈ 1.2B FLOPs**
- **Feed-Forward 연산량**:
  - 사전 단계: **O(1,200·13,312²) ≈ 211B FLOPs**
  - 디코드 단계: **O(300·13,312²) ≈ 52.8B FLOPs**

---

### 정리
TetriInfer의 입력 데이터와 추론 과정은 고정 크기 청킹과 디코드 요청 예측 기반 스케줄링을 통해 효율적으로 처리됩니다. 모델 아키텍처는 대규모 Transformer(OPT-13B) 기반으로, **사전 단계는 계산량이 주로 집중되고**, **디코드 단계는 메모리 사용량이 주요 병목**으로 작용합니다. TetriInfer의 설계는 이러한 연산 및 메모리 요구량의 비대칭성을 고려해 사전과 디코드를 분리하고 최적화했습니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘




### 이 논문의 한계와 이를 극복하기 위한 연구 흐름

논문에서는 **TetriInfer**가 대규모 언어 모델(LLM) 추론 효율성을 크게 개선했지만, 몇 가지 한계를 명시적으로 제시하거나 간접적으로 암시했습니다. 이 한계들은 향후 연구 방향의 기회를 제공합니다. 아래는 한계를 극복하기 위한 관련 연구 흐름과 가능성을 정리한 내용입니다.

---

### 1. **Heavy Prefill + Heavy Decode 작업에서의 한계**
#### 한계점:
- 프롬프트와 디코드 요청이 모두 무거운 워크로드에서 성능 향상이 제한적.
- 디코드 분리로 인한 **전송 오버헤드**와 **청킹 과정의 추가 계산**으로 인해 오히려 자원 효율이 감소.

#### 극복 방안:
##### (1) **Prefill-Decode 통합 최적화 연구**
- 기존의 Sarathi(2023)와 같은 접근법을 적용해 **청킹 과정에서도 디코드 단계와 동시 처리**를 수행.
- 예: Prefill-Decode를 동시에 처리하는 **혼합 청킹 방식**.

##### (2) **메모리-계산 통합 최적화**
- 모델의 **KV Cache 크기**를 줄이고 전송 효율을 높이는 연구:
  - **압축 기술**: KV Cache를 압축하여 네트워크 전송 크기 감소 (예: 양자화, Sparse Matrix 적용).
  - **기존 연구**: FlexGen(2023)은 메모리 사용량을 줄이는 기법으로 높은 워크로드 처리 효율을 달성.
  - **미래 흐름**: 전송 시 **레이어 단위로 KV Cache를 압축**하거나 필요 시에만 부분적으로 전송.

##### (3) **Pipeline Parallelism 최적화**
- 모델을 단계별로 나누어 사전 단계의 일부 계산을 디코드 단계와 병렬 처리.
- Splitwise(2023)에서 제안한 **Prefill-Decode 교차 처리 방식**의 확장.

---

### 2. **디코드 요청 예측 정확도의 한계**
#### 한계점:
- 디코드 요청 길이 예측의 정확도가 약 **74.9%** 수준으로 제한적.
- 예측 오류는 디코드 스케줄링에서 비효율을 초래.

#### 극복 방안:
##### (1) **더 정교한 길이 예측 모델**
- LLM 자체를 활용한 길이 예측 연구:
  - 최근 GPT-4 기반 모델을 활용하여 디코드 요청의 길이와 자원 사용량을 정밀하게 추정.
  - LLM 기반 학습 데이터 증가로 예측 정확도 개선 가능.

##### (2) **확률적 스케줄링**
- 길이 예측의 불확실성을 고려한 스케줄링 알고리즘 개발:
  - 요청 길이를 범위로 예측해 불확실성을 허용하는 **확률적 모델 기반 스케줄링**.
  - Reserve-Dynamic 스케줄링 알고리즘에 예측 오차를 보정하는 매커니즘 추가.

##### (3) **실시간 학습 기반 예측**
- 디코드 중간 결과를 실시간으로 피드백 받아 모델 재학습:
  - Splitwise에서 적용된 **온라인 학습 기반 길이 보정 기법** 확장.

---

### 3. **디코드 단계에서 메모리 병목**
#### 한계점:
- 디코드 요청은 주로 메모리 소모가 많은 작업으로, 여러 요청이 동시에 처리될 경우 메모리 충돌 및 병목 발생.

#### 극복 방안:
##### (1) **메모리 재사용 최적화**
- vLLM에서 제안된 **Paged Attention** 방식을 확장:
  - KV Cache를 **요청 길이에 따라 동적으로 관리**하여 메모리 사용 효율 개선.
  - 기존 연구: FlexGen은 메모리 페이징과 GPU-CPU 간 전송 최적화를 통해 메모리 사용량 감소.

##### (2) **메모리 분할 및 공유**
- 디코드 요청 길이에 따라 메모리를 논리적으로 분할:
  - 가벼운 요청과 무거운 요청이 **별도의 메모리 풀**을 사용하도록 최적화.
  - Splitwise의 **Prefill-Decode 병렬 구조**와 결합하여 메모리 상호작용 감소.

##### (3) **캐시 데이터 전송 효율 개선**
- 네트워크 전송에서 **Zero-Copy 방식** 도입:
  - GPU 간 데이터 전송에서 CPU 메모리를 거치지 않고 **Direct Memory Access(DMA)**를 사용.

---

### 4. **Heavy Decode에서 TTFT 개선의 한계**
#### 한계점:
- 디코드 단계에서 TTFT(Time-To-First-Token)는 길이에 비례하여 증가.
- 가벼운 요청과 긴 요청이 혼합된 상황에서는 스케줄링의 병목 발생 가능.

#### 극복 방안:
##### (1) **Token-Level 병렬화**
- FlashAttention(2022)처럼 **토큰 단위 병렬화**로 TTFT를 단축.
- 디코드 요청이 긴 경우, 첫 번째 토큰 생성과 이후 토큰 생성을 병렬화.

##### (2) **가중치 공유 및 캐싱**
- 긴 디코드 요청의 자원을 줄이기 위해 동일한 맥락에서의 요청끼리 가중치를 공유.
- 공유 기반 추론은 GPTZip(2023)과 같은 **가중치 압축 및 캐싱 방식** 활용 가능.

##### (3) **디코드 단위 스케줄링 개선**
- 디코드 요청을 더 작게 나눠 미리 생성된 KV Cache를 효율적으로 사용:
  - 디코드 요청을 더 세분화하여 병렬로 처리하고 응답 시간 단축.

---

### 5. **워크로드 예측 및 인스턴스 전환**
#### 한계점:
- TetriInfer의 **Instance Flip**은 정적 규칙에 의존(예: 1분 동안 유휴 상태).
- 워크로드 변화에 적응하기 위한 동적 정책 부족.

#### 극복 방안:
##### (1) **강화 학습 기반 인스턴스 전환**
- 강화 학습(RL)을 활용해 동적 워크로드 환경에 최적화된 전환 정책 설계:
  - 클러스터의 실시간 상태를 기반으로 사전/디코드 인스턴스 전환을 자동화.

##### (2) **다중 모드 전환**
- 사전과 디코드를 분리하는 대신 **혼합 인스턴스**를 도입:
  - 경량 워크로드에서는 통합 처리, 중량 워크로드에서는 분리 처리.

---

### 6. **실험 환경의 제한**
#### 한계점:
- 논문에서 제시한 결과는 NVIDIA V100 GPU 기반 환경에서 테스트.
- 최신 GPU 아키텍처(A100, H100)와 고속 네트워크 환경(NVLink, InfiniBand)에서의 검증 부족.

#### 극복 방안:
##### (1) **최신 하드웨어 활용**
- 최신 GPU와 고속 네트워크를 활용한 벤치마크 실험 수행.
- H100의 **Transformer Engine**과 같은 하드웨어 가속기 활용.

##### (2) **Cross-Cloud 배포 실험**
- Google Cloud TPU, AWS Inferentia와 같은 클라우드 전용 하드웨어에서 성능 비교 실험.

---

### 결론: 연구 흐름의 방향성
TetriInfer의 한계를 극복하기 위해 **디코드 최적화**, **KV Cache 전송 효율화**, **워크로드 예측 개선**, 그리고 **강화 학습 기반 적응형 스케줄링**이 주요 연구 흐름으로 나타나고 있습니다. 이러한 접근법은 대규모 언어 모델 추론의 자원 효율성과 성능을 더욱 높이는 데 기여할 것입니다.



# Q : 사전(prefill)과 디코드(decode) 작업의 분리(disaggregation)가 혼합 워크로드에서의 간섭(interference)을 어떻게 줄이는지, 그리고 기존 통합 방식(vLLM 등) 대비 성능 향상이 발생하는 근본적인 이유는 무엇인가?

# A: 

### **사전(prefill)과 디코드(decode) 작업의 분리가 간섭(interference)을 줄이는 이유**
---

#### 1. **사전과 디코드의 작업 특성 차이**
- **사전(prefill)**:
  - 주로 **계산 중심(compute-heavy)** 작업으로, 프롬프트 입력의 길이에 따라 계산 복잡도가 **O(N²)**로 증가.
  - GPU의 계산 유닛 활용도가 높으며, 일정 길이 이상의 프롬프트는 하드웨어 포화 상태에 도달.

- **디코드(decode)**:
  - **메모리 중심(memory-intensive)** 작업으로, 토큰 단위로 처리되며, 생성된 토큰 수에 따라 메모리와 대역폭 요구량이 선형적으로 증가.
  - 실시간 응답 속도(Time-to-First-Token, TTFT)와 생성 토큰 수가 중요한 성능 지표.

- **문제점**:
  - 기존 방식(vLLM 등)은 **사전과 디코드 작업을 동일한 인스턴스에서 실행**하여 계산과 메모리 요구량이 상충(interference)을 초래.
    - **사전 작업**의 계산 병목으로 인해 **디코드 작업**의 응답 지연 발생.
    - 메모리 대역폭이 디코드 작업에 집중되지 못해 **TTFT와 전체 작업 완료 시간(JCT)**이 증가.

---

#### 2. **사전/디코드 분리(disaggregation)의 효과**
TetriInfer는 사전(prefill)과 디코드(decode) 작업을 별도 인스턴스로 분리하여 각각 독립적으로 실행합니다. 이로 인해 다음과 같은 개선이 이루어집니다:

1. **간섭(interference) 제거**:
   - 사전 작업이 계산 자원을 소모해도 디코드 작업에 영향을 주지 않음.
   - 디코드 작업은 필요한 메모리 대역폭을 확보하여 실시간 처리가 가능.

2. **작업 스케줄링 최적화**:
   - 사전 작업과 디코드 작업의 특성을 고려해 별도로 스케줄링 가능.
     - 사전 작업: 고정 크기 청킹(chunked prefill)으로 계산 부담 균등화.
     - 디코드 작업: 예상 길이 기반 메모리 최적화 스케줄링.

3. **자원 활용 효율 극대화**:
   - 사전 인스턴스는 계산 자원(GPU 코어)을 최대로 활용.
   - 디코드 인스턴스는 메모리 대역폭과 캐싱 효율을 최적화.

---

### **성능 향상이 발생하는 근본적인 이유**
---

#### 1. **Time-to-First-Token(TTFT) 감소**
- **문제**: 기존 통합 방식은 디코드 요청을 기다리는 동안 사전 작업이 완료될 때까지 대기.
- **TetriInfer의 개선**:
  - 사전 작업이 완료된 KV 캐시를 디코드 인스턴스로 즉시 전달하여 디코드 작업 시작.
  - 디코드 요청 길이에 따라 배치 크기를 동적으로 조정, **최대 97% TTFT 개선**.

#### 2. **Job Completion Time(JCT) 감소**
- **문제**: 기존 방식은 사전과 디코드의 간섭으로 디코드 처리 속도가 느려짐.
- **TetriInfer의 개선**:
  - 사전과 디코드를 독립 처리하여 대기 시간이 감소.
  - 디코드 작업 간 메모리 자원 충돌을 예측 기반 스케줄링으로 완화, **최대 50% JCT 개선**.

#### 3. **자원 효율성(performance per dollar) 증가**
- **문제**: 기존 방식은 사전 작업이 디코드 단계의 자원 활용도를 저하시킴.
- **TetriInfer의 개선**:
  - 사전과 디코드의 분리로 인스턴스의 자원 사용률 증가.
  - 혼합 워크로드에서 작업 간 자원 사용량을 균등하게 분배, **최대 2.4배 효율 개선**.

---

### **구체적인 성능 향상 이유를 실험 결과로 설명**
#### 실험 결과:
논문에서 제시한 실험 데이터를 기반으로 구체적인 성능 향상을 확인할 수 있습니다.

| 워크로드 유형                | **TTFT 개선** | **JCT 개선** | **자원 효율성(perf/$)** |
| ---------------------------- | ------------- | ------------ | ----------------------- |
| Light Prefill + Heavy Decode | 97%           | 47%          | 2.4배                   |
| Heavy Prefill + Heavy Decode | 19%           | 19%          | 1.1배                   |
| 혼합 워크로드                | 85%           | 50%          | 1.9배                   |

#### 성능 향상의 이유:
1. **Light Prefill + Heavy Decode**:
   - 디코드 중심 작업에서 분리된 인스턴스 설계로 디코드 작업이 계산 간섭 없이 최적의 메모리 대역폭 활용.
2. **Heavy Prefill + Heavy Decode**:
   - 긴 프롬프트와 긴 디코드 요청의 자원 사용량이 모두 높아 분리 효과가 상대적으로 적음.
3. **혼합 워크로드**:
   - 각 작업의 특성에 따라 스케줄링을 최적화하여 전체적인 효율 개선.

---

### **결론**
사전(prefill)과 디코드(decode)의 작업 특성 차이를 고려한 분리(disaggregation)는 간섭을 최소화하여 **TTFT, JCT, 자원 효율성** 모두에서 개선을 이끌어냈습니다. 이는 대규모 언어 모델 추론에서 혼합 워크로드가 발생하는 클라우드 환경에서 특히 효과적이며, 기존 통합 방식(vLLM 등)이 해결하지 못했던 자원 간 충돌 문제를 근본적으로 해결한 접근법으로 평가됩니다.

# Q : TetriInfer의 고정 크기 청킹(chunked prefill) 및 예측 기반 디코드 스케줄링에서 발생하는 오버헤드는 무엇이며, 이를 최소화하기 위해 어떤 추가적인 최적화가 가능할까?




### **TetriInfer의 고정 크기 청킹 및 예측 기반 디코드 스케줄링의 오버헤드**
---

#### **1. 고정 크기 청킹(chunked prefill)에서의 오버헤드**
TetriInfer는 사전(prefill) 처리 시 입력 프롬프트를 고정 크기(예: 512 tokens)로 나눠 처리하여 하드웨어 계산 자원을 최적화합니다. 그러나 이 과정에서 다음과 같은 오버헤드가 발생합니다:

1. **추가적인 청킹 비용**:
   - 긴 프롬프트를 고정 크기 청크로 분리하고 패딩하는 과정에서 추가적인 계산 비용 발생.
   - 청킹의 구현 복잡도가 증가하며, 이로 인해 작업 준비 시간이 늘어날 수 있음.

2. **불필요한 패딩 처리**:
   - 마지막 청크가 고정 크기를 채우지 못할 경우, 패딩된 부분이 모델의 계산에서 불필요한 연산을 유발.
   - 결과적으로 계산 자원이 효율적으로 사용되지 못함.

3. **청킹된 요청의 캐싱 및 전송 오버헤드**:
   - 각 청크별로 생성된 KV 캐시를 디코드 인스턴스로 전송해야 하므로, 네트워크 대역폭 소모가 증가.

---

#### **2. 예측 기반 디코드 스케줄링에서의 오버헤드**
디코드 요청 길이를 사전에 예측하고, 이를 기반으로 자원을 분배하는 과정에서 발생하는 오버헤드는 다음과 같습니다:

1. **예측 모델 실행 비용**:
   - TetriInfer는 작은 모델(예: OPT-125M)을 사용해 디코드 요청 길이를 예측하지만, 이 역시 추가 계산 비용을 유발.
   - 특히 디코드 요청이 짧거나 단순한 경우, 예측 단계 자체가 비효율적일 수 있음.

2. **예측 부정확성으로 인한 자원 낭비**:
   - 길이 예측 정확도가 **74.9%**로 제한적이므로, 잘못된 예측으로 인해 메모리 또는 계산 자원이 비효율적으로 할당될 가능성이 있음.
   - 예측 오류가 클 경우 디코드 간 간섭이 발생해 작업 완료 시간이 증가.

3. **스케줄링 복잡성**:
   - 예측 정보를 기반으로 각 디코드 요청을 적합한 인스턴스로 배정하는 과정에서 스케줄링 지연 발생.

---

### **오버헤드를 최소화하기 위한 추가적인 최적화**
---

#### **1. 고정 크기 청킹 최적화**
##### (1) **동적 크기 청킹(Dynamic Chunking)**
- **개선점**:
  - 고정 크기 대신 **동적 크기**로 청킹을 수행하여 패딩 오버헤드를 줄임.
  - 예: 청킹 크기를 하드웨어 리소스와 요청 길이에 따라 조정.
- **효과**:
  - 계산 비용과 메모리 소모를 줄이고, 작업 준비 시간을 단축.

##### (2) **레이어 단위 캐싱 및 전송**
- **개선점**:
  - 각 청크의 KV 캐시를 한꺼번에 전송하지 않고 **레이어 단위**로 전송.
  - 필요할 때만 해당 레이어의 캐시 데이터를 디코드 인스턴스로 전달.
- **효과**:
  - 네트워크 전송 비용 감소.
  - 디코드 단계에서 필요한 데이터만 사용하므로 메모리 효율 개선.

##### (3) **프롬프트 병합(Merging Short Prompts)**
- **개선점**:
  - 짧은 프롬프트를 그룹화하여 하나의 청크로 처리.
  - 예: 여러 짧은 요청(예: 50 tokens 이하)을 묶어 청킹.
- **효과**:
  - 사전 단계에서 빈번한 작업 호출을 줄여 계산 자원 활용 최적화.

---

#### **2. 예측 기반 디코드 스케줄링 최적화**
##### (1) **예측 모델의 경량화**
- **개선점**:
  - 기존의 OPT-125M 대신 더 작은 모델(예: OPT-30M)을 사용하거나, **지표 기반 통계 모델**로 대체.
  - 예: 과거 요청 패턴을 활용한 히스토리 기반 길이 예측.
- **효과**:
  - 예측 모델의 실행 비용 감소.

##### (2) **실시간 보정 알고리즘**
- **개선점**:
  - 디코드 진행 상황을 실시간으로 모니터링하여 예측 길이를 동적으로 보정.
  - 예: 디코드 중간에 작업이 예상보다 길어지거나 짧아질 경우 스케줄링 수정.
- **효과**:
  - 잘못된 예측으로 인한 자원 낭비 감소.

##### (3) **확률적 스케줄링(Probabilistic Scheduling)**
- **개선점**:
  - 요청 길이를 범위로 예측하고, 예측 범위의 상한/하한을 기반으로 스케줄링.
  - 예: 요청 길이가 100~200 tokens로 예측될 경우, **메모리 여유가 있는 인스턴스**에 할당.
- **효과**:
  - 예측 오류의 영향을 완화하고, 자원 활용 효율성 증가.

---

#### **3. 하드웨어 최적화**
##### (1) **Zero-Copy 메모리 전송**
- **개선점**:
  - KV 캐시 전송에서 GPU 간 직접 메모리 접근(DMA)을 활용하여 CPU 메모리 개입 제거.
- **효과**:
  - 네트워크 전송 지연 및 메모리 복사 오버헤드 감소.

##### (2) **GPU-CPU 메모리 계층 최적화**
- **개선점**:
  - FlexGen(2023)에서 제안된 방식처럼, GPU와 CPU 간 페이징 기법을 활용.
  - 디코드 단계에서 사용하지 않는 캐시는 CPU 메모리로 오프로드.
- **효과**:
  - GPU 메모리 부담 완화 및 병목 감소.

---

#### **4. 워크로드 기반 스케줄링 정책**
##### (1) **워크로드 특화 인스턴스 구성**
- **개선점**:
  - 디코드 요청의 길이에 따라 인스턴스를 **경량 디코드**와 **중량 디코드**로 분리.
- **효과**:
  - 메모리와 계산 요구량의 상충을 줄이고, 스케줄링 효율 극대화.

##### (2) **강화 학습 기반 스케줄링**
- **개선점**:
  - 강화 학습(RL)을 통해 워크로드 패턴을 학습하고, 최적의 디코드 자원 배치 전략을 설계.
- **효과**:
  - 스케줄링 성능이 워크로드 변화에 자동 적응.

---

### **결론**
TetriInfer의 **고정 크기 청킹**과 **예측 기반 스케줄링**은 혼합 워크로드 처리에서 성능 개선에 기여했지만, 오버헤드(청킹 비용, 예측 부정확성 등)가 여전히 존재합니다. 이를 최소화하기 위해 **동적 청킹, 실시간 보정 알고리즘, 하드웨어 최적화, 워크로드 특화 스케줄링**과 같은 추가적인 최적화를 통해 효율성을 더욱 높일 수 있습니다. 이러한 최적화는 대규모 언어 모델 추론의 실용적 적용 가능성을 한층 강화할 것입니다.


# Q : TetriInfer의 설계와 결과가 최신 하드웨어(A100, H100) 및 클라우드 기반 환경(Google Cloud TPU, AWS Inferentia)에서 동일한 성능 이점을 제공할 수 있을까? 하드웨어나 네트워크 환경에 따라 어떤 변형이 필요할까?



### **TetriInfer의 설계와 최신 하드웨어 및 클라우드 기반 환경에서의 성능 이점**

TetriInfer는 NVIDIA V100 GPU 기반 환경에서 설계되고 테스트되었으며, 최신 하드웨어(A100, H100) 및 클라우드 기반 환경에서 성능 이점을 제공할 가능성이 높습니다. 그러나 일부 설계 요소는 최신 하드웨어 및 네트워크 환경의 특성에 맞게 조정이 필요합니다.

---

### **1. 최신 하드웨어 환경에서의 성능 이점 가능성**
#### **1.1 A100 및 H100 GPU**
- **특징**:
  - **A100**: Tensor Core 지원, 대규모 배치 처리 및 Mixed Precision 연산 최적화.
  - **H100**: **Transformer Engine**을 통해 INT8 및 FP8 연산을 가속화, 메모리 대역폭 증가(NVLink 4.0).
- **TetriInfer의 이점**:
  - **고정 크기 청킹(chunked prefill)**:
    - 최신 GPU의 향상된 계산 성능과 메모리 대역폭으로 인해 더 큰 청킹 크기를 설정할 수 있음.
    - FP8 연산과 같은 새로운 데이터 형식을 활용하면 추가적인 속도 향상 가능.
  - **예측 기반 디코드 스케줄링**:
    - H100의 Transformer Engine은 디코드 요청 처리 속도를 크게 향상시킬 수 있으며, 메모리 병목이 감소.
    - A100의 NVSwitch는 여러 GPU 간의 데이터 전송 속도를 증가시켜 KV 캐시 전송 지연을 줄일 수 있음.

#### **1.2 최신 하드웨어에서의 추가 가능성**
- **NVLink 4.0 (H100)**:
  - 900GB/s의 대역폭으로 GPU 간 데이터 전송을 가속화.
  - TetriInfer의 KV 캐시 전송 시간이 V100보다 현저히 줄어들어 사전/디코드 간 분리의 효과가 더욱 극대화될 가능성.
- **더 큰 GPU 메모리(A100: 40GB, H100: 80GB)**:
  - 디코드 요청의 메모리 병목이 줄어들어 더 큰 디코드 배치를 처리 가능.

---

### **2. 클라우드 기반 환경에서의 성능 이점 가능성**
#### **2.1 Google Cloud TPU**
- **특징**:
  - Google Cloud TPU v4는 대규모 모델 추론에 최적화된 하드웨어로, 높은 대역폭 메모리와 네트워크 상호 연결 제공.
  - **Mesh TensorFlow** 기반 데이터 병렬 처리 가능.
- **TetriInfer의 이점**:
  - **고정 크기 청킹**:
    - TPU의 대규모 병렬 처리 기능을 활용해 청킹된 프롬프트를 빠르게 처리.
  - **예측 기반 디코드 스케줄링**:
    - TPU는 디코드 요청을 효율적으로 분배하는 데 적합한 **시스템 아키텍처**를 제공.

#### **2.2 AWS Inferentia**
- **특징**:
  - AWS Inferentia2는 대규모 추론 작업에 최적화된 전용 하드웨어로, 낮은 대기 시간과 높은 효율성을 목표로 설계.
  - **Neuron SDK**를 통해 메모리와 연산을 최적화 가능.
- **TetriInfer의 이점**:
  - 사전/디코드 분리를 통해 워크로드를 효율적으로 분배하고, Neuron SDK의 최적화 기능을 활용 가능.
  - 디코드 요청을 Neuron Cores에 분배하여 병렬 처리 효율 극대화.

---

### **3. 하드웨어 및 네트워크 환경에 따른 변형**
#### **3.1 KV 캐시 전송 최적화**
- **문제**:
  - TetriInfer의 KV 캐시 전송은 V100의 PCIe 대역폭에 의존하여 느린 전송 속도를 보임.
- **최신 하드웨어에서의 변형**:
  - **NVLink(NVIDIA)**:
    - GPU 간 전송을 NVLink를 활용한 **Zero-Copy**로 변경하여 전송 지연 감소.
  - **Cloud TPU**:
    - TPU Pod 간 고속 네트워크를 활용하여 디코드 캐시를 실시간 전송.
  - **AWS Inferentia**:
    - Neuron Cores와 호환되는 **Neuron Direct Memory Access(DMA)** 기술 적용.

#### **3.2 동적 청킹(Dynamic Chunking)**
- **문제**:
  - 고정 크기 청킹은 최신 하드웨어의 향상된 성능을 충분히 활용하지 못할 가능성.
- **변형**:
  - 동적 청킹을 도입하여 하드웨어 자원 활용도를 극대화.
  - 하드웨어의 계산 및 메모리 한계에 따라 청킹 크기를 조정.

#### **3.3 디코드 요청 스케줄링 최적화**
- **문제**:
  - 디코드 요청 길이 예측이 부정확한 경우 자원 배분의 효율성이 낮아질 수 있음.
- **변형**:
  - 강화 학습 기반 스케줄링:
    - A100/H100의 멀티-GPU 환경에서 디코드 요청을 실시간으로 분배.
  - 클라우드 환경에서는 강화 학습을 활용한 워크로드 패턴 학습으로 요청 분배 최적화.

#### **3.4 메모리 사용 최적화**
- **문제**:
  - 고정 크기 메모리 예약은 최신 하드웨어의 유연한 메모리 관리 기능을 충분히 활용하지 못함.
- **변형**:
  - FlexGen과 같은 페이징 기반 메모리 최적화를 TetriInfer에 도입:
    - GPU 메모리가 초과될 경우 디코드 요청을 CPU로 오프로드.
    - AWS Inferentia와 같은 하드웨어의 로컬 메모리와 연계.

---

### **결론: 최신 환경에서의 TetriInfer 성능 전망**
#### **동일한 성능 이점 가능성**
- 최신 GPU(A100/H100)와 클라우드 하드웨어는 TetriInfer의 설계와 잘 맞아떨어지며, 특히 KV 캐시 전송 및 디코드 스케줄링에서 성능 향상이 기대됩니다.

#### **필요한 변형**
- KV 캐시 전송에서 최신 네트워크 기술(NVLink, TPU Pod Interconnect)을 활용해 병목 현상을 줄이고, 고정 크기 청킹 대신 동적 청킹을 통해 자원 활용 효율을 극대화해야 합니다.
- 클라우드 기반 환경에서는 강화 학습 기반 스케줄링 및 TPU/Neuron SDK와의 통합 최적화가 필요합니다.

#### **미래 연구 방향**
- 최신 하드웨어와 클라우드 환경에서 TetriInfer의 성능을 정량적으로 검증하고, 변형된 설계 요소의 효과를 실험적으로 입증하는 연구가 필요합니다.