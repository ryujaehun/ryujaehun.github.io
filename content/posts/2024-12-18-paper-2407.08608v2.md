---
categories:
- paper-review
- with-gpt
date: "2024-12-18"
title: 'FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision'
---

[논문 링크](https://arxiv.org/abs/2407.08608v2)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### FlashAttention-3 논문 분석

FlashAttention-3는 Transformer의 Attention 연산을 최적화하여 성능과 메모리 효율성을 크게 향상시킨 알고리즘입니다. 이 논문은 기존 FlashAttention-2의 한계를 극복하며, NVIDIA Hopper GPU의 특화된 하드웨어 기능을 활용하여 **비동기 처리(asynchrony)** 와 **저정밀도 연산(low-precision)** 을 도입했습니다.

---

### 1. 강점과 독창적인 지점

#### 강점
1. **성능 개선**:
   - 기존 FlashAttention-2 대비 최대 **1.5~2.0배 속도 향상**.
   - H100 GPU에서 FP16 연산에서 최대 **740 TFLOPs/s**, FP8 연산에서 **1.2 PFLOPs/s** 성능 달성.
   - **Warp-specialized pipelining** 을 통해 메모리 대역폭 병목 문제 해소.

2. **저정밀도 연산 활용**:
   - FP8 연산을 도입하여 처리량을 FP16 대비 두 배로 증가.
   - **Block Quantization**과 **Incoherent Processing** 기술로 FP8의 수치 정확도를 유지.

3. **비동기 처리 최적화**:
   - Tensor Core와 메모리 로드(TMA)를 병렬 처리하여 계산과 데이터 이동을 동시에 수행.
   - **Pingpong Scheduling**을 통해 Softmax와 GEMM 연산 간의 병렬성 극대화.

4. **오픈소스화 및 접근성**:
   - PyTorch와 HuggingFace와의 통합 계획으로 연구자 및 개발자들이 쉽게 활용 가능.

---

### 2. 핵심 알고리즘 설명 (예시 포함)

FlashAttention-3는 크게 **비동기 처리**, **2-스테이지 파이프라이닝**, **FP8 최적화**의 세 가지 주요 기술로 구성됩니다.

#### (1) 입력 데이터
- Query (\( Q \)), Key (\( K \)), Value (\( V \)) 행렬:
  - 크기: \( Q, K, V \in \mathbb{R}^{N \times d} \) (시퀀스 길이 \( N \), 차원 \( d \)).
  - 예: \( N = 1024, d = 64 \).

#### (2) 알고리즘 단계

1. **Warp Specialization과 비동기 처리**:
   - 각 Warp는 Producer(데이터 로드)와 Consumer(연산)로 분리.
   - TMA(Tensor Memory Accelerator)를 활용해 메모리 로드와 Tensor Core 연산을 병렬 수행.
   - **Pingpong Scheduling**:
     - Softmax 연산을 한 Warp에서 수행하는 동안, 다른 Warp는 GEMM 연산 수행.

2. **2-스테이지 파이프라이닝**:
   - GEMM과 Softmax 연산을 교차 수행하여 연산 대기 시간 최소화.
   - GEMM-Softmax 간 종속성을 줄이기 위해 추가 버퍼를 사용.
   - 예:
     - 1단계: \( S = QK^\top \) 계산.
     - 2단계: \( P = \text{softmax}(S) \), \( O = PV \) 계산.

3. **FP8 최적화**:
   - FP8 연산은 낮은 비트 표현으로 처리량 증가.
   - **Block Quantization**:
     - 블록 단위로 스케일링하여 값의 표현 범위를 최적화.
   - **Incoherent Processing**:
     - Q와 K를 무작위 직교 행렬로 변환해 outlier 값의 영향을 줄임.

---

### 3. 한계점

#### (1) **FP8의 제약**
- 저정밀도 연산에서 수치 오류 가능성.
- 일부 아키텍처에서는 FP8 활용이 제한적.

#### (2) **H100 GPU 의존성**
- NVIDIA Hopper GPU의 하드웨어 특화 기능에 강하게 의존.
- 다른 하드웨어 플랫폼(TPU, AMD GPU)에서 구현 어려움.

#### (3) **복잡성 증가**
- 비동기 처리와 파이프라이닝으로 커널 설계 및 디버깅 복잡성 증가.
- FP8 변환을 위한 추가 데이터 전처리 필요.

---

### 4. 결론

FlashAttention-3는 비동기 처리, 저정밀도 연산, 하드웨어 최적화를 통해 Transformer의 Attention 병목을 극복하는 강력한 알고리즘입니다. 긴 시퀀스 처리와 고성능 연산이 요구되는 모델에서 유용하며, PyTorch 및 HuggingFace와 통합됨으로써 연구자와 개발자에게 널리 활용될 가능성이 있습니다. 그러나 **FP8 제약, 하드웨어 의존성** 및 **복잡한 설계**는 후속 연구와 구현에서 추가적인 개선이 필요합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### FlashAttention-3의 핵심 알고리즘 설명

FlashAttention-3는 **GPU의 비동기 처리(asynchrony)** 와 **저정밀도 연산(FP8)** 을 결합하여 Transformer의 Attention 연산 성능을 획기적으로 개선한 알고리즘입니다. 이 알고리즘은 **Warp Specialization**, **Pingpong Scheduling**, 및 **2-스테이지 파이프라이닝**을 활용하여 메모리 이동과 연산 병목을 제거합니다.

---

### 1. 입력 데이터
- **Query (\( Q \)), Key (\( K \)), Value (\( V \))**:
  - \( Q, K, V \in \mathbb{R}^{N \times d} \): 시퀀스 길이 \( N \), 차원 \( d \).
  - 예시:
    - \( N = 8, d = 4 \).
    - \( Q = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ ... \end{bmatrix} \),
      \( K = \begin{bmatrix} 2 & 1 & 4 & 3 \\ ... \end{bmatrix} \),
      \( V = \begin{bmatrix} 1 & 0 & 1 & 0 \\ ... \end{bmatrix} \).

---

### 2. 알고리즘 단계 (Forward Pass)

FlashAttention-3는 기존 Attention 연산:
1. \( S = QK^\top \) 계산.
2. \( P = \text{softmax}(S) \) 계산.
3. \( O = PV \) 계산.

이를 최적화하기 위해 다음 과정을 따릅니다.

---

#### 단계 1: Warp Specialization (생산자-소비자 분리)

1. **Warp 역할 분리**:
   - Producer Warp: 데이터 로드.
     - \( Q, K, V \) 블록을 HBM(Higher Bandwidth Memory)에서 Shared Memory로 비동기 로드.
   - Consumer Warp: 연산 수행.
     - \( QK^\top \) (GEMM) 및 \( \text{softmax} \), \( PV \) 수행.

2. **비동기 처리**:
   - TMA(Tensor Memory Accelerator)를 사용하여 연산과 메모리 로드를 병렬 수행.

---

#### 단계 2: Pingpong Scheduling (GEMM과 Softmax 병렬 처리)

1. **Pingpong Scheduling**:
   - Warp 1이 \( QK^\top \) 연산(GEMM)을 수행하는 동안, Warp 2는 Softmax 연산 수행.
   - Warp 간 작업을 교차(pingpong)하여 병렬성 극대화.

2. **예시**:
   - **Iteration 1**:
     - Warp 1: \( S = Q_1 K_1^\top \) 계산.
     - Warp 2: \( P = \text{softmax}(S) \) 계산.
   - **Iteration 2**:
     - Warp 1: \( S = Q_2 K_2^\top \) 계산.
     - Warp 2: \( O = PV \) 계산.

---

#### 단계 3: 2-스테이지 파이프라이닝 (GEMM-Softmax Overlap)

1. **연산 간 파이프라이닝**:
   - \( S \), \( P \), \( O \) 연산의 종속성을 줄이기 위해 추가 버퍼를 사용.
   - 한 블록의 \( S \)를 계산하는 동안, 이전 블록의 \( P, O \)를 계산.

2. **파이프라인 예시**:
   - 1단계: \( S_{\text{cur}} = Q_{\text{cur}} K_{\text{cur}}^\top \) 계산.
   - 2단계: \( P_{\text{prev}} = \text{softmax}(S_{\text{prev}}) \), \( O_{\text{prev}} = P_{\text{prev}} V_{\text{prev}} \).

---

#### 단계 4: FP8 최적화 (Block Quantization 및 Incoherent Processing)

1. **FP8 연산**:
   - FP8으로 연산량을 FP16 대비 2배 증가.
   - \( Q, K, V \)를 블록 단위로 스케일링(Block Quantization).
2. **Incoherent Processing**:
   - \( Q, K \)에 무작위 직교 변환을 적용하여 outlier 효과 완화.

---

### 3. 예제: \( N = 4, d = 2 \)

#### 입력
- \( Q = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \\ 7 & 8 \end{bmatrix} \),
  \( K = \begin{bmatrix} 2 & 1 \\ 4 & 3 \\ 6 & 5 \\ 8 & 7 \end{bmatrix} \),
  \( V = \begin{bmatrix} 1 & 1 \\ 2 & 2 \\ 3 & 3 \\ 4 & 4 \end{bmatrix} \).

#### 단계별 연산
1. \( S = QK^\top \):
   \[
   S = \begin{bmatrix} 4 & 10 \\ 8 & 22 \\ 12 & 34 \\ 16 & 46 \end{bmatrix}.
   \]

2. Softmax:
   \[
   P = \text{softmax}(S) = \begin{bmatrix} 0.12 & 0.88 \\ ... \end{bmatrix}.
   \]

3. \( O = PV \):
   \[
   O = \begin{bmatrix} 1.76 & 1.76 \\ ... \end{bmatrix}.
   \]

---

### 4. 성능 개선 요약

| **기법**                | **효과**                                 |
| ----------------------- | ---------------------------------------- |
| Warp Specialization     | 메모리 로드와 연산 병렬화.               |
| Pingpong Scheduling     | Warp 간 연산 교차로 연산 대기 시간 제거. |
| 2-스테이지 파이프라이닝 | GEMM-Softmax 종속성 최소화.              |
| FP8 최적화              | 처리량 2배 증가, 수치 오류 최소화.       |

---

### 5. 정리

FlashAttention-3는 **비동기 처리와 파이프라이닝**을 통해 GPU의 성능을 극대화하고, **FP8 최적화**를 통해 연산량을 줄이면서 정확도를 유지합니다. 긴 시퀀스 처리와 대규모 모델에서 중요한 병목을 제거하여 Transformer의 Attention 효율성을 혁신적으로 향상시켰습니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### FlashAttention-3 논문의 결과 분석 및 다른 방법론과의 비교

FlashAttention-3는 **Transformer의 Attention 연산을 최적화**하기 위해 도입된 기법으로, 최신 GPU 아키텍처(NVIDIA Hopper H100)의 특성을 적극 활용하여 기존 방법론 대비 뛰어난 성능을 보여줍니다. 이 논문은 **속도, 메모리 효율성, 수치 안정성**에서 획기적인 개선을 제시하며, 이를 뒷받침하는 방법론과 이유를 아래에 분석하였습니다.

---

### 1. 논문에서 제시한 주요 결과

#### (1) **성능 측면**
- **FlashAttention-3 vs. FlashAttention-2**:
  - Forward Pass:
    - FlashAttention-3는 FP16에서 FlashAttention-2 대비 **1.5~2.0배 더 빠른 속도**.
    - H100 GPU에서 최대 **740 TFLOPs/s (75% 이론 최대치)** 성능 달성.
  - Backward Pass:
    - FlashAttention-3는 FlashAttention-2 대비 **1.5~1.75배 속도 향상**.
  - FP8 연산에서 **1.2 PFLOPs/s**(FP16 대비 약 2배 처리량).

- **긴 시퀀스에서의 우수성**:
  - 시퀀스 길이가 16k일 때, cuDNN, Triton 등 최신 벤치마크 대비 우위.
  - FP8로 연산 시에도 정확도 손실 없이 뛰어난 성능 유지.

#### (2) **정확도 측면**
- FP8 도입 시에도 **2.6배 낮은 수치 오차**를 달성.
  - 기존 FP8 방식(Per-tensor Quantization) 대비 블록 단위 스케일링(Block Quantization)과 비선형 변환(Incoherent Processing) 도입으로 정확도 유지.

---

### 2. 다른 방법론과의 비교 (표 요약)

| **기법**               | **Forward 속도 (TFLOPs/s)** | **Backward 속도** | **FP8 정확도 유지** | **긴 시퀀스 처리** | **특징**                                   |
| ---------------------- | --------------------------- | ----------------- | ------------------- | ------------------ | ------------------------------------------ |
| **Standard Attention** | 낮음 (200~400)              | 낮음              | 지원 안 됨          | 비효율적           | HBM 과도 사용, 성능 병목                   |
| **FlashAttention-2**   | 500~600                     | 중간              | 제한적              | 최대 8k            | 병렬 처리 개선, IO 최적화                  |
| **FlashAttention-3**   | 740 (FP16), 1200 (FP8)      | 빠름              | **2.6× 개선**       | **16k 이상**       | 비동기 처리, FP8 최적화, 파이프라이닝 도입 |

---

### 3. FlashAttention-3의 특출난 점

#### (1) **비동기 처리와 병렬성 극대화**
- **Pingpong Scheduling**:
  - Softmax와 GEMM 연산을 Warp 단위로 병렬 처리.
  - Tensor Core와 TMA의 비동기 특성을 활용하여 메모리 로드와 연산을 중첩 수행.
- **Warp Specialization**:
  - Warp를 Producer(메모리 로드)와 Consumer(연산)로 나누어 각 역할 최적화.

#### (2) **2-스테이지 파이프라이닝**
- GEMM(행렬 곱셈)과 Softmax 연산 간 종속성을 제거하여 연산 대기 시간 최소화.
- **효과**:
  - Forward Pass 속도 최대 30% 증가.

#### (3) **FP8 최적화**
- FP8 연산으로 처리량을 FP16 대비 두 배로 증가.
- **Block Quantization**:
  - 각 블록을 독립적으로 스케일링하여 수치 오류 감소.
- **Incoherent Processing**:
  - Q와 K에 무작위 직교 변환 적용으로 outlier 효과 완화.

---

### 4. 논문에서 제시한 결과를 가능하게 한 이유

#### (1) **GPU 하드웨어의 특화된 활용**
- NVIDIA Hopper H100의 Tensor Core와 TMA를 적극적으로 활용하여 메모리 대역폭과 연산 병목을 제거.
- GPU의 비동기 처리 특성을 알고리즘 설계에 통합하여 최적화.

#### (2) **저정밀도 연산에 대한 새로운 접근**
- FP8 연산의 정확도 문제를 해결하기 위해 Block Quantization과 Incoherent Processing 도입.
- 기존 Per-tensor Quantization 대비 정확도를 크게 향상.

#### (3) **알고리즘의 병렬성 강화**
- Warp 단위의 작업 분리를 통해 연산 간 대기 시간 제거.
- Softmax와 GEMM 간의 연산 병렬성을 극대화.

---

### 5. 내 생각: FlashAttention-3의 강점과 향후 과제

#### 강점
1. **효율성과 정확성의 균형**:
   - FP8 연산을 도입하면서도 정확도를 유지하는 점은 고성능 컴퓨팅에서 매우 중요한 기여.
   - 긴 시퀀스 처리에서 경쟁 우위 확보.

2. **하드웨어 특화 알고리즘**:
   - NVIDIA Hopper 아키텍처의 하드웨어 특성을 알고리즘 설계에 통합하여 하드웨어 성능을 극대화.

3. **범용성**:
   - PyTorch와 HuggingFace와 통합 계획으로 실용적이고 널리 사용될 가능성.

#### 한계점 및 향후 과제
1. **하드웨어 종속성**:
   - NVIDIA Hopper GPU에 강하게 의존하며, 다른 플랫폼(TPU, AMD GPU)에서의 적용 제한.
   - 향후 멀티플랫폼 지원을 위한 연구 필요.

2. **복잡한 구현**:
   - 비동기 처리와 파이프라이닝 설계로 인해 커널 구현과 디버깅 복잡성 증가.

3. **FP8 연산의 범용성**:
   - FP8 연산이 작은 시퀀스 또는 특정 작업에서 성능 저하를 보일 수 있음.

---

### 결론

FlashAttention-3는 Transformer의 Attention 연산에서 성능과 효율성을 획기적으로 개선한 혁신적인 알고리즘입니다. **비동기 처리, 저정밀도 연산, 병렬화 최적화**를 통해 긴 시퀀스와 고성능 모델에서 뛰어난 성능을 발휘하며, FP8 연산의 수치 안정성 문제를 해결함으로써 새로운 가능성을 열었습니다. 그러나 **하드웨어 독립성과 구현 복잡성**은 후속 연구를 통해 보완해야 할 중요한 과제로 보입니다.


# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### FlashAttention-3: 입력 데이터와 추론 과정, 모델 아키텍처 분석

FlashAttention-3는 Transformer의 Attention 연산을 최적화하여 메모리와 연산 효율성을 극대화한 알고리즘입니다. 아래에서는 입력 데이터와 추론 과정을 예시를 들어 설명한 뒤, 모델 아키텍처와 관련된 연산 및 메모리 요구량을 분석하겠습니다.

---

### 1. 입력 데이터 구성

#### (1) 데이터 정의
- 입력 데이터는 Query (\( Q \)), Key (\( K \)), Value (\( V \)) 행렬로 구성됩니다.
  - \( Q, K, V \in \mathbb{R}^{N \times d} \):
    - \( N \): 시퀀스 길이 (문장 내 토큰 수).
    - \( d \): Attention Head 차원.

#### (2) 예시 입력
- \( N = 4, d = 2 \)로 설정.
- 입력 데이터:
  - \( Q = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \\ 7 & 8 \end{bmatrix} \),
  - \( K = \begin{bmatrix} 2 & 1 \\ 4 & 3 \\ 6 & 5 \\ 8 & 7 \end{bmatrix} \),
  - \( V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \\ 0 & 0 \end{bmatrix} \).

---

### 2. 추론 과정 (Forward Pass)

FlashAttention-3의 추론 과정은 다음 주요 단계를 포함합니다:

#### 단계 1: Similarity 계산 (\( S = QK^\top \))
- \( S[i, j] = \text{Dot Product}(Q[i, :], K[j, :]) \).
- 예시 계산:
  \[
  S = \begin{bmatrix}
    1 \cdot 2 + 2 \cdot 1 & 1 \cdot 4 + 2 \cdot 3 & 1 \cdot 6 + 2 \cdot 5 & 1 \cdot 8 + 2 \cdot 7 \\
    3 \cdot 2 + 4 \cdot 1 & 3 \cdot 4 + 4 \cdot 3 & 3 \cdot 6 + 4 \cdot 5 & 3 \cdot 8 + 4 \cdot 7 \\
    \cdots & \cdots & \cdots & \cdots
  \end{bmatrix}
  = \begin{bmatrix}
    4 & 10 & 16 & 22 \\
    10 & 25 & 40 & 55 \\
    \cdots & \cdots & \cdots & \cdots
  \end{bmatrix}.
  \]

---

#### 단계 2: Softmax 계산 (\( P = \text{softmax}(S) \))
- 각 행에 대해 Softmax 수행:
  \[
  P[i, j] = \frac{\exp(S[i, j])}{\sum_k \exp(S[i, k])}.
  \]
- 예시 계산:
  - \( P[0, :] \)에 대해:
    \[
    P[0, :] = \frac{\exp([4, 10, 16, 22])}{\sum \exp([4, 10, 16, 22])}.
    \]

---

#### 단계 3: 가중 합 계산 (\( O = PV \))
- 각 행 \( O[i, :] \)는 \( V \)의 열을 \( P[i, :] \)로 가중합:
  \[
  O[i, :] = \sum_j P[i, j] \cdot V[j, :].
  \]
- 예시 계산:
  - \( O[0, :] = P[0, 0] \cdot V[0, :] + P[0, 1] \cdot V[1, :] + \cdots \).

---

### 3. FlashAttention-3의 최적화

#### (1) 비동기 처리
- TMA(Tensor Memory Accelerator)로 메모리 로드와 Tensor Core 연산을 비동기 처리.
- Producer Warp가 데이터를 로드하고 Consumer Warp가 연산 수행.

#### (2) 2-스테이지 파이프라이닝
- \( QK^\top \) (GEMM) 연산과 Softmax 계산을 교차 수행하여 연산 병목 제거.

#### (3) FP8 연산 최적화
- 저정밀도 연산(FP8)을 도입하여 처리량 증가 및 메모리 절약.
- 블록 단위로 \( Q, K, V \)를 스케일링(Block Quantization)하여 outlier 영향을 완화.

---

### 4. 모델 아키텍처

#### (1) Transformer 모델 구조
1. **Input Embedding**:
   - 입력 텍스트를 \( d \)-차원 벡터로 변환.
2. **Multi-Head Attention (MHA)**:
   - FlashAttention-3 기반의 self-attention 연산.
3. **Feedforward Layer**:
   - 비선형 변환 및 활성화 함수 적용.
4. **Layer Normalization 및 Skip Connection**:
   - 출력 안정화.

#### (2) FlashAttention-3 통합
- MHA 모듈 내에서 기존 Attention 연산을 FlashAttention-3로 대체.

---

### 5. 연산 및 메모리 요구량

#### 연산량 (FLOPs)
1. **Forward Pass**:
   - \( S \): \( O(N^2 \cdot d) \) FLOPs.
   - \( P \): \( O(N^2) \) FLOPs.
   - \( O \): \( O(N^2 \cdot d) \) FLOPs.
   - 총: \( 2N^2 \cdot d + N^2 \).
2. **Backward Pass**:
   - Forward의 약 2.5배.

#### 메모리 요구량
- 기존 Attention: \( O(N^2) \) 메모리 (HBM).
- FlashAttention-3:
  - \( O(N \cdot d) \) 메모리 (Shared Memory 사용).
  - IO 요구량 약 **10배 감소**.

---

### 6. 결론

FlashAttention-3는 **입력 데이터를 블록 단위로 처리하며, 비동기 처리와 FP8 최적화를 통해 연산량과 메모리 사용량을 크게 줄이는 방식**으로 Transformer의 성능을 향상시킵니다. 이는 긴 시퀀스 처리와 고성능 모델 학습에서 중요한 병목을 제거하며, 특히 최신 GPU 하드웨어의 기능을 최대한 활용한 혁신적인 접근법으로 평가됩니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### FlashAttention-3의 한계와 이를 극복하기 위한 연구 흐름

FlashAttention-3는 Transformer 모델의 Attention 연산 최적화에 있어 매우 강력한 기법을 제안했지만, 몇 가지 한계가 존재합니다. 이를 극복하기 위한 연구 흐름은 **하드웨어 종속성, FP8 활용 제약, 분산 환경 확장성, 및 일반화 문제**를 중심으로 정리할 수 있습니다.

---

### 1. FlashAttention-3의 주요 한계

#### (1) **하드웨어 종속성**
- FlashAttention-3는 NVIDIA Hopper GPU의 특화된 기능(Tensor Core, TMA 등)에 강하게 의존.
- 다른 하드웨어(TPU, AMD GPU) 또는 FPGA, ASIC과 같은 특수 하드웨어에서 사용하기 어렵다.

#### (2) **FP8 활용의 제약**
- FP8 연산은 수치 정확도 유지가 어려우며, outlier 데이터에 취약.
- FP8 기반 연산은 특정 모델과 데이터셋에 최적화되지 않을 경우 성능 저하 가능.

#### (3) **분산 학습 환경에서의 제약**
- FlashAttention-3는 단일 GPU에서의 최적화를 중점으로 설계되었으며, 멀티-GPU 또는 분산 환경에서 IO 병목 문제가 발생할 가능성이 있음.

#### (4) **알고리즘의 복잡성**
- 비동기 처리 및 파이프라이닝 설계로 인해 구현 및 디버깅 복잡성이 높다.
- PyTorch와 같은 고수준 프레임워크와 통합하기 어렵다.

---

### 2. 한계를 극복하기 위한 연구 흐름

#### (1) **하드웨어 독립성 확보**
1. **TPU 및 AMD GPU 호환성 확보**
   - FlashAttention-3를 GPU 외의 플랫폼에서 실행할 수 있도록 변환.
   - TPU에서는 Google XLA와 통합, AMD GPU에서는 ROCm과 통합 필요.

2. **FPGA 및 ASIC용 경량화**
   - FlashAttention-3의 복잡한 커널을 FPGA나 ASIC 환경에서 효율적으로 실행할 수 있도록 최적화.
   - Hierarchical Memory 접근 방식을 활용하여 메모리 사용량을 줄임.

3. **멀티플랫폼 컴파일러 통합**
   - Triton 또는 TVM과 같은 컴파일러를 활용하여 CUDA 커널을 플랫폼 독립적으로 생성.
   - LLVM 기반의 중간 표현(IR)을 사용해 다양한 하드웨어에서 실행 가능.

---

#### (2) **FP8의 정밀도 문제 해결**
1. **Dynamic Precision Tuning**
   - FP8, FP16, BF16, FP32를 동적으로 전환하는 알고리즘 도입.
   - 중요도가 높은 연산은 FP32로 처리하고, 나머지는 FP8로 처리.

2. **Quantization 기술 개선**
   - **Block Quantization** 외에 데이터 분포 기반의 스케일링 기법 도입.
   - **Adaptive Quantization**:
     - 학습 중 동적으로 스케일링을 조정하여 FP8의 정확도를 유지.

3. **Outlier 대응 기술 강화**
   - Incoherent Processing 외에 더 정교한 outlier 보정 기법 도입:
     - 예: Outlier-aware Regularization.

---

#### (3) **멀티-GPU 및 분산 환경 확장**
1. **Cross-GPU Communication 최적화**
   - 분산 환경에서 GPU 간 데이터 교환량을 최소화하는 알고리즘 개발.
   - Ring-Reduce 또는 All-Reduce 통신 패턴 최적화.

2. **Sharded Attention**
   - \( Q, K, V \) 데이터를 GPU에 분산 저장하고 필요한 부분만 교환.
   - 대규모 시퀀스 학습에서 노드 간 병목을 완화.

3. **Pipeline Parallelism 도입**
   - GPU 간 Attention 연산을 단계별로 나누어 처리하여 병렬 처리 강화.

---

#### (4) **알고리즘 단순화 및 통합**
1. **PyTorch/TensorFlow 통합**
   - FlashAttention-3를 고수준 딥러닝 프레임워크와 통합하여 개발자 접근성을 높임.
   - CUDA 커널의 복잡성을 숨기고 API 수준에서 간단히 호출 가능하도록 설계.

2. **Auto-Tuning Framework**
   - GPU 아키텍처별로 최적 블록 크기와 파이프라이닝 전략을 자동으로 탐색.

3. **컴파일러 기반 최적화**
   - NVIDIA의 TensorRT 또는 Triton과 같은 컴파일러와 통합하여 커널 자동 생성 및 최적화.

---

### 3. 향후 연구 방향

| **한계**                  | **극복 방안**                                   | **기대 효과**                                   |
| ------------------------- | ----------------------------------------------- | ----------------------------------------------- |
| 하드웨어 종속성           | 멀티플랫폼 컴파일러 지원 (Triton, LLVM 등)      | 다양한 하드웨어에서 FlashAttention-3 사용 가능. |
| FP8 활용의 제약           | Dynamic Precision Tuning, Adaptive Quantization | 수치 정확도 유지 및 FP8 활용 범위 확대.         |
| 분산 학습 환경에서의 제약 | Cross-GPU Communication, Sharded Attention      | 대규모 모델 학습에서 병목 최소화.               |
| 알고리즘 복잡성           | PyTorch/TensorFlow 통합, Auto-Tuning Framework  | 구현 난이도 감소 및 연구자 접근성 향상.         |

---

### 4. 결론

FlashAttention-3의 한계를 극복하기 위해서는 **FP8의 정밀도 개선**, **멀티플랫폼 호환성 확보**, **분산 환경 최적화**, 그리고 **알고리즘 단순화**가 필요합니다. 이러한 연구 방향은 FlashAttention-3를 더 넓은 환경에서 활용 가능하게 하고, Transformer 모델의 효율성을 극대화할 수 있는 기반을 제공합니다.

# Q: FlashAttention-3에서 도입된 Pingpong Scheduling과 2-스테이지 파이프라이닝 기법이 기존 FlashAttention-2 대비 연산 효율성을 어떻게 향상시키며, 이를 통해 어떤 병목 문제가 해결되었는가?



### FlashAttention-3의 **Pingpong Scheduling**과 **2-스테이지 파이프라이닝** 기법 분석

FlashAttention-3는 기존 FlashAttention-2에서 발생하던 연산 병목과 메모리 대역폭 문제를 해결하기 위해 **Pingpong Scheduling**과 **2-스테이지 파이프라이닝**을 도입했습니다. 이 두 기법은 GPU의 하드웨어 특성을 활용하여 병렬성을 극대화하고 비동기성을 강화함으로써 연산 효율성을 크게 향상시킵니다.

---

### 1. 기존 FlashAttention-2의 병목 문제

#### (1) Sequential Processing
- FlashAttention-2는 \( QK^\top \) (GEMM), Softmax, \( PV \) (GEMM) 연산이 순차적으로 수행됨.
- 각 연산은 이전 연산의 출력을 필요로 하므로, **종속성**으로 인해 GPU 리소스의 활용도가 낮아짐.

#### (2) Non-GEMM Operation Overhead
- GPU는 GEMM(General Matrix Multiplication) 연산에서 높은 처리량을 보이지만, Softmax와 같은 비-GEMM 연산은 처리량이 훨씬 낮음.
- FlashAttention-2는 GEMM과 Softmax 연산을 별도로 실행하여 비효율이 발생.

#### (3) Memory Access Latency
- GPU의 Tensor Core는 고속 연산을 지원하지만, 메모리 액세스 지연이 연산 속도를 제한.

---

### 2. FlashAttention-3의 주요 기법

#### (1) Pingpong Scheduling
- **동작 방식**:
  - 두 개의 Warp 그룹이 교대로 연산을 수행.
    - Warp 1: \( QK^\top \) 연산 수행 (현재 블록).
    - Warp 2: Softmax 수행 (이전 블록).
  - Tensor Core와 TMA(Tensor Memory Accelerator) 간 작업을 **교차**하여 대기 시간을 제거.

- **효과**:
  - GEMM과 Softmax 연산을 중첩(overlap)하여 GPU 리소스 활용도를 극대화.
  - H100 GPU에서 GEMM 연산의 고속 처리량을 유지하면서 Softmax 연산 병목 완화.

#### (2) 2-스테이지 파이프라이닝
- **동작 방식**:
  - GEMM과 Softmax 연산을 교차 단계로 분리:
    1. 첫 번째 단계:
       - 현재 블록 \( QK^\top \) 연산 수행 → \( S \) 계산.
    2. 두 번째 단계:
       - 이전 블록 Softmax 연산 수행 → \( P = \text{softmax}(S) \), \( O = PV \) 계산.
  - 각 단계에서 결과를 **버퍼**에 저장하여 다음 단계에서 사용.

- **효과**:
  - 연산 간의 **종속성 문제**를 완화하여 파이프라인 처리 가능.
  - GPU의 Tensor Core와 Softmax 유닛을 병렬적으로 활용.

---

### 3. 연산 효율성 향상 분석

#### (1) 성능 개선
- FlashAttention-3는 FP16 기준 H100 GPU에서 최대 **740 TFLOPs/s** 성능을 기록.
  - FlashAttention-2 대비 약 **1.5~2.0배 속도 향상**.
- Pingpong Scheduling으로 GEMM 연산의 GPU 사용률을 높이고, Softmax 병목을 제거.

#### (2) 병목 문제 해결
| **병목 문제**         | **FlashAttention-2**               | **FlashAttention-3**                     |
| --------------------- | ---------------------------------- | ---------------------------------------- |
| Sequential Processing | GEMM과 Softmax 연산이 순차적 실행  | GEMM과 Softmax 연산 병렬 처리            |
| Non-GEMM Overhead     | Softmax 처리량 낮아 GEMM 대기 발생 | Softmax와 GEMM 병렬 수행으로 처리량 유지 |
| Memory Access Latency | Tensor Core 대기 시간 발생         | TMA와 Tensor Core 작업 중첩              |

---

### 4. 예시: Pingpong Scheduling과 2-스테이지 파이프라이닝

#### 입력
- \( Q, K, V \in \mathbb{R}^{4 \times 2} \) (시퀀스 길이 4, 차원 2).
- 블록 크기 \( B_r = 2 \), \( B_c = 2 \).

#### 단계별 수행
1. **Warp 1**:
   - 1단계: \( S_{\text{cur}} = QK^\top \) 계산 (현재 블록).
2. **Warp 2**:
   - 2단계: \( P_{\text{prev}} = \text{softmax}(S_{\text{prev}}) \), \( O_{\text{prev}} = PV \) 계산.

#### 교차 수행
- Warp 1이 현재 블록의 \( QK^\top \)를 계산하는 동안, Warp 2는 이전 블록의 Softmax와 \( PV \) 계산.

---

### 5. 성능 분석 및 결과

| **기법**         | **속도**                | **GPU 활용도** | **병목 제거**                           |
| ---------------- | ----------------------- | -------------- | --------------------------------------- |
| FlashAttention-2 | 중간 (500~600 TFLOPs/s) | 50~60%         | Sequential Processing 제한              |
| FlashAttention-3 | 높음 (740 TFLOPs/s)     | 75%            | GEMM-Softmax 병렬화 및 메모리 병목 제거 |

---

### 6. 결론

FlashAttention-3에서 도입된 **Pingpong Scheduling**과 **2-스테이지 파이프라이닝** 기법은 기존 FlashAttention-2에서 발생하던 연산 병목을 효과적으로 제거하고, GPU의 병렬성과 비동기성을 극대화하여 연산 효율성을 크게 향상시켰습니다. 특히, GEMM과 Softmax 연산을 중첩 수행함으로써 처리 속도와 GPU 활용도를 크게 높인 점이 주요 기여로 평가됩니다.

# Q: FP8 연산을 도입하면서 발생하는 수치 정확도 저하 문제를 Block Quantization과 Incoherent Processing이 어떻게 완화하며, 다른 대안적인 접근법은 어떤 것이 있을 수 있는가?

# A: 

### FlashAttention-3의 FP8 연산에서 수치 정확도 문제 해결: Block Quantization과 Incoherent Processing

FlashAttention-3는 FP8 연산을 도입하여 처리량을 FP16 대비 두 배로 증가시키는 데 성공했지만, FP8은 낮은 비트수로 인해 **수치 정확도 저하** 문제가 발생할 가능성이 있습니다. 이를 해결하기 위해 **Block Quantization**과 **Incoherent Processing**을 활용합니다.

---

### 1. FP8의 수치 정확도 저하 문제

#### (1) 낮은 표현력
- FP8은 **4비트 exponent**와 **3비트 mantissa**를 사용하므로 FP16, FP32에 비해 표현 가능한 숫자의 정밀도가 낮음.
- Outlier 값(특이적으로 큰 값)이 존재할 경우, Quantization(양자화) 과정에서 중요한 정보가 손실될 가능성이 큼.

#### (2) 연산 불안정성
- FP8 연산 중 Softmax나 MatMul에서 값의 범위가 급격히 변하면 수치적 불안정성이 발생.

---

### 2. Block Quantization: 정확도 문제 완화 방법

#### (1) 기법 설명
- FP8 연산 시, 전체 \( Q, K, V \) 행렬이 아닌 **작은 블록 단위**로 스케일링하여 Quantization 수행.
  - 예: \( Q \in \mathbb{R}^{N \times d} \)를 \( B_r \times d \) 블록으로 나눔.
  - 각 블록에 대해 별도의 스케일링 인자를 유지하여 값의 범위를 최적화.

#### (2) 적용 방법
1. \( Q, K, V \)를 블록 단위로 나눔.
2. 각 블록에서 최대값과 최소값을 계산하여 스케일링 팩터 결정.
3. 스케일링 후 FP8로 Quantization 수행.

#### (3) 효과
- 각 블록의 범위를 독립적으로 최적화하여 **outlier의 영향을 최소화**.
- Block Quantization이 자연스럽게 FlashAttention-3의 **블록 단위 처리 방식**과 결합되어 추가적인 성능 저하 없이 구현 가능.

---

### 3. Incoherent Processing: 정확도 문제 완화 방법

#### (1) 기법 설명
- Outlier 값이 주로 \( Q, K \) 행렬의 특정 위치에 집중되는 문제를 해결하기 위해 **무작위 직교 변환**을 적용.
- \( Q \)와 \( K \)에 랜덤 직교 행렬 \( M \)을 곱하여 값의 분포를 균등화:
  \[
  Q' = QM, \quad K' = KM
  \]
  - \( M \): Hadamard 행렬과 ±1 값을 랜덤하게 곱한 직교 행렬.

#### (2) 적용 방법
1. 랜덤 직교 행렬 \( M \) 생성.
2. \( Q \)와 \( K \)에 \( M \)을 곱하여 변환.
3. 변환된 \( Q', K' \)에 대해 FP8 Quantization 수행.

#### (3) 효과
- 특정 위치에 집중된 outlier 값을 분산시켜 **Quantization 오류를 완화**.
- 직교 변환은 Attention 계산의 결과를 보존하므로 성능 손실 없이 구현 가능.

---

### 4. 성능 및 정확도 개선 결과

#### 실험 결과
- FP8 FlashAttention-3는 기존 Per-tensor Quantization 대비 **2.6배 낮은 RMSE**(Root Mean Squared Error)를 기록.
- Block Quantization과 Incoherent Processing을 결합하여 정확도를 유지하면서 처리량(FLOPs/s)을 크게 증가.

#### 정리
| **기법**              | **장점**                                 | **단점**                            |
| --------------------- | ---------------------------------------- | ----------------------------------- |
| Block Quantization    | Outlier 영향 최소화, 추가 연산 부담 적음 | 블록 크기 설정 최적화 필요          |
| Incoherent Processing | 값 분포 균등화로 Quantization 오류 감소  | 추가 행렬 변환으로 연산량 약간 증가 |

---

### 5. 대안적인 접근법

#### (1) Dynamic Scaling
- FP8 연산 중 값의 범위에 따라 스케일링 팩터를 동적으로 조정.
- **장점**: Outlier가 있는 경우에도 정확도를 유지.
- **단점**: 구현 복잡성과 추가 연산 부담.

#### (2) Mixed Precision
- 중요한 연산은 FP16 또는 FP32로 수행하고, 부차적인 연산은 FP8로 수행.
- **장점**: 정확도와 성능 간 균형 유지.
- **단점**: 연산 흐름이 복잡해질 수 있음.

#### (3) Regularization
- 학습 과정에서 Outlier 값을 완화하는 추가 Regularization 도입.
- 예: Weight Clipping 또는 Gradient Clipping.

#### (4) Adaptive Quantization
- 데이터의 분포를 학습하여 각 값에 맞는 최적의 Quantization 방식을 적용.

---

### 6. 결론

FlashAttention-3는 **Block Quantization**과 **Incoherent Processing**을 통해 FP8 연산의 수치 정확도 저하 문제를 효과적으로 완화했습니다. 이러한 접근법은 기존 FP8 도입의 한계를 극복하며, 긴 시퀀스와 고성능 연산이 필요한 환경에서 중요한 역할을 합니다. 대안적인 접근법으로 Dynamic Scaling, Mixed Precision, Regularization 등을 고려할 수 있으며, 이를 통해 FP8 연산의 활용 범위를 더욱 확장할 수 있습니다.

# Q: FlashAttention-3가 멀티-GPU 환경 또는 TPU와 같은 다른 하드웨어 플랫폼에서 효과적으로 동작하려면 어떤 추가적인 최적화 또는 알고리즘 변경이 필요한가?

 

### FlashAttention-3의 멀티-GPU 및 다른 하드웨어 플랫폼(TPU 등)에서 효과적 동작을 위한 최적화 및 알고리즘 변경

FlashAttention-3는 NVIDIA Hopper GPU의 하드웨어 특성을 활용하여 최적화된 알고리즘이지만, 멀티-GPU 환경과 TPU와 같은 다른 하드웨어 플랫폼에서 적용하려면 몇 가지 추가적인 최적화와 알고리즘 변경이 필요합니다. 아래에서 이러한 문제를 해결하기 위한 구체적인 방안과 기술을 제안합니다.

---

### 1. 멀티-GPU 환경에서의 최적화

#### (1) Cross-GPU Communication 최적화
- **문제점**:
  - 멀티-GPU 환경에서는 \( Q, K, V \) 데이터를 분산 처리해야 하며, GPU 간 데이터 전송이 병목이 될 수 있음.

- **해결 방안**:
  1. **Sharded Attention**:
     - \( Q, K, V \) 데이터를 각 GPU에 분산 저장하고 필요한 데이터만 교환.
     - 예: 시퀀스 길이를 \( N \)에서 \( N/k \)로 나눠 GPU \( k \)에 할당.
  2. **All-Reduce 최적화**:
     - Attention 계산 후 노드 간 데이터를 합산할 때, All-Reduce 알고리즘을 최적화.
     - Ring-Reduce 또는 Tree-Reduce 방식을 활용해 통신량 최소화.
  3. **Mixed Precision 통신**:
     - GPU 간 데이터 전송 시 FP8 또는 FP16 형식으로 압축하여 전송 속도 향상.

---

#### (2) Pipeline Parallelism
- **문제점**:
  - Attention 계산의 연산량이 크기 때문에 GPU 간 부하 분배가 어려움.

- **해결 방안**:
  - Attention 연산을 단계별로 분리하여 GPU 간 파이프라인 처리.
    - GPU 1: \( QK^\top \) 계산.
    - GPU 2: Softmax 계산.
    - GPU 3: \( PV \) 계산.
  - 파이프라인 동기화를 위해 **Bubble Filling** 기법 사용.

---

#### (3) Tensor Memory Access (TMA) 최적화
- **문제점**:
  - HBM에서 Shared Memory로의 데이터 이동이 멀티-GPU 환경에서 비효율적으로 동작.

- **해결 방안**:
  - GPU 간 데이터 교환 시 **Prefetching**과 **Double Buffering** 기술을 적용하여 TMA 성능 최적화.
  - TMA 명령어를 GPU 클러스터 간 통신에서도 활용 가능하도록 설계.

---

### 2. TPU와 같은 다른 하드웨어 플랫폼에서의 최적화

#### (1) TPU용 XLA 통합
- **문제점**:
  - FlashAttention-3는 CUDA를 기반으로 설계되어 TPU에서 동작하지 않음.

- **해결 방안**:
  - Google XLA와 통합하여 TPU의 HBM과 L2 Cache를 활용.
  - TPU의 SPMD(Scatter-Parallel Model Data) 모델에 맞춰 Attention 연산을 병렬 처리.

---

#### (2) ROCm 및 AMD GPU 호환
- **문제점**:
  - NVIDIA Tensor Core에 특화된 설계로 ROCm 생태계를 활용하는 AMD GPU에서 비효율적.

- **해결 방안**:
  1. **HIP(Heterogeneous-computing Interface for Portability)**:
     - CUDA 커널을 HIP 코드로 변환하여 AMD GPU에서도 실행 가능하게 변경.
  2. **FP8 최적화**:
     - AMD GPU의 FP8 지원 여부를 확인하고, FP8 연산 최적화를 적용.

---

#### (3) FPGA 및 ASIC 환경에서의 최적화
- **문제점**:
  - FPGA 및 ASIC은 메모리 용량이 제한적이고 병렬 처리가 제한적.

- **해결 방안**:
  1. **RTL(Register Transfer Level) 변환**:
     - FlashAttention-3를 FPGA용 RTL 코드로 변환.
  2. **Hierarchical Memory Design**:
     - FPGA 내부의 SRAM과 DRAM 계층 구조를 활용하여 메모리 병목을 해소.

---

### 3. 알고리즘 변경을 통한 범용성 확보

#### (1) 플랫폼 독립적인 커널 설계
- **문제점**:
  - CUDA 기반 커널이 플랫폼 의존성을 가짐.

- **해결 방안**:
  - **Triton 또는 TVM 통합**:
    - Triton 컴파일러를 사용하여 플랫폼 독립적인 커널 생성.
    - LLVM 기반 중간 표현(IR)을 사용해 다양한 하드웨어에서 실행 가능.

#### (2) Auto-Tuning Framework
- **문제점**:
  - 플랫폼별 최적 블록 크기와 파이프라인 설정이 필요.

- **해결 방안**:
  - Auto-Tuning 프레임워크를 개발하여 각 하드웨어 아키텍처에 최적화된 설정 자동 탐색.

#### (3) Mixed Precision Support
- **문제점**:
  - FP8 연산이 일부 플랫폼에서 지원되지 않을 수 있음.

- **해결 방안**:
  - 중요한 연산은 FP16/FP32로 처리하고, 부수적인 연산은 FP8로 처리하는 Mixed Precision 전략 도입.

---

### 4. 요약 및 기대 효과

| **문제점**                    | **최적화/변경 방안**                                 | **기대 효과**                                   |
| ----------------------------- | ---------------------------------------------------- | ----------------------------------------------- |
| 멀티-GPU 환경에서의 통신 병목 | Cross-GPU Communication 최적화, Pipeline Parallelism | 병렬 처리 성능 향상, 대규모 모델 학습 지원.     |
| TPU와 같은 다른 플랫폼 지원   | XLA 통합, HIP 변환, 플랫폼 독립적 커널 설계          | TPU, AMD GPU 등에서 FlashAttention-3 사용 가능. |
| FP8 지원의 범용성 부족        | Mixed Precision Support, Adaptive Quantization       | 더 많은 플랫폼에서 FP8 성능 활용 가능.          |
| 알고리즘 복잡성               | Auto-Tuning Framework, Triton/TVM 통합               | 구현 난이도 감소 및 연구자 접근성 향상.         |

---

### 결론

FlashAttention-3가 멀티-GPU 환경 또는 TPU와 같은 다른 플랫폼에서 효과적으로 동작하려면 **Cross-GPU 통신 최적화**, **플랫폼 독립적인 커널 설계**, **Auto-Tuning Framework**, **Mixed Precision 지원** 등의 기술적 개선이 필요합니다. 이러한 최적화를 통해 FlashAttention-3는 다양한 하드웨어에서 범용적으로 사용 가능하며, 대규모 모델 학습 및 추론에서도 높은 성능을 유지할 수 있습니다.

# Q : flash attention 1,2,3논문의 inference과정에서의 차이점을 예시로 들어서 아주 자세하게 설명해 줘 그리고 장단점을 테이블로 정리해줘

 

### FlashAttention 1, 2, 3 논문의 Inference 과정 차이점 및 분석

FlashAttention 1, 2, 3은 Transformer의 Attention 연산에서 성능 병목을 제거하고 연산 효율성을 극대화하기 위해 설계되었습니다. 각 버전의 Inference 과정은 GPU의 병렬성 활용, 메모리 최적화, 하드웨어 특화 기법에서 차이를 보입니다. 아래에서 예시와 함께 각 버전의 Inference 과정을 자세히 설명하고, 장단점을 비교 분석합니다.

---

### 1. FlashAttention 1: IO-aware Attention

#### Inference 과정
- **핵심 아이디어**: 메모리 이동(IO)을 줄이기 위해 Attention 연산 전체를 단일 CUDA 커널로 병합.
1. \( S = QK^\top \): Query와 Key 간 유사도를 계산.
2. Row-wise Softmax:
   - \( P = \text{softmax}(S) \).
   - Softmax 계산 중간 값을 유지하여 메모리 액세스를 최소화.
3. \( O = PV \): Value와 가중치를 곱해 최종 출력 생성.

#### 예시
- 입력:
  - \( Q = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, K = \begin{bmatrix} 2 & 1 \\ 4 & 3 \end{bmatrix}, V = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \).
- 과정:
  - \( S = QK^\top = \begin{bmatrix} 4 & 10 \\ 8 & 22 \end{bmatrix} \).
  - \( P = \text{softmax}(S) \): 각 행에서 Softmax 적용.
  - \( O = PV \): \( P \)와 \( V \) 곱셈.

---

### 2. FlashAttention 2: Parallelism and Work Partitioning

#### Inference 과정
- **핵심 아이디어**: 시퀀스 길이와 연산 간 병렬성을 극대화.
1. **블록 단위 처리**:
   - \( Q, K, V \)를 GPU의 Shared Memory에 적합한 크기의 블록으로 나누어 처리.
2. \( QK^\top \) (GEMM) 연산:
   - 블록 단위로 \( S \) 계산.
3. Row-wise Softmax:
   - Softmax 계산을 블록 단위로 수행하여 중간 결과를 저장하지 않음.
4. \( PV \):
   - \( S \) 계산 결과와 \( V \)를 병렬로 곱하여 \( O \) 생성.

#### 예시
- 입력:
  - \( Q, K, V \)는 \( 2 \times 2 \) 블록으로 분할.
- 과정:
  - 첫 번째 블록 \( Q_1, K_1, V_1 \)에 대해:
    - \( S_1 = Q_1 K_1^\top \).
    - \( P_1 = \text{softmax}(S_1) \), \( O_1 = P_1 V_1 \).

---

### 3. FlashAttention 3: Asynchrony and Low Precision

#### Inference 과정
- **핵심 아이디어**: 비동기 처리와 FP8 연산으로 처리량을 극대화.
1. Warp Specialization:
   - GPU Warp를 Producer(데이터 로드)와 Consumer(연산)로 분리.
2. **Pingpong Scheduling**:
   - Softmax와 \( QK^\top \), \( PV \) 연산을 병렬로 수행.
3. Low-Precision 연산:
   - FP8을 활용하여 연산량 증가 및 메모리 절약.

#### 예시
- 입력:
  - 동일한 \( Q, K, V \) 행렬.
- 과정:
  - Warp 1: \( QK^\top \) 연산 수행.
  - Warp 2: 이전 블록 Softmax 수행.
  - FP8 변환: 블록 단위로 \( Q, K, V \)를 스케일링.

---

### 4. FlashAttention 1, 2, 3 비교 (장단점 테이블)

| **특징**          | **FlashAttention 1**                   | **FlashAttention 2**     | **FlashAttention 3**                                  |
| ----------------- | -------------------------------------- | ------------------------ | ----------------------------------------------------- |
| **병렬성**        | 제한적 (연산 순차 처리)                | 블록 단위 병렬 처리      | Warp 간 병렬 처리 (Pingpong Scheduling)               |
| **메모리 최적화** | Softmax 통계 유지로 메모리 이동 최소화 | 블록 단위 처리로 IO 감소 | 비동기 처리로 HBM-Shared Memory 이동 최적화           |
| **정확도**        | FP32 연산으로 높은 정확도              | FP16 지원                | FP8로 처리량 증가, Block Quantization으로 정확도 유지 |
| **하드웨어 활용** | Tensor Core 활용 제한                  | Tensor Core 활용         | Tensor Core 및 TMA 활용 극대화                        |
| **성능 개선**     | 기존 Attention 대비 2배                | 기존 대비 1.5~2배        | 기존 대비 1.5~2.0배, FP8에서 1.2 PFLOPs/s 달성        |
| **한계**          | 연산 병렬성 부족                       | 비동기 처리 미지원       | FP8 의존성, 특정 하드웨어 종속성                      |

---

### 5. 결론

FlashAttention 1, 2, 3는 점진적으로 병렬성과 비동기성을 강화하여 성능을 개선하였으며, FlashAttention-3는 **Pingpong Scheduling**과 **FP8 연산**을 활용해 가장 뛰어난 성능을 보여줍니다. 그러나 **FP8 의존성** 및 **하드웨어 특화 설계**는 멀티플랫폼 적용에 제약을 가하며, 이는 향후 연구에서 해결해야 할 과제로 보입니다.