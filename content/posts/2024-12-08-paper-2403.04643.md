---
categories:
- paper-review
- with-gpt
- LLM-Inference
date: "2024-12-08"
title: 'QAQ: Quality Adaptive Quantization for LLM KV Cache'
---

[논문 링크](https://arxiv.org/abs/2403.04643)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문 요약: **QAQ: Quality Adaptive Quantization for LLM KV Cache**

#### 주요 기여
1. **품질 적응형 양자화(Quality Adaptive Quantization)**: LLM의 Key-Value(KV) 캐시를 위한 새로운 양자화 방식 제안. 이는 GPU 메모리 사용량을 10배까지 줄이면서 성능 손실을 최소화함.
2. **이론적 분석 및 실험적 검증**:
   - Key와 Value 캐시는 양자화에 대해 서로 다른 민감도를 보임. 이에 따라 각기 다른 양자화 전략을 설계.
   - 기존 중요성 지속성 가설(Persistence of Importance)을 보완해, 예외적인 토큰 중요도 변화를 처리하는 방법 제안.
   - Outliers(이상치)의 중요성을 강조하고 이를 개별적으로 처리하여 성능 손실 최소화.
3. **우수한 성능**: 기존 방법 대비 1.6~1.8배 높은 압축률을 달성하면서 성능 손실을 거의 없는 수준으로 유지.

---

#### 강점과 독창성

1. **독창적 설계**
   - Key와 Value 캐시의 양자화 민감도를 구별해 설계한 것은 기존 연구와 차별화됨.
   - 주의(attention) 창(window)을 통해 중요도 변화 예외를 효율적으로 처리.

2. **성능 및 실용성**
   - GPU 메모리 절약 효과를 제공하면서도 LLM 성능 저하를 방지.
   - 특히, LLaMA 모델 계열(7B 및 13B)에 대한 실험에서 높은 압축률을 보여줌.

3. **적용 가능성**
   - 코드가 공개되어 있어, 실무에서의 적용 가능성이 큼.
   - Zero-shot 설정에서도 압축 효과가 유지됨을 보여, 다양한 NLP 태스크에 적합.

---

#### 핵심 알고리즘: QAQ의 동작 과정 (예제 입력 설명 포함)

1. **입력 데이터**:
   - 예: LLaMA 모델에서 생성된 Key와 Value 캐시 (행렬 형태).
   - Key: \(K \in \mathbb{R}^{T \times D}\), Value: \(V \in \mathbb{R}^{T \times D}\).

2. **양자화 과정**:
   - **Value 캐시**:
     - Value 캐시의 값은 주의(attention) 가중치에 비례하여 양자화.
     - 예: \( \sigma(V)_t = \frac{\sigma_{max}^{(X)}}{\sqrt{T} \cdot |S_t|} \), \( S_t \): Attention Score.
   - **Key 캐시**:
     - Key는 Value보다 민감하므로, 엄격한 양자화 기준 적용.
     - 예: \( \sigma(K)_t \leq \frac{1}{\sum Q^2} \cdot \log(\text{상수} + 1) \).

3. **Outlier 처리**:
   - Key와 Value에서 이상치 검출 (상위/하위 α% 값).
   - 이상치는 양자화하지 않고 고정된 고정밀도로 저장.

4. **주의 창(Window)**:
   - n개의 이전 주의 점수 중 최대값 사용해 미래 중요도를 예측.
   - 예: \( S_t = \max(S_{t-n}, \dots, S_{t-1}) \).

5. **결과**:
   - 양자화된 KV 캐시를 모델에 다시 통합하여 메모리 절약 효과 제공.
   - 압축률 예시: 원래 메모리의 10% 크기로 줄이면서 성능 유지.

---

#### 한계점

1. **추가 계산 비용**:
   - 주의 창 및 Outlier 처리는 추가적인 계산 및 메모리 전송 비용을 요구함.
   - 특히 CPU-GPU 간의 데이터 전송이 빈번할 경우 성능 저하 우려.

2. **일부 특수 상황 처리 부족**:
   - 드물게 발생하는 극단적 이상치(outlier)에 대해 더 정교한 처리가 필요할 수 있음.
   - 특정 모델 구조나 태스크에 최적화되지 않은 경우 성능 저하 가능.

3. **일반화 문제**:
   - 실험이 LLaMA 모델에 집중되어 있어, 다른 LLM 구조에서의 성능 보장이 부족.

4. **복잡성**:
   - 제안된 방법은 기존 양자화 기법보다 더 많은 하이퍼파라미터(예: attention window 크기, outlier 비율)를 요구.

---

#### 결론
QAQ는 LLM의 KV 캐시 문제를 해결하기 위한 매우 유망한 접근법으로, 특히 메모리 제약이 큰 환경에서 효율성을 극대화할 수 있음. 하지만 실용화를 위해 추가적인 최적화 및 다양한 모델에서의 검증이 요구됨.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### 핵심 알고리즘: QAQ의 동작 과정

QAQ는 LLM(Key-Value 캐시) 양자화에 특화된 알고리즘으로, Key와 Value 캐시를 분리하여 최적화된 양자화를 수행합니다. 아래는 알고리즘의 주요 단계와 예시를 통해 자세히 설명하겠습니다.

---

#### 1. **예제 상황 설정**
- 모델: LLaMA 2-7B.
- 입력: "The quick brown fox jumps over the lazy dog."
- 토큰 시퀀스: 9개의 토큰으로 구성됨.
- KV 캐시 크기: 
  - Key 캐시 (\( K \)): \( 9 \times 768 \).
  - Value 캐시 (\( V \)): \( 9 \times 768 \).

---

#### 2. **Value 캐시 양자화 과정**
Value 캐시는 Key보다 덜 민감하여, attention 가중치를 기반으로 양자화합니다.

##### **수식**
- Attention 가중치 \( S_t \)에 비례하여 값의 양자화 오차를 설정:
  \[
  \sigma(V)_t = \frac{\sigma_{max}^{(X)}}{\sqrt{T} \cdot |S_t|}
  \]
  여기서:
  - \( T \): 전체 토큰 수.
  - \( \sigma_{max}^{(X)} \): 양자화 손실 허용 한계.
  - \( S_t \): Attention 점수.

##### **예시**
- \( T = 9 \), \( \sigma_{max}^{(X)} = 0.01 \), Attention 점수 \( S_t = [0.3, 0.2, 0.15, \dots] \).
- 계산:
  \[
  \sigma(V)_t = \frac{0.01}{\sqrt{9} \cdot |S_t|}
  \]
  \( t = 1 \)일 때 \( \sigma(V)_1 \approx 0.037 \)로 양자화 허용 범위를 계산.

##### **양자화**
- Value 범위 (\( V_{min} \), \( V_{max} \))를 2진수로 나눔:
  \[
  B(V)_t = \lceil \log_2(\frac{V_{max} - V_{min}}{2\sqrt{3} \cdot \sigma(V)_t}) \rceil
  \]
- 범위를 좁힐수록 필요한 비트 수 감소.

---

#### 3. **Key 캐시 양자화 과정**
Key 캐시는 Attention 점수 계산에 민감하므로 더 정교한 처리가 필요합니다.

##### **수식**
- Attention 스코어 \( S_t \)가 손실되지 않도록 Key 값의 오차를 제어:
  \[
  \sigma(K)_t \leq \frac{1}{\sum_d Q_d^2} \cdot \log\left(\frac{T^3}{T-1} \cdot \sigma_{max}^{(S)} + 1\right)
  \]
  여기서:
  - \( Q_d \): Query 벡터 값.
  - \( \sigma_{max}^{(S)} \): Attention 스코어 허용 오차.

##### **예시**
- Query 값 (\( Q \))의 크기: \( \| Q \|^2 = 1000 \), \( T = 9 \), \( \sigma_{max}^{(S)} = 0.01 \).
- 계산:
  \[
  \sigma(K)_t \leq \frac{1}{1000} \cdot \log\left(\frac{9^3}{9-1} \cdot 0.01 + 1\right)
  \]
  \( \sigma(K)_t \approx 0.0013 \).

##### **양자화**
- Key 범위 (\( K_{min} \), \( K_{max} \))를 양자화:
  \[
  B(K)_t = \lceil \log_2(\frac{K_{max} - K_{min}}{2\sqrt{3} \cdot \sigma(K)_t}) \rceil
  \]

---

#### 4. **Outlier 처리**
Key와 Value에서 이상치를 검출하고 별도로 저장합니다.

##### **방법**
- 이상치 기준: 상위 \( \alpha\% \) 값.
- 예: Key 값 \( K \)에서 상위 1% 이상치 검출 후, 고정된 고정밀도로 저장.

---

#### 5. **Attention Window**
특정 토큰의 중요도가 시간에 따라 변할 수 있으므로, 주의 창(Window)을 사용해 예외 처리를 수행.

##### **동작**
- 이전 \( n \)개의 Attention 점수에서 최대값을 저장:
  \[
  S_t = \max(S_{t-n}, \dots, S_{t-1})
  \]
- 예: \( n = 3 \)일 때, \( S_{t-3}, S_{t-2}, S_{t-1} \) 중 가장 큰 값을 사용해 다음 단계의 양자화 결정.

---

#### 6. **결과 통합**
양자화된 KV 캐시를 텍스트 생성 프로세스에 다시 통합:
- 새로 생성된 KV 캐시를 CPU로 전송 및 저장.
- 필요한 경우 이전 양자화된 값을 재조정.

---

### 요약
- **Value 캐시**: Attention 가중치에 따라 양자화 강도 결정.
- **Key 캐시**: Query의 민감도 분석을 기반으로 정밀한 양자화 적용.
- **Outlier**: 중요 이상치를 고정밀도로 저장해 성능 손실 방지.
- **Attention Window**: 토큰 중요도 변화를 추적해 양자화 오류 감소.

이 알고리즘은 GPU 메모리 사용량을 10배까지 줄이면서, 성능 저하를 최소화하는 데 성공했습니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### 논문에서 제시한 결과와 다른 방법론 대비 특출난 점

#### **1. 주요 실험 결과**
QAQ는 LLaMA 2-7B 및 13B 모델에서 수행된 실험을 통해, Key-Value(KV) 캐시의 메모리 사용량을 최대 10배 줄이면서도 성능 저하를 최소화했다고 주장합니다.

- **압축률과 정확도**:
  - 최대 **10배 압축률**에서 성능 손실은 미미하거나 전혀 없었음.
  - Zero-shot 환경에서 HellaSwag, PIQA, MathQA 등의 태스크에서 SOTA(Scissorhands, H2O) 대비 **1.6~1.8배 향상된 압축 성능**을 보임.

- **SOTA와의 비교**:
  - Scissorhands: Attention 기반의 중요성 지속성 가설(Persistence of Importance)에 의존하나, 중요 토큰의 예외 처리 부족.
  - H2O: 토큰의 중요성을 기반으로 효율적인 KV 캐시 제거 수행. 그러나 압축률이 제한적(최대 5배).
  - QAQ: 더 정교한 양자화와 Outlier 처리로 압축률과 정확도를 동시에 극대화.

---

#### **2. QAQ의 특출난 점**
QAQ는 기존 방법론의 한계를 극복하며 아래의 세 가지 주요 설계를 통해 두드러진 성능을 보여줍니다.

1. **Key-Value 캐시의 민감도 구분**:
   - Key와 Value 캐시가 양자화에 대해 서로 다른 민감도를 보인다는 점을 발견하고, 이에 맞는 양자화 전략을 개별적으로 설계.
   - Key 캐시는 Attention 점수 계산에 민감하므로 더 높은 정밀도를 유지해야 하지만, Value 캐시는 상대적으로 덜 민감해 더 강한 압축 적용 가능.
   - 이러한 접근법은 압축률을 높이면서도 성능 손실을 최소화하는 데 기여.

2. **Outlier 처리**:
   - KV 캐시에서 발견되는 이상치(outliers)가 모델 성능에 미치는 영향을 강조.
   - 기존 방법은 이상치를 무시하거나 동일하게 처리해 성능 저하를 초래했으나, QAQ는 혼합 정밀도(mixed precision) 방식을 도입해 이상치를 별도로 저장하고, 나머지 값을 더 효율적으로 압축.
   - 실험적으로, 이상치 처리 시 정확도가 평균 12~26% 향상됨.

3. **주의 창(Attention Window)**:
   - 기존 중요성 지속성 가설(Persistence of Importance)의 예외를 처리.
   - 특정 토큰의 중요도가 시간에 따라 급격히 변할 수 있음을 발견하고, 이전 \( n \)개의 Attention 점수에서 최대값을 저장하여 미래 중요도를 예측.
   - 이를 통해, 중요도가 낮은 토큰을 지나치게 압축하거나 잘못 삭제하는 오류를 방지.

---

#### **3. QAQ의 성과를 뒷받침하는 방법론**
QAQ의 성공은 아래의 이론적 및 실험적 발견에 기반합니다.

1. **Key와 Value 캐시의 민감도 차이**:
   - Key는 Attention 점수 계산에 직접 관여하므로 더 민감함을 이론적 분석과 실험을 통해 검증.
   - Value는 결과 출력의 가중치로만 사용되므로 상대적으로 덜 민감.

2. **Outlier의 중요성**:
   - KV 캐시의 값이 정규 분포를 따르지 않으며, 극단값이 모델 성능에 미치는 영향을 강조.
   - Outlier를 별도로 처리함으로써 모델 성능을 안정적으로 유지.

3. **Attention Score의 예측 가능성**:
   - 대부분의 토큰은 중요성이 일정하게 유지되지만, 일부 토큰은 예외적으로 중요도가 변동.
   - Attention Window 기법으로 예외 상황을 예측하고 처리.

---

#### **4. 필자의 의견**
QAQ는 모델 메모리 사용 문제를 해결하기 위해 고도로 정교한 접근법을 제안하며, 아래 이유로 우수한 성과를 보였다고 생각합니다.

1. **실제 문제를 정확히 정의하고 해결**:
   - 기존 방법론은 Attention 기반 중요도 추정이나 단순 양자화에 초점을 맞추었으나, QAQ는 Key와 Value 캐시의 본질적인 특성을 구분하여 설계.

2. **설계의 유연성**:
   - Outlier 처리와 Attention Window 같은 기법은 다양한 시나리오에 적용 가능하며, 향후 더 긴 문맥의 모델에서도 효과를 발휘할 가능성이 높음.

3. **모델 효율성 향상**:
   - QAQ는 단순히 압축률을 높이는 것에 그치지 않고, 메모리와 성능의 균형을 맞춘 최적화 방법론을 제시.

4. **한계와 개선 가능성**:
   - CPU와 GPU 간의 빈번한 데이터 전송은 실시간 애플리케이션에서 성능 병목이 될 수 있음. 이를 해결하기 위한 추가 최적화가 필요.
   - 이상치(outliers)에 대한 처리 방식은 고정적이며, 더 동적인 접근법으로 개선될 여지가 있음.

QAQ는 메모리 효율성과 모델 성능 사이의 균형을 달성한 뛰어난 접근법이며, 향후 더 큰 LLM과 다양한 태스크에서의 확장이 기대됩니다.

# Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니? 



### QAQ 구현에 필요한 추가 작업 및 리소스 요구 사항 분석

QAQ는 기존 LLM(Key-Value 캐시 관리) 구현을 확장하여 다음과 같은 추가 구현이 필요합니다. 각 구현 요소별로 소모되는 공수와 연산 리소스를 분석합니다.

---

### 1. **Key-Value 캐시 민감도 기반 양자화**
#### **구현 추가점**
- Key와 Value 캐시를 구분하여 개별 민감도 분석.
- 민감도 계산(예: \( \sigma(V)_t, \sigma(K)_t \)) 및 적절한 양자화 비트 결정.
- 기존 KV 캐시 구조를 양자화 버전으로 변경.

#### **컴퓨팅 자원 요구**
1. **연산**:
   - \( \sigma(V)_t \): \( O(T) \) (전체 토큰 수 \( T \)).
   - \( \sigma(K)_t \): \( O(T \cdot D) \) (Query 벡터 크기 \( D \)).
   - 양자화 비트 결정: 각 토큰의 범위 계산에 \( O(T) \).
   - **추정**:
     - \( T = 1024 \), \( D = 768 \)일 경우, 연산량은 \( O(1024 \times 768) \approx 10^6 \) 연산.

2. **메모리**:
   - 양자화된 KV 캐시 저장 공간은 기존 대비 약 1/10 수준으로 감소.
   - 민감도 계산을 위한 중간 데이터는 토큰별로 추가 \( O(T) \) 메모리 요구.

---

### 2. **Outlier 검출 및 처리**
#### **구현 추가점**
- KV 캐시 값의 분포를 분석하고 이상치를 검출.
- 이상치를 고정밀도로 별도 저장(혼합 정밀도 사용).

#### **컴퓨팅 자원 요구**
1. **연산**:
   - 분포 계산: 각 캐시 값의 분포를 분석(예: 히스토그램 계산, \( O(T \cdot D) \)).
   - 이상치 검출: \( O(T \cdot D) \) (정렬 기반).
   - 이상치 저장: \( O(T \cdot D \cdot \alpha) \), \( \alpha \)는 이상치 비율 (보통 1% 미만).
   - **추정**:
     - \( T = 1024, D = 768, \alpha = 0.01 \)일 경우 이상치 연산량은 \( O(1024 \times 768 \times 0.01) \approx 10^4 \).

2. **메모리**:
   - 이상치를 고정밀도로 저장(32비트).
   - 압축되지 않은 이상치 데이터는 총 KV 캐시의 \( \alpha \% \) 추가 메모리 요구.

---

### 3. **Attention Window 구현**
#### **구현 추가점**
- 각 토큰의 과거 \( n \)개의 Attention 점수 추적.
- 주의 창에서 최대값을 사용하여 중요도 예측.

#### **컴퓨팅 자원 요구**
1. **연산**:
   - Attention 점수 저장 및 갱신: \( O(T \cdot n) \).
   - 최대값 계산: \( O(T \cdot n) \) (n은 창 크기, 일반적으로 5~10).
   - **추정**:
     - \( T = 1024, n = 5 \)일 경우, \( O(1024 \times 5) = 5120 \) 연산 추가.

2. **메모리**:
   - Attention 점수 저장: \( O(T \cdot n) \) 추가 메모리 필요.

---

### 4. **CPU-GPU 데이터 전송 관리**
#### **구현 추가점**
- CPU에 비양자화된 KV 캐시 저장.
- 양자화 비트 변경 시 CPU-GPU 간 데이터 전송.

#### **컴퓨팅 자원 요구**
1. **연산**:
   - 데이터 전송 크기: \( O(T \cdot D) \) (KV 캐시 크기).
   - 전송 빈도: 양자화 비트 재계산 빈도에 따라 결정.
   - **추정**:
     - \( T = 1024, D = 768 \), 전송 10회 시 \( 1024 \times 768 \times 10 \approx 8 \text{MB} \) 데이터 이동.

2. **병목**:
   - 데이터 전송 시간: GPU-CPU 간 대역폭(약 10GB/s)에서 데이터 크기 기반 계산.
   - **예시**:
     - 8MB 데이터 전송 시, \( \frac{8}{10} \approx 0.8 \text{ms} \).

---

### 5. **종합 공수 및 리소스 분석**
#### **구현 공수**
- 민감도 계산 및 양자화 구현: 약 2~3주.
- Outlier 처리: 1~2주.
- Attention Window: 1주.
- CPU-GPU 데이터 관리: 2~3주.
- **총 공수**: 약 6~9주.

#### **컴퓨팅 자원 총합**
1. **추가 연산**:
   - 총 \( O(T \cdot D) + O(T \cdot n) + O(T) \approx O(T \cdot D) \) 연산량.
   - LLaMA 2-7B 모델(\( T = 1024, D = 768 \)) 기준 약 \( 10^6 \) 연산 추가.

2. **메모리**:
   - Attention Window, 이상치 저장 등으로 \( T \cdot (n + \alpha \cdot D) \) 추가 메모리.
   - 압축으로 줄어든 메모리의 일부만 재사용.

3. **실시간 병목 가능성**:
   - CPU-GPU 전송은 실시간 애플리케이션에서 병목 가능성 존재.

---

### 결론
QAQ 구현은 기존 KV 캐시 양자화에 비해 약간의 추가 연산 및 메모리 요구가 있지만, GPU 메모리 사용량을 10배 감소시키면서 성능 손실을 최소화합니다.  
**병목 우려가 있는 CPU-GPU 전송 문제를 해결하기 위해 전송 빈도 최적화 또는 GPU 내 양자화를 통한 연산 병렬화가 권장됩니다.**

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### 논문의 입력 데이터 및 추론 과정

#### **1. 입력 데이터**
- **입력 형식**: 자연어 문장.
  - 예: *"The quick brown fox jumps over the lazy dog."*
  - 토큰화 후, 시퀀스 \( T = 9 \)개의 토큰으로 변환.

- **토큰 임베딩**:
  - 각 토큰은 모델의 입력으로 변환되며, 크기 \( D = 768 \) (LLaMA 2-7B 기준)의 벡터로 매핑됨.

---

#### **2. 추론 과정**
추론 과정은 입력 데이터를 처리하여 다음 토큰을 생성하거나 시퀀스를 이해하는 과정입니다. 

##### **단계별 설명**
1. **KV 캐시 초기화**:
   - \( T \)개의 토큰 입력에 대해 Key 벡터 \( K \)와 Value 벡터 \( V \)가 생성됨.
   - \( K \in \mathbb{R}^{T \times D} \), \( V \in \mathbb{R}^{T \times D} \).

2. **주의(attention) 계산**:
   - Query \( Q \in \mathbb{R}^{1 \times D} \)를 입력받아 Key와 Value에 대한 주의 가중치 \( S \)를 계산:
     \[
     S = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{D}}\right)
     \]
     - \( Q K^\top \in \mathbb{R}^{1 \times T} \): 각 토큰의 중요도를 나타냄.
     - Softmax로 가중치를 정규화.

3. **출력 계산**:
   - 최종 출력 \( X \in \mathbb{R}^{1 \times D} \)는 Value 벡터와 주의 가중치의 가중 합:
     \[
     X = S V
     \]
   - 이를 기반으로 다음 토큰 또는 태스크의 출력 생성.

4. **양자화 적용**:
   - **QAQ 알고리즘 적용**:
     - Key와 Value 캐시를 양자화하여 메모리 사용량 감소.
     - 민감도 기반으로 비트수를 조정하고, 이상치는 별도로 처리.

5. **새 토큰 추가**:
   - 새로 생성된 토큰의 KV 벡터를 기존 KV 캐시에 추가:
     - \( K \to [K; K_{\text{new}}] \), \( V \to [V; V_{\text{new}}] \).

---

#### 프롬프트모델 아키텍처 구성

#### **1. Transformer 구조**
QAQ는 Transformer 기반 모델(LLaMA 2)을 대상으로 설계되었으며, 주요 아키텍처는 다음과 같습니다.

1. **임베딩 층**:
   - 입력 토큰을 \( D \)-차원 벡터로 변환.
   - 크기: \( \text{InputSize} \times D \).

2. **Self-Attention 층**:
   - Query, Key, Value 계산:
     \[
     Q = X W_Q, \, K = X W_K, \, V = X W_V
     \]
     - \( W_Q, W_K, W_V \): 선형 변환 가중치, 크기 \( D \times D \).
   - Attention 가중치 계산:
     \[
     S = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{D}}\right)
     \]
   - 최종 출력:
     \[
     X_{\text{out}} = S V W_O
     \]
     - \( W_O \): 선형 변환 가중치.

3. **Feed-Forward Network (FFN)**:
   - 활성화 함수와 함께 추가 변환:
     \[
     FFN(X) = \text{ReLU}(X W_1 + b_1) W_2 + b_2
     \]
     - \( W_1 \in \mathbb{R}^{D \times 4D} \), \( W_2 \in \mathbb{R}^{4D \times D} \).

4. **Layer Normalization 및 잔차 연결**:
   - 각 블록에서 LayerNorm과 Residual 연결을 통해 안정적인 학습 보장.

5. **출력 층**:
   - 최종 출력 확률 계산:
     \[
     P(\text{토큰}|입력) = \text{Softmax}(X_{\text{out}})
     \]

---

### 연산 및 메모리 요구량 분석

#### **1. 주요 파라미터**
- 모델 크기: LLaMA 2-7B.
  - Layer 수: 32.
  - Hidden Dimension: \( D = 768 \).
  - Attention Head: \( h = 12 \) (헤드당 크기: \( \frac{D}{h} = 64 \)).
  - Sequence 길이: \( T = 1024 \).

#### **2. 연산량 분석**
- **Self-Attention**:
  1. Query-Key 내적: \( T \times D \) 크기의 행렬 곱:
     \[
     O_{\text{QK}} = O(T \cdot D \cdot h) = O(1024 \cdot 768) \approx 10^6
     \]
  2. Softmax 및 Scaling: \( O(T \cdot T) \).
  3. Value 곱: \( O(T \cdot D) \).

- **Feed-Forward Network**:
  - \( W_1 \), \( W_2 \): \( D \times 4D \)와 \( 4D \times D \) 행렬 곱 수행.
  - 총 연산량:
    \[
    O_{\text{FFN}} = O(T \cdot D^2) = O(1024 \cdot 768^2) \approx 10^8
    \]

- **전체 연산량 (레이어당)**:
  - Self-Attention + FFN: \( O(T \cdot D^2) \).

#### **3. 메모리 요구량**
- **KV 캐시**:
  - Key/Value 크기: \( 2 \times T \times D \).
  - \( T = 1024, D = 768 \) 기준:
    \[
    \text{메모리} = 2 \cdot 1024 \cdot 768 \cdot 4 \text{(float32)} \approx 6 \text{MB}
    \]
  - 96 레이어의 LLaMA 2-7B에서 KV 캐시 전체 크기:
    \[
    6 \text{MB} \times 96 = 576 \text{MB}
    \]

#### **4. QAQ 적용 시 요구량**
- **압축 후 메모리**:
  - 10배 압축률 적용 시 \( \frac{576}{10} \approx 57.6 \text{MB} \).
- **추가 연산**:
  - KV 민감도 계산: \( O(T \cdot D) \approx 10^6 \) 연산.
  - 이상치 처리: 약 \( O(T \cdot \alpha D) \approx 10^4 \) 연산.

---

### 결론
- **추론 과정**: 기존 Transformer의 Self-Attention과 FFN 단계를 거쳐 KV 캐시를 생성하고 재사용.
- **연산 및 메모리**: LLaMA 2-7B 기준, 한 레이어의 Self-Attention과 FFN에서 약 \( O(10^8) \) 연산이 발생하며, 메모리는 \( 576 \text{MB} \)가 요구됨.
- **QAQ의 효과**:
  - 메모리 사용량은 10배 감소(약 57.6MB).
  - 추가 연산량은 기존 연산 대비 미미함. 
  - KV 캐시 관리 최적화로 실시간 추론에서 큰 이점을 제공.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### 논문의 한계와 이를 극복하기 위한 연구 흐름

이 논문은 QAQ(Quality Adaptive Quantization) 알고리즘을 통해 대규모 언어 모델(LLM)의 Key-Value(KV) 캐시 메모리 사용량을 줄이고 성능 손실을 최소화하는 데 성공했습니다. 하지만 몇 가지 한계점이 존재하며, 이를 해결하기 위한 잠재적인 연구 흐름을 정리해 보겠습니다.

---

### **1. 한계점**
#### **(1) CPU-GPU 간 데이터 전송 병목**
- KV 캐시를 CPU에 저장하고 필요할 때 GPU로 전송하는 방식은 데이터 전송 대역폭에 의존하며, 실시간 추론에서 병목현상이 발생할 가능성이 있음.

#### **(2) 이상치(outlier) 처리의 고정적 접근**
- 이상치를 고정된 비율(예: 상위 1%)로 처리하므로, 실제 이상치 분포가 변화하는 시나리오에서 최적 성능을 보장하기 어려움.

#### **(3) Attention Window 크기 설정의 제한**
- Attention Window 크기 \( n \)이 고정되어 있어, 동적으로 변화하는 중요도 패턴에 대응하기 어려움.
- 적절한 \( n \) 값은 태스크와 모델 크기에 따라 달라지며, 이를 최적화하기 위한 추가적인 작업이 필요.

#### **(4) 메모리와 연산 최적화의 균형 부족**
- 양자화를 통해 메모리 사용량을 줄였지만, 민감도 계산 및 이상치 처리를 위한 추가 연산이 필요.
- 연산량이 증가하면 대규모 실시간 추론의 처리 효율이 저하될 가능성이 있음.

#### **(5) 모델 일반화 부족**
- QAQ는 특정 LLM(LLaMA 2) 모델 계열에 대해 검증되었으나, 다른 구조나 태스크에 대한 일반화는 제한적.

---

### **2. 한계를 극복하기 위한 연구 흐름**
#### **(1) 데이터 전송 병목 해결: GPU 내 양자화**
- **문제**: CPU와 GPU 간의 데이터 전송이 병목을 초래.
- **해결 방향**:
  - **GPU 내 양자화 구현**:
    - 양자화 및 민감도 계산을 GPU에서 수행하여 데이터 전송을 최소화.
    - GPU 메모리 내에서 비트 변환 및 이상치 처리를 위한 효율적인 알고리즘 설계.
  - **연구 흐름**:
    - GPU 내부 메모리 관리 최적화 연구.
    - CUDA 기반 병렬 처리를 활용한 실시간 양자화 구현.

#### **(2) 이상치 처리를 위한 동적 전략**
- **문제**: 고정된 이상치 기준이 변화하는 분포에 적응하지 못함.
- **해결 방향**:
  - **동적 이상치 감지**:
    - 이상치 비율을 고정하지 않고, 현재 KV 캐시 값의 통계적 분포(예: 평균, 분산)를 동적으로 분석.
    - 특정 임계값(예: 3-시그마 법칙)을 기반으로 이상치를 실시간으로 감지.
  - **연구 흐름**:
    - 분포 기반 동적 이상치 탐지 알고리즘 연구.
    - KV 캐시에서 이상치 처리가 모델 성능에 미치는 영향에 대한 실험적 검증.

#### **(3) Attention Window 최적화**
- **문제**: Attention Window 크기가 고정되어 다양한 패턴을 반영하지 못함.
- **해결 방향**:
  - **적응형 Attention Window**:
    - 모델이 입력 시퀀스의 중요도 변화 패턴을 학습하여, 동적으로 Window 크기를 조정.
    - 예: 중요도가 급격히 변동하는 경우 Window 크기를 늘리고, 안정적인 경우 줄임.
  - **연구 흐름**:
    - 강화학습 기반 Window 크기 최적화.
    - Attention 가중치 변화 패턴 분석을 통해 자동 조정 메커니즘 개발.

#### **(4) 연산 최적화와 메모리 사용 간 균형**
- **문제**: 양자화 민감도 계산이 추가 연산을 요구하여 실시간 애플리케이션에 부담.
- **해결 방향**:
  - **근사 민감도 계산**:
    - 민감도 계산을 단순화하여 근사적인 값을 사용.
    - 예: 토큰 중요도(Attention 가중치)를 이전 값의 이동 평균으로 근사.
  - **연구 흐름**:
    - 근사 민감도 계산 알고리즘 연구.
    - 토큰 간 상관관계를 활용한 민감도 추정.

#### **(5) 모델 일반화 연구**
- **문제**: LLaMA 모델에 특화되어 다른 모델 구조에서 성능 보장이 어려움.
- **해결 방향**:
  - **다양한 모델 및 태스크에 대한 검증**:
    - GPT, OPT, BLOOM 등 다양한 LLM에 QAQ를 적용하고 성능을 평가.
    - 다른 NLP 태스크(기계 번역, 문서 요약 등)에서의 성능 실험.
  - **연구 흐름**:
    - 다양한 Transformer 모델에 QAQ 적용 및 성능 비교.
    - 태스크 특화 양자화 기법 연구.

#### **(6) 압축 외의 추가 최적화**
- **문제**: QAQ는 메모리 압축에 중점을 두었으나, 다른 최적화 기법과 병합이 제한적.
- **해결 방향**:
  - **다중 최적화 기법 통합**:
    - QAQ와 Pruning, Distillation, Sparse Attention 등의 기법을 결합하여 전반적인 효율성 증대.
    - 예: Sparse Attention을 사용하여 계산량을 줄이고, QAQ로 메모리 최적화.
  - **연구 흐름**:
    - 다중 최적화 기법의 상호작용 분석.
    - 메모리와 계산 효율성을 동시에 개선할 수 있는 통합 알고리즘 설계.

---

### **3. 결론**
QAQ는 LLM의 메모리 사용량 문제를 해결하기 위한 강력한 방법론이지만, 다음과 같은 연구 흐름이 이를 보완하거나 확장할 수 있습니다:
1. **GPU 내 연산으로 병목 제거**.
2. **이상치에 적응하는 동적 탐지**.
3. **Attention Window의 동적 최적화**.
4. **근사 민감도 계산을 통한 연산 부담 감소**.
5. **다양한 모델과 태스크에서의 일반화 연구**.
6. **다중 최적화 기법의 통합**.

이러한 연구 방향은 QAQ의 실용성을 높이고, 실시간 애플리케이션과 다양한 모델에 적용 가능성을 확대하는 데 기여할 것입니다.
