---
categories:
- paper-review
- with-gpt
date: "2025-07-09"
tags:
- 2402.17762v2
- Transformer
- SelfAttention
- BiasMechanism
- RepresentationLearning
- Interpretability
- NeuralMechanisms
- Massive Activations
- Explicit Attention Bias
title: '[Paper Review] Massive Activations in Large Language Models'
showRelated: true
relatedPostsCount: 3
katex:
  enable: true
mermaid: true
cover: https://eric-mingjie.github.io/massive-activations/assets/main_teaser_final.png
---

[Paper Link](https://arxiv.org/abs/2402.17762v2)

# Massive Activations, Hidden Biases: A Reinterpretation of Self-Attentionâ€™s Secrets

---

## TL;DR

Just **4â€“10 extreme scalar values (Ã—10,000)** out of tens of millions can single-handedly uphold the performance of LLMs and ViTs.  
These scalars act as **input-invariant, constant self-attention biases**â€”zeroing them causes immediate model collapse (PPL â†’ âˆ).  
The authors replace these implicit phenomena with an **Explicit Attention Bias (EAB)** module, fully **restoring performance** while enabling better analysis, compression, and safety.

---

## Core Ideas

1. **Discovery of Massive Activations (MA)**  
   * A few scalars in hidden states reach values up to **10â´ times** larger than the median.
2. **Self-Attention Bias Hypothesis**  
   * MAs can be interpreted as **constant biases inserted into key/value vectors**.
3. **Replacement Experiments**  
   * Zeroing MAs â†’ catastrophic performance drop; replacing with **mean values** â†’ 100% recovery.
4. **Explicit Attention Bias (EAB)**  
   * Appends `(kâ€², vâ€²)` directly to attention heads as learnable constants.  
   * Re-trained GPT-2 shows **no MAs at all**, while matching original performance.

---

## Background â€“ The Problem They Solve

Previous works observed phenomena such as â€œoutlier featuresâ€ and â€œattention sinks,â€ but failed to answer:

* **Why** do a few scalars grow so large?
* **How** do they influence the whole modelâ€™s behavior?
* **What** could replace them safely?

As a result, efforts in quantization and interpretability were hindered by unexplained tail-value behaviors.  
This paper bridges the gap by reinterpreting **Massive Activations as constant attention biases**.

---

## A New Approach â€“ **Explicit Attention Bias (EAB)**

$$
\text{Attention}\!\bigl(Q,K,V;\,k',v'\bigr)
  = \operatorname{softmax}\!\left(
        \frac{Q\,[\,K^{\top}\;{k'}^{\top}\,]}{\sqrt{d}}
    \right)
    \,[\,V;\,{v'}^{\top}]
$$

* **`kâ€², vâ€² âˆˆ â„áµˆ`** are learnable vectors, invariant to input.
* All tokens now compete with `(kâ€², vâ€²)` during softmax, enabling the model to achieve the same â€œfocusâ€ effectâ€”without relying on MA.

This single line is the secret weapon of the paper.

---

## How It Works â€“ Step-by-Step with an Example

| Token | Hidden vector `h` (d = 4) | Description |
| ----- | ------------------------- | ----------- |
| `<s>` | **\[50, 0.1, -0.2, 0]**   | **MA = 50** |
| â€œcatâ€ | \[0.2, 0.1, -0.1, 0]      | Normal      |
| â€œ.â€   | **\[45, 0, 0, -0.1]**     | **MA â‰ˆ 45** |

1. **Detection** â€“ Scalars satisfying `|h| â‰¥ 100 âˆ§ |h|/median â‰¥ 10Â³` are flagged as MA. Two activations detected here.
2. **Attention Dynamics** â€“ Assuming Q and K copy only the first dim, softmax logits heavily favor `<s>` and â€œ.â€ â†’ simulates a bias.
3. **Intervention**  
   * Setting MA = 0 â†’ logit drops sharply â†’ next layer input collapses.
   * Setting MA = mean (â‰ˆ 47.5) â†’ logits preserved â†’ performance intact.
4. **EAB** â€“ Use `kâ€² = 47.5`, train `vâ€²` â†’ same attention pattern restored, with no MA needed.

---

## Performance Evaluation â€“ Key Results

| Model / Setting        | MA State | **PPL â†“ / Top-1 â†‘** | Notes                     |
| ---------------------- | -------- | ------------------- | ------------------------- |
| **LLaMA2-7B** Original | Present  | **5.47**            | Baseline                  |
| MA = 0                 | Removed  | **âˆ**               | Collapsed performance     |
| **MA = mean**          | Replaced | **5.47**            | **Fully restored**        |
| **GPT-2 + EAB**        | None     | **3.04** (baseline) | MA = 0, retraining needed |
| **CLIP ViT-L**         | Present  | **75.5 %**          | ImageNet classification   |
| MA = 0                 | Removed  | 59.8 %              | -15.7 p accuracy drop     |
| MA = mean              | Replaced | **75.5 %**          | Fully recovered           |

* **Generality** â€“ Same phenomenon observed across 20+ LLMs and 12+ ViTs.
* **Replaceability** â€“ Using mean substitution or EAB fully preserves performance.


## Our Take â€“ Strengths, Limitations, and Why It Matters

### Strengths ğŸŒŸ

1. **Interpretability** â€“ Offers a unified explanation for previously puzzling behaviors like attention sinks and outlier features.
2. **Stability Potential** â€“ Suggests that removing MA may resolve long-tail activation issues in quantization and model stability.
3. **Simplicity** â€“ EAB requires just two extra vectors per headâ€”simple yet powerful.
4. **Security Implications** â€“ The idea that just 4 scalars can crash a model opens a new avenue for security research.

### Limitations âš ï¸

* **Retraining Required** â€“ EAB currently needs full model retraining.
* **No Performance Gain** â€“ EAB doesnâ€™t improve SOTA; it matches it.
* **Origin Unknown** â€“ Why MAs explode in layers 2â€“4 is still a mystery.
* **Unverified in Low Precision / Multimodal Models** â€“ No evidence yet for INT4 LLMs or audio transformers.

### Why This Matters ğŸ”‘

> More than boosting raw benchmarks, this paper introduces a **modular bias design paradigm** that advances interpretability, compression, and safetyâ€”all at once.


## Next Steps â€“ Where Do We Go From Here?

1. **Plug-in EAB** â€“ Introduce `(kâ€², vâ€²)` via LoRA-style fine-tuning, avoiding full retraining.
2. **Model the Origins of MA** â€“ Theorize how initialization, data statistics, or LayerNorm lead to MA formation.
3. **Low-Precision and Hardware Experiments** â€“ Evaluate whether MA-free models save power or improve latency on INT4/FP8 accelerators.
4. **Multimodal Extension** â€“ Investigate whether similar patterns exist in audio LLMs or video ViTs.
5. **Security Research** â€“ Study MA perturbations as a potential attack or defense mechanism.

The era of **â€œBias-as-a-Moduleâ€** might soon become standard in LLM designâ€”and this paper stitches its first thread.



<details markdown="block">
<summary>â–¶ï¸ <strong>Click to expand for detailed Q&A on the paper</strong></summary>


## Prompt 1.1.1 â€“ Research Gap

> â€œAnalyze the 'Introduction' and 'Related Work' sections of the paper to identify the specific research gap, limitations of prior work, or unresolved questions that this study explicitly addresses. Also summarize the state-of-the-art (SOTA) as described at the time of publication.â€

### One-Line Summary

This study identifies that a few (typically <10) extreme scalar valuesâ€”up to 10,000Ã— larger than the medianâ€”appear in early LLM layers and act as **input-invariant fixed biases**, critically steering self-attention toward specific tokens. Prior works on outlier features and attention sinks observed the symptoms but failed to explain or replace them. This paper fills that gap.


### 1. Defining the Research Gap

| Target Phenomenon           | Unit                  | Frequency           | Magnitude (max)     | Positional Pattern                                | Limitation of Prior Work              |
| --------------------------- | --------------------- | ------------------- | ------------------- | ------------------------------------------------- | ------------------------------------- |
| **Massive Activation (MA)** | Scalar (1 activation) | â‰¤10 out of millions | 15,000 (LLaMA2-70B) | Confined to a few feature dims and special tokens | Never previously reported             |
| Outlier Feature (Dettmers)  | Vector (full feature) | Dozens per layer    | <1,000              | Spread across many tokens                         | Causal role unclear; no bias modeling |

#### Core Gaps

1. **Unknown Mechanism** â€“ Prior research observed anomalies like outlier features or LayerNorm scale spikes but couldnâ€™t explain how just a few scalars influence model behavior.
2. **Unexplained Attention Sink** â€“ Start-token attention dominance was documented but lacked causal explanation.
3. **Quantization and Interpretability Limits** â€“ Extreme tails in activation distributions hindered model compression and analysis, yet lacked a clear origin.

This paper addresses all three by reinterpreting Massive Activations as **constant self-attention biases** and verifying it through controlled interventions (e.g., setting just 4 values to zero makes LLaMA2-7B PPL â†’ âˆ).


### 2. State-of-the-Art at Time of Publication

| Category                      | Representative Works        | Limitations                                             |
| ----------------------------- | --------------------------- | ------------------------------------------------------- |
| **Outlier Feature**           | Dettmers (2022), Lin (2023) | Only discuss vectors; do not explain sparse scalars     |
| **Attention Sink**            | Xiao (2023)                 | Visualizations exist, but no causal mechanism           |
| **LayerNorm Weight Outliers** | Kovaleva (2021)             | Focused on weights; does not explain hidden activations |
| **Register Token in ViT**     | Darcet (2023)               | Proposed performance gains; no link to MA or biases     |

In short, SOTA in early 2024 acknowledged that â€œlarge features existâ€ but couldnâ€™t address the **why, where, and how much they matter**. This paper provides empirical answers across 20+ LLMs and 12+ ViTs using precise numbers (e.g., Mixtral-8Ã—7B: max 7,100 vs. median 0.3), positioning MAs as both a mechanism and bottleneck in modern models.


### Quantitative Highlights

* **Ratio (max/median)**: Up to 10,000Ã— (e.g., LLaMA2-7B: 2,622 vs. 0.2)
* **Frequency**: ~4 MAs out of 40,000 activations â†’ <0.01%
* **Impact**: Zeroing 4 scalars â†’ 69% accuracy â†’ 37% (â†“32 pts)
* **Layer Locations**: Explodes in layers 2â€“4, then stabilizes or vanishes
* **ViT**: CLIP ViT-L also shows 200+ MAs functioning as biases

> **In sum**: The paper fills a major empirical gap by demonstrating that MAs act as **rare but decisive biases** across LLMs and ViTs, answering questions that outlier/sink studies left openâ€”this has deep implications for interpretability, quantization, and model safety.


## Prompt 1.1.2 â€“ Central Hypothesis

> â€œWhat is the central hypothesis or core claim of this paper? Express it in a clear, single sentence, ideally in the form: â€˜The authors hypothesize that by using [method], they can overcome [limitation] and achieve [result].â€™â€

**Central Hypothesis**  
The authors hypothesize that by interpreting sparse Massive Activations as explicit, constant self-attention biases and replacing them with learnable parameters `(kâ€², vâ€²)`, they can eliminate mysterious behaviors like attention sinks and outlier features, and **fully restore model performance** (e.g., in LLaMA2-7B where setting just 4 scalars to zero collapses PPL to âˆ, but mean replacement yields full recovery).


## Prompt 1.2.1 â€“ Key Contributions

> â€œBased on the entire paper, list the top 1â€“3 most important and original contributions. For each, indicate whether it represents a new architecture component, training method, theoretical insight, dataset, or novel application of existing methods.â€

**One-line summary**  
This paper discovers that a few extremely large scalar activations (**Massive Activations**)â€”up to 10,000Ã— larger than the medianâ€”function as implicit attention biases. The authors replace them with explicit, learnable parameters and reinterpret them as a design element that exists across LLMs and ViTs.

| #   | Contribution                                                                                                                      | Category                                         | Representative Evidence                              |
|-----|------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|------------------------------------------------------|
| 1ï¸âƒ£ | First empirical measurement of **Massive Activations** (â‰¤10 per layer, max/median â‰ˆ 10â´:1) across 20+ LLMs and ViTs               | **New theoretical insight**                      | LLaMA2-7B max = 2,556, median = 0.2                   |
| 2ï¸âƒ£ | Verified that MAs function as **input-invariant self-attention biases** and proposed **explicit (kâ€², vâ€²)** to replace them        | **New architectural component + training method**| Zeroing MA â†’ PPL âˆ; Mean/EAB â†’ identical performance |
| 3ï¸âƒ£ | Extended analysis to CLIP, DINOv2, and ViT-G, reinterpreting **register tokens as bias carriers**                                 | **New interpretation of existing methods**       | CLIP ViT-L: Top-1 drop of -15.7 p if MAs are removed |

These are the paperâ€™s key original contributions, each offering a new design, theory, or generalization.


## Prompt 1.2.2 â€“ Strengths from the Authorsâ€™ View

> â€œFrom the authorsâ€™ perspective, what makes their approach superior to previous ones? Include any core arguments they use to support the novelty or effectiveness of their method.â€

**One-line summary**  
The authors argue that reinterpreting Massive Activations as **constant attention biases**â€”then replacing them with `(kâ€², vâ€²)`â€”enables them to fully preserve performance while eliminating mysterious behaviors. This offers a simple, generalizable, and interpretable solution across 30+ models.


### 5 Key Strengths Claimed by the Authors

1. **Critical Performance Role**

   * In LLaMA2-7B, setting just 4 MAs to zero causes:  
     â†’ WikiText PPL 5.47 â†’ âˆ  
     â†’ Zero-shot accuracy 68.9% â†’ 36.8%
   * Same test with outlier features or median-scaled values does not affect performance, proving **MAs are uniquely decisive**.

2. **Cross-Model, Cross-Domain Generality**

   * Same phenomenon found in **24 LLMs and 12 ViTs**, including LLaMA, Mixtral, OPT, GPT-2, and CLIP/DINOv2.
   * Unlike attention sink studies (limited to start tokens), MAs follow consistent patterns across token types and layers.

3. **Explicit Design Makes the Implicit Observable**

   * Appending `(kâ€², vâ€²)` to attention layers in GPT-2 **completely eliminates MA formation** during training.
   * Model achieves the same PPL, indicating that performance does not depend on implicit MAs.

4. **Practical Extension to Vision Models**

   * In CLIP ViT-L, zeroing 2 MAs â†’ Top-1 accuracy drops from 75.5% to 59.8%.  
     Replacing them with mean values â†’ Accuracy fully restored.
   * Reinterprets ViT register tokens not as â€œinformation aggregatorsâ€ but as **bias containers**.

5. **Quantization and Stability Benefits**

   * MAs are extremely large (e.g., 10,000Ã— median), but have **coefficient of variation (CV) â‰ˆ 0.06** â†’ almost constant.
   * Because they are input-invariant, they cause long-tail distributionsâ€”mean replacement flattens this, helping compression.



### Key Comparison (Intervention Results)

| Model         | Modification | PPL     | Zero-shot / Top-1 | Effect      |
|---------------|--------------|---------|--------------------|-------------|
| LLaMA2-7B     | Original     | 5.47    | 68.9%              | Baseline    |
|               | MA = 0       | âˆ       | 36.8%              | **Collapse**|
|               | MA = mean    | 5.47    | 68.9%              | No change   |
| CLIP ViT-L    | Original     | â€”       | 75.5%              | Baseline    |
|               | MA = 0       | â€”       | 59.8%              | -15.7 pts   |
|               | MA = mean    | â€”       | 75.5%              | Fully restored |


> **In summary**  
> The authors claim superiority based on â€œmechanical clarityâ€ (MAs can be isolated, verified, and replaced) and â€œempirical impactâ€ (works across domains, removes mysterious behaviors). No other method achieves both MA removal and zero performance loss.



## Prompt 1.3.1 â€“ Step-by-Step Algorithm Explanation

> â€œExplain the core algorithm, architecture, or methodology step-by-step, assuming a graduate-level AI audience. Use simple toy examples (e.g., 3Ã—3 image, small vectors), and define all key variables on first use. Show how input transforms into output through each stage.â€


### Summary First â€“ Core Algorithm at a Glance

1. **Detect** â€“ Identify scalars in hidden states whose magnitude is â‰¥100 and â‰¥1,000Ã— larger than the median â†’ call them **Massive Activations (MAs)**.
2. **Verify** â€“ Set MAs to 0 â†’ performance collapses; replace with mean â†’ full recovery. Shows MAs act as **implicit self-attention biases**.
3. **Replace** â€“ Add explicit key/value bias vectors `(kâ€², vâ€²)` to self-attention â†’ retrain â†’ same performance, no MAs.


### 1. Step-by-Step Algorithm Description

#### Step 0 â€“ Notation & Variables

| Symbol             | Meaning                              | Shape               |
|--------------------|---------------------------------------|---------------------|
| *T*                | Sequence length (tokens)              | â€”                   |
| *d*                | Embedding / feature dimension         | â€”                   |
| *hâ‚—* âˆˆ â„<sup>TÃ—d</sup> | Hidden states at layer â„“              | â€”                   |
| *MA*               | Massive Activation                    | â€”                   |
| *W<sub>q/k/v</sub>*| Projection matrices for Q/K/V         | â„<sup>dÃ—d</sup>     |
| *Q, K, V*          | Query, key, value matrices            | â„<sup>TÃ—d</sup>     |
| *kâ€², vâ€²*           | Learnable bias vectors (proposed)     | â„<sup>d</sup>       |


#### Step 1 â€“ Detect MAs

1. Compute `median = median(|hâ‚—|)` after LayerNorm or RMSNorm.
2. Identify MAs:

```
MA = {(i,j) | |hâ‚—\[i,j]| â‰¥ 100 and |hâ‚—\[i,j]| / median â‰¥ 1,000}
```

e.g., in LLaMA2-7B: â‰¤4 out of ~40,000 values pass this test.


#### Step 2 â€“ Trace the Attention Path

1. Compute:
`Q = hâ„“ Wq`, `K = hâ„“ Wk`, `V = hâ„“ Wv`.
2. For tokens with MAs (set C), attention output becomes:
   ```math
   Attention(Q,K,V)k = Î£_{iâˆˆC} p_{k,i} Â· v_i  +  Î£_{iâˆ‰C} â€¦      (Eq 2)
   ```

â†’ The first term dominates and acts as a **fixed bias** for all k.


#### Step 3 â€“ Intervention Experiments

* Set MA = 0 â†’ LLaMA2-7B collapses (PPL â†’ âˆ; acc â†“32 pts)
* Replace MA with mean â†’ performance identical to original.


#### Step 4 â€“ Inject Explicit Bias (EAB)

Replace attention function with:

```math
Attention(Q,K,V;kâ€²,vâ€²) = softmax([Q]Â·[Káµ€â€†kâ€²] / âˆšd) Â· [V; vâ€²áµ€]   (Eq 3)
```

Training with this:

* Keeps PPL constant (e.g., GPT-2: 3.04)
* Prevents MAs from forming entirely


### 2. Toy Example (Text, d = 4, T = 3)

| Token | Hidden vector `h[i]`      | Description |
| ----- | ------------------------- | ----------- |
| `<s>` | **\[50, 0.1, -0.2, 0.0]** | MA = 50     |
| â€œcatâ€ | \[0.2, 0.1, -0.1, 0.0]    | Normal      |
| â€œ.â€   | **\[45, 0.0, 0.0, -0.1]** | MA â‰ˆ 45     |

1. Median â‰ˆ 0.1 â†’ 50 / 0.1 = 500 â‰¥ 1,000 âŒ, but 50 â‰¥ 100 âœ… â†’ MA detected.

2. Let `Wq = Wk = diag(1, 0, 0, 0)` â†’ copies only first dim:

   ```
   Q = K = [[50], [0.2], [45]]
   ```

3. Compute logits:

   ```
   S = QÂ·Káµ€ / âˆšd â‰ˆ [[2500, 10, 2250],
                    [  10,  0,    9],
                    [2250,  9, 2025]] / 2
   ```

4. Softmax highly favors `<s>` and â€œ.â€ â†’ Bias-like behavior.

5. Replace MA with 0 â†’ logits collapse â†’ downstream layers fail.

6. Replace with mean (47.5) or use `kâ€² = 47.5, vâ€² = ...` â†’ recovers attention.


### 3. Toy Example (3Ã—3 Grayscale Image â€“ ViT)

* Input: 9 grayscale pixels (0â€“1)
* Patch embedding â†’ vector `x âˆˆ â„â´`, assume `x[2] = 30` (MA), rest â‰ˆ 0.05
* Median = 0.05 â†’ 30 / 0.05 = 600

â†’ MA detected â†’ adds large constant to logits across all patches
â†’ Set to 0 â†’ CLIP ViT-L drops from 75.5% â†’ 59.8%
â†’ Replace with mean or use register token â†’ full recovery


### 4. Pseudo-code Summary

```python
for layer in model:
    h = hidden_states[layer]
    median = np.median(np.abs(h))
    MA_idx = [(i,j) for i in range(h.shape[0])
                     for j in range(h.shape[1])
                     if abs(h[i,j]) >= 100 and abs(h[i,j])/median >= 1e3]
    if intervene:
        h[MA_idx] = np.mean([h[i,j] for (i,j) in MA_idx])  # or 0
    hidden_states[layer] = h

# Optionally replace attention with Eq. (3)
```


### Key Takeaways

* **Sparse, constant scalar MAs** act as implicit self-attention biases
* Removing just 4â€“10 values can collapse the model
* Eq. (3) enables explicit bias injection that fully preserves performance while eliminating MAs



## Prompt 1.3.2 â€“ â€œSecret Weaponâ€ Identification

> â€œIdentify the single most critical formula, step, or component that enables the key contribution. Explain its function and why it is essential to the paperâ€™s success.â€

**One-Line Summary**  
> The paperâ€™s core enabler is **Equation (3)**, which appends a learnable constant key/value pair `(kâ€², vâ€²)` to self-attention, transforming the implicit Massive Activations into a **modular, interpretable bias injection**.


### 1. The Formula / Component

> Attention(Q,K,V;kâ€²,vâ€²) = softmax([Q]Â·[Káµ€â€†kâ€²] / âˆšd) Â· [V; vâ€²áµ€]   (Eq 3)

* `kâ€², vâ€² âˆˆ â„áµˆ`: Learnable bias vectors (per head)
* `[Â·;Â·]`: Concatenation over sequence axis (adds bias slot to softmax)


### 2. What It Does

1. **Injects Constant Bias**
   Turns what was previously an emergent phenomenon (MA) into a learnable, controlled bias mechanism.

2. **Removes MAs**
   In GPT-2 experiments, training with EAB results in no MAs forming at allâ€”yet PPL = 3.04 (same as baseline).

3. **Proves the Hypothesis**
   This design is essential for demonstrating that MAs are not accidents but can be functionally replaced with explicit design.


### 3. Why Itâ€™s Crucial

| Problem              | Without Eq. (3)                   | With Eq. (3)                         |
| -------------------- | --------------------------------- | ------------------------------------ |
| MA explanation       | Unverifiable emergent behavior    | Testable, tunable parameterization   |
| Performance collapse | MA = 0 â†’ PPL âˆ                    | Bias vector â†’ performance restored   |
| Quantization issues  | MA causes long-tailed activations | Mean-fixed bias â†’ smooth activations |

Thus, Eq. (3) is the **lever that turns MA from mystery into mechanism**, enabling every contribution that follows.

## Prompt 1.4.1 â€“ Key Results Analysis


> "Analyze the core results from the â€˜Experimentsâ€™ or â€˜Resultsâ€™ section, including figures and tables. What metrics were used? What benchmarks? What are the authorsâ€™ strongest pieces of evidence?"



### Summary â€“ Performance in Numbers

* **Language**: In LLaMA2-7B, zeroing just 4 MAs â†’ PPL 5.47 â†’ âˆ  
   â†’ Replacing with mean: PPL 5.47 (no change)
* **Vision**: In CLIP ViT-L, removing 2 MAs â†’ Top-1 drops from 75.5% â†’ 59.8%  
   â†’ Mean replacement restores accuracy
* **Redesign**: GPT-2 with EAB trained from scratch â†’ PPL = 3.04 (same as baseline), and **no MAs** ever formed


### 1. Evaluation Metrics & Benchmarks

| Domain            | Metric                      | Benchmark Dataset(s)                                      |
|-------------------|-----------------------------|------------------------------------------------------------|
| **Language**      | Perplexity â†“                | WikiText-103, C4, PG-19                                   |
| **Understanding** | Avg. Zero-shot Accuracy â†‘   | BoolQ, PIQA, WinoGrande, ARC-Easy, ARC-Challenge          |
| **Vision**        | Top-1 Accuracy â†‘            | ImageNet-1K                                               |
| **Internal Stats**| max/median, Ïƒ/Î¼             | Hidden states (internal probes, Table 2)                  |


### 2. Key Results (Tables)

#### 2-1. Language Model Intervention (Table 3)

| Model        | Action         | WikiText PPL | C4 PPL | PG-19 PPL | Zero-shot Acc | Result        |
|--------------|----------------|--------------|--------|-----------|----------------|----------------|
| LLaMA2-7B     | Original       | **5.47**     | 7.85   | 8.57      | **68.95%**     | Baseline       |
|              | MA = 0         | âˆ            | âˆ      | âˆ         | 36.75%         | **Collapse**   |
|              | MA = mean      | 5.47         | 7.86   | 8.59      | 68.94%         | Full recovery  |
| LLaMA2-13B    | Original       | 4.88         | 7.22   | 7.16      | 71.94%         | Baseline       |
|              | MA = 0         | 5â€“6k         | 5â€“6k   | 4â€“5k      | 37.50%         | Collapse       |
|              | MA = mean      | 4.88         | 7.22   | 7.16      | 71.92%         | Recovery       |

#### 2-2. Vision Model Intervention (Table 4)

| Model       | Action         | ImageNet Top-1 | Change        |
|-------------|----------------|----------------|----------------|
| CLIP ViT-L  | Original       | **75.5%**       | â€”              |
|             | MA = 0         | 59.8%           | **â€“15.7 pts**  |
|             | MA = mean      | 75.5%           | Full recovery  |

#### 2-3. GPT-2 Re-training (Fig. 9â€“10, 39)

| Setup                    | MAs Present? | Validation PPL (OpenWebText2) |
|--------------------------|--------------|-------------------------------|
| Vanilla GPT-2            | âœ…           | **3.04**                      |
| + Sink Token             | âœ…           | 3.04                          |
| **+ EAB (kâ€², vâ€²)**        | âŒ (Removed) | **3.04**                      |


### 3. What Do the Authors Emphasize?

1. **Decisive Influence**  
   * Removing just 4â€“10 scalars â†’ complete failure in both LLMs and ViTs
   * Replacing with mean â†’ full performance recovery

2. **Replaceability**  
   * Swapping MAs for `(kâ€², vâ€²)` or mean shows 100% recovery â†’ MAs not essential, only functional

3. **Generality**  
   * Observed across 20+ LLMs and 12+ ViTs (Table 7, 8; Fig. 45â€“47)

4. **Practical Impact**  
   * Quantization tails can be removed by flattening MAs  
   * ViT register tokens can be reinterpreted as learnable bias slots (Table 6)


> **Conclusion**  
> These results support the hypothesis that MAs are not incidental but core self-attention biases.  
> Their removal causes collapse, and their functional replacement via mean or EAB fully restores performanceâ€”across domains.


## Prompt 1.4.2 â€“ Critical Comparison


> "How does the proposed method compare against baselines and SOTA models? What are the strongest proof points for superiority? Where does it fall short? How do the authors address these?"




### One-Line Summary

The proposed **Explicit Attention Bias (EAB)** methodâ€”adding `(kâ€², vâ€²)` to attentionâ€”matches baseline performance **while completely removing MAs**. This validates its strength in **interpretability and stability**, but it does **not exceed SOTA** in raw metrics.


### 1. Where It Excels â€“ Strongest Comparison Points

| Dimension             | Baseline                     | EAB Approach                    | Key Differentiator                |
|-----------------------|------------------------------|----------------------------------|-----------------------------------|
| **LLM Performance**   | LLaMA2-7B PPL = 5.47         | MA = 0 â†’ âˆ, MA = mean/EAB â†’ 5.47 | EAB restores performance 100%     |
| **ViT Accuracy**      | CLIP ViT-L = 75.5%           | MA = 0 â†’ 59.8%, mean â†’ 75.5%     | Mean = EAB in effect              |
| **MA Elimination**    | Sink-token, tweaks â†’ MAs remain | GPT-2 + EAB â†’ no MAs at all      | Only method that fully eliminates |


### 2. Where It Falls Short â€“ No Raw Metric Gains

| Area                  | Metric               | Result              | Authorsâ€™ Comment                            |
|-----------------------|----------------------|---------------------|---------------------------------------------|
| **Absolute Performance** | GPT-2 Val PPL       | 3.04 (same as base) | â€œGoal is not better performanceâ€            |
| **Bias Variant Tests**  | Alt. tricks (e.g., QK-bias) | MAs reduced but persist | â€œOnly Eq. (3) removes them completelyâ€       |
| **ViT EAB Re-training** | Not attempted        | Only mean tested    | Left for future work                        |

> In short: EAB doesnâ€™t beat SOTA in scores, but it provides **interpretability, modularity, and compression safety**â€”which previous methods lacked.


### 3. Summary Table â€“ Comparative Performance

| Model & Setting     | MA State   | PPL â†“ / Top-1 â†‘ | Relative Change | Note        |
|---------------------|------------|------------------|------------------|-------------|
| LLaMA2-7B (original) | Present    | 5.47             | â€”                | Baseline    |
| + MA = 0            | Removed    | âˆ                | Collapse         | â€”           |
| + MA = mean         | Replaced   | 5.47             | 0%               | EAB â‰ˆ mean  |
| GPT-2 (vanilla)     | Present    | 3.04             | â€”                | Baseline    |
| + Sink token        | Partial    | 3.04             | 0%               | MAs remain  |
| + EAB               | None       | 3.04             | 0%               | No MAs      |
| CLIP ViT-L          | Present    | 75.5%            | â€”                | Baseline    |
| + MA = 0            | Removed    | 59.8%            | -15.7 pts        | Collapse    |
| + MA = mean         | Replaced   | 75.5%            | 0%               | Restored    |


> **In summary**  
> Unlike prior attempts (e.g., softmax tweaks, sink-token tricks), EAB is the **only approach that fully removes MAs while preserving full model performance**.  
> Its strength lies not in outperforming others numerically, but in enabling new modular design patterns and safety analysis.

## Prompt 1.5.1 â€“ Stated and Potential Limitations

> "What limitations or weaknesses do the authors explicitly acknowledge? Additionally, based on your analysis, what other potential weaknesses or open risks might exist (e.g., scalability, assumptions, hardware constraints)?"


### Summary

The authors admit that while they observed and replaced the Massive Activation (MA) phenomenon, they have not fully explained its **origin**, **generality**, or **performance implications** in all settings. Furthermore, the proposed Explicit Attention Bias (EAB) design still requires **full retraining**, and its effectiveness in low-precision or security-critical environments remains **unproven**.


### 1. Explicitly Stated Limitations

| ID    | Limitation                                                                                           | Source                          |
|-------|------------------------------------------------------------------------------------------------------|----------------------------------|
| A1    | **Not aimed at improving accuracy** â€“ Goal is interpretability, not higher SOTA                      | â€œour goal is not to improveâ€¦â€   |
| A2    | **ViT tests incomplete** â€“ EAB re-training in CLIP, DINOv2 left for future work                      | Discussion section               |
| A3    | **Unexplained MA origin** â€“ Why MAs explode in layers 2â€“4 remains a mystery                          | Discussion                       |
| A4    | **No quantization tests yet** â€“ Hypothesized impact on INT4/INT8 models is not empirically tested    | Conclusion                       |


### 2. Unstated (Potential) Weaknesses

| Category               | Potential Issue                                                                                     | Why It Matters                                      |
|------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------|
| B1. Threshold Sensitivity | Detection rule (|h| â‰¥ 100 and |h| / median â‰¥ 1e3) may need re-tuning per model/norm config        | Reduces automation and reproducibility              |
| B2. Retraining Cost     | EAB requires full training from scratch; not compatible with frozen checkpoints                    | Blocks low-cost deployment                          |
| B3. Cross-Modality Gaps | No verification for audio Transformers, video ViTs, or multimodal models                           | Limits generalization                               |
| B4. Low-Precision Limit | MA detection likely fails after quantization (e.g., FP16/INT8) due to insufficient resolution       | Affects deployment on real hardware                 |
| B5. Security Surface    | Few scalars control behavior â†’ risk of adversarial backdoor/trigger manipulation                   | Needs robustness testing                            |
| B6. Streaming Uncertainty | MAs occur near start tokens â†’ behavior may change in streaming or mid-sequence editing scenarios | Affects dialogue, real-time apps                    |
| B7. Diversity Tradeoff  | Constant bias could reduce generation diversity or controllability in autoregressive LLMs          | Potential side effect of hardcoded focus mechanism  |


### 3. Quantified Risk Examples

| Scenario               | Effect on Performance                      | Notes                        |
|------------------------|--------------------------------------------|------------------------------|
| LLaMA2-7B: MA = 0      | PPL = âˆ, accuracy drop of â€“32 pts          | Collapse                     |
| LLaMA2-7B: MA = mean   | PPL = 5.47 (unchanged)                     | Full recovery                |
| GPT-2 + EAB            | PPL = 3.04, MAs = 0                        | Successful, but needs retrain|
| CLIP ViT-L: MA = 0     | Top-1 accuracy drops from 75.5 â†’ 59.8%     | â€“15.7 pts                    |

> While these figures show the power of just a few scalars, they also highlight the **fragility** and **attack potential** of such a bottleneck.


### 4. Implications for Future Research

1. **Theoretical modeling** â€“ Explain why MA forms abruptly in early layers
2. **Plug-in solutions** â€“ Can we add EAB without full retraining (e.g., via LoRA)?
3. **Quantized inference tests** â€“ Validate impact of MA removal on INT4/FP8 latency, energy
4. **Adversarial robustness** â€“ Study backdoor-like attacks using MA injection or removal

These limitations donâ€™t invalidate the contributionsâ€”but they underscore the need for **downstream validation** before adopting this paradigm in production LLMs or real-time systems.


## Prompt 1.5.2 â€“ Future Research Directions

> â€œWhat concrete future directions do the authors suggest? What logical extensions or new paths follow from this workâ€™s findings or limitations?â€


### Summary

While the authors empirically validated the hypothesis â€œMassive Activation â‰ˆ Constant Attention Bias,â€ they did not address its origin, real-world deployability, or generalization to all model types. The table below combines their stated suggestions with additional promising directions.


### 1. Authorsâ€™ Proposed Future Work

| ID    | Direction                                                                                          | Context                       |
|-------|----------------------------------------------------------------------------------------------------|-------------------------------|
| F1    | **Train ViTs with EAB** â€“ Currently only mean-replacement tested on CLIP/DINOv2                    | â€œleft for future workâ€        |
| F2    | **Model MA formation** â€“ Explain abrupt emergence in layers 2â€“4                                    | Discussion                    |
| F3    | **Test in quantized settings** â€“ See if MA removal aids INT4/INT8 inference                        | Conclusion                    |
| F4    | **Check multimodal models** â€“ Investigate MA in audio, video, and image-language models            | Conclusion                    |


### 2. Additional Research Directions (Suggested)

| ID    | Idea                                                                                       | Why It Matters                            |
|-------|--------------------------------------------------------------------------------------------|-------------------------------------------|
| S1    | **Plug-in EAB (LoRA-style)** â€“ Add `(kâ€², vâ€²)` via adapters or fine-tuning only             | Avoids retraining (see B2)                |
| S2    | **Auto-tuned thresholding** â€“ Detect MAs via dynamic stats (instead of fixed 100Ã—, 1e3Ã—)   | Solves model-specific hyperparam issues   |
| S3    | **Security evaluation** â€“ Test if modifying MAs can function as a stealthy backdoor        | Explores B5 attack surface                 |
| S4    | **Streaming/Editing LLMs** â€“ Study how MAs behave in token insert/delete settings          | Affects LLM chat, code editing, etc.      |
| S5    | **Controllable generation via `(kâ€², vâ€²)`** â€“ Tune styles or tones through bias control     | New use case: conditioning                |
| S6    | **Mathematical origin of MA** â€“ Derive how initialization, norms, and data drive MA spikes| Grounding F2 with theory                  |
| S7    | **Hardware-aware deployment** â€“ Test if MA-free models save power on real accelerators     | Complements F3 for deployment readiness   |


> By combining theoretical modeling (F2, S6) with plug-in solutions (S1, S4) and quantization-aware designs (F3, S7), this work could mature into a standard design pattern:  
> **â€œBias-as-a-Moduleâ€** for interpretable, safe, and efficient LLMs.

</details>