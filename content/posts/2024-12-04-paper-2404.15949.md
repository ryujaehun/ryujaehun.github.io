---
categories:
- paper-review
- with-gpt
date: "2024-12-04"
title: 'CORM: Cache Optimization with Recent Message for Large Language Model Inference'
---

[논문 링크](https://arxiv.org/abs/2404.15949)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### 논문의 강점, 독창성, 핵심 알고리즘, 그리고 한계점에 대한 분석

---

#### **논문의 강점**
1. **효율적인 메모리 사용**:
   - LLM 추론 과정에서 가장 큰 병목 중 하나인 키-값(KV) 캐시 메모리를 최적화하여 최대 70%까지 메모리 사용을 줄였습니다.
   - 기존 모델 성능을 거의 유지하면서도 메모리 효율성을 대폭 향상했습니다.

2. **추가 학습 불필요**:
   - CORM(Caching Optimization with Recent Message)은 모델 재학습 없이 기존 모델에 바로 적용 가능해 실용성이 높습니다.
   - 학습 없이 메모리 사용량을 줄이는 방법은 현장의 적용 가능성을 극대화합니다.

3. **확장 가능성**:
   - 다양한 LLM 모델(LLaMA2-7B, Vicuna-7B 등)에 대한 실험 결과를 통해 CORM의 범용성과 확장 가능성을 입증했습니다.
   - 다른 메모리 최적화 기법(GQA 등)과의 통합도 가능하여 추가적인 압축을 지원합니다.

4. **이론과 실험의 균형**:
   - 유사 쿼리가 유사한 키에 집중하는 경향에 대한 이론적 분석과 실험적 증거를 통해 설계된 정책의 타당성을 뒷받침합니다.

---

#### **논문의 독창적인 지점**
1. **쿼리 유사성 활용**:
   - CORM은 현재 쿼리가 최근 쿼리와 가장 유사하다는 관찰을 바탕으로 최신 쿼리 정보만을 활용하여 메모리 최적화를 수행합니다.
   - 이 접근법은 기존의 고정된 캐시 크기 정책 대신, 동적인 캐시 관리 메커니즘을 도입합니다.

2. **다층, 다헤드의 차별적 관리**:
   - 서로 다른 Transformer 계층과 헤드가 다양한 희소성을 가지므로, 이를 개별적으로 처리하는 정책을 설계했습니다.

3. **최신 메시지 기반의 캐시 관리**:
   - `최근 w 개의 쿼리`와 `최근 r 개의 키`를 바탕으로 덜 중요한 키-값 페어를 제거하는 동적 관리 정책을 제안합니다.

---

#### **핵심 알고리즘과 예시**
**CORM 알고리즘의 주요 단계:**
1. **쿼리와 메시지 관리**:
   - 각 생성 단계에서 현재 쿼리와 이전 w개의 쿼리에 기반하여 중요하지 않은 키-값 페어를 결정합니다.
   - 중요한 키만 유지하고 나머지는 제거합니다.

2. **기록된 메시지 활용**:
   - 최신 쿼리의 메시지를 저장하고 이를 기반으로 중요 키를 확인합니다.

3. **최소 최근 사용 정책**:
   - r개의 최신 키는 항상 유지하여 중요 정보의 조기 삭제를 방지합니다.

**예시 입력**:
- 입력 시퀀스: "The quick brown fox jumps over the lazy dog."
- 쿼리 \(Q\) 및 키 \(K\), 값 \(V\)가 계산되었으며, \(w=4, r=2\)로 설정.

**CORM 실행**:
1. 최근 \(w=4\)개의 쿼리를 고려하여 중요도가 낮은 키-값 페어를 식별.
2. 중요도가 낮은 키-값 페어를 제거(예: \(\alpha_{t,i} < 1/t\)).
3. 최신 \(r=2\)개의 키는 항상 유지.

---

#### **논문의 한계점**
1. **모델별 성능 차이**:
   - 일부 Transformer 모델(예: Bloom-7B, OPT-6.7B)에서 성능 감소 및 낮은 압축률을 보였습니다. 이는 모델의 포지션 인코딩 방식(RoPE가 아닌 경우)에 따른 차이로 분석됩니다.

2. **운영 복잡성**:
   - 캐시 최적화를 위해 추가적인 데이터 구조(메시지 기록 등)를 유지해야 하며, 이는 약간의 추가 메모리를 요구합니다.

3. **초기화 비용**:
   - 초기 파라미터(예: \(w, r\)) 선택이 최적화 성능에 중요한 영향을 미칠 수 있어 사용자 설정이 필요합니다.

4. **응답 속도**:
   - 최근 쿼리와의 유사성을 계산하는 데 약간의 추가 연산 비용이 필요하며, 대규모 데이터에서 실시간 처리에는 추가 검증이 필요합니다.

---

CORM은 LLM 추론에서 메모리 병목을 해결하는 데 탁월한 기여를 했으며, 실제 적용 가능한 혁신적 알고리즘으로 평가받을 만합니다. 하지만 모델의 다양성과 초대형 배포 시 발생할 수 있는 문제를 해결하기 위한 후속 연구가 필요합니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



논문의 핵심 알고리즘인 CORM(Cache Optimization with Recent Message)을 단계별로 설명하고, 예시를 들어 전체적인 과정을 구체적으로 보여드리겠습니다.

---

### **핵심 알고리즘: CORM**
CORM은 **쿼리의 유사성**과 **주의 집중 정보**를 활용하여 LLM의 키-값(KV) 캐시 메모리를 동적으로 관리하고 압축하는 방법입니다. 중요한 단계는 다음과 같습니다.

---

#### **1. 초기화**
- **주요 파라미터**:
  - \( w \): 최근 쿼리를 고려할 윈도우 크기.
  - \( r \): 항상 유지할 최신 키-값 페어의 개수.
- 각 단계에서 쿼리와 키, 값 정보가 저장됩니다.
  
---

#### **2. 입력 데이터**
- **입력 시퀀스**: 
  - 예를 들어, "The quick brown fox jumps over the lazy dog."
  - 각 토큰이 쿼리(\(Q\)), 키(\(K\)), 값(\(V\))로 변환됩니다.

---

#### **3. 중요 키 판단 기준**
- 현재 쿼리 \( q_t \)의 주의(attention) 점수 \(\alpha_{t,i}\)를 계산:
  \[
  \alpha_{t,i} = \frac{\exp(q_t k_i^T / \sqrt{d})}{\sum_{j=1}^t \exp(q_t k_j^T / \sqrt{d})}
  \]
  - 여기서 \( \alpha_{t,i} \)는 \( q_t \)가 \( k_i \)에 주는 주의 비율.
  - 중요한 키는 \(\alpha_{t,i} \geq 1/t\)일 때로 정의됩니다.

---

#### **4. 최근 메시지 기반 삭제 정책**
- 최근 \(w\)개의 쿼리에서 중요하지 않은 키를 판단:
  - 각 쿼리가 중요하다고 판단한 키들만 남기고 나머지는 삭제.
- 최신 \(r\)개의 키는 항상 유지:
  - 중요한 키라도 반복적으로 확인되지 않으면 삭제되지만, 최신 키는 예외적으로 보존.

---

#### **5. 알고리즘 동작**
- 각 생성 단계에서 다음을 수행:
  1. **주의 점수 업데이트**: 이전 메시지 기록에 현재 쿼리의 점수를 추가.
  2. **중요 키 필터링**: 최근 \(w\)개의 쿼리 메시지에서 중요하지 않은 키 제거.
  3. **최신 키 유지**: 최신 \(r\)개의 키는 항상 유지.

---

### **예시를 통해 CORM 이해하기**

#### **입력 데이터**
- 시퀀스: "The quick brown fox jumps over the lazy dog."
- 각 토큰이 쿼리(\(Q\)), 키(\(K\)), 값(\(V\))로 변환됨.
  - \( Q_1, Q_2, \dots, Q_9 \)
  - \( K_1, K_2, \dots, K_9 \)
  - \( V_1, V_2, \dots, V_9 \)

#### **파라미터 설정**
- \( w = 4 \) (최근 4개의 쿼리를 고려).
- \( r = 2 \) (최신 2개의 키-값 페어 유지).

---

#### **단계별 진행**

1. **1단계** (쿼리 \( Q_1 \) 처리):
   - \( Q_1 \)이 생성되고, \( K_1 \), \( V_1 \) 저장.
   - 현재는 캐시가 비어 있으므로 삭제 없음.

2. **2단계** (쿼리 \( Q_2 \) 처리):
   - \( Q_2 \) 생성. 주의 점수 계산:
     \[
     \alpha_{2,1}, \alpha_{2,2}
     \]
   - \(\alpha_{2,i} \geq 1/2\)인 \( K_1, K_2 \) 유지.
   - 삭제 없음.

3. **3단계** (쿼리 \( Q_3 \) 처리):
   - \( Q_3 \) 생성. \( Q_1, Q_2 \) 메시지를 참조하여 \( K_3 \), \( V_3 \) 추가.
   - 중요하지 않은 키-값(\(\alpha_{3,i} < 1/3\)) 제거.

4. **9단계** (마지막 토큰 처리):
   - 최근 \(w=4\)개의 쿼리(\(Q_6, Q_7, Q_8, Q_9\)) 메시지 기반으로 중요 키 판단.
   - 최신 \(r=2\)개의 키(\(K_8, K_9\))는 무조건 유지.
   - 나머지 \( K_1, \dots, K_7 \) 중 중요도가 낮은 키 삭제.

---

### **CORM의 주요 결과**
- **메모리 절감**:
  - 중요하지 않은 키-값 페어 삭제로 최대 70% 메모리 절감.
- **성능 유지**:
  - 삭제 정책에도 불구하고 거의 동일한 성능을 유지.
- **적응형 관리**:
  - 고정 크기 대신 동적 캐시 크기 관리로 메모리와 성능 간 균형 달성.

---

CORM 알고리즘은 입력 데이터의 길이가 길어질수록 메모리 효율성을 극대화하며, 특히 추론 과정에서 메모리 병목을 해결하는 데 효과적입니다.

# Q : 이 논문에서 사용하는 방법을 학습하기 위하여 어떤 데이터셋이 필요할까? 그리고 어떻게 학습을 진행하면 될지 예시를 들어서 아주 자세하게 설명해줘



CORM(Cache Optimization with Recent Message)은 모델 재학습이 필요 없는 방식으로 설계되었습니다. 따라서 이 논문에서 제안한 방법을 구현하거나 최적화를 실험하려면, **학습 데이터셋** 대신 **평가 데이터셋**이 필요합니다. 하지만 관련된 평가 실험과 메모리 최적화를 위한 검증 과정을 준비하기 위해 어떤 데이터셋과 절차가 필요한지 설명드리겠습니다.

---

### **1. 데이터셋 선택**
CORM의 실험은 **긴 문맥 처리**를 다루는 데이터셋에서 수행되었습니다. 필요한 데이터셋은 주로 다음과 같은 조건을 만족해야 합니다.

#### **데이터셋 요구사항**
1. **긴 문맥을 포함**:
   - 각 시퀀스 길이가 길어야 함(최소 4,000~16,000 토큰).
   - 이유: 긴 문맥에서 키-값 캐시 메모리 병목이 발생하기 때문.
   
2. **다양한 태스크**:
   - 요약, 질문 응답, 코딩, 문서 생성 등 다양한 작업.
   - 이유: 캐시 최적화가 태스크별로 다른 영향을 미칠 수 있음.

#### **추천 데이터셋**
1. **LongBench**:
   - 긴 문맥 기반 평가를 위한 데이터셋.
   - 요약, 질문 응답, 코드 완성 등 다양한 작업 포함.
   - 논문에서도 사용된 데이터셋.

2. **PG-19**:
   - 긴 문맥 추론을 테스트하기 위한 대규모 텍스트 데이터셋.
   - 소설, 문학 작품 등으로 구성되어 긴 시퀀스 처리가 가능.

3. **OpenWebText**:
   - 웹 문서 기반의 데이터셋으로 긴 텍스트 포함.

4. **WikiText-103**:
   - 긴 문맥에 적합한 위키 문서로 구성된 데이터셋.

---

### **2. 학습 및 평가 과정**
CORM 자체는 **모델 재학습이 필요 없으므로**, 주로 **평가 프로세스**에 초점이 맞춰져야 합니다. 아래는 이를 구현하기 위한 절차입니다.

---

#### **2.1. 데이터 준비**
1. **데이터 전처리**:
   - 데이터를 토큰화하여 LLM에서 처리 가능한 입력 형식으로 변환.
   - 예: "The quick brown fox jumps over the lazy dog." → 토큰 ID.

2. **문맥 길이 조정**:
   - 시퀀스 길이를 다양하게 설정(예: 4,000, 8,000, 12,000 토큰).
   - 이유: 캐시 크기가 시퀀스 길이에 따라 다르게 작동하기 때문.

---

#### **2.2. 알고리즘 적용**
1. **기존 LLM 모델 준비**:
   - 사전 학습된 LLaMA2-7B 또는 Vicuna-7B와 같은 모델을 준비.

2. **CORM 알고리즘 구현**:
   - 논문에 나온 Pytorch 코드를 활용하여 다음을 구현:
     - 최근 쿼리 메시지를 저장하는 구조.
     - 중요하지 않은 키-값 페어를 식별하고 삭제하는 정책.

3. **실험 설정**:
   - \( w \) (윈도우 크기)와 \( r \) (최근 키 유지 크기) 값 조정:
     - \( w = [64, 128, 256, 512] \)
     - \( r = [16, 32, 64] \)

---

#### **2.3. 평가 방법**
1. **평가 기준**:
   - **메모리 사용량**: 키-값 캐시 크기를 측정하여 압축률 확인.
   - **성능 유지**: 모델의 정확도, F1 스코어, 또는 로그 확률(perplexity) 측정.

2. **실험 프로세스**:
   - **Baseline 모델**:
     - CORM을 적용하지 않고 전체 키-값 캐시를 유지한 상태에서 수행.
   - **CORM 적용 모델**:
     - CORM 알고리즘을 적용하여 키-값 캐시 크기 조정.
   - **비교**:
     - 성능 차이와 메모리 절감률 비교.

3. **실험 예시**:
   - 작업: 문서 요약.
   - 입력: "In a faraway land, there lived a fox who was known for his cleverness..." (4,096 토큰).
   - 출력:
     - **Baseline 모델**: "The fox was clever and lived in a faraway land."
     - **CORM 적용 모델**: "The clever fox lived in a faraway land."
   - 결과:
     - 성능 차이: BLEU 점수 기준 95%.
     - 메모리 절감: 70%.

---

### **3. 코드 예시**
아래는 CORM 알고리즘의 단순화된 PyTorch 코드입니다.

```python
import torch

def corm_eviction(keys, values, message, attn_score, w, r, t):
    """
    Args:
        keys: 이전 키 상태 (Tensor [l, d])
        values: 이전 값 상태 (Tensor [l, d])
        message: 최근 주의 메시지 (Tensor [m, l-1])
        attn_score: 현재 단계의 주의 점수 (Tensor [1, l])
        w: 윈도우 크기
        r: 최근 크기
        t: 현재 단계

    Returns:
        업데이트된 키, 값, 메시지
    """
    # 메시지 업데이트
    message = torch.cat([message, torch.zeros_like(attn_score)], dim=0)
    cur_message = (attn_score >= 1 / t).float()
    message = torch.cat([message[-w:], cur_message.unsqueeze(0)], dim=0)

    # 중요하지 않은 키-값 페어 삭제
    long_term_minor_keys = (message.sum(dim=0) == 0).nonzero().squeeze()
    keys = torch.index_select(keys, 0, long_term_minor_keys)
    values = torch.index_select(values, 0, long_term_minor_keys)

    return keys, values, message
```

---

### **4. 학습 및 평가 전략**
1. **파라미터 튜닝**:
   - 다양한 \( w, r \) 값에 대해 실험을 반복.
   - 예: 긴 시퀀스에서는 \( w \)를 크게, 짧은 시퀀스에서는 작게 설정.

2. **작업별 평가**:
   - LongBench를 기반으로 요약, 질문 응답 등 작업별로 메모리 효율성과 성능 비교.

3. **최적 정책 도출**:
   - 가장 높은 압축률과 최소한의 성능 손실을 제공하는 \( w, r \) 조합 도출.

---

이 과정은 실제로 모델 추론 시 메모리 효율성을 극대화하면서 성능을 유지하는 최적화 전략을 구축하는 데 유용합니다.

# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### **논문에서 제시한 결과와 비교 분석**

#### **1. 논문에서 제시한 주요 결과**
1. **메모리 절감**:
   - CORM은 키-값(KV) 캐시의 메모리 사용량을 최대 **70%까지 절감**.
   - 이는 다른 최신 기법들(예: StreamingLLM, Scissorhands, H2O) 대비 더 높은 압축률을 보여줍니다.

2. **성능 유지**:
   - 6개의 태스크(요약, 질문 응답, 코드 생성 등)에서 **성능 저하가 미미**.
   - 몇몇 작업에서는 심지어 **기존 캐시를 전부 사용하는 경우보다 더 나은 결과**를 도출.

3. **모델 확장성**:
   - 다양한 LLM 모델(LLaMA2-7B, Vicuna-7B 등)과 통합 가능.
   - 특히, Grouped Query Attention(GQA)와 함께 사용했을 때도 성능과 압축률 모두 유지.

---

#### **2. 다른 방법론과의 비교**
논문에서 CORM은 기존 메모리 최적화 기법들과 비교되며, 다음과 같은 특출난 점을 보입니다.

| **기법**                             | **압축률** | **성능 유지** | **재학습 필요 여부** | **주요 특징**                |
| ------------------------------------ | ---------- | ------------- | -------------------- | ---------------------------- |
| **CORM**                             | 최고 70%   | 거의 동일     | 불필요               | 최근 쿼리 기반, 유사성 활용. |
| **StreamingLLM**                     | 50~60%     | 약간의 저하   | 불필요               | 초기 및 최신 토큰 유지.      |
| **Scissorhands**                     | 60%        | 약간의 저하   | 불필요               | 중요성 지속 가설 기반.       |
| **H2O**                              | 50~65%     | 약간의 저하   | 불필요               | 누적 주의 점수 활용.         |
| **Dynamic Memory Compression (DMC)** | 70%        | 동일 수준     | 재학습 필요          | 레이어별 압축률 학습.        |

---

#### **3. CORM의 특출난 점**
1. **쿼리의 유사성 활용**:
   - CORM은 **쿼리 간 유사성**과 **최근 쿼리 메시지**를 기반으로 캐시를 동적으로 관리.
   - 최근 쿼리와 현재 쿼리가 유사하다는 관찰에 따라, 중요하지 않은 키-값 페어를 제거하면서도 성능을 유지.

2. **동적 관리**:
   - 다른 기법들은 고정된 캐시 크기(예: Scissorhands, H2O)로 동작하지만, CORM은 입력 길이와 태스크에 따라 **적응형 압축**을 적용.

3. **추가 학습 불필요**:
   - DMC와 같은 기법은 추가 학습이 필요한 반면, CORM은 모델의 구조나 파라미터를 수정하지 않음.

---

#### **4. 논문에서 제시하는 성공 요인**
1. **쿼리의 유사성 기반 정책**:
   - CORM은 현재 쿼리가 **최근 \(w\)개의 쿼리와 유사**하다는 점을 활용.
   - 이로 인해 현재 쿼리가 중요하다고 판단하는 키는 최근 쿼리에서도 중요할 가능성이 높음.

2. **주의 희소성 활용**:
   - Transformer 모델에서 대부분의 주의(attention)가 **소수의 키**에 집중된다는 사실에 기반.
   - 이를 통해 중요하지 않은 키-값 페어를 제거하여 캐시 크기를 줄임.

3. **유지 정책 (Eviction Policy)**:
   - 최신 \(r\)개의 키를 항상 유지하여 중요 키의 조기 삭제를 방지.
   - 중요도가 낮은 키를 동적으로 제거하여 캐시의 효율성을 극대화.

---

#### **5. CORM의 결과를 도출한 이유에 대한 내 의견**
1. **설계의 효율성**:
   - 최근 쿼리와 현재 쿼리 간의 유사성 관찰은 Transformer 구조의 근본적 특성을 잘 이해한 결과로 보임.
   - 특히, **로터리 포지션 인코딩(RoPE)**을 사용하는 모델에서 쿼리의 연속적 유사성을 극대화한 점이 효과적.

2. **희소성에 대한 정확한 활용**:
   - 대부분의 키가 주의 계산에서 중요하지 않음을 이론적으로 증명하고, 이를 실제로 적용.
   - 많은 메모리를 낭비하는 기존 방식을 개선한 점이 큰 장점.

3. **동적 적응**:
   - CORM의 동적 캐시 관리 정책은 모든 입력 시퀀스와 태스크에 적응할 수 있는 유연성을 제공.
   - 이는 실제 환경에서 중요하며, 다른 고정 크기 접근법보다 실용적이라고 생각됨.

4. **모델의 노이즈 제거 효과**:
   - 논문에서 일부 작업에서 CORM이 기존 모델보다 더 나은 성능을 보이는 이유를 **노이즈 제거**로 설명.
   - 실제로, 중요하지 않은 키-값 페어를 제거하면서 모델이 불필요한 정보를 처리하지 않아 성능이 더 좋아질 가능성이 있음.

---

#### **6. 추가적으로 제안할 수 있는 보완 아이디어**
1. **동적 파라미터 최적화**:
   - 현재 \(w\)와 \(r\) 값은 고정되어 실험되었지만, 태스크별 동적 조정 알고리즘을 도입하면 더 높은 성능과 효율성을 기대할 수 있음.

2. **다른 포지션 인코딩 방식 테스트**:
   - RoPE 외에도 다양한 포지션 인코딩 방식(ALiBi, 절대 포지션 등)을 테스트하여, 최적의 유사성 조건을 규명.

3. **실시간 환경에서의 응답 시간 테스트**:
   - CORM이 추가 연산(예: 쿼리 간 유사도 계산)을 요구하므로, 실시간 환경에서의 응답 속도를 더욱 최적화할 필요가 있음.

---

CORM은 메모리 병목 문제를 해결하기 위한 매우 실용적이고 혁신적인 접근법입니다. 논문의 설계와 결과는 Transformer의 근본적 특성을 잘 활용한 사례로, 앞으로 실시간 추론과 같은 더욱 복잡한 환경에서도 응용 가능성이 높습니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### **논문의 입력 데이터와 추론 과정**

#### **입력 데이터**
CORM의 적용 대상은 긴 시퀀스를 처리하는 Transformer 기반 대형 언어 모델(LLM)입니다. 입력 데이터는 일반적으로 다음과 같습니다:

1. **형식**:
   - 긴 자연어 텍스트 시퀀스.
   - 예: `"The quick brown fox jumps over the lazy dog. The story continues with..."`

2. **전처리**:
   - 텍스트는 토큰화(tokenization) 과정을 통해 숫자 ID로 변환.
   - 예:
     ```plaintext
     Input: "The quick brown fox jumps over the lazy dog."
     Tokenized: [101, 2023, 2086, 5672, 3767, 2862, 102]
     ```

3. **시퀀스 길이**:
   - 논문에서는 최대 16,000 토큰의 시퀀스를 처리하는 데이터를 평가.
   - 긴 문맥이 캐시 메모리 병목을 유발하기 때문에 긴 데이터셋이 중요.

---

#### **추론 과정**
CORM의 추론 과정은 Transformer 모델의 기본 추론 과정과 유사하지만, **KV 캐시 관리**를 동적으로 수행하여 메모리를 최적화합니다.

1. **기본 Transformer 추론 과정**:
   - 입력 토큰 시퀀스를 Transformer 계층으로 전달하여 각 계층에서 쿼리(\(Q\)), 키(\(K\)), 값(\(V\))를 생성.
   - 각 계층에서 \(Q\), \(K\), \(V\)는 다음과 같은 연산으로 계산:
     \[
     Q = XW_Q, \quad K = XW_K, \quad V = XW_V
     \]
     - \(X\): 입력 토큰 임베딩 또는 이전 계층 출력.
     - \(W_Q, W_K, W_V\): 쿼리, 키, 값 생성 가중치 행렬.
   - 주의(attention) 계산:
     \[
     \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d}}\right) V
     \]
     - \(\sqrt{d}\): 스케일링 요인.

2. **CORM 적용 과정**:
   - CORM은 각 단계에서 KV 캐시를 관리하여 메모리를 최적화:
     1. 현재 쿼리 \(Q_t\) 생성.
     2. \(Q_t\)와 기존 키 \(K_i\)를 사용하여 주의 점수 \(\alpha_{t,i}\) 계산.
     3. 최근 \(w\)개의 쿼리 메시지를 기반으로 중요하지 않은 키-값 페어 삭제.
     4. 최신 \(r\)개의 키-값 페어는 항상 유지.

---

#### **예시: 입력 데이터와 추론**
**입력 데이터**:
- 문장: `"The quick brown fox jumps over the lazy dog."`
- 토큰화: `[101, 2023, 2086, 5672, 3767, 2862, 102]`
- Transformer의 입력: 토큰 임베딩.

**CORM 적용 예시**:
1. **1단계**:
   - \(Q_1, K_1, V_1\) 생성.
   - 캐시에 저장: \(\{K_1, V_1\}\).

2. **2단계**:
   - \(Q_2, K_2, V_2\) 생성.
   - \(\alpha_{2,1}\) 계산: \(Q_2\)와 \(K_1\) 간의 주의 점수.
   - \(K_1, K_2\) 모두 유지(아직 삭제 없음).

3. **n단계**:
   - 최근 \(w\)개의 쿼리 메시지로 중요하지 않은 키 식별.
   - 예: \(K_{n-w}\)이 중요하지 않은 것으로 판명되면 제거.

4. **출력**:
   - 최종 토큰 생성.

---

### **모델 아키텍처 및 요구 사항**

#### **1. 모델 아키텍처**
CORM이 적용된 Transformer 아키텍처는 다음과 같습니다:
1. **입력 임베딩**:
   - 입력 토큰이 고차원 벡터로 임베딩됨.
2. **다중 계층의 Self-Attention**:
   - 각 계층은 \(Q\), \(K\), \(V\)를 생성하고 주의 계산 수행.
3. **포지션 인코딩**:
   - \(Q, K\)를 계산할 때 토큰의 위치 정보를 포함.
   - 논문에서는 RoPE(Rotary Position Embedding)를 주로 활용.

#### **2. 연산 요구량**
1. **주의 계산**:
   - Attention 연산의 복잡도: \(O(n^2 \cdot d)\) (\(n\): 시퀀스 길이, \(d\): 히든 크기).
   - CORM은 중요하지 않은 키를 제거하여 \(n\)을 동적으로 줄임.

2. **KV 캐시 관리**:
   - 기존 키-값 크기: \(O(n \cdot d)\).
   - CORM 적용 후: \(O(w \cdot d)\) (\(w\): 최근 쿼리 윈도우 크기).

#### **3. 메모리 요구량**
- **기존 방법**:
  - KV 캐시 메모리: \(O(n \cdot d)\).
  - 예: \(n=4,096, d=1,024\)일 때 16GB 필요.
- **CORM**:
  - \(n\) 대신 \(w\)를 기반으로 메모리 사용량 절감.
  - \(w = 256\)일 때 메모리 사용량 1GB로 감소.

---

### **결론**
- **입력 데이터**는 긴 문맥 텍스트를 포함하며, 이를 토큰화하여 Transformer에 입력.
- **추론 과정**에서 CORM은 주의 계산 후 중요하지 않은 키-값 페어를 제거하여 메모리를 동적으로 관리.
- **모델 아키텍처**는 Transformer의 기본 구성과 유사하지만, RoPE를 사용하여 쿼리의 유사성을 강화.
- **연산 및 메모리 요구량**은 CORM 덕분에 기존 방식 대비 크게 감소하여 긴 시퀀스 처리에 매우 효율적.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘






### **논문의 한계를 극복하기 위한 연구 흐름**

CORM(Cache Optimization with Recent Message)은 LLM 추론에서 메모리 병목을 해결하기 위한 혁신적인 접근법입니다. 그러나 논문은 몇 가지 한계를 가지고 있으며, 이를 해결하기 위한 연구 흐름은 다음과 같은 방향으로 발전할 수 있습니다.

---

### **1. 한계와 이를 극복하기 위한 연구 방향**

#### **1.1 모델별 성능 차이**
- **한계**:
  - 일부 모델(예: OPT-6.7B, Bloom-7B)에서 CORM의 압축률이 낮거나 성능 저하가 발생.
  - 이는 포지션 인코딩 방식(RoPE, ALiBi, 절대 포지션 등) 차이에 기인.

- **연구 방향**:
  1. **포지션 인코딩 방식의 일반화**:
     - 다양한 포지션 인코딩 방식에서 쿼리 유사성을 강화하는 방법론 개발.
     - 예: RoPE와 ALiBi의 장점을 결합한 하이브리드 포지션 인코딩 설계.
  2. **모델별 최적화 정책 학습**:
     - 각 Transformer 모델에 맞춘 캐시 관리 최적화를 학습하는 메타 학습 접근법 도입.
     - 모델별 희소성 분석을 자동화하여 적합한 정책을 생성.

---

#### **1.2 캐시 관리 정책의 고정성**
- **한계**:
  - CORM의 정책은 \(w\)와 \(r\) 파라미터를 사전에 설정해야 하며, 이는 태스크나 입력 데이터의 다양성을 충분히 반영하지 못할 수 있음.

- **연구 방향**:
  1. **동적 파라미터 최적화**:
     - \(w\)와 \(r\) 값을 입력 데이터의 길이, 태스크 유형, 또는 실시간 조건에 따라 동적으로 조정.
     - Reinforcement Learning(RL) 기반으로 최적의 \(w, r\) 값 선택.
  2. **학습 기반의 캐시 정책**:
     - 캐시 유지 또는 삭제 결정을 학습 가능한 모델(예: 경량화된 Neural Network)로 처리.
     - 주의 희소성, 쿼리 유사성, 키 중요도를 입력으로 사용하여 정책 결정.

---

#### **1.3 실시간 환경에서의 응답 시간**
- **한계**:
  - CORM은 쿼리 간 유사성을 계산하기 때문에 실시간 응답에서 추가 계산 비용이 발생.
  - 특히 긴 시퀀스에서는 추가 연산이 병목이 될 가능성.

- **연구 방향**:
  1. **효율적인 유사성 계산 알고리즘**:
     - Cosine Similarity 계산을 근사화하는 방법(예: 로컬 민감 해싱, LSH) 도입.
     - 최근 쿼리에서 중요도가 높은 키만을 사전에 필터링하는 방식을 추가.
  2. **병렬 처리 최적화**:
     - GPU나 TPU의 병렬 연산을 활용해 유사성 계산과 캐시 관리 병렬화.
     - Tensor Core를 활용한 경량화 연산 최적화.

---

#### **1.4 메모리와 성능 간 트레이드오프**
- **한계**:
  - 메모리 절감률을 높일수록 성능 저하가 발생할 수 있음.
  - 모든 태스크에서 최적의 압축률과 성능 균형을 달성하기 어려움.

- **연구 방향**:
  1. **태스크별 최적화**:
     - 각 태스크에 따라 압축률과 성능 균형을 조정하는 방법론 개발.
     - 예: 요약 태스크에서는 높은 압축률 허용, 질문 응답 태스크에서는 낮은 압축률 유지.
  2. **다중 압축 정책**:
     - 단일 압축 정책 대신, 다양한 태스크와 입력 길이에 맞는 여러 정책을 동시에 유지.
     - 다중 정책 프레임워크에서 최적 정책을 선택하는 방식을 도입.

---

#### **1.5 노이즈 제거 및 정밀도 유지**
- **한계**:
  - CORM이 불필요한 정보를 제거하며 성능을 높이는 경우가 있지만, 일부 중요한 정보가 과도하게 삭제될 가능성.
  - 특히 노이즈와 중요한 정보를 정확히 구분하는 데 한계.

- **연구 방향**:
  1. **노이즈 식별 개선**:
     - Attention Score 외에도 Gradient 정보나 모델 출력 민감도를 활용해 중요 정보를 식별.
     - 중요 정보와 노이즈 간의 경계를 학습 가능한 방식으로 개선.
  2. **중요도 예측 모델**:
     - Attention Score와 함께 과거의 Query-Response 관계를 활용해 키의 중요도를 예측하는 경량 모델 추가.

---

### **2. 현재 연구 흐름과 관련된 접근법**
현재 논문과 유사한 맥락에서 진행 중인 연구 흐름은 다음과 같습니다:

#### **2.1 Adaptive Sparse Attention**
- 희소성을 모델 학습 과정에서 동적으로 학습하여, 중요하지 않은 키를 제거.
- 주요 연구:
  - Dynamic Memory Compression(DMC): 레이어 및 헤드별로 희소성 수준을 학습하여 메모리 최적화.

#### **2.2 Lightweight Inference Techniques**
- LLM 추론 비용을 줄이기 위해 경량화된 구조를 설계.
- 주요 연구:
  - StreamingLLM: 최신 토큰과 초기 토큰만 유지.
  - Scissorhands: 중요성 지속 가설을 활용해 키를 선택적으로 삭제.

#### **2.3 Approximation Algorithms**
- Attention 연산의 비용을 줄이는 근사 알고리즘 활용.
- 주요 연구:
  - FlashAttention: I/O 효율성을 활용하여 Attention 계산 가속화.
  - Kernelized Attention: 근사 기법으로 Attention Score를 빠르게 계산.

---

### **3. 제안 연구 흐름 요약**
CORM의 한계를 극복하기 위해 다음 연구 흐름이 유망합니다:
1. **동적 캐시 관리 정책 개발**:
   - 입력 및 태스크 특성에 맞춰 정책을 자동 조정.
2. **효율적인 유사성 계산**:
   - Cosine Similarity의 근사화 및 병렬 연산 최적화.
3. **모델 일반화**:
   - RoPE 외 다양한 포지션 인코딩 방식에서 쿼리 유사성을 보장.
4. **태스크별 최적화**:
   - 태스크 맞춤형 압축률과 성능 균형 설계.

이러한 흐름은 CORM의 한계를 보완하며, 실제 환경에서 더욱 높은 실용성과 성능을 제공할 것입니다.
