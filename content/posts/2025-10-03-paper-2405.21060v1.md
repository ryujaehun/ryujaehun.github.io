---
title: "Transformers are SSMs: Generalized Models and Efficient Algorithms
Through Structured State Space Duality"
date: "2025-10-03"
categories:
 - paper-review
 - with-gpt
tags: [
  "Mamba", "Mamba-2", "Structured State Space Duality", "SSD",
  "State Space Models", "SSM", "Transformer", "Attention Mechanism",
  "Long Context", "Efficient Training", "FlashAttention",
  "Sequence Modeling", "Scaling Laws", "Parallelism", "GPU Acceleration",2405.21060v1
]
cover: https://icml.cc/media/PosterPDFs/ICML%202024/32613.png
---

[논문 링크](https://arxiv.org/abs/2405.21060v1)


# Structured State Space Duality: Mamba-2로 본 SSM↔Attention 통일과 2–8× 가속

## 한 줄 요약 (TL;DR)

**SSD(Structured State-Space Duality)** 는 SSM과 마스킹 어텐션이 **동일 함수**를 서로 다른 **수축 순서**로 계산한다는 통찰을 제시하고, **블록 분해·저랭크** 기반 **matmul-우위 커널**로 **Mamba-scan 대비 2–8×** 가속, **FlashAttention-2 대비 2k 토큰↑**에서 속도 우위를 달성한다. **2.7B / 300B tokens** 사전학습에서는 **Transformer++와 품질 동급**, **주의 6층 혼합** 시 **ppl 5.95**로 추가 개선을 보인다(근거: Abstract, Fig.10, Tab.3, §9.2–9.3).

---

## 핵심 아이디어

* **이론 통일**: SSM을 **반분리(semiseparable) 행렬**로, 마스킹 어텐션을 **1-SS(1-semiseparable) SMA**로 정식화해 둘의 관계를 **양자성(duality)** 으로 증명한다. 즉, 같은 선형 변환을 **이차(어텐션)** vs **선형(SSM 재귀)** 로 계산할 뿐이다(근거: §5.1–§5.3, Fig.1).
* **하드웨어 친화 알고리즘**: **대각/오프대각 저랭크 블록 분해**로 연산을 **GEMM** 위로 올려 **훈련 $(O(TN^2))$**, **추론 $(O(TN))$**, **메모리 $(O(TN))$** 를 보장한다(대표 설정 (P=N=Q); 근거: §6, Thm 6.1).
* **아키텍처화(Mamba-2)**: **입력-기반 병렬 프로젝션 + GroupNorm** 으로 **블록당 all-reduce 1회**(TP 절반화), ppl **11.76→11.49(−0.27)** 개선을 달성한다(근거: Fig.7/§8, Tab.4).

---

## 배경: 그들이 해결한 문제

* **Transformer 병목**: 훈련 $(O(T^2))$, 생성 시 **KV-cache $(O(T))$** 로 길이에 비선형적으로 비용이 커진다(근거: §1).
* **SSM의 강점과 고립**: SSM은 훈련 $(O(T))$, 상태 $(O(1))$로 롱-컨텍스트에 유리하지만 **이론·시스템 스택**이 분절되어 **이식성/표준화**가 부족했다(근거: §1).
* **필요한 것**: 두 진영을 잇는 **공통 수학**과 **하드웨어-친화 커널**, 그리고 **대규모 학습에서 통용되는 병렬화 체계**(TP/SP/CP)의 정합(근거: §1, §8).

---

## 새로운 접근법: SSD(Structured State-Space Duality)

* **정의**: **스칼라-아이덴티티 SSM ≡ 1-SS SMA**. 효율적 자기회귀 주의가 되려면 **반분리 마스크**가 필요하다는 **필요충분 조건**을 제시한다(근거: §5.2–§5.3).
* **알고리즘**: $(T\times T)$ 행렬을 **$(Q\times Q)$ 블록**으로 나누고,

  * **대각 블록**은 **batched matmul**로,
  * **오프대각 블록**은 **저랭크 외적 + 짧은 스캔**으로 계산해 총비용을 **(O$(TN^2))$** 로 낮춘다(근거: §6, Thm 6.1).
* **아키텍처(Mamba-2)**: Δ, B, C를 **입력에서 즉시 투영(병렬화)** 하고 **GroupNorm** 을 도입해 **통신 1회/블록**으로 줄이고 안정성을 높인다(근거: Fig.7, §8, Tab.4).

---

## 작동 원리: 구체적인 예시로 살펴보기

### 1) “같은 함수, 다른 수축 순서”

3×3 이미지 (x)를 row-major로 편평화: $(x_{1:9}=[1,2,\dots,9])$.
**스칼라 SSM**(1-SS): $(h_t=a,h_{t-1}+x_t,\ y_t=c,h_t)$ with $(a=0.5,\ c=1)$.

* 재귀로 계산하면 (y_3=4.25), …, (y_9=16.0039) (선형 시간).
  **동일 SMA(1-SS)** 의 마스크 $(L_{j,i}=a^{j-i})로 (y_j=\sum_{i\le j} a^{j-i}x_i)$ 를 계산해도 **동일 결과**(이차 시간).
  → **동일 선형 변환**을 **스캔(선형)** 과 **마스크 행렬곱(이차)** 로 모두 구현 가능(근거: §5.1–§5.3).

### 2) 블록-저랭크 한 눈에

오프대각 블록의 항 $(L_{j,i}=a^{j-i}=u_j,v_i)$ 형태(랭크-1)로 분해되어 **외적 한 번 + 짧은 스캔**으로 처리 가능 → **GEMM 우위 경로**에 부합(근거: §6).

### 3) 수식 스니펫(복잡도)

[
\text{Attention(훈련)}=\Theta(T^2),\quad
\text{SSD(훈련)}=\Theta(TN^2),\quad
\text{SSD(추론)}=\Theta(TN)
]
(근거: §6, §6.3)

---

## 성능 검증: 주요 결과

### 1) 커널/시스템 효율

* **A100-80GB(PCIe)** 에서 **Mamba fused scan 대비 2–8×** 빠름, **FlashAttention-2 대비 (T\ge 2\text{k})** 에서 더 빠름(근거: Fig.10, §9.3).
* **메모리 스케일링**: SSD는 **KV-cache가 없고** 상태가 (O(N))라 긴 컨텍스트에서 유리(근거: §6.3).

> [
> \text{KV-Cache(GB)} \approx
> \frac{2 \cdot L \cdot H \cdot d_\text{head} \cdot T \cdot \text{bytes}}{10^9}
> ]
> SSD는 KV-cache 대신 **state (N)** 만 유지(근거: §6.3).

### 2) 언어모델링 & 스케일

* **중형(≈1.3–1.4B)**: **Mamba-2-1.3B ppl 6.66**로 **Mamba-1.4B 6.80** 대비 개선, Zero-shot 평균은 동률(56.4)(근거: Tab.1).
* **대형(2.7B / 300B tokens)**: **Mamba-2 ≈ Transformer++**(동일 레시피), **주의 6층 혼합** 시 **ppl 5.95, Avg 61.0**로 둘 다 상회(근거: Tab.3).
* **스케일링 법칙**: 125M–1.3B에서 **Perplexity–이론 FLOPs–벽시계** 파레토 상 **Transformer++ 지배**(근거: Fig.9).

### 3) 기억 과제(MQAR)

* **N=16**에서도 **Mamba-2 ≫ Mamba-1**, **N=16→64→256**으로 **정확도 지속 상승**(상태 용량 효과)(근거: Fig.8, §9.1).

---

## 우리의 관점: 강점, 한계, 그리고 이 연구가 중요한 이유

### 강점

* **통일 이론 + 실용 커널**: **반분리 행렬** 관점이 **선형/이차 알고리즘**을 한 프레임에 올리고, 이를 바로 **텐서코어-친화 커널**로 구체화(근거: §5–§6).
* **시스템 설계의 정합**: **입력-병렬 프로젝션 + GroupNorm** 으로 **TP 통신 2→1회**(동일 블록당), **SP/CP** 로 자연 확장(근거: Fig.7, §8).
* **롱-컨텍스트 효율의 실증**: FA-2와의 **교차점이 2k 토큰**, **16k에서 큰 격차**(근거: Fig.10).

### 한계(명시 + 잠재)

* **짧은 시퀀스(≈2k) 학습 효율**은 Transformer 대비 불리할 수 있어 **SSD:MLP 혼합** 권장(근거: §9.3).
* **순수 Mamba-2 품질 상한**: **주의 6층** 혼합이 **일관된 추가 이득**을 보임(근거: Tab.3).
* **평가 범위**: 보고 스케일은 **≤2.8B**, 벤치는 **Pile/Zero-shot 중심**으로, 수학·코드·멀티모달은 제한적(근거: Tab.1/§9.2).

### 왜 중요한가?

* **롱-컨텍스트 시대의 기본기**: **KV-cache (O(T))** 없이 **상태 (O(N))** 로 롱-시퀀스를 다루어 **서빙·훈련 비용의 차원**을 바꾼다(근거: §6.3).
* **SSM↔Attention의 교량**: 두 패러다임을 **교체 가능한 구성요소**로 바라보게 하여 **하이브리드 설계 최적화**의 문을 연다(근거: §5, §9.2.3).

---

## 다음 단계는?: 앞으로의 길

* **SMA 일반화**: **다른 structured masks** 탐색으로 **새 효율형 attention** 설계(유한 특성맵 (\psi); 근거: §10).
* **하이브리드 최적화**: **주의층 비율(≈5–20%) × 배치 위치 × 헤드 폭** 매트릭스로 **ICL/복사/MQAR**-효율의 **Pareto 지도** 작성(근거: §9.2.3, Tab.2).
* **표현력 trade-off**: **상태확장 (N)** vs **전역 캐시** 간 **복사·ICL 성능** 교환관계의 정량화(근거: §10).
* **해석 가능성 이전**: **attention sinks** 와 유사 현상이 **Mamba/SSM**에도 존재하는지 검증(근거: §10).
* **스케일 확장·벤치 다양화**: **≥7B** 모델, **지시따르기·수학·코딩·멀티모달**까지 평가 확장.

---

### 부록: 간단 수치 메모(학습/추론·메모리)

* **사전학습**: **2.7B** 파라미터를 **Pile 300B tokens**로, **batch 1.0M tokens**로 학습(단순 계산으로 **≈300k steps**)(근거: §9.2.3, Appx §D.3).
* **커널 처리량(상대)**: **Mamba-scan 대비 2–8×**, **FA-2 대비 2k 토큰↑ 우위**(A100-80GB PCIe; 근거: Fig.10, §9.3).
* **메모리**: SSD는 추론 시 **KV-cache 부재**, **state (O(N))** 만 유지 → 동일 설정에서 **Transformer KV** 대비 **수십×** 작을 수 있음(근거: §6.3).

> 모든 수치와 주장에는 본문 §/Fig./Tab. 태그로 **증거**를 명시했습니다. 세부 도출식, 블록 설계, 병렬화 스킴은 §5–§8, Fig.7–10, Tab.1–4를 참조하세요.


### 토글을 클릭하면 논문에 대한 자세한 LLM 질의응답 내용을 확인할 수 있습니다.

<details markdown="block">
<summary>▶️<strong>클릭하여 펼치기</strong></summary>






## 프롬프트 1.1.1 (연구의 공백)

```
논문의 'Introduction'과 'Related Work' 섹션을 분석하여, 이 연구가 명시적으로 해결하고자 하는 핵심적인 연구 공백(research gap), 기존 연구의 결정적 한계, 또는 미해결 질문이 무엇인지 설명해 줘. 저자들이 설명하는, 이 논문 출판 시점의 '최신 기술(state of the art)'은 어떤 상태였는지 요약해 줘.
```


**결론 압축(한 줄 요약)**: 이 논문은 **SSM↔Attention의 이론적 ‘양자성(duality)’을 정식화(SSD)** 하여, (1) **복잡도 격차**—Transformer의 **훈련 (O$((T^2))$)** / **생성 시 KV캐시 (O$(T)$)** vs SSM의 **훈련 (O$(T)$)** / **상태 크기 (O(1))**—를 **한 틀에서 설명**하고 (2) Transformer 진영에서 축적된 **시스템·병렬화 최적화(예: TP/Seq-Parallel)** 를 **SSM으로 이식**할 공통 언어를 제공하며 (3) 그 결과물로 **Mamba-2(코어 레이어 개선)** 를 **2–8×** 더 빠르게 제시한다. (근거: §1, §5, §7–§9)    


## 1) 핵심 연구 공백 (Research Gap)

1. **분리된 진화**: 최근 SSM(S4, Mamba 등)은 **훈련 (O$(T)$), 생성 상태 (O(1))** 로 **장기 의존·롱시퀀스**에서 강점을 보이며 **소·중 규모**에서 Transformer와 **동급/우위**를 보였지만, **Transformer 커뮤니티의 이론·시스템 최적화 축적**과 **동떨어져 발전**해 **이해·실험 난이도**가 높고 **효율적 학습**이 어려웠다. (근거: §1) 
2. **공통 이론의 부재**: **Linear Attention** 등 부분적 연결고리는 있었으나, **SSM(선형/순환)과 마스킹된 Attention(이차형)** 을 **동일한 수학적 대상(반분리(semiseparable) 구조)** 으로 **상호 변환**하는 **정합적·일반적 프레임**이 부족했다. (근거: §1, §4–§5, Fig.1)  
3. **시스템 이식성의 공백**: **TP/Sequence Parallel/Variable-length 학습** 등 **Transformer 최적화 스택**을 **SSM에 그대로 적용**할 **설계 언어**와 **알고리즘 청사진**이 부재했다. (근거: §8 개요) 


## 2) 기존 연구의 결정적 한계

* **복잡도/메모리 병목(Transformer)**: **훈련 시 (O$((T^2))$)**, **생성 시 KV-cache (O$(T)$)** 로 길이 $(T)$ 증가에 **정량적 비용 급증**. (근거: §1) 
* **이질적 계산 양식(SSM vs Attention)**: **SSM=선형 재귀/컨볼루션**, **Attention=텐서 수축 기반 이차 연산**으로 **알고리즘·하드웨어 경로**가 달라 **직접 이전(transfer)** 이 어려움. (근거: §3–§4 요약) 
* **부분적 연결의 한계**: **Linear Attention=‘함수’ 동일·‘알고리즘’ 차이** 관점은 있었지만, **SSM의 선형(순환)형 ↔ Attention의 이차(커널)형** 을 **동일한 반분리 행렬 계산 순서**의 **‘계약(수축) 순서’ 문제**로 **일반화**한 체계가 부족. (근거: §4.3.1 요약) 


## 3) 이 논문이 제시하는 해법(공헌)과 그로 인한 해소

* **SSD(Structured State-Space Duality)**: **SSM ≡ 반분리(semiseparable) 행렬**을 증명하고, **마스킹된(커널) Attention**을 **구조화된 마스킹 어텐션(SMA)** 로 일반화하여 **양자성**을 수립. ⇒ **SSM의 선형-시간 알고리즘** ↔ **커널 어텐션의 이차-시간 알고리즘**이 **동일 함수의 다른 수축 순서**임을 명시. (근거: §3–§5, Fig.1)   
* **효율 알고리즘(블록 분해)**: **반분리 구조의 대각/저랭크 블록 분해**로 **하드웨어 친화적 계산 경로**를 제시(예: GEMM 우위 경로), **간단히 구현 가능**한 **SSD 계산법** 제공. (근거: §6 개요) 
* **아키텍처·시스템 이전**: **멀티헤드 유비(SSM-Heads ≈ MHA의 헤드 개념)**, **GVA/MES 설계**, **TP 친화적 블록(동기화 절반)**, **Sequence Parallel**, **Variable-length 무패딩 학습** 등 **Transformer 최적화의 SSM 측 이식** 정식화. (근거: §7–§8)  
* **실증(대표 수치)**: **Mamba-2 코어 레이어**가 **Mamba 대비 2–8×** 속도 향상(동일 조건), **언어모델링에서 경쟁력 유지**. (근거: Abstract) 


## 4) 출판 시점의 SOTA 요약 (저자 서술 기반)

| 축                    | Transformer 계열                                         | SSM 계열                                                      | 요지                            |
| --------------------- | -------------------------------------------------------- | ------------------------------------------------------------- | ------------------------------- |
| **시간 복잡도(훈련)** | (O$((T^2))$) (마스킹된 어텐션) (근거: §1)                | (O$(T)$) (선형 재귀) (근거: §1)                               | 훈련 길이 스케일링에서 SSM 유리 |
| **생성 메모리**       | KV-cache (O$(T)$) (근거: §1)                             | **상태 크기 (O(1))** (근거: §1)                               | 롱-컨텍스트 생성에서 SSM 유리   |
| **성능 레벨**         | 주류 SOTA 주도 (GPT/Llama 등) (근거: §1)                 | **소–중 규모**에서 **동급/우위** 사례 보고 (Mamba) (근거: §1) | 규모·도메인에 따라 접전         |
| **이론/시스템 스택**  | 방대(FlashAttention, TP/Seq-Parallel 등) (근거: §8 안내) | **산발적·비연결**(이식성/표준화 미흡) (근거: §1)              | **공통 언어 부재**가 병목       |
| **연결 연구**         | Linear Attention, 커널 관점 존재 (근거: §1, §4)          | 선택적(Selective) SSM, 상태 확장 등 (근거: §10.1)             | **전면적 양자성** 정식화 필요   |

> **요약**: 2024.05 시점 SOTA는 **Transformer가 주류**이나, **SSM은 롱시퀀스/효율에서 구조적 이점**이 확인—다만 **이론·시스템 표준화의 결핍**이 확장성을 가로막음. 본 논문은 이를 **SSD로 해소**하고 **속도·이식성**을 수치로 입증(Mamba-2 **2–8×**). (근거: §1, Abstract, §8–§10)    


## 5) Related Work에서의 위치 지정(요지)

* **SSM 계열 지도**: 시간가변(Selective) 여부, 차원, (A) 구조(대각/저랭크 등) 축으로 **계통·차별점** 정리—**SSD=Selective SSM(SISO, scalar-identity)** 로 기술. (근거: §10.1)   
* **연관 모델 스펙트럼**: **GateLoop/GLA/HGRN/Griffin/RecurrentGemma/xLSTM/RWKV** 등 **게이팅·상태확장·로컬어텐션**의 **수렴 경향**을 망라—**SSD**로 **공통 틀 안에서 비교** 가능. (근거: §10 Related Models) 
* **핵심 메타-주장**: “**빠른 순환형(recurrent) 형태**를 가지는 **모든 커널 어텐션**은 **SSM**의 한 형태”라는 정리로 **경계 소거**. (근거: §5 요약) 



### 참고 수식 (복잡도 대비)

$$
\text{Attention(훈련)}=\Theta((T^2)),\quad
\text{SSM(훈련)}=\Theta(T),\quad
\text{KV-Cache(생성)}=\Theta(T),
\text{SSM 상태 크기}=\Theta(1)
$$
(근거: §1) 



> 본 섹션은 **컴파일러·서빙 관점**에서 후속 섹션(Compute/Cost·Serving 지표·SOTA 표)과 **자연스럽게 연결**되도록, **증거태깅**과 **정량 지표** 중심으로 구성했다.





## 프롬프트 1.1.2 (핵심 가설)

```
이 논문의 중심 가설(central hypothesis) 또는 핵심 주장은 무엇인가? '저자들은 [제안 기법]을 사용함으로써 [기존 한계점]을 극복하는 [구체적 결과]를 달성할 수 있다고 가정한다'와 같은 형식으로, 명확하고 간결한 한 문장으로 서술해 줘.
```

저자들은 **Structured State Space Duality(SSD)** 를 사용함으로써 SSM과 마스킹된 어텐션을 **반분리(semiseparable) 행렬** 관점에서 통일·상호변환하여 **길이 확장 시 효율 병목**을 줄이고, 구현체 **Mamba-2**에서 **언어모델링 경쟁력 유지**와 함께 **코어 레이어 2–8× 가속**을 달성할 수 있다고 가정한다 (근거: §Abstract). ([arxiv.org][1])

[1]: https://arxiv.org/abs/2405.21060 "[2405.21060] Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"


## 프롬프트 1.2.1 (독창성 식별)

```
논문 전체를 바탕으로, 가장 중요하고 독창적인 기여(contribution) 1~3가지를 구별되는 항목으로 나열해 줘. 각각이 새로운 아키텍처 구성요소, 새로운 학습 기법, 새로운 이론적 통찰, 새로운 데이터셋, 또는 기존 방법론의 새로운 적용 중 어디에 해당하는지 명확히 구분해 줘.
```


**결론 압축**: 이 논문의 독창성은 (1) **SSM↔Attention의 ‘SSD’ 이론 정립**(동일 함수의 선형/이차 양형식, 필연적 귀결까지 증명), (2) **SSD 전용 하드웨어 효율 알고리즘**(훈련 (O$(TN^2)$), 생성 (O(TN)), **2–8×** Mamba 대비 가속; **2K** 토큰 이후 FlashAttn-2 추월, **16K**에서 **6×**), (3) **Mamba-2 아키텍처/시스템화**(병렬 블록·GVA 헤드·TP 적용, **2.7B @ 300B** 토큰 학습에서 동급·상위 모델 대비 우위)이다 (근거: §3–§6, §7, §9; Fig.1/6/10).     



### 1) SSD 이론 틀 정립 — *새로운 이론적 통찰*

* **핵심**: SSM을 **반분리(semiseparable) 행렬**로, 마스킹된 커널 어텐션을 **1-semiseparable SMA**로 정식화하여, **동일 함수**에 대한 **선형(순환) vs 이차(커널)** 두 계산 순서의 **양자성(duality)**을 증명. 더 나아가 **빠른(순환형) 커널 어텐션은 반드시 SSM**임을 제시. (근거: §5.1–§5.3, Fig.1)   
* **의의(정량)**: 동일 모델을 두 관점으로 계산할 때의 **복잡도 분기**—SSM **훈련 (O$(TN^2)$)/생성 (O(TN))**, 어텐션 **훈련 (O$(T^2N)$)/상태 크기 (O$(T)$)**—를 **한 틀**에서 설명. (근거: §6 요약표) 

### 2) SSD 전용 하드웨어 효율 알고리즘 — *새로운 효율 알고리즘*

* **핵심**: **블록 분해(대각/저랭크)** 기반 SSD 계산법을 제안해, **행렬곱(matmul) 우위**의 경로로 맵핑(텐서 코어 활용). 이 알고리즘은 **훈련 (O$(TN^2)$)**, **추론 (O(TN))**, **추론 메모리 (O(N^2))** 보장(Theorem 6.1). (근거: §6, Thm 6.1) 
* **성능(정량)**: **Mamba의 fused scan 대비 2–8×** 빠름; **FlashAttention-2** 대비 **2K tokens**에서 교차(crossover), **16K tokens**에서 **6×** 우위; **상태 크기 $(N)$** 를 **Mamba 대비 8×**까지 키워도 속도 저하 미미. (근거: §9.3 Fig.10, §6)  

### 3) Mamba-2 아키텍처 & 시스템 이전 — *새로운 아키텍처/기존 방법의 체계적 적용*

* **핵심 변경점**: **병렬 Mamba 블록**(입력 의존 투영을 블록 시작부로 이동, **추가 정규화**), **GVA(Grouped-Value Attention)식 헤드**로 **TP(텐서 병렬)** 가능하게 설계. (근거: §7.1, Fig.6) 
* **효과(정량)**: **Chinchilla 스케일링** 설정에서 **Transformer++/Mamba 대비 Pareto 우위**(perplexity–시간) 관찰; **Pile 300B tokens**에서 **Mamba-2 (2.7B Params)** 가 **Mamba-2.8B/Pythia-2.8B/6.9B**를 다수 벤치에서 상회. (근거: §7 개요, §9.2) 

> 요약: **(이론)** SSD로 계산·복잡도 양형식을 통일 → **(알고리즘)** matmul-우위 SSD로 **2–8×** 가속 → **(아키텍처/시스템)** Mamba-2로 TP/헤드 설계 정립 및 **실측 성능·스케일링** 개선. (근거: §5–§7, §9)   



## 프롬프트 1.2.2 (저자 관점에서의 강점)

```
저자들의 관점에서, 자신들의 접근법이 이전 방법들보다 우월한 이유는 무엇인가? 그들이 자신들의 연구가 지닌 독창성과 강점을 뒷받침하기 위해 사용하는 핵심 논거를 인용하거나 알기 쉽게 설명해 줘.
```

**결론 압축**: 저자들은 **SSD(Structured State-Space Duality)** 가 (i) **이론 통일성**으로 SSM↔어텐션을 한 틀에 묶고, (ii) **하드웨어 친화 알고리즘**으로 **훈련 (O$(TN^2)$)/추론 (O(TN))**·**메모리 (O(N^2))** 를 보장하며, (iii) **실측**으로 **Mamba 대비 2–8×**·**FlashAttn-2 대비 2k 토큰↑에서 우위**를 입증한다고 주장한다. (근거: §5–§6, §9.3/Fig.10)   



## 저자 관점의 강점 (핵심 논거)

1. **이론적 통일성 & 일반성(독창성) — “SSM↔어텐션의 양자성”**

* **1-semiseparable SMA ≡ 대각 SSM의 특수형**(Cor. 5.1)이며, **효율적 자기회귀 어텐션 ⇒ 반분리(Semiseparable) 마스크**가 필요(Thm 5.2)라는 양방향 결과로, **선형/이차 두 계산형식**을 동일 함수의 **수축 순서** 차이로 정식화. (근거: §5.2–§5.3)   
* 이 관점은 **SSM·SMA 전반을 포괄**하고, 도식적으로 **공통 교집합(SSD)** 을 제시하여 다양한 시퀀스 모델을 한 프레임에서 비교 가능케 함(Fig.4). (근거: §5 개요/Fig.4) 

2. **하드웨어 친화적 SSD 알고리즘 — “Matmul 우위 + 복잡도 보장”**

* **Theorem 6.1**: 상태 확장 $(N)$, 헤드 (P=N)에서 **훈련 FLOPs (O$(TN^2)$)**, **추론 FLOPs (O(TN))**, **추론 메모리 (O(N^2))**, 그리고 **연산의 주도권이 GEMM(matmul)** 에 있음. (근거: §6/Thm 6.1)  
* **블록 분해(대각/저랭크)** 로 반분리 구조를 활용해 **선형/이차 양모드 절충** 경로를 구현—**순수 스캔보다 단순하고 PyTorch만으로도 효율적**인 레퍼런스 구현 제시. (근거: §6 요지) 

3. **실측 성능(속도·스케일링) — “2–8×, 2k 교차, 16k에서 ~6×”**

* **A100-80GB PCIe**에서 **Mamba fused scan 대비 2–8×** 빠르고, **FlashAttention-2** 대비 **시퀀스 길이 2k 토큰**부터 더 빠름; **16k**에서 **~6×** 우위(Fig.10 좌/우). (근거: §9.3/Fig.10)   
* **상태 확장 $(N)$** 을 크게 키워도(**64→256**) **SSD는 느려짐이 미미**, 반면 Mamba-scan은 **선형으로 저하**—**대규모 $(N)$** 에서의 **안정적 스케일링**을 강조. (근거: Fig.10 우) 

4. **언어모델링·효율 파레토 — “동급/우위 곡선”**

* **2.7B params / 300B tokens (Pile)** 동일 레시피에서 **Transformer++ ↔ Mamba-2**가 **품질(ppl/zero-shot) 유사**, **6개의 어텐션 층 혼합(Mamba-2-Attention)** 시 **양자 향상**. (근거: §9.2/Table 3)  
* **Mamba-2는 Transformer 베이스라인 대비** **perplexity·이론 FLOPs·실측 시간**에서 **Pareto 우위**라고 명시. (근거: §9 종합) 

5. **어려운 기억 과제(MQAR)·대상 확장 — “N↑ ⇒ 성능↑”**

* **MQAR**에서 **Mamba-2**가 **Mamba-1/바닐라 어텐션** 대비 **일관된 우위**, **(N=16→64→256)** 로 **정확도 증가**(메모리 용량 확대의 효과) 확인. (근거: §9.1/Fig.8)  

6. **대규모 학습·서빙 시스템 이식성 — “TP/SP/변동 길이”**

* **TP 친화 설계**: **블록 당 동기화 포인트 1회(절반)**, **GroupNorm** 채택으로 **추가 통신 회피**. (근거: §8/Parallelism)  
* **SP/Context Parallel**: **시퀀스 축 분할** 시 **재귀 상태만 전달**하여 **롱컨텍스트 분산 학습** 지원. (근거: §8.2) 
* **가변 길이 무패딩 학습·추론**: **패딩 제거/정렬 없이** 효율적 파이프라인 구현. (근거: §8 요지) 

7. **주의(Attention) 기법과의 호환성 — “커널/정규화 흡수”**

* **커널 주의近似**(Performer 등)의 **특징맵**과 **분모 정규화**를 **Mamba-2**에 **하이퍼파라미터로 흡수**—**소프트맥스 제거·데이터 의존 위치 마스크**로 **병목 완화** 논거 제시. (근거: §4.1.3, §7 설계 메모)   



**요약(수치 중심)**: **A100-80GB**에서 **2–8×**(vs Mamba-scan), **≥2k 토큰**에서 **FlashAttn-2 추월**, **16k**에서 **~6×**, **2.7B/300B** 학습에서 **Transformer++ 동급 품질**·**Pareto 우위**(ppl·FLOPs·벽시계). (근거: §9.3/Fig.10, §9.2)   




## 프롬프트 1.3.1 (알고리즘 단계별 설명)

```
"핵심 알고리즘, 모델 아키텍처, 또는 주요 방법론을 단계별(step-by-step)로 설명해 줘. 독자는 AI 분야의 대학원생 수준이라고 가정해. 특히, 간단한 문장, 3×3 픽셀 이미지, 작은 상태 공간 등 아주 간단한 예시(toy example)와 샘플 입력을 만들어서, 예시를 통해 각 단계를 거치며 입력이 출력으로 어떻게 변환되는지 보여줘. 등장하는 모든 핵심 용어와 변수는 그 즉시 정의해 줘."
```


**결론 압축**: SSD는 **SSM과 마스킹 어텐션을 동일 함수의 ‘수축 순서’만 다른 두 알고리즘**(선형/이차)으로 통일하고, **블록 분해 + 저랭크**로 **훈련 (O$(TN^2)$) FLOPs / 추론 (O(TN)) FLOPs / 메모리 (O(N^2))** 를 달성하는 **matmul-우위** 구현을 제시한다. 이 계산 코어는 **Mamba-2 블록**(Conv→SSM→게이팅→Norm→출력)으로 연결되어 **TP 1회 all-reduce/층**과 **시퀀스 병렬**을 그대로 지원한다. (근거: §5 요약; Thm 6.1; Fig.7)   



## A. 핵심 객체와 표기(정의)

* **시퀀스 길이** $(T)$ (tokens), **헤드/특징 차원** (P), **상태 확장** $(N)$, **블록 크기** (Q). (근거: §6 개요) 
* **SSM 파라미터**: $(A_t\in\mathbb{R}^{N\times N}), (B_t,C_t\in\mathbb{R}^N)$ (시간 종속 가능). (근거: Def.3.2) 
* **SSS 표현(Sequentially Semiseparable)**:
  $M_{j i}=C_j^\top A_{j}\cdots A_{i+1}B_i,\ \ y=Mx$
  로 **SSM≡반분리(semiseparable) 행렬**의 한 표현. (근거: Eq.(3), Def.3.1/3.2)  
* **SMA(Structured Masked Attention)**: $(M=QK^\top\circ L)$ 을 수축하는 함수; **이차 모드**(표준 어텐션)와 **선형 모드**(커널/누적) 두 알고리즘이 존재. (근거: §4.3.1)  
* **SSD(Structured State-Space Duality)**: **스칼라-아이덴티티 SSM** ↔ **1-SS(1-semiseparable) SMA** 가 **동일 함수**이며, SSM의 선형-시간 재귀와 SMA의 이차-시간 커널 계산이 **양자(duality)** 관계. (근거: §5.1–5.3)   

> 요점: **반분리 행렬**은 하삼각 모든 부분행렬의 **랭크 ≤ N**을 갖고(정의), **오프대각 블록이 저랭크**여서 빠른 알고리즘이 가능. (근거: Def.3.1; Lemma/Prop.3.x)  



## B. “두 경로”로 같은 함수를 계산하기

### B-1) **이차(Attention) 경로** — SMA의 표준 수축

* **순서**: $(G=QK^\top) → (M=G\circ L) → (Y=M V)$. (근거: Eq.(13); 도식 요약) 
* **복잡도**: **훈련 (O(T^2P))**, **상태 크기 (O$(T)$)** (KV-cache). (근거: 비교표) 

### B-2) **선형(SSM) 경로** — 스캔/재귀

* **스칼라 SSM**(1-SS)의 재귀는 **누적 형태**로 **선형 시간**: $(h_t=a_t h_{t-1}+b_t x_t;\ y_t=c_t h_t)$. (근거: §3.2.2 개요) 
* **동일 함수성**: 1-SS SSM의 **나이브(이차) 행렬곱**은 **마스킹 커널 어텐션과 동일**. (근거: §5.1 Eq.(16) 부연) 



## C. **SSD 블록 알고리즘**(핵심 단계)

> 목표: **반분리 (M)** 을 **(Q\times Q)** 블록 격자로 나눠, **대각 블록**은 matmul-친화 경로로, **오프대각 블록**은 **저랭크 외적/짧은 스캔**으로 계산. (근거: Thm 6.1, 블록 아이디어) 

1. **블록 분해**: $(M\in\mathbb{R}^{T\times T}) 를 ((T/Q)\times(T/Q))$ 블록으로 분할. (근거: §6) 
2. **대각 블록** $(M_{bb})$:

   * **마스크 (L)** 와 **로컬 Gram**(또는 $(CB^\top))$을 사용해 **블록 내** $(M_{bb})$ 계산 → **BMM**(배치 matmul)로 구현. (근거: §6 단계 요약) 
3. **오프대각 블록** $(M_{b>b'})$: **저랭크** $(\Rightarrow)$ **Right/Center/Left** 3-단계

   * **Right**: 우요인 곱 $( \mathrm{BMM}(T/Q,;N,P,Q))$. (근거: §6) 
   * **Center**: **1-SS 스캔** $(O(TN P/Q))$ — 길이가 **(T/Q)** 로 **짧아짐**. (근거: §6) 
   * **Left**: 좌요인 곱 $( \mathrm{BMM}(T/Q,;Q,P,N))$. (근거: §6) 
4. **총 비용**(대표 설정 (N=P=Q)): **훈련 (O$(TN^2)$) FLOPs**, **메모리 (O(TN))**, 작업은 **(N,N) matmul** 위주. (근거: §6 Total Cost) 
5. **정리(Thm 6.1)**: (P=N)일 때 **훈련 (O$(TN^2)$)**, **추론 (O(TN))**, **추론 메모리 (O(N^2))**, **주연산=matmul**. (근거: Thm 6.1) 

> **비교(요지)**: **어텐션** $(T^2N)$ FLOPs↔**SSD/선형SSM** $(TN^2)$ FLOPs, **상태 크기**: $(T)$↔$(N)$. (근거: 비교표) 



## D. **토이 예시** — 3×3 픽셀 → 9-토큰, 1-SS(스칼라 SSM) vs 1-SS SMA

### D-0) 셋업

* **입력 이미지(정수 intensity)** $(3\times3)$ → **row-major 플래튼** $(x_{1:9}=[1,2,3,4,5,6,7,8,9])$ (무단위 예시).
* **스칼라 SSM**: (a=0.5) (감쇠), $(b_t=c_t=1)$ (고정). 상태 $(h_t\in\mathbb{R})$.
* **동일 함수의 SMA(1-SS)**: 마스크 $(L_{j,i}=a^{,j-i})$ if $(j\ge i)$, else 0.

### D-1) **선형(SSM 재귀) 경로** — 한 토큰씩 스캔

$$
h_t=0.5,h_{t-1}+x_t,\quad y_t=h_t,\quad h_0=0
$$

* $t=1$: $h_1=0+1=1.0\Rightarrow y_1=1.0$
* $t=2$: $h_2=0.5\cdot1+2=2.5\Rightarrow y_2=2.5$
* $t=3$: $h_3=0.5\cdot2.5+3=4.25\Rightarrow y_3=4.25$
* $t=4$: $6.125$
* $t=5$: $8.0625$
* $t=6$: $10.03125$
* $t=7$: $12.015625$
* $t=8$: $14.0078125$
* $t=9$: $16.00390625$

### D-2) **이차(SMA) 경로** — 1-SS 마스크 행렬곱

$$
y_j=\sum_{i=1}^j a^{,j-i} x_i
$$

* 예: $j=3$일 때 $(y_3=[a^2,a^1,1]\cdot[1,2,3]=0.25+1+3=4.25)$.
* 전체 $(j=1..9)$의 $(y_j)$는 D-1과 **완전히 일치**(수치 동일).

> **관찰**: **같은 함수**를 **두 수축 순서**로 계산—**재귀(선형)** vs **마스크 행렬곱(이차)**. (근거: §5.1 결론) 

### D-3) **블록/저랭크 맛보기** — (Q=3) 블록으로 쪼개기


* 수치: $(u=[1,0.5,0.25],\ v=[0.125,0.25,0.5])$.
  $(\ v^\top x_{1:3}=0.125\cdot1+0.25\cdot2+0.5\cdot3=2.125)$.
  $(\ \Rightarrow) 해당 블록 기여 (=u\cdot2.125=[2.125,\ 1.0625,\ 0.53125])$.
* 이렇게 **모든 오프대각을 저랭크로 처리**하고, **대각은 로컬 matmul**로 처리하면 **SSD 블록 알고리즘**이 된다. (근거: 세미세퍼러블 오프대각 저랭크 성질) 



## E. **Mamba-2 블록** — SSD 코어를 감싼 레이어(학습/서빙 친화)

Mamba-2 레이어의 **한 헤드 경로**(모든 텐서는 길이 (L) 배치):

1. **투영**: $(x^{(i)}=u W^{(x)}_i\in\mathbb{R}^{L\times e_d/2})$, $(z^{(i)}=u W^{(z)}_i)$. (근거: Fig.7 식) 
2. **로컬 Conv1D**: $(x_c^{(i)}=\mathrm{conv1d}(x^{(i)}))$. (근거: Fig.7 식) 
3. **SSD/SSM 혼합**: $(y^{(i)}=\mathrm{SSM}_{A,B,C,\Delta}(x_c^{(i)}))$. (근거: Fig.7 식) 
4. **게이팅**: $(y_g^{(i)}=y^{(i)}\cdot\phi(z^{(i)}))$. (근거: Fig.7 식) 
5. **정규화**: $(\mathrm{GroupNorm}(y_g^{(i)}))$ — **TP 차수로 나눠떨어지는 그룹 수**. (근거: Fig.7 캡션) 
6. **출력 투영/집계**: $( \text{out}^{(i)}=y_g^{(i)} W^{(o)}_i)$, **모든 GPU 간 all-reduce 1회/층**. (근거: Fig.7 캡션) 

**병렬성**

* **TP(텐서 병렬)**: **헤드/투영 분할**, **GroupNorm**으로 추가 통신 회피, **층당 all-reduce 1회**. (근거: Fig.7 설명) 
* **SP/CP(시퀀스/컨텍스트 병렬)**: 시퀀스 축 분할, **블록 경계에서 상태만 전달**. (근거: §8.2) 



## F. 요약 (복잡도·계산 경로)

| 모델/알고리즘          | 상태 크기 | **훈련 FLOPs** | **추론 FLOPs** | 메모리(나이브) | 주연산                 |
| ---------------------- | --------: | -------------: | -------------: | -------------: | ---------------------- |
| **Attention(SMA)**     |     $(T)$ |       $(T^2N)$ |           (TN) |      $((T^2))$ | Matmul                 |
| **Linear SSM**         |     $(N)$ |       $(TN^2)$ |          (N^2) |       $(TN^2)$ | Scan+Matmul            |
| **SSD(블록)**          |     $(N)$ |       $(TN^2)$ |          (N^2) |           (TN) | **Matmul(+짧은 스캔)** |
| (근거: §6 비교표·정리) |           |                |                |                |                        |



### 체크 포인트 (용어/기호)

* **1-SS**: $(C_j,B_i)$가 **스칼라**인 반분리(랭크-1) 케이스. (근거: §3.2.2) 
* **SSD=1-SS SMA ↔ 스칼라-아이덴티티 SSM**(정확한 특수화 관계). (근거: Cor.5.1) 
* **효율적 자기회귀 어텐션** ⇒ **반분리 마스크 (L)** 가 **필요충분**(경계 성질). (근거: Thm 5.2) 

> **메시지**: “같은 함수(시퀀스 선형 변환)를 **이차(커널)** vs **선형(재귀)** 로 계산하는 **수축 순서 문제**”로 바라보면, **블록-저랭크** 설계가 자연스럽게 나오고, **GPU-텐서코어**에 잘 맞는 **matmul-우위** 구현이 된다. (근거: §4.3.1 요약; §6 Thm 6.1)  



> 위 예시는 **직관·단위 테스트용 토이**이며, 모든 정량 주장은 본문 각 절/정리/그림으로 **증거 태깅**하였다.




## 프롬프트 1.3.2 ('비밀 병기' 식별)

```
"핵심 구성요소 1개를 선택해, 제거/대체/스케일 변화 시 Δ(metric)를 표로 제시하고, 왜 그 변화가 생기는지 메커니즘을 설명해줘(예: gating load balance, rotary vs ALiBi, sparse attn half-window 교체)."
```

**Mamba-2 블록의 “병렬 프로젝션 + Extra Normalization(GroupNorm)”**

**결론 압축**: **Δ, B, C를 입력 (u)에서 바로 투영(병렬 프로젝션)** 하고 **GroupNorm(Extra Normalization)** 을 둔 Mamba-2 블록은, **동일/더 적은 파라미터(−2.2%)** 로 **perplexity −0.27** 개선(11.76→11.49)하며, **TP 통신을 블록당 all-reduce 2→1회**로 절반화한다. 이는 **추가 all-reduce를 유발하던 Mamba-1의 구조적 병목**을 제거한 결과다. (근거: §9.4 Tab.4, Fig.7/§8.1)   



## 1) Ablation 표 — 구성요소 제거/대체/스케일 변화에 따른 Δ(metric)

### (A) 블록 내부 설계 변화(파라미터·정규화·프로젝션)

| Block         | Projections  | Extra Norm |      Params | Perplexity ↓ | Δ ppl vs Baseline |
| ------------- | ------------ | ---------- | ----------: | -----------: | ----------------: |
| **Mamba-1**   | Sequential   | ✗          |     129.3 M |    **11.76** |                 — |
| Mamba-1       | Sequential   | ✓          |     129.3 M |        11.54 |         **−0.22** |
| Mamba-2(변형) | Parallel     | ✗          |     126.5 M |        11.66 |             −0.10 |
| **Mamba-2**   | **Parallel** | **✓**      | **126.5 M** |    **11.49** |         **−0.27** |

(근거: §9.4 Tab.4 “Ablations: Mamba-2 block”) 

> 해석: **(i)** 정규화 추가만으로도 **−0.22** 개선. **(ii)** 병렬화만으론 −0.10이지만, **병렬화+정규화 동시 적용** 시 **최대 개선(−0.27)** 과 **파라미터 −2.2%**(129.3→126.5 M). (근거: §9.4 Tab.4) 

### (B) 텐서 병렬(TP) 통신 라운드 변화

| 구조        | Δ,B,C 계산 경로                       | 블록당 all-reduce 횟수 | 메커니즘 요지                                                                                                                               |
| ----------- | ------------------------------------- | ---------------------: | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Mamba-1** | **$(x_c)$(conv 결과)**에서 Δ,B,C 산출 |               **2 회** | Δ,B,C 계산 전에 **$(x_c)$** 를 **TP 전역 집계(추가 all-reduce)** 해야 함 → 통신 2배. (근거: Fig.7/설명)                                     |
| **Mamba-2** | **입력 (u)** 에서 **직접** Δ,B,C 투영 |               **1 회** | Δ,B,C를 **GPU별 그룹**으로 독립 계산, **마지막에만 all-reduce**. **GroupNorm** 채택으로 **블록 내 추가 통신 없음**. (근거: Fig.7 캡션/§8.1) |



## 2) 왜 이런 변화가 생기는가? — 메커니즘 설명

1. **프로젝션 위치 이동(Sequential→Parallel)**

   * **Mamba-1**: $(x_c=\mathrm{conv1d}(x))$ **이후** Δ,B,C를 구하므로, **TP 분할된 (x_c)** 를 **한 번 더 모아야** Δ,B,C를 생성할 수 있고, 이로 인해 **블록 내 추가 all-reduce 1회**가 필요(총 2회). (근거: Fig.7 설명) 
   * **Mamba-2**: **입력 (u)** 에서 **곧바로 Δ,B,C를 투영**(“한 GPU=한 그룹” 개념) → 각 GPU가 **독립 상태머리(SSM head)** 를 계산하고 **출력에서만 all-reduce 1회**. **통신량/동기화 지연 감소**가 학습 효율에 직결. (근거: Fig.7/§8.1)  

2. **Extra Normalization = GroupNorm(GN)**

   * **TP 친화 정규화**: GN의 **그룹 수를 TP 차수로 나눠떨어지게** 설정해 **크로스-GPU 통신 불필요**. **Residual/Norm** 연산은 **Sequence Parallel(SP)** 까지 그대로 적용 가능. (근거: §8.1–8.2)  
   * **품질 개선(−0.22~−0.27 ppl)**: Conv→SSM→Gating 사이 **분포 변동을 완충**하여 **상태 (h_t)** 의 스케일 안정화 → **수렴·일반화** 향상으로 귀결(정량은 Tab.4). (근거: §9.4 Tab.4) 

3. **스케일링/실무 효과(부가)**

   * 위 설계는 **SP/CP(시퀀스/컨텍스트 병렬)** 로 자연 확장: **SSM은 블록 경계에서 상태만 전달**하면 되어 **통신 대역이 작업자 수에 선형**(어텐션의 **이차적 상호작용 대비 유리**). (근거: §8.2) 



## 3) 추가 검증 신호(맥락)

* **효율 벤치**: SSD 코어는 **FlashAttn-2** 대비 **≥2k 토큰**부터 빠르고, **16k**에서 **~6×** 우위. 이는 **블록 분해 + matmul 우위** 경로의 시스템적 효과와 결합해 **대상 확장$(N)$** 시에도 **속도 저하가 미미**함을 보인다. (근거: Fig.10/§9.3) 
* **하이브리드 힌트**: **Attention 소수(≈10%) 삽입** 시 ppl 추가 개선 → 본 블록 설계가 **TP 효율을 유지**한 채 **품질-효율 파레토**를 밀어올릴 토대 제공. (근거: §9.2.3 Table 2/3)  



### 요약(한 줄)

**Mamba-2 블록의 “입력-기반 병렬 프로젝션 + GroupNorm”은** ppl **−0.27**(11.76→11.49), **params −2.2%**, **TP all-reduce 2→1회**를 동시에 달성하는 **핵심 비밀 병기**다. (근거: §9.4 Tab.4, Fig.7)  




## 프롬프트 1.4.1 (핵심 결과 분석)

```
"'Experiments' 또는 'Results'의 표/그림을 포함한 주요 결과를 분석해 줘. 핵심 성능 지표는 무엇인가? 어떤 벤치마크에서 보고되었는가? 저자들이 성공 증거로 가장 강조하는 결과를 요약해 줘."
```

# 프롬프트 1.4.1 — 핵심 결과 분석

**결론 압축**: (i) **언어모델링 품질**—Mamba-2는 **동일/더 작은 파라미터**에서 Mamba 대비 **ppl ↓**(예: 1.3B vs 1.4B, **6.66 vs 6.80**)이며, 동급 규모의 **Transformer++**와 **동등**(2.7B·300B 토큰) 또는 **하이브리드(주의 6층)** 구성 시 **우위**(**ppl 5.95**)를 보인다 (근거: Tab.1/Tab.3).  
(ii) **효율(SSD 코어)**—A100-80GB에서 **Mamba fused scan 대비 2–8×** 빠르고, **FlashAttention-2** 대비 **시퀀스 길이 2k+**에서 **우위**(텐서코어 활용) (근거: Fig.10/§9.3).  
(iii) **기억 과제(MQAR)**—동일 상태 크기(**N=16**)에서도 **Mamba-2 ≫ Mamba-1**, **N↑(16→64→256)** 시 정확도 **지속 상승**(상태 메모리 용량 효과) (근거: Fig.8/§9.1).  
(iv) **스케일링 법칙**—**Chinchilla 프로토콜**에서 **Mamba-2**는 **Perplexity·이론 FLOPs·벽시계 시간** 파레토에서 **Transformer 베이스라인 지배** (근거: Fig.9). 



## 핵심 성능 지표 & 벤치마크

* **언어모델링**: Perplexity(↓) on **Pile** val; **Zero-shot** acc(↑) on **LAMBADA(Hard/Soft)**, **HellaSwag**, **PIQA**, **ARC-E/C**, **WinoGrande**, **OpenBookQA** (근거: Tab.1/Tab.3).  
* **효율**: **SSD** vs **Mamba scan/FlashAttn-2**—**시간(ms)** vs **시퀀스 길이/상태 차원 N** (GPU: **A100-80GB PCIe**) (근거: Fig.10). 
* **기억/검색 능력**: **MQAR** 정확도(↑)—키-값 연상 회상 난이도 증대 버전 (근거: Fig.8/§9.1).  



## 주요 결과(표·그림 요약)

| 영역                      | Metric                       | 세팅                         | 모델                                       |                                                결과(수치) |                                                                               Δ vs 기준 |
| ------------------------- | ---------------------------- | ---------------------------- | ------------------------------------------ | --------------------------------------------------------: | --------------------------------------------------------------------------------------: |
| **언어모델링(Zero-shot)** | Pile ppl ↓, 평균 acc ↑       | ~1.3–1.4B, Pile              | **Mamba-2-1.3B**                           |                      **ppl 6.66**, LAMBADA 5.02, Avg 56.4 | vs **Mamba-1.4B**: ppl **−0.14**(6.80→6.66), LAMBADA **−0.02**, Avg **=** (근거: Tab.1) |
| **스케일링**              | Perplexity–FLOPs–Time        | 125M–1.3B, 8k ctx            | **Mamba-2**                                | **Transformer++** 대비 **Pareto 우위**(ppl↓/FLOPs↓/시간↓) |                                                                         — (근거: Fig.9) |
| **하이브리드(2.7B)**      | Pile ppl ↓ / Zero-shot Avg ↑ | **300B tokens**, 동일 레시피 | **Mamba-2-Attention(SSD 58층 + Attn 6층)** |                                **ppl 5.95**, Avg **61.0** |             vs Mamba-2: **ppl −0.14**(6.09→5.95), **Avg +0.8**(60.2→61.0) (근거: Tab.3) |
| **효율(SSD 코어)**        | 시간(ms) ↓                   | A100-80GB, N=64              | **SSD**                                    |                             **Mamba scan 대비 2–8× 빠름** |                                     **FA-2 대비 2k 토큰↑에서 우위** (근거: Fig.10/§9.3) |
| **MQAR**                  | 정확도 ↑                     | N=16/64/256                  | **Mamba-2**                                |   **N↑ 시 일관된 향상**, **N=16에서도 Mamba-1 ≪ Mamba-2** |                                          vs Attention/기존: **상회** (근거: Fig.8/§9.1) |



## 저자들이 강조하는 “성공 증거”

1. **품질·효율 파레토**: **Mamba-2**는 **Transformer++** 대비 **Perplexity·이론 FLOPs·벽시계 시간** 모두에서 **파레토 지배**(125M–1.3B, 8k ctx) (근거: Fig.9). 
2. **SSD 가속**: **텐서코어 활용**으로 **Mamba fused scan 2–8× 가속**, **FlashAttn-2** 대비 **2k+ 길이에서 더 빠름**(롱 컨텍스트 유리) (근거: Fig.10/§9.3).  
3. **대규모 학습 재현(동일 레시피)**: **2.7B·300B 토큰**에서 **Transformer++ ≈ Mamba-2**, **주의 6층 혼합** 시 **추가 개선**(ppl 6.09→**5.95**) (근거: Tab.3).   
4. **기억·장기 의존**: **MQAR**에서 **Mamba-2 ≫ Mamba-1**, **상태 확장 $(N)$** 이 **정확도 상승**을 직접 유발(메모리 용량 증가) (근거: Fig.8/§9.1). 

> 보조 신호: **Ablation**에서 **병렬 프로젝션 + Extra Normalization**이 **ppl 11.76→11.49**(−0.27) 및 **TP 친화성**을 동시에 달성(파라미터 **−2.8M**)—블록 설계의 기여를 분리 확인 (근거: Tab.4).  



**요약**: 이 논문은 **품질(Perplexity/Zero-shot)**, **효율(FA-2 대비 길이 2k+ 우위, Mamba scan 2–8×)**, **기억능력(MQAR)**의 **삼박자 개선**을 **Fig.8–10, Tab.1/3/4**로 실증하며, **2.7B·300B 토큰** 스케일에서의 **재현 가능성**까지 제시한다. (근거: Fig.8–10, Tab.1/3/4)     


## 프롬프트 1.4.2 (비판적 비교)

```
"제안된 방법론은 논문에서 언급된 주요 베이스라인 및 SOTA 모델들과 비교하여 어떤 성능을 보이는가? 우월성 주장을 가장 강력하게 뒷받침하는 특정 비교 지점을 식별해 줘. 반대로, 능가하지 못했거나 개선이 미미했던 결과가 있다면 이유를 정리해 줘."
```

# 프롬프트 1.4.2 — 비판적 비교 (Baselines / SOTA 대비)

**결론 압축**:

* **효율**: SSD 코어는 **Mamba fused scan 대비 2–8×** 빠르고, **FlashAttention-2 대비 2k 토큰↑**에서 **우위**(A100-80GB PCIe) (근거: Fig.10). 
* **품질(2.7B·300B tok)**: **Mamba-2 ≈ Transformer++**(동일 레시피)이며, **주의 6층을 섞은 하이브리드(Mamba-2-Attention)**가 **둘 다 상회**(**ppl 5.95**, avg 61.0) (근거: Tab.3).  
* **중형(≈1.3–1.4B)**: **Mamba-2-1.3B ppl 6.66**으로 **Mamba-1.4B 6.80** 대비 개선, **동일/근접 avg**(56.4) (근거: Tab.1). 
* **기억(MQAR)**: **Mamba-2 ≫ Mamba-1**, **N=16→64→256**로 **정확도 지속 ↑**(상태용량 효과) (근거: Fig.8/§9.1). 


## SOTA/베이스라인 대비 정량 비교표

| 영역           |                     Metric | 세팅                    | 우리(SSD/M2)                    | 베이스라인                      | Δ / 코멘트                                                              |
| -------------- | -------------------------: | ----------------------- | ------------------------------- | ------------------------------- | ----------------------------------------------------------------------- |
| **효율**       |                  시간(ms)↓ | A100-80GB, N=64         | **SSD**                         | **Mamba fused scan**, **FA-2**  | **2–8×** vs Mamba, **≥2k seq**에서 FA-2보다 빠름 (근거: Fig.10).        |
| **품질(중형)** | Pile ppl↓ / Zero-shot avg↑ | ~1.3–1.4B, 300B tok     | **M2-1.3B: ppl 6.66, avg 56.4** | Mamba-1.4B: 6.80, 56.4          | ppl **−0.14**, avg 동률 (근거: Tab.1).                                  |
| **스케일링**   |      ppl–FLOPs–시간 파레토 | 125M–1.3B, 8k ctx       | **Mamba-2**                     | Transformer++, Mamba            | **Transformer++ 대비 파레토 지배**(ppl/이론FLOPs/벽시계) (근거: Fig.9). |
| **대형(2.7B)** |           Pile ppl↓ / avg↑ | 2.7B, **300B tok**      | **M2: 6.09/60.2**               | **Transformer++: 6.13/60.2**    | 거의 동급 (근거: Tab.3).                                                |
| **하이브리드** |           Pile ppl↓ / avg↑ | 2.7B, 300B tok          | **M2-Attn: 5.95/61.0**          | M2: 6.09/60.2, Trf++: 6.13/60.2 | **둘 다 상회**(주의 6층≈10%) (근거: Tab.3, Tab.2).                      |
| **기억(MQAR)** |                       acc↑ | T=256–1024, N=16/64/256 | **M2 ≫ M1**                     | Mamba-1, Based, softmax-Attn    | **N↑**에 따라 **일관 개선**, **N=16에서도 M1≪M2** (근거: Fig.8/§9.1).   |

> 핵심 성능 지표: **Perplexity(↓)**, **Zero-shot 평균 정확도(↑)**, **시간(ms)/스루풋**(효율), **MQAR 정확도(↑)**. (근거: Tab.1/3, Fig.9–10, §9.1)    


## 우월성 주장을 가장 강하게 뒷받침하는 “비교 지점” Top-3

1. **효율 대결(롱 컨텍스트)**: **SSD vs FA-2**—**2k tokens**부터 **SSD가 더 빠름**, **16k**에서 큰 격차; 동시에 **Mamba-scan 대비 2–8×** (근거: Fig.10). (근거: Fig.10) 
2. **동일 레시피 대형 학습(2.7B·300B tok)**: **M2 ≈ Trf++**이며, **주의 6층 혼합(M2-Attn)**이 **둘 다 상회**(**ppl 5.95**, avg 61.0) → **품질·효율 파레토 개선의 여지** (근거: Tab.3). 
3. **기억 난이도(MQAR)**: **M2가 M1을 전범위에서 상회**, **N↑**가 **정확도↑**로 직결—**상태용량의 실질적 가치** 입증 (근거: Fig.8/§9.1). 



## 반대로, **능가하지 못했거나** 개선이 **미미**했던 부분과 이유

* **Transformer와의 품질 ‘동률’ 구간**: **2.7B·300B tok**에서 **M2 vs Transformer++**는 **ppl 6.09 vs 6.13**, **avg 60.2 vs 60.2**로 사실상 **동급**(우월은 아님). **주의 6층**을 섞어야 **우위**가 안정적 (근거: Tab.3). (근거: Tab.3) 

  * **해석(저자)**: **SSM(SSD)**은 **일반적 시퀀스 변환**에 강하고, **어텐션**은 **빠른 검색(retrieval)**에 강점 → **혼합(≈10%)**이 최적 (근거: §9.2.3/Tab.2 설명). (근거: §9.2.3, Tab.2)  

* **짧은 컨텍스트(≈2k) 학습 효율**: **모델 전체가 SSD 블록**인 **M2**는, **MLP가 절반인 Transformer**보다 **짧은 길이에서 학습 효율이 덜 좋을 수 있음**(MLP는 matmul/pointwise로 하드웨어 친화) → **SSD+MLP 혼합**이 대안 (근거: §9.3 코멘트). (근거: §9.3) 

* **중형(1.3–1.4B) Zero-shot 평균**: **M2-1.3B vs Mamba-1.4B**에서 **avg 56.4 동일**—**ppl 개선(−0.14)**에도 **다운스트림 평균은 대등**(토크나이저/데이터 동일) (근거: Tab.1). (근거: Tab.1) 


## 맥락 보강(저자 서술)

* **스케일링 법칙**: **125M–1.3B**에서 **Mamba-2가 Transformer++ 대비 ppl/FLOPs/시간 파레토 지배** (근거: Fig.9). (근거: Fig.9) 
* **Table 1 주석**: **M2는 동일 크기 Mamba 상회**, **Pythia(2× 크기)**에 **대체로 필적** (근거: Tab.1 캡션). (근거: Tab.1 caption) 


### 한 줄 요약

**롱 컨텍스트 효율(2–8×, 2k↑ 우위)**과 **하이브리드에서의 품질 우위(2.7B·300B tok, ppl 5.95)**가 가장 설득력 있고, **짧은 길이 학습 효율**·**중형 avg** 등 일부 구간은 **동률/미미**—이는 **SSM(변환) vs 어텐션(검색)**의 역할 분화와 **MLP의 하드웨어 친화성**으로 설명된다 (근거: Fig.10, Tab.3, §9.3/§9.2.3).    



## 프롬프트 1.5.1 (언급된 한계와 잠재적 한계)

```
"저자들이 명시적으로 인정한 한계/약점/실패 사례는 무엇인가? 분석을 바탕으로 잠재적 한계(강한 가정, 확장성, 연산 비용, 일반화 한계, 사회적 영향 등)는 무엇이라고 보나?"
```

# 언급된 한계와 잠재적 한계 (요약)

* **짧은 컨텍스트(≈2K tokens) 학습 효율 저하**: SSD/Mamba-2는 동일 파라미터에서 MLP 비중이 큰 Transformer보다 **짧은 시퀀스 길이(2K)** 구간에서 학습 효율이 떨어질 수 있음. 저자들은 **SSD:MLP 혼합(½:½)**로 단기 시퀀스 학습을 가속화할 것을 권고(근거: §9.3). 
* **순수 Mamba-2의 품질 한계 → 소량의 Attention 추가가 유의미한 개선**: 2.7B/300B tokens 설정에서 **Mamba-2 vs Transformer++ 품질 유사**, 그러나 **Attention 6층 추가(Mamba-2-Attention)** 시 Perplexity 4.10→3.85, 평균 Zero-shot 60.2→61.0으로 개선(근거: Tab.3).  
* **속도 이점의 구간 의존성**: SSD 커널은 **FA-2보다 시퀀스 길이 ≥2K**에서 더 빠름. 2K 미만에선 이점이 약할 수 있음(근거: §9.3). 
* **평가 범위의 제약**: 보고된 규모는 **≤2.8B Params**, 주요 평가는 **Pile/Zero-shot** 중심(326B급 대형·지시따르기·수학·코딩 등 광범위 벤치 미포함)(근거: Tab.1, §9.2).    


## 세부 분석

### 1) 명시적 한계(저자 인정)

1. **짧은 시퀀스 학습 효율**

   * 진술: “짧은 시퀀스 길이(예: 2K)에서는 Transformer가 더 효율적일 수 있다… ½ SSD + ½ MLP로 학습 가속 가능”(요지). (근거: §9.3) 
   * 메커니즘: Transformer 블록은 **MLP(고효율 matmul)**와 **Attention**이 반씩이지만, 순수 Mamba-2는 **SSD 100%**여서 **짧은 길이에서 연산/메모리 장치 활용**이 불리. (근거: §9.3) 

2. **순수 Mamba-2 품질의 상한 & 하이브리드의 필요성**

   * 관찰: 2.7B에서 **Transformer++와 품질 “대체로 비슷”**, **Attention 6층 추가** 시 **ppl 4.10→3.85(-6.1%)**, **Avg 60.2→61.0(+0.8pt)**. (근거: Tab.3)  
   * 해석: **장거리 상호작용/랜덤 액세스**가 필요한 일부 과제에서 **소량의 전역 어텐션**이 여전히 이득. (근거: Tab.3) 

3. **속도 이점이 ‘롱컨텍스트’에서 두드러짐**

   * 사실: SSD는 **FA-2 대비 2–8× 빠름**(Mamba-scan 대비) & **FA-2보다 빠른 지점이 L≈2K**부터. (근거: §9.3)  
   * 함의: **짧은 컨텍스트 워크로드**(예: 튜닝/서빙의 짧은 창)에서는 **Transformer 커널**이 여전히 경쟁력. (근거: §9.3) 

4. **평가/스케일의 범위**

   * 스케일: 보고 최대 **2.8B Params**(예: Mamba-2-2.8B), **2.7B 실험은 300B tokens@Pile**. (근거: Tab.1, §9.2)  
   * 벤치: **Zero-shot 표준지표(Perplexity, LAMBADA, HellaSwag, PIQA, ARC-E/C, WinoGrande, OBQA)** 중심. 고난도 추론(MMLU/GSM8K), 코드·멀티모달, 지시따르기·안전성 **미포함**. (근거: Tab.1) 

> 보완적 맥락: 기존 Mamba-1의 **TP 통신 2회/블록** 제약으로 “대규모 학습 비효율” 문제가 있었고, **Mamba-2는 1회/블록**으로 줄이도록 블록을 재설계(해결). (근거: §8)  

### 2) 잠재적 한계(분석 기반)

* **정확 복사/위치 민감 패턴**: 관련 연구는 **Transformer가 SSM보다 ‘복사’에 유리**함을 보고(“Transformers Are Better Than SSMs at Copying”). 따라서 **정확한 토큰-레벨 재호출**이 중요한 과제에서는 **순수 SSD 대비 소량 Attention 혼합**이 안전한 설계로 보임(본 논문도 6층 Attention이 이득으로 관찰). (근거: §References; Tab.3)  
* **워크로드 적합성**: SSD 커널의 이점은 **긴 시퀀스/롱-컨텍스트 프리필·디코드**에 클수록 큼. **짧은 시퀀스 튜닝·서빙**이 주인 환경에서는 **속도/비용 이점이 제한**될 수 있음. (근거: §9.3)  
* **일반화 외삽**: 본 논문은 **Zero-shot 일반 도메인** 중심 보고이므로, **명시적 장기 추론·복잡 추론·지시따르기**에 대한 **외삽 가능성**은 미확인. (근거: Tab.1) 


## (보너스) 수치 비교—순수 vs 하이브리드 (2.7B @ 300B tokens)

| 모델                        | Pile ppl ↓ | Avg Zero-shot ↑ |
| --------------------------- | ---------: | --------------: |
| Transformer++               |       3.99 |            60.2 |
| Mamba-2 (순수)              |       4.10 |            60.2 |
| **Mamba-2-Attention (6층)** |   **3.85** |        **61.0** |

(근거: Tab.3) 



### 정리

* **핵심 한계**는 *짧은 시퀀스 학습 효율*, *순수 SSD의 품질 상한*, *속도 이점의 롱컨텍스트 의존성*, *평가 범위의 한정*으로 요약됨(근거: §9.2–§9.3, Tab.1–3).   



## 프롬프트 1.5.2 (미래 연구 궤적)

```
"저자들이 제안하는 향후 연구 방향은 무엇인가? 한계에 비추어 합리적인 다음 단계나 대안적 방향을 제안해 줘."
```

### 결론(요약)

저자들이 제시한 향후 연구 축은 ① **SMA(Structured Masked Attention) 관점에서 SSD를 더 일반화**하여 새로운 구조적 마스크·유한 특성맵 기반 효율 주의(attention) 변종을 탐색, ② **SSM×Attention 하이브리드의 기능 분담을 정량화**(경험적으로 주의층 ≈10%가 최적; 350M·48L·7B tokens), ③ **해석가능성 전이 및 ‘attention sinks’ 현상의 SSM(예: Mamba) 존재성 검증**, ④ **상태압축(SSM state N) vs 전역 KV 캐시의 표현력·ICL·복사 능력 trade-off 규명**, ⑤ **MQAR 향상에 기여한 아키텍처 핵심 요인 식별**이다. (근거: §10, Tab.2, Fig.8)


## 저자 제안: 향후 연구 방향 (정리)

1. **SMA 관점의 SSD 확장 (새 마스크·새 알고리즘)**

* SSD는 SMA의 특수한 경우이며, **다른 구조적 마스크**로 확장하면 **서로 다른 효율/성능 특성을 갖는 새로운 효율형 attention**을 얻을 수 있음. 또한 SSD는 **유한 특성맵 𝜓**가 있는 경우로 제한되므로, **유한 𝜓를 갖는 변종의 분류/설계**가 필요. (근거: §10 “SMA는 SSD의 일반화”, “다른 structured masks→새 variants”) 

2. **SSM↔Attention 연결에 기반한 해석·현상 연구**

* **Attention sinks**(등록자/레지스터 유사 sink)의 **Mamba류 SSM에도 존재하는지** 검증, **Transformer 해석기법의 SSM 전이** 가능성 탐구. (근거: §10 “attention sinks가 Mamba에 존재하는지 궁금”, “해석가능성 전이”) 

3. **표현력 trade-off 규명(메모리 압축 N vs 완전 캐시)**

* **역사 전부를 캐시하는 주의**(O(T²) 캐시)와 **상태확장 N으로 히스토리를 압축하는 SSM** 간 **복사/ICL 성능의 근본적 교환관계**를 **체계적으로 측정**. 저자들은 이 분야에서 “더 이해가 필요”하다고 명시. (근거: §10 “tradeoffs… more remains to be understood”) 

4. **하이브리드 아키텍처의 최적 혼합 비율 규명**

* 350M(48L) 모델을 **7B tokens**로 학습 시 **주의층 약 10%**가 **perplexity 최적**(8.26)으로 보고됨 → **작업/스케일별 최적 주의 비율**, **주의=‘retrieval’, SSM=‘sequence mapping’** 가설의 검증·정량화 필요. (근거: Tab.2 “10% attn best”, §9.2.3 가설)

5. **아키텍처 기여 요인 식별(MQAR)**

* **Mamba-2가 Mamba-1 대비 MQAR에서 유의 향상**(동일 N=16에서도)하나, **“어떤 구성요소가 주효했는지 불명”**이라 **후속 규명이 필요**. (근거: Fig.8 설명문 “remains a question to explore in future work”) 

6. **SSD 설계공간의 축 확장(§10.1)**

* **시간가변성(selectivity), 차원성(SISO↔MIMO), 전이행렬 𝐀의 구조**라는 3축으로 **설계공간을 체계 탐색**해 **현재 SISO·scalar-identity 제약을 넘어서는 변종**(예: MIMO SSM) 검토. (근거: §10.1 “axes… time variance / dimensionality / structure on A”) 


## 한계 대비 “합리적 다음 단계”(제안)

### A. 하이브리드 최적화: 주의층 비율·배치 위치·크기

* **실험 매트릭스**: 주의층 비율 {0, 5, 10, 15, 20}% × **배치 위치**(균등/상부 집중/하부 집중/주기적) × **주의 헤드/hidden 폭**.
* **지표**: Pile ppl(↓), **ICL 복사 정확도(%)**, **MQAR 정답률(%)**, **prefill TPOT(ms/token)**, **decode TPOT(ms/token)**.
* **가설 검증**: “주의=빠른 retrieval, SSM=압축 메모리” 역할 가설을 **ablations**(주의 제거/교체, 상태확장 N 스윕)로 검증. (근거: Tab.2 가설 진술) 

### B. MQAR 성능 기여 원인 분해

* **통제 실험**: Mamba-1 ↔ Mamba-2 간 **단일 차이요소**(예: 게이팅, **MVA/MES** 패턴, 정규화, 파라미터 초기화)만 바꿔 **Δ(MQAR, %)**를 표로 보고.
* **샘플 크기**: 길이 {1k, 4k, 16k tokens}; **N**∈{16, 64, 256}. → **효과크기** 추정. (근거: Fig.8에서 N 스윕과 향상 보고, 원인 미상) 

### C. SMA 기반 새 마스크 설계·학습

* **목표**: **유한 특성맵 𝜓** 보장 + **선택적 상호작용(거리·패턴 제어)** 마스크 패밀리 설계.
* **연산비**: **O(N·T) 또는 O(T·log T)** 타깃, **GPU 친화 커널**(block-sparse matmul/scan) 동반.
* **벤치**: **ICL copying(정확도, %)**, **needle-in-a-haystack**(hit@k, %), **long-range arena**(점수). (근거: §10 “SMA 일반화→새 variants”) 

### D. 해석·현상 검증(Attention sinks in SSM)

* **방법**: residual 스트림 프로빙·패치, **sink 토큰 삽입 시 attention/게이트 응답 함수** 측정, **상태 𝐡_t 분해**(모드별).
* **산출**: **sink 민감도 곡선**, **게이트 포화율(%)**, **정보 보존률(%)**. (근거: §10 “sinks 존재성·해석 이전”) 

### E. 표현력 trade-off 지도화

* **축**: **상태확장 N(무차원)**, **컨텍스트 길이 T(tokens)**, **작업군(복사/추론/지식회상)**.
* **지도**: (N, T) 격자에서 **정확도(%)·TPOT(ms/token)·메모리(GB)** 등 **Pareto 전선** 산출 → **SSM 압축 vs 주의 캐시**의 **영역 분할**. (근거: §10 “tradeoffs… more remains to be understood”) 



## 메모(수치 포인트 재인용)

* **주의층 비율 최적**: **≈10 %**(350M, 48L, 7B tokens, ppl **8.26**). (근거: Tab.2) 
* **오픈퀘스천**: **Mamba-2의 MQAR 향상을 유발한 핵심 요소 미상** → **후속 연구 필요**. (근거: Fig.8 캡션) 
* **SMA 일반화**: **새 structured masks** 설계가 **새 효율형 attention**으로 이어질 여지. (근거: §10) 
* **해석가능성 전이/attention sinks**: **SSM에의 존재성·전이성 탐구 제안**. (근거: §10) 
* **연결 표현 trade-off**: **복사/ICL에서의 비교 연구 필요**. (근거: §10) 

> 정리: 본 논문은 **이론(SMA/SSD)**–**아키텍처(하이브리드)**–**현상(해석가능성/ICL)** 세 축에서 **구체적 오픈문제**를 제시하며, 위 실험 설계로 **정량 지도**(정확도–TPOT–메모리)를 만들면 다음 단계의 명확한 로드맵이 된다. (근거: §9–§10, Tab.2, Fig.8)



## 추가질문

```
"핵심 소프트웨어 의존성(예: CUDA, MPI, 특정 라이브러리)은 무엇인가? 학습 및 추론 중 예상되는 메모리 점유량(GPU의 경우 VRAM, CPU의 경우 RAM)은 얼마인가? 명시된 하드웨어에서의 처리량(throughput)은 얼마인가(예: images/sec, tokens/sec)? 총 연산 비용(예: 총 FLOPs 또는 Petaflop-days)에 대한 상세한 내역을 제공해 줘."
```

# Compute·메모리·처리량·의존성 (Mamba-2 / SSD 관점 요약)

**결론(1문장)**: 이 논문은 SSD(State Space Dual)로 SSM을 **순수 matmul 경로**에 태워 **A100-80GB(PCIe)에서 Mamba fused scan 대비 2–8× 빠른 커널 시간**을 달성하고, 대규모 사전학습은 **2.7B 파라미터 모델을 Pile 300B 토큰·배치 1M 토큰**으로 수행했음을 보고한다(정확한 tokens/s·CUDA 버전은 미기재). (근거: Fig.10, §9.3; §9.2.3, §D.3)   


## Key Numbers (요약)

* **Params**: 2.7B (주요 대규모 실험) (근거: §9.2.3, §D.3)  
* **Architecture**: Dense SSM(SSD 블록), attention/MLP와 혼합 가능 (근거: §7, §9.2.3, Tab.2–3)  
* **Pretrain Data**: Pile **300B tokens** (2.7B) | **Batch**: **1M tokens** (1.3B/2.7B) (근거: §9.2.3, §D.3)  
* **Train Steps(계산)**: **300B / 1M = 300k steps** (추정, 수식 기반) — 근거 수치: 토큰수·배치 (근거: §D.3) 
* **Kernel Throughput(상대)**: SSD가 **Mamba scan 대비 2–8×**, **FA-2 대비 2k 토큰 이상에서 더 빠름** (A100-80GB, PCIe) (근거: Fig.10, §9.3)  
* **메모리 복잡도**: SSD **O(T·N)**, Attention **O(T²)**, 선형 SSM **O(T·N²)** (나이브) (근거: §6.3) 

> 표기: $(T)$=시퀀스 길이, $(N)$=state dim, $(d_\text{model})$=모델 폭.


## 핵심 소프트웨어 의존성

* **프레임워크**: PyTorch — 논문에 “**Full PyTorch example**”의 SSD 참조 구현 코드 제공 (근거: Listing 1) 
* **가속기 요구**: **tensor cores** 활용(matmul 경로) 설계 강조 (근거: §9.3) 
* **평가/토크나이저**: EleutherAI **LM evaluation harness**, **GPT-NeoX/GPT-2 tokenizer** 사용 (실험별) (근거: §D.3)  
* **벤치마크 비교 라이브러리**: FlashAttention-2는 **비교 대상**으로 사용(SSD 자체 의존성 아님) (근거: Fig.10, §9.3)  

> CUDA/트리톤/드라이버 **버전 명시는 없음**(N/R). 논문은 **알고리즘·아키텍처**와 **상대 성능**을 중심으로 보고. (근거: 전반) 


## 메모리 점유 (추론/학습)

### 이론 복잡도(요지)

* **SSD(본 논문)**: 메모리 (O(TN)), 학습 FLOPs (O$(TN^2)$), 추론 FLOPs (O(N^2)) — **matmul 주도** (근거: §6.3)  
* **Quadratic Attention**: 메모리 (O$((T^2))$), 학습 FLOPs (O$(T^2N)$) (근거: §6.3) 
* **Linear SSM(naive scan)**: FLOPs (O$(TN^2)$)이나 **scan/상태물질화**로 **메모리·병렬성 비효율** (근거: §6.3) 

### 수치 예시(추정, FP16, bs=1)

아래는 **2.7B 모델 사양(32 layers, d_model=2560)**과 **(N=64)** 가정을 둔 **대략치**다. (근거: §D.3에서 2.7B: 32층·2560) 

| 항목                    | 공식/가정                                                                                |                                              결과 |
| ----------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------: |
| **SSD 상태 메모리**     | $(\text{bytes} \approx L \cdot d_\text{model}\cdot N \cdot \text{bytes/elt}$)            |                     **≈ 10.0 MB** (32×2560×64×2B) |
| **Transformers KV(4k)** | $(\displaystyle \text{KV} \approx \frac{2L H d_\text{head} T \cdot \text{bytes}}{10^9})$ | **≈ 1.34 GB** (L=32, H=32, d_head=80, T=4096, 2B) |

> 해석: SSD는 **KV-cache가 없고** per-layer 상태가 **(O$(N)$)**라 **긴 컨텍스트에서 메모리 이점**이 큼(예: 위 조건에서 약 **~100×** 작음). (근거: §6.3 “State size / Memory”) 


## 처리량(throughput)·지연

* **커널 레벨**: **A100-80GB(PCIe)**에서 SSD가 **Mamba fused scan 대비 2–8× 빠름**, 그리고 **FlashAttention-2보다 T≥2k에서 빠름**(시퀀스 길이 스윕, ms) (근거: Fig.10, §9.3)  
* **모델 레벨 tokens/s**: **절대값(tokens/s)·TTFT는 미기재**. 논문은 **상대 커널 시간**과 **복잡도 분석** 중심으로 보고. (근거: §9.3, §6.3)  


## 통신·병렬화(학습 시 중요 포인트)

* **Tensor Parallel(TP)**: Mamba-1은 블록 내부 **2회 all-reduce**가 필요해 비효율적이나, **Mamba-2는 Δ,B,C를 입력 (u)에서 병렬 투영 + GroupNorm 채택**으로 **블록당 1회 all-reduce**만 수행 (Transformer의 attention/MLP와 동일) (근거: §8, Fig.7)  
* **Sequence/Context Parallel**: SSD 알고리즘과 **동형**, 시퀀스 축 분할 후 **경계 상태만 전달** (근거: Fig.7 caption) 


## Compute & Cost (훈련 총량 정리)

* **훈련 토큰**: **300B tokens** (2.7B) (근거: §9.2.3) 
* **배치(토큰)**: **1.0M tokens** (1.3B/2.7B) → **약 300k steps** (계산) (근거: §D.3) 
* **FLOPs 추정(형식)**: SSD 학습 **( \Theta$(TN^2)$ )**, 추론 **( \Theta(N^2) )** — 절대 상수 미기재로 **PF-days 산출 불가**(논문 정보 한계) (근거: §6.3) 
* **HW 명시**: 마이크로벤치는 **A100-80GB(PCIe)**로 보고; 대규모 학습의 **GPU 수/기간/벽시계**는 **미기재** (근거: Fig.10, 본문) 


## 참고: 구현 난이도·코드 경로

* **PyTorch만으로 구현 가능한 SSD 예제**(Listing 1) 제공 — 핵심 연산은 **rearrange + cumsum + einsum**으로 구성 (근거: Listing 1)  
* **연산 분해**: SSD는 **대각/저랭크 블록(우·중앙·좌)**로 분해되어 모두 **BMM**(batched matmul) 위에 올라가 **tensor cores 활용**에 유리 (근거: Fig.5, §6.2–6.3)  


### 한줄 요약

* **의존성**: PyTorch 중심(+평가용 EAI harness); 비교대상으로 FlashAttention-2. (근거: Listing 1, Fig.10, §D.3)   
* **메모리**: SSD는 (O(TN)), **KV-cache 부재**로 긴 컨텍스트에서 유리. 예시로 **~10 MB**(vs Transformer KV **~1.34 GB/4k**) (근거: §6.3) 
* **처리량**: A100-80GB(PCIe)에서 **Mamba-scan 대비 2–8×**, **FA-2 대비 2k↑에서 우세**(상대 커널 시간) (근거: Fig.10, §9.3)  
* **훈련량**: 2.7B를 **300B 토큰**, **배치 1M** → **≈300k steps**(계산). PF-days는 논문에 미기재. (근거: §9.2.3, §D.3)  




</details>