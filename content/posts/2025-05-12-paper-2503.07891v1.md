---
categories:
- paper-review
- with-gpt
date: "2025-05-12"
title: 'Gemini Embedding: Generalizable Embeddings from Gemini'
---

[ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/2503.07891v1)

# Q : ì´ ë…¼ë¬¸ì„ ì•„ì£¼ ìì„¸í•˜ê²Œ ì½ê³  ë…¼ë¬¸ì˜ ê°•ì ê³¼ ë…ì°½ì ì¸ ì§€ì ì„ ì„¤ëª…í•´ì£¼ê³  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œ ì…ë ¥ì„ ë“¤ì–´ì„œ ì „ì²´ì ì¸ ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜ ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ì˜ í•œê³„ì ì— ëŒ€í•´ì„œë„ ì•Œë ¤ì¤˜



### âœ… ê²°ë¡  ìš”ì•½

**Gemini Embedding**ì€ Google Gemini LLMì—ì„œ ì´ˆê¸°í™”ëœ **ë²”ìš© ì„ë² ë”© ëª¨ë¸ë¡œ**, MTEB(Multilingual) ê¸°ì¤€ í‰ê·  +5.09 ì ì˜ ì„±ëŠ¥ í–¥ìƒê³¼ SOTA ë‹¬ì„±ì„ ê¸°ë¡í•˜ë©° **ë¶„ë¥˜, ê²€ìƒ‰, í´ëŸ¬ìŠ¤í„°ë§** ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì—ì„œ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

* í•µì‹¬ ê°•ì : **LLM í™œìš©í•œ pretrain + fine-tune + model soup** ì¡°í•©
* ì£¼ìš” ê¸°ë²•: **NCE ê¸°ë°˜ contrastive í•™ìŠµ**, **mean pooling**, **ë‹¤ì°¨ì› embedding(MRL)**
* ëŒ€í‘œ ìˆ˜ì¹˜: MTEB(Multilingual)ì—ì„œ Task Mean 68.32, Type Mean 59.64
* í•œê³„: ì‚¬ì „ í•™ìŠµ LLM ê¸°ë°˜ ëª¨ë¸ì´ë¯€ë¡œ **ëª¨ë¸ ê²½ëŸ‰í™”/ì„œë¹™ latency** ë¬¸ì œ ì¡´ì¬, modality ì œí•œë„ ìˆìŒ

---

### ğŸ” ê°•ì  ë° ë…ì°½ì ì¸ ê¸°ì—¬

| í•­ëª©                               | ê¸°ì—¬ ë‚´ìš©                                                                       | ë…ì°½ì„±                                            |
| ---------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------- |
| **1. ê¸°ë°˜ ëª¨ë¸**                   | Gemini LLMì—ì„œ embedding ëª¨ë¸ ì´ˆê¸°í™”                                            | ìµœì‹  ë©€í‹°ëª¨ë‹¬ LLM íŒŒë¼ë¯¸í„° í™œìš©                   |
| **2. ë°ì´í„° í’ˆì§ˆ ê°œì„ **            | Geminië¡œ synthetic data ìƒì„±, hard negative mining, noisy ë°ì´í„° filtering ìˆ˜í–‰ | LLMìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì •ì œ                          |
| **3. í•™ìŠµ ì „ëµ**                   | 2ë‹¨ê³„ í•™ìŠµ: Pre-finetuning â†’ Task-specific Fine-tuning                          | Large batch pre-finetuning â†’ ì‘ì€ batch fine-tune |
| **4. ì„ë² ë”© êµ¬ì¡°**                 | Mean pooling + Linear projection + Multi-Resolution Loss (MRL)                  | ë‹¤ì–‘í•œ ì„ë² ë”© ì°¨ì› ì§€ì›                           |
| **5. ëª¨ë¸ ì†ŒìŠ¤ í•©ì„± (Model Soup)** | ë‹¤ì–‘í•œ fine-tuned checkpoint í‰ê· í™”                                             | task ê°„ generalization í–¥ìƒ                       |
| **6. í‰ê°€**                        | MTEB, XTREME-UP, XOR-Retrieve ë“± 100+ taskì— ëŒ€í•´ ì••ë„ì  ì„±ëŠ¥                   | ë‹¤êµ­ì–´/ì½”ë“œ/low-resourceê¹Œì§€ ëª¨ë‘ SOTA            |

---

### ğŸ§  í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (ì˜ˆì‹œ í¬í•¨)

#### êµ¬ì¡° ìš”ì•½:

* ì…ë ¥ í…ìŠ¤íŠ¸: `T = "What is the capital of France?"`
* Tokenize â†’ Gemini ëª¨ë¸ í†µê³¼ â†’ Token Embedding `T_embed âˆˆ â„^{L Ã— d_M}`
* Mean Pooling â†’ `P_embed âˆˆ â„^{d_M}`
* Linear projection â†’ ìµœì¢… ì„ë² ë”© `E âˆˆ â„^d`

#### í•™ìŠµ ì˜ˆì‹œ:

| êµ¬ì„± ìš”ì†Œ             | ë‚´ìš©                                   |
| --------------------- | -------------------------------------- |
| Query (`qáµ¢`)          | "What is the capital of France?"       |
| Positive (`pâºáµ¢`)      | "Paris is the capital city of France." |
| Hard negative (`pâ»áµ¢`) | "France is known for wine and cheese." |
| Task prompt (`t`)     | "question answering"                   |

#### ìˆ˜ì‹:

* ì„ë² ë”© ë²¡í„°:
  `qáµ¢ = f(mean_pool(M(t âŠ• qáµ¢)))`
  `pâºáµ¢ = f(mean_pool(M(pâºáµ¢)))`

* ì†ì‹¤ í•¨ìˆ˜ (NCE):

  $$
  L = -\frac{1}{B} \sum_{i=1}^B \log \frac{e^{\text{sim}(qáµ¢, pâºáµ¢)/Ï„}}{e^{\text{sim}(qáµ¢, pâ»áµ¢)/Ï„} + \sum_{j \ne i} e^{\text{sim}(qáµ¢, pâºâ±¼)/Ï„}}
  $$

* MRL(Multi-Resolution Loss):
  ê° ì°¨ì›(768, 1536, 3072)ì— ëŒ€í•´ ë…ë¦½ì ì¸ ì†ì‹¤ ê³„ì‚° í›„ ê²°í•©

---

### ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

| Benchmark                    | Gemini Embedding | 2nd Best Model         | ì°¨ì´            |
| ---------------------------- | ---------------- | ---------------------- | --------------- |
| MTEB(Multilingual) Task Mean | 68.32            | 63.23 (gte-Qwen2-7B)   | **+5.09**       |
| MTEB(Eng, v2) Task Mean      | 73.30            | 71.40 (jasper)         | **+1.90**       |
| MTEB(Code)                   | 74.66            | 65.40 (Linq-Embed)     | **+9.26**       |
| XTREME-UP                    | 64.33            | 39.2\~18.7 (ë‹¤ìˆ˜ ëª¨ë¸) | **ì••ë„ì  ì°¨ì´** |

---

### ğŸš« í•œê³„ì  ë° ê°œì„  ë°©í–¥

| í•œê³„                                  | ì„¤ëª…                                                     | ê°œì„  ë°©í–¥                                            |
| ------------------------------------- | -------------------------------------------------------- | ---------------------------------------------------- |
| **1. Latency & Inference Cost**       | Gemini ê¸°ë°˜ ëŒ€í˜• LLM ì´ˆê¸°í™”ë¡œ ì¶”ë¡  ë¦¬ì†ŒìŠ¤ ë¶€ë‹´ í¼        | Knowledge distillation or hybrid routing (e.g., MoE) |
| **2. Uni-modal ì œí•œ**                 | í…ìŠ¤íŠ¸ ì „ìš© embedding â†’ ì´ë¯¸ì§€/ìŒì„± ë“± multimodal ë¯¸ì§€ì› | Gemini ë©€í‹°ëª¨ë‹¬ í™•ì¥ ê³„íš ì–¸ê¸‰                       |
| **3. Task-specific ìµœì í™” ë¶€ì¡±**      | Zero-shot ê¸°ë°˜ì´ë¯€ë¡œ íŠ¹ì • ë„ë©”ì¸ ìµœì í™” ì–´ë ¤ì›€           | Task-adaptive fine-tuning, LoRA ë„ì… ê°€ëŠ¥ì„±          |
| **4. Cross-lingual fine-tuning í•œê³„** | íŠ¹ì • low-resource ì–¸ì–´ëŠ” ë¯¸ì„¸í•˜ê²Œ ì„±ëŠ¥ ì €í•˜ ìˆìŒ         | Few-shot task-balanced sampling í•„ìš”                 |

---

# Q : Gemini Embeddingì˜ training pipelineì—ì„œ pre-finetuningê³¼ fine-tuningì€ ì–´ë–»ê²Œ ê¸°ëŠ¥ì ìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, ê° ë‹¨ê³„ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?

# A: 

ë‘ ì§ˆë¬¸ ëª¨ë‘ Gemini Embeddingì˜ ì„±ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° ë§¤ìš° í•µì‹¬ì ì¸ ìš”ì†Œì…ë‹ˆë‹¤. ì•„ë˜ì— ê°ê°ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìˆ˜ì¹˜ì™€ ë©”ì»¤ë‹ˆì¦˜ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

---

## âœ… Q1. **Pre-finetuningê³¼ Fine-tuningì˜ êµ¬ë¶„ ë° ì„±ëŠ¥ ê¸°ì—¬**

### ğŸ“Œ ê¸°ëŠ¥ì  ì°¨ì´ ìš”ì•½

| í•­ëª©            | Pre-finetuning                                        | Fine-tuning                                |
| --------------- | ----------------------------------------------------- | ------------------------------------------ |
| **ëª©ì **        | Gemini LLMì—ì„œ ì„ë² ë”© ëª¨ë¸ë¡œ ì „ì´ í•™ìŠµ ì´ˆê¸°í™”         | íƒœìŠ¤í¬ë³„ ì •ë°€ ì¡°ì •                         |
| **ë°ì´í„°**      | ì›¹ì—ì„œ ìˆ˜ì§‘í•œ noisy (query, passage) ìŒ               | (query, positive, hard negative) triples   |
| **ì†ì‹¤ í•¨ìˆ˜**   | NCE + in-batch negative, **no hard negatives**        | NCE + in-batch + **hard negatives**        |
| **ë°°ì¹˜ ì‚¬ì´ì¦ˆ** | **ë§¤ìš° í¼ (1024+)** â†’ ì•ˆì •ëœ gradient                 | **ì‘ìŒ (<1024)** â†’ ë¯¸ì„¸í•œ íƒœìŠ¤í¬ êµ¬ë¶„ ìš©ì´ |
| **íš¨ê³¼**        | LLMì˜ autoregressive íŠ¹ì„±ì„ **encoder ë°©ì‹ìœ¼ë¡œ ì „í™˜** | íŠ¹ì • íƒœìŠ¤í¬ì—ì„œì˜ ì„±ëŠ¥ ë¯¸ì„¸ ì¡°ì • ë° ì¼ë°˜í™” |

---

### ğŸ“ˆ ì„±ëŠ¥ ê¸°ì—¬ (ë…¼ë¬¸ Table 6 ê¸°ì¤€)

| í•™ìŠµ ë‹¨ê³„               | MTEB(Multi.) | MTEB(Eng) | MTEB(Code) | XOR-Retrieve | XTREME-UP |
| ----------------------- | ------------ | --------- | ---------- | ------------ | --------- |
| **No Training**         | 30.55        | 28.17     | 9.86       | -            | -         |
| **Pre-finetuning Only** | 48.89        | 50.99     | 46.18      | 76.64        | 21.22     |
| **ì „ì²´ (Pre + Fine)**   | **68.32**    | **73.28** | **74.66**  | **90.42**    | **64.33** |

* **+37.77 ì  í–¥ìƒ** (MTEB) â†’ Pre-finetuningë§Œìœ¼ë¡œë„ í° íš¨ê³¼
* **Fine-tuningì€ ì„±ëŠ¥ì„ ì¶”ê°€ë¡œ +20pt ì´ìƒ ì¦ê°€**, íŠ¹íˆ classification, retrievalì—ì„œ íš¨ê³¼ì 

â†’ **Pre-finetuning: ì–¸ì–´ì  ê¸°ë°˜ êµ¬ì¶• / Fine-tuning: íƒœìŠ¤í¬ ìµœì í™”**


# Q Multi-resolution loss(MRL) ê¸°ë²•ì´ embedding space êµ¬ì„±ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ëŠ” ë¬´ì—‡ì´ë©°, ì™œ 768/1536/3072 ë‹¨ìœ„ê°€ íš¨ê³¼ì ì¸ê°€?

# A: 
### ğŸ“Œ ëª©ì : **í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ embedding ì°¨ì›ì„ ì§€ì›í•˜ê¸° ìœ„í•¨**

> ì‹¤ì„¸ê³„ì—ì„œëŠ” ğ‘‘=768 (retrieval ë“±), ğ‘‘=3072 (classification ë“±) ì²˜ëŸ¼ **íƒœìŠ¤í¬ë³„ë¡œ ìµœì  ì„ë² ë”© ì°¨ì›ì´ ë‹¤ë¦„**
> MRLì€ ì´ ë‹¤ì–‘ì„±ì„ ìˆ˜ìš©í•˜ë©´ì„œë„ **í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì—¬ëŸ¬ ì°¨ì›ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ì œê³µ**í•˜ê²Œ í•¨.

---

### âš™ï¸ ì‘ë™ ë°©ì‹

* ì „ì²´ ì„ë² ë”© ë²¡í„° `E âˆˆ â„^3072`ì—ì„œ ì•ë¶€ë¶„ 768, 1536, 3072ê¹Œì§€ ì˜ë¼ ì‚¬ìš©
* í•™ìŠµ ì¤‘ì—ëŠ” ê° ì„œë¸Œ ì°¨ì›ì— ëŒ€í•´ **ë…ë¦½ì ì¸ NCE Loss**ë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ **ê°€ì¤‘í•© or í‰ê· **í•¨

#### ìˆ˜ì‹:

$$
\mathcal{L}_{total} = \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}_{NCE}(E_{1:d_i})
\quad \text{where } d_i âˆˆ \{768, 1536, 3072\}
$$

---

### ğŸ§ª íš¨ê³¼ (ë…¼ë¬¸ ë‚´ ì§ì ‘ ìˆ˜ì¹˜ ì—†ìŒ â†’ ì„¤ê³„ ëª©ì  ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…)

| ì°¨ì›     | ì‚¬ìš© ì‚¬ë¡€                      | ì¥ì                                    |
| -------- | ------------------------------ | -------------------------------------- |
| **768**  | retrieval, low-latency serving | ì‘ì€ ëª¨ë¸ê³¼ í˜¸í™˜ì„±, fast indexing      |
| **1536** | reranking, pairwise scoring    | trade-off between size and performance |
| **3072** | classification, clustering     | ì •ë³´ëŸ‰ ìµœëŒ€, ì •í™•ë„ ìš°ì„  íƒœìŠ¤í¬ì— ì í•© |

â†’ **ë‹¨ì¼ í•™ìŠµìœ¼ë¡œ ë‹¤ëª©ì  ì„ë² ë”© ì œê³µ** â†’ íŒŒë¼ë¯¸í„° ê³µìœ ì˜ ì¼ë°˜í™” ê°€ëŠ¥ì„± ê·¹ëŒ€í™”

---

### ğŸ“Œ ê²°ë¡  ìš”ì•½

| ì§ˆë¬¸                | ê²°ë¡                                                                                          |
| ------------------- | -------------------------------------------------------------------------------------------- |
| Pre vs. Fine-tuning | PreëŠ” êµ¬ì¡° ì „í™˜, Fineì€ íƒœìŠ¤í¬ ì •ë°€ ìµœì í™” ì—­í• . ë‘ ë‹¨ê³„ í•©ì³ì•¼ ìµœê³  ì„±ëŠ¥                    |
| MRLì˜ íš¨ê³¼          | ë‹¤ì–‘í•œ ì°¨ì›ì˜ íƒœìŠ¤í¬ì— ì ì‘ ê°€ëŠ¥í•œ ì„ë² ë”© ì œê³µ. ëª¨ë¸ í•˜ë‚˜ë¡œ multiple downstream support ê°€ëŠ¥ |



# Q : ë…¼ë¬¸ì˜ í•µì‹¬ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ê³  ì •ë¦¬í•´ ì¤˜



ë…¼ë¬¸ \*â€œGemini Embedding: Generalizable Embeddings from Geminiâ€\*ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ë©°, **LLM ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ contrastive learningìœ¼ë¡œ í•™ìŠµ**í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
ì•„ë˜ì— ì˜ˆì‹œì™€ í•¨ê»˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

---

## âœ… í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê°œìš”

1. **ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ embedding ìƒì„±**
2. **Noise-Contrastive Estimation (NCE) lossë¡œ í•™ìŠµ**
3. **Multi-Resolution Loss (MRL)ë¡œ ë‹¤ì–‘í•œ embedding dimension ì§€ì›**

---

## ğŸ§ª ì˜ˆì‹œ ê¸°ë°˜ ì „ì²´ íŒŒì´í”„ë¼ì¸

### ğŸ¯ íƒœìŠ¤í¬: *ì§ˆë¬¸-ì‘ë‹µ ë§¤ì¹­ (retrieval)*

* Query (ì§ˆë¬¸):
  `qáµ¢`: "What is the capital of France?"
* Positive passage (ì •ë‹µ):
  `pâºáµ¢`: "Paris is the capital city of France."
* Hard negative (í˜¼ë™ ìœ ì‚¬ë¬¸):
  `pâ»áµ¢`: "France is known for its wine and cheese."

---

## ğŸ” Step 1: ì…ë ¥ â†’ ì„ë² ë”© ë²¡í„° ìƒì„±

### â‘  ì…ë ¥ í† í°í™” ë° Transformer ì²˜ë¦¬

```text
ì…ë ¥ ì¿¼ë¦¬:  t âŠ• qáµ¢  
â†’ ì˜ˆ: "question answering âŠ• What is the capital of France?"
```

* Transformer M (Geminiì—ì„œ ì´ˆê¸°í™”ëœ encoder ëª¨ë¸)ì„ í†µê³¼ì‹œì¼œ,

  ```
  T_embed = M(t âŠ• qáµ¢) âˆˆ â„^{LÃ—d_M}
  ```

### â‘¡ Mean Pooling

```math
P_embed = mean_pool(T_embed) âˆˆ â„^{d_M}
```

### â‘¢ Linear projection (ì°¨ì› ì¶•ì†Œ)

```math
E_q = f(P_embed) âˆˆ â„^d
```

* ì—¬ê¸°ì„œ d âˆˆ {768, 1536, 3072}

**ë™ì¼í•˜ê²Œ `pâºáµ¢`, `pâ»áµ¢`ë„ ìœ„ ê³¼ì •ì„ í†µí•´ `E_p+`, `E_p-`ë¡œ ë³€í™˜**

---

## ğŸ¯ Step 2: Contrastive Loss ê³„ì‚° (NCE with in-batch negatives)

> ì¿¼ë¦¬ì™€ ì •ë‹µ ìŒì˜ ìœ ì‚¬ë„ë¥¼ ë†’ì´ê³ , ë‹¤ë¥¸ ìŒê³¼ëŠ” ë©€ì–´ì§€ë„ë¡ í•™ìŠµ
> í•µì‹¬ì€ **cosine similarity ê¸°ë°˜ softmax**

### ğŸ“ ìœ ì‚¬ë„ ê³„ì‚°

```math
sim(x, y) = \frac{x^\top y}{\|x\|\|y\|}
```

### ğŸ’¡ Loss ìˆ˜ì‹ (batch size = B)

$$
\mathcal{L} = -\frac{1}{B} \sum_{i=1}^{B} \log \frac{e^{sim(q_i, p^+_i)/\tau}}{
e^{sim(q_i, p^-_i)/\tau} + \sum_{j=1}^{B} \text{mask}(i, j) e^{sim(q_i, p^+_j)/\tau}
}
$$

* `mask(i, j)`ëŠ” `i == j`ì¸ ê²½ìš° ì œì™¸í•´ false negative ë°©ì§€
* `Ï„`ëŠ” temperature (ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜)

### âš ï¸ ì¤‘ìš” ì˜µì…˜

* **Hard negative ì—†ì„ ê²½ìš°** â†’ ì²« í•­ ì œê±°
* **In-batch negativesë§Œ ì‚¬ìš©** â†’ íš¨ìœ¨ì„± í–¥ìƒ

---

## ğŸ¯ Step 3: MRL (Multi-Resolution Loss)

> í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ì°¨ì›ì˜ embeddingì„ ì§€ì›í•˜ëŠ” ë°©ì‹

### êµ¬ì¡°

* ì „ì²´ ì„ë² ë”© `E âˆˆ â„^{3072}`ë¥¼ ì•ì—ì„œë¶€í„° ë‚˜ëˆ ì„œ ì‚¬ìš©
* ì˜ˆ:

  * `Eâ‚‡â‚†â‚ˆ = E[0:768]`
  * `Eâ‚â‚…â‚ƒâ‚† = E[0:1536]`
  * `Eâ‚ƒâ‚€â‚‡â‚‚ = E[0:3072]`

### í•™ìŠµ ì‹œ ë‹¤ì¤‘ loss í•©ì„±

$$
\mathcal{L}_{total} = \frac{1}{3} \sum_{d \in \{768, 1536, 3072\}} \mathcal{L}_{NCE}(E_d)
$$

â†’ ì´ êµ¬ì¡° ë•ë¶„ì— **1ê°œì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ë¦¬ì†ŒìŠ¤ í™˜ê²½ì— ëŒ€ì‘ ê°€ëŠ¥**

---

## ğŸ“ˆ ì „ì²´ êµ¬ì¡° ìš”ì•½ (ìˆœì„œë„)

```
         Input Query: "What is the capital of France?"
                   â¬‡ (task prompt concat)
       "question answering âŠ• What is the capital of France?"
                   â¬‡
        Gemini Transformer Encoder (bidirectional)
                   â¬‡
           Mean Pooling over token outputs
                   â¬‡
     Linear projection â†’ Embedding (768/1536/3072)
                   â¬‡
    Contrastive NCE Loss with in-batch & hard negatives
                   â¬‡
          Multi-Resolution Loss (MRL) ì ìš©
```

---

## ğŸ”š ì •ë¦¬

| ë‹¨ê³„               | ëª©ì                       | ê¸°ë²•                                        | ì¥ì                      |
| ------------------ | ------------------------- | ------------------------------------------- | ------------------------ |
| **1. ì„ë² ë”© ìƒì„±** | ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ | Gemini encoder + mean pooling + linear proj | Fast & simple            |
| **2. NCE í•™ìŠµ**    | positiveì™€ negative êµ¬ë¶„  | cosine sim ê¸°ë°˜ contrastive loss            | Efficient & scalable     |
| **3. MRL ì ìš©**    | ë‹¤ì°¨ì› embedding ì œê³µ     | 768/1536/3072 ì°¨ì› ê°ê°ì— loss              | ë‹¤ì–‘í•œ downstreamì— ëŒ€ì‘ |

# Q : PyTorch ê¸°ë°˜ì˜ ë¯¸ë‹ˆë©€ êµ¬í˜„ ì˜ˆì‹œë¥¼ ë§Œë“¤ì–´ì¤„ ìˆ˜ ìˆê² ë‹ˆ?

ë¬¼ë¡ ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” **Gemini Embedding ë…¼ë¬¸ ê¸°ë°˜**ìœ¼ë¡œ êµ¬ì„±í•œ **PyTorch ë¯¸ë‹ˆë©€ êµ¬í˜„ ì˜ˆì‹œ**ì…ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥:

* Mean Pooling ê¸°ë°˜ì˜ embedding ìƒì„±
* Multi-resolution embedding
* In-batch NCE loss + MRL (768/1536/3072)

---

## ğŸ§© PyTorch ë¯¸ë‹ˆë©€ ì½”ë“œ ì˜ˆì‹œ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GeminiEmbeddingModel(nn.Module):
    def __init__(self, base_encoder: nn.Module, hidden_dim=3072, proj_dim=3072):
        super().__init__()
        self.encoder = base_encoder  # e.g., BERT without pooler
        self.proj = nn.Linear(self.encoder.config.hidden_size, proj_dim)
        self.dimensions = [768, 1536, 3072]  # MRL ì ìš© ì°¨ì›

    def mean_pooling(self, token_embeds, attention_mask):
        mask = attention_mask.unsqueeze(-1).float()
        summed = torch.sum(token_embeds * mask, dim=1)
        counts = torch.clamp(mask.sum(dim=1), min=1e-9)
        return summed / counts

    def forward(self, input_ids, attention_mask):
        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = output.last_hidden_state  # [B, L, H]
        mean_embed = self.mean_pooling(token_embeddings, attention_mask)  # [B, H]
        proj_embed = self.proj(mean_embed)  # [B, 3072]
        return proj_embed  # full MRL embedding

def compute_mrl_nce_loss(queries, positives, temperature=0.05, dims=[768, 1536, 3072]):
    losses = []
    for d in dims:
        q = F.normalize(queries[:, :d], p=2, dim=1)
        p = F.normalize(positives[:, :d], p=2, dim=1)

        logits = torch.matmul(q, p.T) / temperature
        labels = torch.arange(len(q), device=q.device)
        loss = F.cross_entropy(logits, labels)
        losses.append(loss)

    return sum(losses) / len(losses)  # í‰ê·  loss

# âœ… ì‚¬ìš© ì˜ˆì‹œ
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
encoder = AutoModel.from_pretrained("bert-base-uncased")

model = GeminiEmbeddingModel(encoder)

# ìƒ˜í”Œ ì¿¼ë¦¬/íŒ¨ì‹œì§€
texts_q = ["What is the capital of France?", "Who wrote Hamlet?"]
texts_p = ["Paris is the capital of France.", "Hamlet was written by Shakespeare."]

encoded_q = tokenizer(texts_q, padding=True, truncation=True, return_tensors="pt")
encoded_p = tokenizer(texts_p, padding=True, truncation=True, return_tensors="pt")

# ì„ë² ë”© ê³„ì‚°
emb_q = model(**encoded_q)
emb_p = model(**encoded_p)

# MRL ê¸°ë°˜ NCE loss ê³„ì‚°
loss = compute_mrl_nce_loss(emb_q, emb_p)
print(f"Loss: {loss.item():.4f}")
```

---

## ğŸ§  ì„¤ëª… ìš”ì•½

| êµ¬ì„± ìš”ì†Œ              | ê¸°ëŠ¥                                                   |
| ---------------------- | ------------------------------------------------------ |
| `mean_pooling`         | attention mask ê¸°ë°˜ í‰ê·  í’€ë§                          |
| `self.proj`            | Linear layerë¡œ embedding ì°¨ì› ì¡°ì •                     |
| `compute_mrl_nce_loss` | ì—¬ëŸ¬ ì°¨ì›(768, 1536, 3072)ì— ëŒ€í•´ NCE loss í‰ê·  ê³„ì‚°   |
| `F.normalize`          | cosine similarity ê¸°ë°˜ contrastive loss ê³„ì‚° ì „ ì •ê·œí™” |

---



# Q : ì´ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê¸° ìœ„í•˜ì—¬ ì–´ë–¤ ë°ì´í„°ì…‹ì´ í•„ìš”í• ê¹Œ? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ë ì§€ ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ì•„ì£¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜



### âœ… ê²°ë¡  ìš”ì•½

**Gemini Embedding**ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ **3ì¢…ë¥˜ì˜ ë°ì´í„°ì…‹**ì´ í•„ìš”í•˜ë©°,
í•™ìŠµì€ \*\*ë‘ ë‹¨ê³„(pre-finetuning â†’ fine-tuning)\*\*ë¡œ ì§„í–‰ë˜ê³ , ê° ë‹¨ê³„ëŠ” ì„œë¡œ ë‹¤ë¥¸ ëª©ì ê³¼ êµ¬ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

| ë‹¨ê³„           | ë°ì´í„° êµ¬ì„±                                       | ëª©ì                     | ì˜ˆì‹œ ë°ì´í„°                         |
| -------------- | ------------------------------------------------- | ----------------------- | ----------------------------------- |
| Pre-finetuning | `(query, positive passage)`                       | LLM â†’ Encoderë¡œ ì ì‘    | ì›¹ ë¬¸ì„œ + ì œëª© ìŒ, Wikipedia ë¬¸ë‹¨   |
| Fine-tuning    | `(query, positive, hard negative)`                | Task-specific ì„±ëŠ¥ í–¥ìƒ | BEIR, MS MARCO, MTEB ë¶„ë¥˜ íƒœìŠ¤í¬ ë“± |
| + Synthetic    | Geminië¡œ ìƒì„±ëœ `retrieval/classification` íƒœìŠ¤í¬ | ë°ì´í„° ë‹¤ì–‘ì„± ë° ì¼ë°˜í™” | Gemini promptingìœ¼ë¡œ ìƒì„±           |

---

## ğŸ§ª ë°ì´í„°ì…‹ êµ¬ì„±: ìì„¸í•œ ì˜ˆì‹œ

### 1ï¸âƒ£ Pre-finetuning ë°ì´í„°: noisy but large

**(query, positive passage)** í˜•íƒœë§Œ ìˆìœ¼ë©´ OK

* **ëª©í‘œ**: Gemini (autogressive decoder) â†’ encoder embedding modelë¡œ ì ì‘
* **ë°ì´í„° í¬ê¸°**: ìˆ˜ì²œë§Œ\~ìˆ˜ì–µ ìŒ (ê·œëª¨ê°€ ì¤‘ìš”)
* **í˜•ì‹ ì˜ˆì‹œ**:

```json
{
  "query": "What is Python used for?",
  "positive": "Python is a general-purpose programming language..."
}
```

* **êµ¬ì„± ë°©ë²•**:

  * ì›¹ í¬ë¡¤ë§ í›„ title + passage ë§¤ì¹­
  * Wikipedia title â†” ë¬¸ë‹¨ ë³¸ë¬¸
  * ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª© â†” ë¦¬ë“œ ë¬¸ë‹¨
  * ê³µê°œ ì½”í¼ìŠ¤: [CC-News](https://commoncrawl.org), [OSCAR](https://oscar-corpus.com), [RedPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data)

---

### 2ï¸âƒ£ Fine-tuning ë°ì´í„°: triple í˜•íƒœ (query, pos, neg)

* **ëª©í‘œ**: íƒœìŠ¤í¬ ì ì‘ (retrieval, classification ë“±)
* **í•„ìˆ˜**: Hard negative ìƒ˜í”Œë§ or ìˆ˜ë™ êµ¬ì¶•

#### ğŸ”¹ Retrieval task (MS MARCO ë“±)

```json
{
  "query": "What is the boiling point of water?",
  "positive": "Water boils at 100 degrees Celsius.",
  "negative": "Water is essential for life on Earth."
}
```

* ì¶”ì²œ ì˜¤í”ˆ ë°ì´í„°ì…‹:

  * **BEIR**: IR, QA íƒœìŠ¤í¬ ë‹¤ì–‘ (TREC, FEVER, HotpotQA ë“±)
  * **MS MARCO**: passage ranking
  * **MIRACL**: multilingual IR

#### ğŸ”¹ Classification task (e.g. sentiment)

```json
{
  "task": "sentiment analysis",
  "query": "I hated the movie so much.",
  "positive": "negative",
  "negative": "positive"
}
```

* ì‚¬ìš© ê°€ëŠ¥ ë°ì´í„°ì…‹:

  * **Amazon polarity**, **Yelp**, **Emotion**, **MultiEURLEX**

---

### 3ï¸âƒ£ Synthetic dataset (Geminië¡œ ìƒì„±)

* **ì‚¬ìš© ì´ìœ **: íƒœìŠ¤í¬ ë‹¤ì–‘ì„±, class counterfactual ìƒì„±
* **í˜•ì‹**:

  * Gemini promptingìœ¼ë¡œ `"query"` + `"positive"` + `"hard negative"` ìƒì„±
  * ë˜ëŠ” `"review"` ìƒì„± â†’ Geminiê°€ `"sentiment"` ë¶„ë¥˜ label ìë™ ìƒì„±

```json
{
  "task": "counterfactual sentiment",
  "query": "I used to like this phone but now it always crashes.",
  "positive": "negative",
  "negative": "positive"
}
```

---

## ğŸ—ï¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ (ë‹¨ê³„ë³„ êµ¬ì„±)

### âœ… Stage 1: Pre-finetuning

| í•­ëª©      | ë‚´ìš©                                     |
| --------- | ---------------------------------------- |
| ë°ì´í„°    | (q, pos) pairs (ìˆ˜ì²œë§Œ ìŒ ì´ìƒ)          |
| ì†ì‹¤ í•¨ìˆ˜ | In-batch NCE (hard negative ì—†ìŒ)        |
| ë°°ì¹˜ í¬ê¸° | **1024+** (í° ê°’ì´ ì•ˆì •ì )               |
| í•™ìŠµ ëª©í‘œ | LLM â†’ encoder í˜•íƒœë¡œ ì „ì´                |
| í•™ìŠµ ì‹œê°„ | ê¸¸ê²Œ, ìˆ˜ epoch ì§„í–‰ (1M steps ì´ìƒ ê°€ëŠ¥) |

```python
loss = compute_mrl_nce_loss(emb_q, emb_pos)  # no negatives
```

---

### âœ… Stage 2: Fine-tuning

| í•­ëª©      | ë‚´ìš©                                                   |
| --------- | ------------------------------------------------------ |
| ë°ì´í„°    | (q, pos, hard neg) triples                             |
| ì†ì‹¤ í•¨ìˆ˜ | NCE + hard negative                                    |
| ë°°ì¹˜ í¬ê¸° | 128\~1024 (ì‘ì€ ê°’ì´ signal ì„ ëª…í•¨)                    |
| ì „ëµ      | í•˜ë‚˜ì˜ taskë§Œ í¬í•¨ëœ batch ì‚¬ìš© (task-specific loss â†‘) |
| ëª©í‘œ      | íƒœìŠ¤í¬ ì„±ëŠ¥ ê°•í™”, ì¼ë°˜í™” í–¥ìƒ                          |

```python
loss = compute_mrl_nce_loss(emb_q, emb_pos, emb_neg)  # with hard negatives
```

---

### ğŸ“Œ Hard Negative êµ¬ì„±ë²• ì˜ˆì‹œ (LLM ê¸°ë°˜)

1. Embedding modelë¡œ top-k candidate retrieve
2. ê° candidateì™€ queryë¥¼ Geminiì—ê²Œ ì£¼ê³  score ê³„ì‚°
3. **score ë‚®ì€ ìƒ˜í”Œ**ì„ hard negativeë¡œ ì„ íƒ (e.g. Reciprocal Rank Fusion ì‚¬ìš©)

---

## âœï¸ ì •ë¦¬

| êµ¬ì„±               | ë‚´ìš©                                                                                                               |
| ------------------ | ------------------------------------------------------------------------------------------------------------------ |
| í•„ìš”í•œ ë°ì´í„° ìœ í˜• | (1) Pre-finetuning (q, pos) ëŒ€ê·œëª¨ ìŒ<br>(2) Fine-tuning (q, pos, neg) triples<br>(3) Gemini ê¸°ë°˜ synthetic ë°ì´í„° |
| ëŒ€í‘œ ì†ŒìŠ¤          | BEIR, MS MARCO, Wikipedia, OSCAR, HuggingFace Datasets                                                             |
| í•™ìŠµ ë‹¨ê³„          | (1) Pre-finetuning: êµ¬ì¡° ì „í™˜<br>(2) Fine-tuning: task ìµœì í™”                                                      |
| ê¸°íƒ€ ê³ ë ¤ì‚¬í•­      | multilingual ëŒ€ì‘ ì‹œ ì–¸ì–´ íƒœê¹… or task prompt ì¶”ê°€ í•„ìš”                                                            |


# Q : ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë¥¼ ìì„¸í•˜ê²Œ ë³´ê³  ë‹¤ë¥¸ ë°©ë²•ë¡ ì— ë¹„í•˜ì—¬ íŠ¹ì¶œë‚œ ì ê³¼ ë…¼ë¬¸ì—ì„œ ì œê¸°í•˜ëŠ” ì–´ë– í•œ ë°©ë²•ì´ ì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ ë˜ì—ˆëŠ”ì§€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ì´ìœ ì™€ ë„ˆì˜ ìƒê°ì„ ì•Œë ¤ì¤˜




### âœ… ê²°ë¡  ìš”ì•½

**Gemini Embeddingì€ ê¸°ì¡´ ëª¨ë“  embedding ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” SOTAë¥¼ ê¸°ë¡í•˜ë©°**,
ì´ëŠ” ë‹¨ì¼ ëª¨ë¸ì´ \*\*ë‹¤êµ­ì–´, ì½”ë“œ, ë‹¤ì–‘í•œ íƒœìŠ¤í¬(Classification, Retrieval, Clustering ë“±)\*\*ì—ì„œ ëª¨ë‘ **ì¼ê´€ëœ ìš°ìˆ˜ ì„±ëŠ¥**ì„ ë‹¬ì„±í–ˆë‹¤ëŠ” ì ì—ì„œ íŠ¹ì¶œë‚©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ì„±ëŠ¥ì€ ê¸°ì¡´ ë°©ì‹ê³¼ ì°¨ë³„í™”ëœ **Gemini ê¸°ë°˜ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ê¸°ë²• + 2ë‹¨ê³„ í•™ìŠµ ì „ëµ + ëª¨ë¸ ìˆ˜í”„(Model Soup)** ì¡°í•©ì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤.

---

## ğŸ“Š 1. ê²°ê³¼ ìš”ì•½ ë° ë¹„êµ ë¶„ì„

### ğŸ“Œ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¹„êµ (ë…¼ë¬¸ Table 1/2/3 ìš”ì•½)

| Benchmark                | Gemini    | 2nd Best           | ì„±ëŠ¥ì°¨          | íŠ¹ì§•                                |
| ------------------------ | --------- | ------------------ | --------------- | ----------------------------------- |
| MTEB (Multilingual)      | **68.32** | 63.23 (gte-Qwen2)  | **+5.09**       | 250+ê°œ ì–¸ì–´, 10ê°œ íƒœìŠ¤í¬            |
| MTEB (Eng v2)            | **73.30** | 71.40 (jasper)     | **+1.90**       | ê³ ìì› ì˜ì–´ ê¸°ë°˜                    |
| MTEB (Code)              | **74.66** | 65.40 (Linq-Embed) | **+9.26**       | ì½”ë“œ ê²€ìƒ‰ ì„±ëŠ¥                      |
| XTREME-UP (low-resource) | **64.33** | \~40 ì´í•˜          | **ì••ë„ì  ì°¨ì´** | íŒë””ì–´, ì•„ì‚¼ì–´ ë“± low-resource ì–¸ì–´ |
| XOR-Retrieve             | **90.42** | 68.76              | **+21.7**       | ë‹¤êµ­ì–´ â†’ ì˜ì–´ cross-lingual ê²€ìƒ‰    |

â†’ ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ëŒ€ë¶€ë¶„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ì–¸ì–´êµ°ì— overfitë˜ì§€ë§Œ, **Gemini Embeddingì€ ë²”ìš©ì„±**ì´ ì••ë„ì ì…ë‹ˆë‹¤.

---

## ğŸ§  2. Gemini Embeddingë§Œì˜ íŠ¹ì¶œë‚œ ì  (ë°©ë²•ë¡  ìš”ì•½)

| êµ¬ì„± ìš”ì†Œ                        | ê¸°ì¡´ ë°©ì‹ê³¼ì˜ ì°¨ë³„ì                                             | íš¨ê³¼                                                 |
| -------------------------------- | --------------------------------------------------------------- | ---------------------------------------------------- |
| **Gemini ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ìƒì„±** | Geminië¥¼ í™œìš©í•œ synthetic data, filtering, hard-negative mining | **ì •í™•í•˜ê³  ë‹¤ì–‘ì„± ìˆëŠ” í•™ìŠµ ë°ì´í„° í™•ë³´**            |
| **2ë‹¨ê³„ í•™ìŠµ (Pre + Fine)**      | Pre-finetuneìœ¼ë¡œ encoder ì ì‘ í›„ fine-tune                      | **LLM ê¸°ë°˜ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ encoderë¡œ ì „ì´**    |
| **Multi-resolution Loss (MRL)**  | ë‹¤ì–‘í•œ ì°¨ì›ì˜ embedding ì§€ì› (768/1536/3072)                    | **íƒœìŠ¤í¬ë³„ íš¨ìœ¨ì„± & í˜¸í™˜ì„± â†‘**                       |
| **Model Soup**                   | ë‹¤ì–‘í•œ fine-tuned ëª¨ë¸ í‰ê· í™”                                   | **íƒœìŠ¤í¬ ê°„ ì¼ë°˜í™” ëŠ¥ë ¥ ê·¹ëŒ€í™”**                     |
| **Prompt ê¸°ë°˜ íƒœìŠ¤í¬ ì§€ì • í•™ìŠµ** | â€œquestion answeringâ€ê³¼ ê°™ì€ íƒœìŠ¤í¬ ì§€ì‹œë¬¸ ì‚½ì…                  | **íƒœìŠ¤í¬ ì¡°ê±´ í•™ìŠµ â†’ zero-shot generalization í–¥ìƒ** |

---

## ğŸ“Œ 3. ë…¼ë¬¸ ë‚´ ì›ì¸ ë¶„ì„ ë° ë„ˆì˜ ì‹œì‚¬ì  ì •ë¦¬

### ğŸ” ë…¼ë¬¸ì´ ì œì‹œí•˜ëŠ” í•µì‹¬ ì„±ëŠ¥ ìš”ì¸

| ìš”ì¸                            | ë…¼ë¬¸ ê·¼ê±°                                                           | ì„±ëŠ¥ ê¸°ì—¬ë„                                   |
| ------------------------------- | ------------------------------------------------------------------- | --------------------------------------------- |
| Gemini í™œìš©í•œ data filtering    | MIRACL ë°ì´í„°ì—ì„œ filtering â†’ í‰ê·  +3.9 ì  í–¥ìƒ                     | **ì˜ëª»ëœ ë ˆì´ë¸” ì œê±°**                        |
| Synthetic data (classification) | AmazonCounterfactual: +17.6ì  í–¥ìƒ (Table 7)                        | **tail ë°ì´í„° ìƒì„± ê°€ëŠ¥**                     |
| Hard negative mining            | Gemini-based negative selectionì´ ì¼ë°˜ randomë³´ë‹¤ ì„±ëŠ¥ ìš°ìˆ˜ (Fig.3) | **semantic confusion ìœ ë„ í•™ìŠµ**              |
| Model soup                      | ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ í‰ê· í™” â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ                         | **grid searchê°€ ì–´ë ¤ìš´ í™˜ê²½ì—ì„œ ë§¤ìš° íš¨ê³¼ì ** |
| MRL                             | ë‹¤ì–‘í•œ ì„œë¹™ í™˜ê²½ ëŒ€ì‘ + ì„±ëŠ¥ ìœ ì§€                                   | **íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì˜ ê· í˜• ë‹¬ì„±**               |

---

### ğŸ§  ë‚˜ì˜ í•´ì„ (ì¶”ê°€ ê´€ì°°)

1. **Gemini LLMì´ ë‹¨ìˆœ backboneì´ ì•„ë‹Œ í•™ìŠµ ë°ì´í„° generatorë¡œ í™œìš©ëœ ì ì´ í•µì‹¬ì **

   * ê¸°ì¡´ embedding ëª¨ë¸ì€ "ëª¨ë¸ ì„¤ê³„ ê°œì„ " ì¤‘ì‹¬ì´ì—ˆë‹¤ë©´,
   * Gemini Embeddingì€ "í•™ìŠµ ë°ì´í„°ì˜ ì§ˆì„ LLMìœ¼ë¡œ ì—­ì „ì‹œí‚¨" ë©”íƒ€ ì „ëµ

2. **ëª¨ë¸ì´ ì•„ë‹ˆë¼ â€œí›ˆë ¨ ì „ëµê³¼ ë°ì´í„°ì˜ ìš°ìœ„â€ì—ì„œ SOTAë¥¼ ë§Œë“¤ì—ˆë‹¤ëŠ” ì ì—ì„œ ì‚°ì—…ì ìœ¼ë¡œë„ ì‹œì‚¬ì ì´ í¼**

   * ë‹¤ë¥¸ LLM ê¸°ë°˜ embedding ëª¨ë¸ë“¤(e.g. e5-mistral)ì€ ëŒ€ê°œ LLMì„ fine-tuneí•˜ê±°ë‚˜ distillí•  ë¿
   * GeminiëŠ” LLMì„ *í›ˆë ¨ì—ë„ ì§ì ‘ í™œìš©*, ë‹¨ìˆœ ëª¨ë¸ ì„±ëŠ¥ ì´ìƒì˜ ë°ì´í„° í’ˆì§ˆ ì „ëµì´ ë‹ë³´ì„

3. **ë¬¸ì œì˜ ë³¸ì§ˆì„ ë°”ê¾¸ëŠ” ë°©ì‹:**

   * ê¸°ì¡´: ì ì€ ìˆ˜ì˜ ì¸ê°„ ë ˆì´ë¸” â†’ ë°ì´í„°ì˜ ì œì•½
   * Gemini ë°©ì‹: ìˆ˜ì²œë§Œ ê°œì˜ ê³ í’ˆì§ˆ synthetic ë°ì´í„° â†’ scalability í™•ë³´

---

## ğŸ“š ì •ë¦¬: Gemini Embeddingì´ SOTAë¥¼ ë‹¬ì„±í•œ êµ¬ì¡°ì  ìš”ì•½

```text
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     Gemini LLM ê¸°ë°˜    â”‚
                     â”‚   (Generator ì—­í• ë„ ìˆ˜í–‰) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Synthetic  â”‚     â”‚ Hard Negative â”‚      â”‚ Data Filterâ”‚
     â”‚ Data ìƒì„±    â”‚     â”‚ Mining by LLM â”‚      â”‚ via Gemini â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                   â”‚                  â”‚
                â–¼                   â–¼                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             Pre-finetuning + Fine-tuning           â”‚
       â”‚      + Prompt-guided Learning + Model Soup         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        Multilingual, Multi-task Embedding     â”‚
         â”‚       with Multi-resolution Loss (MRL)        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        ğŸ“ˆ SOTA: MTEB, XTREME-UP, XOR-Retrieve ëª¨ë‘ ìµœìƒìœ„
```



# Q : ì´ ë…¼ë¬¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì–´ë–¤ ì—°êµ¬íë¦„ì´ ìˆëŠ”ì§€ ì •ë¦¬í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ ì¤˜



### âœ… ê²°ë¡  ìš”ì•½

**Gemini Embeddingì˜ í•œê³„ëŠ” í¬ê²Œ â‘  ê³ ë¹„ìš© ì¶”ë¡ , â‘¡ ë©€í‹°ëª¨ë‹¬ ì œí•œì„±, â‘¢ Task-specific ìµœì í™” ë¶€ì¡±, â‘£ ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ìœ ì—°ì„± ë¶€ì¡±**ìœ¼ë¡œ ìš”ì•½ë©ë‹ˆë‹¤.
ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ì—°êµ¬ íë¦„ì€ **ê²½ëŸ‰í™”, ëª¨ë‹¬ë¦¬í‹° í™•ì¥, ì§€ì‹ ì••ì¶•, ì„ íƒì  ê³„ì‚°(MoE), ë™ì  ì¶”ë¡  ì „ëµ** ì¤‘ì‹¬ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“Œ 1. í•œê³„ë³„ ë¶„ì„ ë° ëŒ€ì‘ ì—°êµ¬ íë¦„

| í•œê³„ì                           | ìƒì„¸ ì„¤ëª…                                           | ëŒ€ì‘ ì—°êµ¬ íë¦„                                                                                             |
| ------------------------------- | --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| â‘  **ì¶”ë¡  ë¹„ìš©ì´ í¼**            | Gemini ê¸°ë°˜ encoderëŠ” í¬ê³  ëŠë¦¼. ì‹¤ì‹œê°„ ì‘ë‹µ ì–´ë ¤ì›€ | ğŸ”¹ **Distillation ê¸°ë°˜ embedding ëª¨ë¸**<br>ğŸ”¹ **Sparse Mixture-of-Experts (e.g. MoE Embedding)**             |
| â‘¡ **ë©€í‹°ëª¨ë‹¬ ì œí•œ**             | í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ ê°€ëŠ¥ (ì´ë¯¸ì§€/ìŒì„± ë¹„ì§€ì›)             | ğŸ”¹ **Multimodal embedding (VLM2Vec, Flamingo, SigLIP)**<br>ğŸ”¹ **Cross-modal contrastive learning (CLIP ë“±)** |
| â‘¢ **Task-specific ìµœì í™” ë¶€ì¡±** | general-purpose â†’ íŠ¹ì • íƒœìŠ¤í¬ì—ì„  ë¯¸ì„¸ ì¡°ì • í•„ìš”    | ğŸ”¹ **Adapter / LoRA ê¸°ë°˜ task-tuning**<br>ğŸ”¹ **Prompt injection + few-shot tuning (e.g. BGE-ICL)**           |
| â‘£ **ì°¨ì› ìœ ì—°ì„± ë¶€ì¡±**          | 768/1536/3072 ì™¸ ì„¤ì • ì–´ë ¤ì›€, êµ¬ì¡° ê³ ì •             | ğŸ”¹ **Modular representation learning (e.g. Matryoshka)**<br>ğŸ”¹ **Query-adaptive dimension reduction (QADR)** |

---

## ğŸ§  2. í•œê³„ ê·¹ë³µì„ ìœ„í•œ ëŒ€í‘œ ì—°êµ¬ íë¦„

### 2.1 ğŸ”¹ **Embedding Distillation & Compression**

| ëŒ€í‘œ ë…¼ë¬¸           | í•µì‹¬ ì•„ì´ë””ì–´                                     | Gemini ë³´ì™„ í¬ì¸íŠ¸                                  |
| ------------------- | ------------------------------------------------- | --------------------------------------------------- |
| **NV-Embed** (2025) | LLMì—ì„œ embeddingì„ distill â†’ fast encoderë¡œ ì „ì´ | Geminiì˜ representationì„ ì‹¤ì‹œê°„ ì¶”ë¡ ìš© ëª¨ë¸ë¡œ ì´ì‹ |
| **E5-Mistral**      | LLM ê¸°ë°˜ few-shot promptë¡œ triplet ìƒì„± â†’ distill | Synthetic Gemini ë°ì´í„°ì™€ ê²°í•© ê°€ëŠ¥                 |
| **SFR-Mistral**     | teacher ì—†ì´ in-domain ìì²´í•™ìŠµ â†’ distill         | resource ì ˆê° ê°€ëŠ¥, domain overfitting ëŒ€ì‘         |

---

### 2.2 ğŸ”¹ **ë©€í‹°ëª¨ë‹¬ í™•ì¥**

| ì—°êµ¬ íë¦„                           | ì˜ˆì‹œ                         | ë³´ì™„ ê°€ëŠ¥ì„±                                    |
| ----------------------------------- | ---------------------------- | ---------------------------------------------- |
| **CLIP-style contrastive learning** | SigLIP, ALIGN                | Gemini ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥ì„ embeddingìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥ |
| **Multimodal embedding space**      | VLM2Vec (Jiang et al., 2024) | Text+Imageë¥¼ ë™ì¼ ë²¡í„° ê³µê°„ì— encoding         |
| **Multimodal task prompting**       | Flamingo, Gemini 1.5         | Instruction + multi-input ëŒ€ì‘ ê°€ëŠ¥            |

---

### 2.3 ğŸ”¹ **Sparse Mixture-of-Experts (MoE Embedding)**

| ë…¼ë¬¸                | ì„¤ëª…                             | Gemini ë³´ì™„ì                            |
| ------------------- | -------------------------------- | --------------------------------------- |
| **MoEUT** (2024)    | MoE êµ¬ì¡° + Uni-Task embedding    | Gemini task ë¶„ë¦¬ â†’ expert ë¶„ì‚° ê°€ëŠ¥     |
| **Duplex** (2024)   | MoE + GQA + Continuous batching  | Cross-lingual embedding taskì—ì„œ íš¨ìœ¨ì  |
| **MegaScale-Infer** | Sparse activation â†’ latency ì ˆê° | Gemini êµ¬ì¡°ì— Sparsity ì ìš© ê°€ëŠ¥ì„± ì¡´ì¬ |

---

### 2.4 ğŸ”¹ **Task-adaptive Embedding**

| ì „ëµ                             | ì˜ˆì‹œ                     | ì ìš© ë°©í–¥                               |
| -------------------------------- | ------------------------ | --------------------------------------- |
| **Adapter-based fine-tuning**    | PEFT, LoRA               | Gemini Embeddingì— task adapter ì‚½ì…    |
| **Prompt-Conditioned Embedding** | BGE-ICL, Promptagator    | Prompt â†’ embedding conditioning ê°€ëŠ¥    |
| **Dynamic Representation Size**  | Matryoshka, TokenMerging | Qì˜ ê¸¸ì´ë‚˜ íƒœìŠ¤í¬ì— ë”°ë¼ ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥ |

---

## ğŸ“š 3. ì •ë¦¬: Gemini Embedding ë³´ì™„ì„ ìœ„í•œ ì—°êµ¬ íë¦„ ë§µ

```text
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       í•œê³„ì                             ëŒ€ì‘ ì—°êµ¬ íë¦„         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ ê³ ë¹„ìš© ì¶”ë¡          â”€â”€â–¶    Distillation / Sparse Experts     â”‚
        â”‚ ë©€í‹°ëª¨ë‹¬ ì œí•œ      â”€â”€â–¶    VLM2Vec, Flamingo, CLIP-like êµ¬ì¡° â”‚
        â”‚ Task generality     â”€â”€â–¶    Adapter tuning / Prompt embeddingâ”‚
        â”‚ Fixed dimension    â”€â”€â–¶    Matryoshka / Dynamic width models â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœï¸ ë‚´ í•´ì„ ë° ì œì•ˆ

* Gemini Embeddingì€ **ëª¨ë¸ êµ¬ì¡°ë³´ë‹¤ëŠ” ë°ì´í„° ì „ëµ ì¤‘ì‹¬ì˜ ì„±ëŠ¥ í˜ì‹ **ì„ ë³´ì—¬ì£¼ì—ˆìŒ
  â†’ ì´í›„ ë°œì „ì€ **ëª¨ë‹¬ë¦¬í‹° í™•ì¥**ê³¼ **ì¶”ë¡  íš¨ìœ¨í™”** ì¤‘ì‹¬ì´ ë  ê²ƒ

* íŠ¹íˆ, **Geminië¡œ ìƒì„±í•œ high-quality synthetic datasetì„ í™œìš©í•œ Distilled Lightweight Embedding** ëª¨ë¸ì€ ì‹¤ìš©ì„±ì´ ë†’ìŒ
  â†’ ì˜ˆ: **Gemini-Teacher + Gecko-Student**

* ë§ˆì§€ë§‰ìœ¼ë¡œ, \*\*"prompt-conditioned dynamic embedding"\*\*ì€ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì™€ inference budgetì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì°¨ì›ì„ ì¡°ì •í•  ìˆ˜ ìˆì–´ ë¯¸ë˜ ì§€í–¥ì ì¸ ë°©í–¥


# Q : Gemini Embeddingì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì—ì„œ Gemini LLMì„ í™œìš©í•œ ë°ì´í„° ìƒì„± ë° í•„í„°ë§ ê¸°ë²•ì€ ê¸°ì¡´ì˜ ìˆ˜ì‘ì—… ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ ìƒ˜í”Œë§ ë°©ì‹ê³¼ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì¥ì ì´ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ë°ì´í„° í’ˆì§ˆ ì „ëµì´ ì„±ëŠ¥ í–¥ìƒì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ê°€?



ë‘ ì§ˆë¬¸ ëª¨ë‘ Gemini Embeddingì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ì— ê°ê° ìì„¸í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

---

## Q1. Gemini LLM ê¸°ë°˜ ë°ì´í„° ìƒì„±/í•„í„°ë§ ê¸°ë²•ì˜ ì¥ì ê³¼ ì„±ëŠ¥ ê¸°ì—¬

### ğŸ“Œ ê¸°ì¡´ ë°©ì‹ vs Gemini ë°©ì‹ ë¹„êµ

| í•­ëª©                       | ê¸°ì¡´ ë°©ë²• (ìˆ˜ì‘ì—… / ë£° ê¸°ë°˜)    | Gemini LLM í™œìš© ë°©ì‹                                      |
| -------------------------- | ------------------------------- | --------------------------------------------------------- |
| **positive/negative ì„ íƒ** | ìˆ˜ë™ ë ˆì´ë¸”ë§, BM25 ê¸°ë°˜ ìœ ì‚¬ë„ | LLM ê¸°ë°˜ semantic íŒë‹¨ (ë¬¸ë§¥/ì˜ë„ ê³ ë ¤)                   |
| **hard negative mining**   | cosine ê±°ë¦¬ ê¸°ë°˜ top-kì—ì„œ ì„ íƒ | Geminiê°€ ìœ ì‚¬í•˜ì§€ë§Œ ì •ë‹µì´ ì•„ë‹Œ ë¬¸ì¥ì„ ì§ì ‘ íŒë‹¨          |
| **synthetic ìƒì„±**         | paraphrase rule, í…œí”Œë¦¿         | Few-shot promptingìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬/íŒ¨ì‹œì§€ ìë™ ìƒì„±   |
| **í•„í„°ë§**                 | í‚¤ì›Œë“œ ê·œì¹™, ìœ ì‚¬ë„ ì„ê³„ê°’      | Geminië¡œ ë¬¸ì¥ í’ˆì§ˆ ë° relevance ì ìˆ˜ í‰ê°€                 |
| **ì–¸ì–´/íƒœìŠ¤í¬ ë‹¤ì–‘ì„±**     | ì œí•œì  (ì£¼ë¡œ ì˜ì–´ ì¤‘ì‹¬)         | Geminiê°€ multilingual promptingìœ¼ë¡œ long-tail ì–¸ì–´ë„ ì§€ì› |

---

### ğŸ“ˆ ì„±ëŠ¥ ê¸°ì—¬ ê·¼ê±° (ë…¼ë¬¸ ê¸°ì¤€)

| ì „ëµ                         | ì‹¤í—˜ ê²°ê³¼                                       | ì„±ëŠ¥ í–¥ìƒ ê·¼ê±°                              |
| ---------------------------- | ----------------------------------------------- | ------------------------------------------- |
| **Gemini filtering**         | MIRACLì—ì„œ í‰ê·  +3.9ì  í–¥ìƒ (Table 8)           | ì˜ëª»ëœ ë ˆì´ë¸” ì œê±°ë¡œ í•™ìŠµ ì•ˆì •ì„± ì¦ê°€       |
| **Synthetic classification** | AmazonCounterfactualì—ì„œ +17.6ì  í–¥ìƒ (Table 7) | ë°ì´í„° ë‹¤ì–‘ì„±ê³¼ ê· í˜• í–¥ìƒ                   |
| **Hard Negative Mining**     | Figure 3ì—ì„œ 3\~7ê°œ negative ì ìš© ì‹œ ì„±ëŠ¥ ì¦ê°€  | Semantic noise ìœ ì…ìœ¼ë¡œ generalization ì¦ê°€ |

---

### ğŸ§  ì¶”ê°€ í•´ì„

* LLM ê¸°ë°˜ filteringì€ **ë‹¨ìˆœ ì˜¤ë¥˜ ì œê±°ë¥¼ ë„˜ì–´ì„œ**, **ì–¸ì–´ì  ì •í•©ì„±**ê³¼ **ì§ˆë¬¸-ì‘ë‹µ ê°„ ì˜ë¯¸ ì—°ê²°**ê¹Œì§€ ê³ ë ¤í•  ìˆ˜ ìˆìŒ
* Geminië¥¼ í†µí•œ ìë™ ë°ì´í„° ìƒì„±ì€ **ì €ë¹„ìš© ê³ í’ˆì§ˆ ëŒ€ê·œëª¨ í•™ìŠµì´ ê°€ëŠ¥**í•˜ë‹¤ëŠ” ì ì—ì„œ ê¸°ì¡´ human-in-the-loop ë°©ì‹ ëŒ€ë¹„ scalability ìš°ìœ„



# Q : Multi-resolution loss(MRL) ê¸°ë²•ì´ í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ ì„ë² ë”© ì°¨ì› ìš”êµ¬ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì†ì‹¤ì„ ê³„ì‚°í•˜ë©°, ì‹¤ì œ í™œìš© ì¸¡ë©´ì—ì„œ ë‹¨ì¼ ê³ ì • ì°¨ì›ì„ ì‚¬ìš©í•˜ëŠ” embedding ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€?


### âš™ï¸ ì†ì‹¤ ê³„ì‚° ë°©ì‹

1. ì „ì²´ embedding vector `E âˆˆ â„^3072` ìƒì„±
2. ì•ë¶€ë¶„ ì˜ë¼ì„œ ì—¬ëŸ¬ sub-embedding ì¶”ì¶œ:

   * `E_768 = E[:, :768]`, `E_1536 = E[:, :1536]`, `E_3072 = E`
3. ê° ì°¨ì›ë³„ë¡œ NCE lossë¥¼ ë”°ë¡œ ê³„ì‚°:

$$
\mathcal{L}_{total} = \frac{1}{3} \left( \mathcal{L}_{768} + \mathcal{L}_{1536} + \mathcal{L}_{3072} \right)
$$

* ëª¨ë“  ì°¨ì›ì—ì„œ **cosine similarity ê¸°ë°˜ contrastive loss**ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì ìš©í•¨

---

### ğŸ“Š ë‹¨ì¼ ì°¨ì› ë°©ì‹ vs MRL ë°©ì‹ ë¹„êµ

| í•­ëª©            | ë‹¨ì¼ ê³ ì • ì°¨ì› ëª¨ë¸         | Multi-resolution Loss (MRL)                  |
| --------------- | --------------------------- | -------------------------------------------- |
| **ì¥ì **        | êµ¬í˜„ ê°„ë‹¨, ì¶”ë¡  ê²½ëŸ‰í™” ìš©ì´ | ë‹¤ì–‘í•œ í™˜ê²½ì— í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ëŒ€ì‘ ê°€ëŠ¥        |
| **ë‹¨ì **        | ì°¨ì› ë³€ê²½ ì‹œ ì¬í•™ìŠµ í•„ìš”    | í•™ìŠµ ì‹œ cost ì¦ê°€ (multi-loss)               |
| **ì ìš© ìœ ì—°ì„±** | íŠ¹ì • íƒœìŠ¤í¬ ì „ìš©            | task/latency ì¡°ê±´ì— ë§ê²Œ ì„œë¹™ ì°¨ì› ì„ íƒ ê°€ëŠ¥ |
| **ëª¨ë¸ í¬ê¸°**   | ì°¨ì› = ëª¨ë¸ êµ¬ì¡°            | êµ¬ì¡°ëŠ” ê³ ì •, í‘œí˜„ë ¥ì€ ì„ íƒ ê°€ëŠ¥              |

---

### ğŸ’¡ ì‹¤ì œ í™œìš© ì˜ˆ

* **768-dim**: vector search engine (FAISS, Weaviate ë“±) ìµœì í™”
* **1536-dim**: reranking, semantic scoring
* **3072-dim**: classification, clustering ë“± ì •ë³´ëŸ‰ ìš”êµ¬ ë†’ì€ íƒœìŠ¤í¬

â†’ MRLì€ **í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ downstream íƒœìŠ¤í¬ì— ëŒ€ì‘**í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ êµ¬ì¡°ì  ì´ì 

---

## ğŸ“š ìš”ì•½

| ì§ˆë¬¸                           | í•µì‹¬ ë‹µë³€                                                                                               |
| ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| Gemini ê¸°ë°˜ ë°ì´í„° ì „ëµì˜ ì¥ì  | ì˜ë¯¸ ê¸°ë°˜ judgment + ìë™í™”ëœ ê³ í’ˆì§ˆ ë°ì´í„° ìƒì„±/ì •ì œë¡œ ê¸°ì¡´ ìˆ˜ì‘ì—…ë³´ë‹¤ ì •í™•ì„±, ë‹¤ì–‘ì„±, í™•ì¥ì„±ì—ì„œ ìš°ìœ„ |
| MRLì˜ ì›ë¦¬ì™€ ì¥ë‹¨ì             | ì„œë¸Œì°¨ì›ë³„ lossë¥¼ ë™ì‹œì— í•™ìŠµí•¨ìœ¼ë¡œì¨ ì¶”ë¡  ìƒí™©/íƒœìŠ¤í¬ì— ë§ì¶° ì„ë² ë”© ì°¨ì›ì„ ìœ ì—°í•˜ê²Œ ì„ íƒí•  ìˆ˜ ìˆìŒ     |
