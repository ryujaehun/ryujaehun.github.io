---
title: "[논문리뷰] Pretraining Large Language Models with NVFP4"
date: "2025-10-09"

cover: https://developer-blogs.nvidia.com/wp-content/uploads/2025/08/Optimizing-LLM-Training-png.webp

---
[논문 링크](https://arxiv.org/abs/2509.25149v1)

# NVFP4로 4-bit 프리트레이닝을 실전으로: 12B를 10T 토큰까지, FP8과 사실상 동급

## TL;DR

12B 하이브리드 Mamba-Transformer를 **10T tokens** 에서 **NVFP4(4-bit)** 로 프리트레이닝하면 **안정 구간 손실차 <1%** , 말기 **~1.5%** 로 **FP8을 근접 추종** 하고, 다운스트림 성능도 대부분 동급(수학·다국어 일부 +0.9~+3.7pp)이다. **MXFP4 대비 동일 손실에 토큰 +36%** (1.36T vs 1.0T)가 필요해 **NVFP4의 토큰 효율 우위** 가 확인된다. (근거: Fig.2, Tab.2, Fig.6)    

---

## 핵심 아이디어

* **NVFP4 포맷** : FP4(E2M1) 값을 **1×16 블록** 과 **FP8(E4M3) 블록 스케일** 로 저장(+전역 FP32 스케일)하여 **제로/포화 오류와 아웃라이어 손실** 을 줄인다. (근거: §NVFP4, Fig.1)  
* **레시피(훈련법)** : (1) **민감 층 약 15%** 는 BF16 유지(주로 마지막 8 블록), (2) **가중치에 2D 16×16 블록 스케일** 로 전·후향 일관성 보장, (3) **RHT(Hadamard) 는 Wgrad에만** , (4) **SR(Stochastic Rounding)은 그라디언트(Dgrad+Wgrad)에만**  적용. (근거: §4, Fig.4, Appx E.2–E.4)    

---

## 배경: 그들이 해결한 문제

4-bit 학습은 **처리량 2×(GB200)/3×(GB300)** , **메모리 ~1/2(FP8 대비)** 의 하드웨어 이점을 제공하지만, **체인룰 불일치** , **아웃라이어** , **양자화 편향** 으로 대규모 프리트레이닝에서 자주 **발산** 한다. NVFP4는 **더 작은 블록(16)** 과 **정밀한 스케일(E4M3)** 로 **MXFP4**  대비 수치적 정확도를 높이며, 해당 한계를 **훈련 레시피** 로 보완한다. (근거: Tab.1, §NVFP4)  

---

## 새로운 접근법: NVFP4 Training Recipe

1. **부분 고정밀 유지** : 모든 선형층 FP4는 발산. **마지막 4~8 블록** 을 BF16로 두면 안정 수렴(12B/1.2B 공통 경향). (근거: Appx E.2 Fig.9) 
2. **2D Weight Scaling(16×16)** : 전·후향이 다른 축으로 스케일되어 **$(w_{\text{fprop}}\neq w_{\text{bprop}})$**  → 체인룰 위반. 가중치에 **2D 블록 스케일** 을 적용해 **같은 양자 표현** 을 유지. (근거: §4.3, Fig.14)  
3. **RHT는 Wgrad 전용** : Fprop/Dgrad에 적용하면 **오히려 악화** , **Wgrad 입력에만**  적용 시 손실 개선. **행렬 크기 d=16** 이 계산-정확도 균형. (근거: Appx E.4 Fig.11–12)  
4. **SR은 Grad에만** : 그라디언트에 SR이 **수렴의 핵심** , 활성/가중치에 SR은 **발산**  유발. 12B는 **Dgrad·Wgrad 모두**  SR 필요. (근거: §4.4, Appx E.3 Fig.10)  

> **요약:**  네 가지가 **세트** 로 동작해야 10T 지평에서 안정 수렴하며, 하나라도 빼면 수렴성·손실이 악화된다. (근거: Fig.4, Fig.8)  

---

## 작동 원리: 구체적인 예시로 살펴보기

아주 단순화한 **선형층(입력 3, 출력 3)**  한 타일(블록)만 생각하자.

* **양자화 스케일링** : 먼저 텐서 전체를 FP32 스케일 $(s_\mathrm{tensor})$로 리맵 → 각 **1×16(여기선 1×3 가정) 블록** 마다 **FP8(E4M3)**  스케일 $(s_\mathrm{block})$을 곱해 **FP4 표현 범위** 로 압축한다. 이렇게 하면 **작은 값의 0-화** 와 **큰 값의 포화** 를 줄인다. (근거: §NVFP4) 
* **체인룰 일관성** : 전향(Fprop)은 행 기준, 후향(Bprop)은 열 기준으로 축이 바뀐다. 1D 스케일이면 (w)의 **양자 표현이 전/후향에서 달라짐** $((w_{\text{fprop}}\neq w_{\text{bprop}}))$ → **2D 16×16**  스케일로 같은 타일에 **동일 스케일** 을 적용해 $(w_{\text{fprop}}=w_{\text{bprop}})$에 가깝게 만든다. (근거: §4.3, Fig.14)  
* **아웃라이어 완화(RHT)** : 활성/그라디언트를 **Hadamard×sign** 로 섞어 **블록 내 분산** 을 키우면 **FP4 오차** 가 완화된다. 그러나 Fprop/Dgrad에 적용하면 **양자화 불일치** 가 커져 역효과 → **Wgrad 전용** 이 안전하다. (근거: Fig.11) 
* **편향 제거(SR)** : FP4 근방에서 최근접 반올림은 **편향** 을 만든다. **SR(확률적 반올림)** 을 그라디언트에만 써서 편향을 줄인 게 수렴의 관건이었다. (근거: §4.4, Fig.10)  

---

## 성능 검증: 주요 결과

* **프리트레인 손실 곡선(12B·10T)** : 안정 구간 **상대 오차 <1%** , lr decay 구간 **~1.5%** 로 **FP8을 근접 추종** . (근거: Fig.2) 
* **다운스트림(BF16 평가)** : **MMLU-Pro 62.58 vs 62.62(−0.04pp)** , **GSM8K-CoT +3.19pp** , **MGSM +3.66pp** , **ARC-C 동률** , **코딩 계열 −2.5~−3.2pp** . (근거: Tab.2)   
* **NVFP4 vs MXFP4(8B)** : **NVFP4 상대오차 ~1.5%** , **MXFP4 ~2.5%** . **MXFP4가 NVFP4 손실을 맞추려면 +36% 토큰(1.36T/1.0T)**  필요. (근거: Fig.6) 
* **말기 정밀 전환(12B·10T)** : **8.2T부터 Fprop만 BF16 전환**  시 **손실 1.5%→0.5%** , 고정밀 연산 **~6%** 만 추가. (근거: Fig.7) 

---

## 우리의 관점: 강점, 한계, 그리고 이 연구가 중요한 이유

**강점** 

* **정확도-비용 Pareto 개선** : 같은 정확도에서 **토큰 예산 36% 절감** (MXFP4 대비) → 시간·에너지·$ 절감 잠재. (근거: Fig.6) 
* **스케일 신뢰성** : **12B·10T** 라는 대규모 지평에서 **안정 수렴** 과 **다운스트림 동급성** 을 동시 달성. (근거: Fig.2, Tab.2)  
* **메서드 투명성** : **아블레이션(Fig.4/8/10/11/12/14)** 로 각 구성요소의 필요조건을 체계적으로 입증. (근거: §4, Appx E)  

**한계** 

* **코딩 태스크 약세** : HE+/MBPP+에서 **−2.5~−3.2pp** . (근거: Tab.2) 
* **부분 고정밀 의존** : 현재 **마지막 8 블록(≈15%)**  BF16 유지 필요성. (근거: §5 설정) 
* **시스템 지표 부재** : 논문은 **런타임/비용/에너지** 의 엔드투엔드 실측은 범위 밖으로 둠. (근거: §Intro/Conclusions 범위 언급) 

**왜 중요한가** 

* **Blackwell 세대 최적화 포맷** (Tensor Core 네이티브, FP4 **2×/3×**  처리량, **1/2 메모리** ): 모델/데이터 스케일이 커질수록 **경제성 임계점** 을 앞당긴다. (근거: Tab.1) 


## 다음 단계는?: 앞으로의 길

* **완전 FP4(전 층)** : **층 민감도 지표(Wgrad 오차 등)** 로 **BF16↔FP4 동적 게이팅** 을 탐색. (근거: Appx E.2) 
* **Forward-only 정밀 전환 자동화** : **lr decay 시작점**  등 **체인지포인트** 를 탐지해 **8.2T 전후** 의 전환을 자동 결정. (근거: Fig.7) 
* **어텐션/통신 경로 FP4화** : GEMM 외 경로까지 FP4를 확장하고 **TTFT/TPOT/Throughput/$·kWh per 1M tok** 를 **동일 세팅** 으로 공시. (근거: Tab.1) 
* **형식별 스케일링 법칙** : **NVFP4 vs MXFP4** 를 **P·T 지평** 별로 정량 모델링하여 **최적 형식·토큰 예산** 을 사전 설계. (근거: Fig.6 코멘트) 

---

### 참고 도표 (요약)

| 항목             |                          수치 | 근거                    |
| -------------- | --------------------------: | --------------------- |
| 프리트레인 스케일      |             12B, 10T tokens | Fig.2                 |
| 손실차(안정/말기)     |                 <1% / ~1.5% | Fig.2                 |
| 다운스트림 하이라이트    | GSM8K +3.19pp, MGSM +3.66pp | Tab.2                 |
| 코딩 태스크         |                 −2.5~−3.2pp | Tab.2                 |
| NVFP4 vs MXFP4 |    MXFP4 동일 손실에 **+36%**  토큰 | Fig.6                 |
| 말기 전환          |     8.2T에서 Fwd만 BF16 → 0.5% | Fig.7                 |
| RHT/SR/2D 필요성  |                각 요소 제거 시 악화 | Fig.4/8/10/11/12/14   |

> 한 줄 결론: **NVFP4 레시피** 는 **FP8-동급 정확도** 를 **FP4 비용구조** 에서 실현하는 실증 사례이며, **형식·스케일·정밀전환** 을 정교화하면 **완전 FP4** 로의 이행과 **엔드투엔드 비용 절감** 이 가속될 것이다. (근거: Fig.2/6/7, Tab.1/2)     


### 토글을 클릭하면 논문에 대한 자세한 LLM 질의응답 내용을 확인할 수 있습니다.

<details markdown="block">
<summary>▶️<strong>클릭하여 펼치기</strong></summary>


</details>
---


## 프롬프트 1.1.1 (연구의 공백)

```
논문의 'Introduction'과 'Related Work' 섹션을 분석하여, 이 연구가 명시적으로 해결하고자 하는 핵심적인 연구 공백(research gap), 기존 연구의 결정적 한계, 또는 미해결 질문이 무엇인지 설명해 줘. 저자들이 설명하는, 이 논문 출판 시점의 '최신 기술(state of the art)'은 어떤 상태였는지 요약해 줘.
```


**결론 압축:**  이 논문이 겨냥한 공백은 “**FP4(4-bit) 정밀도로 수조(token-trillion) 단위 LLM 사전학습을 안정적으로 수행** 하는 방법 부재”다. 저자들은 **NVFP4 포맷 + 2D weight scaling + Random Hadamard Transform(RHT) + stochastic rounding** 로 **12B 모델·10T 토큰** 에서 **FP8 대비 <~1.5%**  손실 격차로 수렴·일치 성능을 달성해, **공개 기준 최초의 ‘멀티-트릴리언 토큰’ FP4 프리트레이닝 근거** 를 제시한다. (근거: §Intro, §3 Fig.2, Tab.2, §6)     


## 1) 무엇이 ‘연구의 공백’인가?

* **FP8 → FP4 이행의 간극:**  산업계 SOTA는 FP8 훈련이 널리 채택되었으나, **FP4(특히 대규모·장기 토큰 지평)** 는 **수렴 안정성/체인룰 불일치/양자화 바이어스/실장 제약** 이 병목이었다. (근거: §Intro) 

* **하드웨어 동인은 충분하지만 알고리즘 격차:**  Blackwell Tensor Core는 FP4에서 **수학 처리량 2–3×(vs FP8)** , **메모리 ~1/2(대비 FP8)** 의 이점을 제공하나, **방법론 부재** 로 실효 이득을 프리트레이닝까지 확장하지 못했다. (근거: Tab.1/§NVFP4 Format) 

* **사전학습 규모의 증거 부족:**  기존 FP4 연구/형식(MXFP4 등)은 있었지만, **다중 트릴리언 토큰 스케일** 에서 **FP8 동급 수렴·정확도** 를 **체계적으로 보인 공개 결과가 부재** 했다. (근거: §Intro, §6)  

* **정확한 문제 진단:**  (i) **forward/backward 축 불일치** 로 같은 텐서가 서로 다른 양자 표현을 갖게 되어 **체인룰 파괴** (weight 기준), (ii) **gradient 양자화 바이어스** (FP4에서 더 심각), (iii) **블록 단위 outlier**  대응 필요. (근거: §4.3, §4.4, Appx E)   

---

## 2) 저자들이 제시하는 해법(=공백 충족)

* **NVFP4 포맷 고도화:**  **더 작은 마이크로 블록(16)** , **FP8 분수 정밀 스케일 + FP32 텐서 스케일의 2-레벨 스케일링** 으로 **국소 동적 범위 포착**  및 정밀도 개선. (근거: §Intro/NVFP4 Format) 

* **2D weight scaling:**  **16×16 블록** 으로 fprop/bprop 모두 **동일 양자 표현** 을 유지해 체인룰 불일치를 완화(손실 개선). (근거: §4.3, Fig.4, Fig.14)   

* **RHT(Random Hadamard Transform):**  블록 outlier 분산을 위해 적용하되, **weights에는 비적용** , **Wgrad에 제한** 하여 일관성 손상 방지. (근거: §4.3, Appx E.4.1) 

* **Stochastic rounding(grad 필수):**  **gradient 양자화 바이어스** 를 줄여 **12B에서 수렴 필수 요건** 임을 실증. (근거: §4.4, Fig.4)  

* **혼합 정밀 계층:**  민감 계층을 상위 정밀도로 유지(일부 BF16 등)해 전체 수렴 안정화. (근거: §Training Methodology, Fig.4) 

---

## 3) 이 논문이 출판 시점에 규정한 ‘SOTA’ 맥락

| 축         | 기존 SOTA(요지)                                                       | 본 논문이 주장·갱신한 점                                                                                                                            |
| --------- | ----------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| 훈련 정밀도    | **FP8 훈련 광범위 채택** (Micikevicius+ 등)                                | **FP4(NVFP4)로 12B·10T 프리트레이닝** 을 안정 수렴, **손실 격차 <~1.5%**  유지, **다운스트림 동급**  (MMLU-Pro 62.58 vs FP8 62.62). (근거: §Intro, §3 Fig.2, Tab.2)     |
| HW 효율     | Blackwell FP4는 **FP8 대비 수학처리량 2×(GB200)/3×(GB300)** , **메모리 ~1/2**  | 알고리즘으로 이를 **대규모 프리트레이닝까지 실효화**  (체계적 방법·어블레이션). (근거: Tab.1, Fig.4)                                                                         |
| FP4 형식 비교 | **MXFP4**  중심 보고                                                   | **NVFP4 vs MXFP4** : **NVFP4가 동일 손실에 MXFP4 대비 토큰 **–36%**  절감(1.36T→1T)** . (근거: §5 Fig.6)                                                   |

---

## 4) 미해결 질문 & 한계(저자 서술)

* **적용 범위 확대:**  **모든 선형층 양자화** , **어텐션/통신 경로로 확장** , **더 큰 모델·더 긴 토큰 지평·MoE 아키텍처** 로의 일반화가 숙제. (근거: §Conclusions/Future Work) 

* **정밀도 전환 전략:**  말기(lr decay) 구간의 **정밀도 상향(BF16/MXFP8)**  전환이 손실 격차 완화에 유용— **최소량의 고정밀 추가**로 대부분을 FP4로 수행 가능. (근거: Fig.2 설명/Appendix D 언급) 

* **체인룰·일관성의 일반해:**  **2D scaling/RHT 범용 설계** 의 **다양 아키텍처·커널** 에 대한 보편성/최적 파라미터(예: Hadamard 크기·랜덤화 규칙)는 **추가 규명 필요** . (근거: §4.3/“Hadamard size & randomization” 실험)  

---

## 5) 핵심 수치(SOTA 컨텍스트)

* **규모:**  **12B 파라미터** , **10T 토큰**  프리트레이닝을 FP4로 수행(공개 기준 최장). (근거: §Abstract/§3)  
* **손실 추적:**  안정 구간 **상대 오차 <1%** , 말기 **~1.5%** . (근거: Fig.2) 
* **다운스트림:**  **MMLU-Pro 62.58 vs FP8 62.62** , 광범위 태스크 동급(코드 제외 약간 열세). (근거: §Abstract, Tab.2)  
* **형식 비교:**  **NVFP4 = MXFP4 대비 -36% 토큰** 으로 동일 손실 도달(1.36T→1T). (근거: §5 Fig.6) 

> 정리: 출판 시점의 SOTA는 **FP8 훈련 표준화 + Blackwell의 FP4 HW 이점** 이 존재했으나, **대규모 FP4 프리트레이닝의 안정·정확 수렴** 은 공백이었다. 본 논문은 **NVFP4+알고리즘 묶음** 으로 **12B·10T** 에서 **FP8 동급** 을 보이며 그 공백을 메웠다. (근거: §Intro, §3, Fig.2, Tab.2, §6)     



## 프롬프트 1.1.2 (핵심 가설)

```
이 논문의 중심 가설(central hypothesis) 또는 핵심 주장은 무엇인가? '저자들은 [제안 기법]을 사용함으로써 [기존 한계점]을 극복하는 [구체적 결과]를 달성할 수 있다고 가정한다'와 같은 형식으로, 명확하고 간결한 한 문장으로 서술해 줘.
```

**가설(한 문장):**  저자들은 **NVFP4 포맷** (16-elem FP4 블록+E4M3 블록 스케일+FP32 텐서 스케일)과 **2D weight scaling(16×16)** , **RHT의 Wgrad 한정 적용** , **gradient stochastic rounding** , **민감 선형층의 BF16 유지** 를 조합하면 **FP4 훈련의 체인룰 불일치·양자화 바이어스·outlier 문제** 를 극복하여 **12B**  LLM을 **10T tokens** 에서 **FP8 대비 검증 손실 격차 ≤≈1.5%** 로 안정 수렴시키고(안정 구간 <1%, 말기 ~1.5%) **다운스트림 정확도 동급** 을 달성할 수 있다고 주장한다. (근거: §NVFP4 Format/§4.3–4.4, Fig.2–3, Tab.2, §6)          


## 프롬프트 1.2.1 (독창성 식별)

```
논문 전체를 바탕으로, 가장 중요하고 독창적인 기여(contribution) 1~3가지를 구별되는 항목으로 나열해 줘. 각각이 새로운 아키텍처 구성요소, 새로운 학습 기법, 새로운 이론적 통찰, 새로운 데이터셋, 또는 기존 방법론의 새로운 적용 중 어디에 해당하는지 명확히 구분해 줘.
```

**결론 압축:**  기여는 (1) **NVFP4 4-bit 마이크로스케일 형식** (16-elem 블록, E4M3 블록 스케일+FP32 텐서 스케일) 제안, (2) **FP4 안정 훈련 레시피** (가중치 2D 스케일링 16×16 + Wgrad 전용 RHT + 그래디언트 SR + 소수 고정밀 층 유지), (3) **12B·10T 대규모 실증과 MXFP4 대비 36% 토큰 절감** 이다. (근거: §2 Fig.1, §4 Fig.4, §3 Fig.2/Tab.2, §5 Fig.6)     

---

## 독창적 기여 1 — NVFP4 데이터 형식(새로운 **숫자표현/형식** )

* **핵심:**  MXFP4(블록 32, UE8M0 스케일) 대신 **블록 16(E2M1)** 에 **E4M3 블록 스케일** 과 **FP32 텐서 스케일** 을 결합한 **2단계 마이크로스케일링** 을 도입. 블록 내 **≥6.25%**  값(amax)이 **준-FP8 정밀** 로 표현되고 나머지는 FP4로 저장되어 동적범위를 보존. (근거: §2 Fig.1) 
* **의의(정량):**  **블록 32→16**  축소로 블록 내 분산 축소, **UE8M0→E4M3** 로 정밀 스케일링, **파워-오브-투 반올림 손실(±4,±6 binade 손실 가능)** 을 회피해 **포화/제로화 감소** . (근거: §2/Appx B.4)  

> 분류: **새로운 데이터 형식(수치 표현)** 

---

## 독창적 기여 2 — FP4 안정화를 위한 훈련 방법론(새로운 **학습 기법/훈련 레시피** )

* **가중치 2D 스케일링(16×16):**  fprop/bprop에서 **동일 양자 표현** 을 강제하여 **체인룰 불일치** 를 완화, 12B에서 손실 개선. (근거: §4.3 Fig.4/14)  
* **RHT 범위 제한:**  **Hadamard 변환은 Wgrad에만 적용(크기 16×16)** , 가중치에는 미적용하여 fprop/bprop 불일치 악화를 방지. (근거: §4.3/Appx E.4.1) 
* **Stochastic Rounding(SR):**  **그래디언트에 한정** 해 **수렴 필수** , forward에 적용 시 오히려 악화. (근거: §4.4 Fig.4/Appx E.3) 
* **혼합정밀 유지:**  **민감 선형층 ≤~15–16%** 를 **BF16/MXFP8** 로 유지하면 10T 토큰 지평에서도 안정 수렴(마지막 4블록만 고정밀로도 가능). (근거: §4 Fig.4, §4.1)  

> 분류: **새로운 훈련 기법(방법론적 레시피)** 

---

## 독창적 기여 3 — 대규모 FP4 프리트레이닝의 공개 **실증**  및 형식 비교(새로운 **경험적 근거/적용** )

* **12B·10T 실증:**  **12B 파라미터** 를 **10T 토큰** 으로 NVFP4로 사전학습 시, 검증 손실이 FP8을 **안정 구간 <1%** , 말기 **~1.5%**  격차로 추종; **다운스트림 정확도 동급** (Tab.2, 일부 코드만 소폭 열세). (근거: §3 Fig.2/3, Tab.2)   
* **NVFP4 vs MXFP4 효율:**  **동일 손실 도달에 MXFP4가 +36% 토큰(1.36T vs 1.0T)**  필요 → **토큰·시간 비용 증가** 를 정량화. (근거: §5 Fig.6)  
* **의미:**  **“멀티-트릴리언 토큰 지속 FP4 프리트레이닝” 공개 근거의 최초 제시** 와 **정밀도 전환(BF16 등)** 으로 말기 손실 격차 축소 전략 제안. (근거: §6, Appx D Fig.7)  

> 분류: **새로운 경험적 증거/스케일 적용 및 형식 비교 인사이트** 


## 프롬프트 1.2.2 (저자 관점에서의 강점)

```
저자들의 관점에서, 자신들의 접근법이 이전 방법들보다 우월한 이유는 무엇인가? 그들이 자신들의 연구가 지닌 독창성과 강점을 뒷받침하기 위해 사용하는 핵심 논거를 인용하거나 알기 쉽게 설명해 줘.
```

**결론 압축:**  저자들은 **NVFP4 수치형식(블록 16·E4M3·FP32 텐서 스케일)** 과 **훈련 레시피(가중치 2D 스케일 16×16, Wgrad 전용 RHT 16×16, Grad SR, 민감층 ~15% 고정밀)** 로 **체인룰 불일치·양자화 바이어스·블록 아웃라이어** 를 눌러 **12B@10T 프리트레이닝** 에서 **FP8 대비 손실 격차 <1%(안정구간) / ~1.5%(말기)** , **다운스트림 동급** 을 달성한다고 주장한다. (근거: §2, §3 Fig.2–3, §4 Fig.4, Tab.2)    

---

## 저자 관점의 핵심 강점 (숫자 중심)

1. **수치 형식 자체의 우월성: NVFP4 → 블록 16·E4M3·FP32 스케일(2단계 MX)** 

* MXFP4(블록 32·UE8M0) 대비 **블록 크기 32→16** 으로 내부 동적범위 축소, 스케일 **UE8M0→E4M3** 로 유효 맨티사 증대, **FP32 텐서 스케일** 로 전역 범위 보존. 블록 내 **≥6.25% 값(amax)** 을 **준-FP8**  정밀로 보존해 포화/제로화를 줄임. (근거: §2 Fig.1, Appx B.4) 
* 결과적으로 **MXFP4의 파워-오브-투 스케일 반올림으로 생기는 1 binade 손실(±4, ±6) 위험** 을 회피. (근거: §2, Appx B.4) 

2. **체인룰 일관성 해결: 가중치 2D 스케일(16×16)** 

* FP4에서 fprop/bprop가 **서로 다른 축** 으로 스케일/변환되어 **같은 가중치 텐서가 서로 다른 양자표현** 이 되면 **체인룰이 깨짐**  → 손실 악화. **가중치 16×16 스케일** 로 **fprop=bprop 표현 일치** 를 강제해 **12B 손실 개선** 을 보임. (근거: §4.3, Fig.4/14)   

3. **아웃라이어 완화는 ‘가중치 제외’ 원칙: RHT를 Wgrad에만 16×16 적용** 

* **Hadamard 변환** 을 가중치에 적용하면 fprop/bprop **불일치 심화**  → 가중치에는 **미적용** , **Wgrad에만 16×16** 으로 제한해 **효과/일관성** 을 양립. (근거: §4.2–4.3, Appx E.4.1)  
* **행렬 크기 d=16** 이 12B·롱호라이즌에서 **수렴-성능 균형 최적** (d가 너무 작으면 수렴 저하, 너무 크면 이득 한계). (근거: Appx E.4.2 Fig.13) 

4. **양자화 바이어스 제거: Grad에만 Stochastic Rounding(SR)** 

* **Deterministic rounding** 은 FP4에서 **바이어스↑** ; **Grad SR** 이 **12B 수렴에 필수** , **fprop 텐서에 SR은 오히려 악화** . (근거: §4.4, Appx E.3, Fig.4)   

5. **혼합정밀 최소화 전략: 민감 선형층만 ≤~15% 고정밀 유지** 

* 전체 **GEMM은 FP4** , 다만 **민감 선형층 일부(주로 말단)** 을 **BF16/MXFP8** 로 두면 **발산 방지/수렴 안정화** . 12B 실험에선 **첫 2블록+마지막 8블록(≈16%)** 을 고정밀로 두되, **마지막 4블록만 고정밀** 이어도 안정. (근거: §4 Fig.4–5, Appx E.2)  

6. **규모·정확도 실증: 12B@10T에서 FP8 동급** 

* **검증 손실:**  안정 구간 **<1%** , 말기 **~1.5%** (lr decay 구간). **다운스트림:**  **MMLU-Pro 5-shot 62.58 vs FP8 62.62** , 범용/수학/다국어 전반 **동급** , 코드만 소폭 열세. (근거: §3 Fig.2–3, Tab.2)  

7. **형식 간 효율 비교의 정량 근거: NVFP4는 MXFP4 대비 토큰-효율↑** 

* **같은 최종 손실** 에 **MXFP4가 +36% 토큰(1.36T vs 1.0T)**  필요 → **훈련 시간·비용 증가** . NVFP4가 **동일 손실을 더 적은 토큰** 으로 달성. (근거: §5 Fig.6) 

8. **HW-정렬 이점의 실효화: Blackwell FP4 수학처리량↑, 메모리↓** 

* Blackwell **FP4 수학 처리량** : **GB200 2× / GB300 3× (vs FP8)** , **메모리 ~1/2** . 저자 레시피가 **이 이점을 ‘프리트레이닝’까지 전이**  가능함을 실증. (근거: Tab.1, §6 결론)   

9. **실용 팁(정밀 전환): 말기 BF16 전환으로 손실 격차 1.5%→~0.5%** 

* **8.2T 이후(학습의 18%)** 에 **forward(주로)** 를 **BF16** 으로 전환 시 **대부분 회복** , **총 연산 중 ~6%만 고정밀**  → **런타임 영향 최소** . (근거: Appx D Fig.7)  

> 요약: **형식(NVFP4)×방법론(2D W-scaling, Wgrad-RHT, Grad-SR, 부분 혼합정밀)** 의 결합이 **수치 불안정성의 근원(일관성·바이어스·아웃라이어)** 을 정면 타격했고, **12B·10T**  규모에서 **FP8 동급 정확도** 를 **토큰·시간 효율** 까지 포함해 **정량적으로 증명** 했다. (근거: §2–§5, Fig.2/4/6, Tab.1/2)     


## 프롬프트 1.3.1 (알고리즘 단계별 설명)

```
핵심 알고리즘, 모델 아키텍처, 또는 주요 방법론을 단계별(step-by-step)로 설명해 줘. 독자는 AI 분야의 대학원생 수준이라고 가정해. 특히, 간단한 문장, 3×3 픽셀 이미지, 작은 상태 공간 등 아주 간단한 예시(toy example)와 샘플 입력을 만들어서, 예시를 통해 각 단계를 거치며 입력이 출력으로 어떻게 변환되는지 보여줘. 등장하는 모든 핵심 용어와 변수는 그 즉시 정의해 줘.
```


**결론 압축:**  한 스텝의 학습은 **(1) NVFP4 양자화(전역 FP32 스케일 → 블록 E4M3 스케일 → FP4 변환)** , **(2) 가중치 2D 스케일(16×16)로 fprop/bprop 일관성 유지** , **(3) Wgrad 입력에만 RHT(16×16)** , **(4) 그래디언트에만 Stochastic Rounding** , **(5) 말단 소수 층 BF16 유지** 의 순으로 진행된다. 이 레시피로 **12B@10T**  프리트레이닝에서 **FP8 대비 안정구간 <1%·말기 ~1.5% 손실 격차** 와 **다운스트림 동급** 을 달성한다. (근거: §2 Fig.1/Tab.1, §3 Fig.2–3, §4.1–4.4, Appx E)     

---

## 0) 준비: 핵심 용어

* **NVFP4** : FP4(요소) + **E4M3 블록 스케일(8비트)**  + **FP32 전역 스케일** 의 2단계 마이크로스케일 형식. 블록 크기 **16** . (근거: §2) 
* **2D 스케일(가중치)** : 가중치 텐서를 **16×16 블록** 으로 스케일링(입력채널×출력채널). fprop/bprop가 같은 양자 표현을 쓰도록 함. (근거: §4.3) 
* **RHT** : 랜덤 하다마드 변환. **Wgrad 입력에만**  적용, 크기 **d=16** , 시드는 **고정 시드** 면 충분. (근거: §4.2, Appx E.4.1–E.4.3)   
* **Stochastic Rounding(SR)** : **그래디언트** 에만 적용해야 수렴, fprop 텐서에 쓰면 악화. (근거: §4.4, Appx E.3)  
* **혼합정밀** : 임베딩/어텐션/정규화/옵티마이저 상태는 고정밀, **말단 일부 선형층(≤~15–16%)** 만 BF16 유지. (근거: §4.1, Appx E.2)  

---

## 1) NVFP4 양자화 파이프라인 (한 텐서 기준)

**입력:**  실수 텐서 $(T\in\mathbb{R}^{m\times n})$.
**출력:**  NVFP4로 저장된 텐서 $(\widehat{T})$ (FP4 값 + 블록 스케일(E4M3) + 전역 스케일(FP32)).

**Step 1 — 전역 스케일 FP32:** 
전역 스케일 $(s_\text{glob}\in\mathbb{R}*{+})$을 곱해 **블록 스케일(E4M3)** 이 표현 가능한 범위로 재매핑. (근거: §2)
$$
T` = s * \text{glob} \cdot T
$$


**Step 2 — 블록 스케일 E4M3 (블록=16요소):** 
각 블록의 **amax** 가 FP4 최대치에 맞도록 **E4M3 스케일 $(s_b)$** 를 잡고,
$$
T`` *{\text{block}} = s_b \cdot T' * {\text{block}}
$$
로 **FP4 표현 가능 구간** 으로 이동. (근거: §2 Fig.1) 

**Step 3 — FP4 변환 & 디코드 규칙:** 
FP4로 **반올림** : 가중치/활성은 **최근접-짝수** , **그래디언트는 SR** (확률은 두 근접값까지의 거리 역비례). (근거: §4.4) 
복원은 $((s_b\cdot s_\text{glob})^{-1})$로 디스케일. (근거: §2) 

> **왜 NVFP4인가?**  MXFP4(블록 32·UE8M0) 대비 **블록 16**  + **E4M3**  + **전역 FP32** 로 **≥6.25% 값(각 블록 amax)** 을 **준-FP8 정밀** 로 보존, **파워-오브-투 스케일 반올림으로 생기는 binade 손실(±4,±6)** 을 회피. (근거: §2)  

---

## 2) 가중치 2D 스케일(16×16)로 체인룰 일관성 보장

문제: fprop은 **행(입력 채널)**  기준, bprop은 **열(출력 채널)**  기준으로 스케일·변환되어 **같은 가중치가 서로 다른 양자 표현**  → **체인룰 위반** . (근거: §4.3) 

해결: **가중치만 16×16 블록 스케일** 을 써서 fprop/bprop **동일 양자 표현** 을 강제. 활성/그래드는 기존 **1×16**  유지. (근거: §4.3, Fig.14)  

---

## 3) RHT(랜덤 하다마드) — **Wgrad 입력에만** , 크기 **d=16** 

* **왜 Wgrad만?**  fprop/dgrad에 RHT를 쓰면 변환 불일치가 커져 품질 악화. **Wgrad 입력에만 적용 시 개선** . (근거: Appx E.4.1) 
* **크기 선택:**  **d=16** 이 **4보다 좋고 128과 유사** , 비용·정확도 절충 최적. (근거: Appx E.4.2) 
* **랜덤화:**  **고정 시드** 면 충분, **무랜덤** 은 열세. (근거: Appx E.4.3 Fig.13) 

---

## 4) Stochastic Rounding — **그래디언트에만**  적용

* **필수 타깃:**  **그래디언트 SR** 이 12B 수렴에 **필수** , 다른 텐서에 SR은 **악화** . (근거: §4.4, Appx E.3)  

---

## 5) 혼합정밀 — **말단 소수 층만 BF16** 

* 모든 선형층 FP4로는 **발산** . **마지막 4블록** 만 BF16이어도 **안정** , 12B 실험에선 **처음 2 + 마지막 8 블록(≈16%)** 을 BF16으로 두는 **보수 설정**  채택. (근거: §4.1, Appx E.2)  

---

## 6) 한 층(Linear) 학습 스텝 — **절차 요약(의사코드)** 

**입력:**  (x) (BF16 활성), (W) (FP32 옵티마이저 가중치)
**출력:**  업데이트된 (W)

1. **Fprop GEMM** 

* $(x\rightarrow)$ NVFP4(전역 FP32 → 1×16 E4M3 → FP4) [최근접-짝수],
* $(W\rightarrow)$ NVFP4(전역 FP32 → **16×16 E4M3**  → FP4),
* $(y = \text{GEMM}(x_\text{FP4}, W_\text{FP4}))$ → BF16/FP32 누산. (근거: Fig.5, §4.3)  

2. **Dgrad GEMM** 

* $(\partial L/\partial y)$ (BF16)와 $(W_\text{FP4})$로 $(\partial L/\partial x)$ 계산. (근거: Fig.5) 

3. **Wgrad GEMM (+RHT, SR)** 

* **RHT(d=16)** 를 $((x, \partial L/\partial y))$의 **Wgrad 입력** 에만 적용 → NVFP4 변환,
* **그래디언트 SR** 로 FP4 변환 편향 완화,
* $(\partial L/\partial W)$ 계산(BF16/FP32 누산). (근거: §4.2, §4.4, Fig.5)   

4. **옵티마이저 업데이트** 

* (W)와 옵티마이저 상태는 FP32 유지, 업데이트 후 다음 스텝. (근거: §4.1) 

---

## 7) **토이 예시** 로 보는 변환 흐름 (4×4 가중치 블록 = 16요소)

가중치 블록 $(W\in\mathbb{R}^{4\times4})$:
$$
W=\begin{bmatrix}
6.2 & -4.3 & 0.48 & 1.7
-0.6 & 3.2 & -1.1 & 0.0
-2.9 & 5.8 & 0.12 & -0.44
1.0 & -6.0 & 4.9 & -0.5
\end{bmatrix}
$$

* **전역 스케일**  $(s_\text{glob}=1)$ (예시).
* **블록 amax**  = 6.2 → **E4M3 블록 스케일**  $(s_b=\frac{6}{6.2}\approx0.968)$로 설정(amax→FP4 최대 6에 매핑). (설명용 예시)
* 스케일 후 요소 예: $(6.2\times0.968=6.00)$, $(-4.3\times0.968=-4.16)$, $(0.48\times0.968=0.465)$, …
* **FP4 반올림(최근접-짝수)**  값 집합 $({\pm0,\pm0.5,\pm1,\pm1.5,\pm2,\pm3,\pm4,\pm6})$:

  * $(6.00\to 6)$, $(-4.16\to -4)$, $(0.465\to 0.5)$, $(1.7\to 1.5)$, $(-0.6\to -0.5)$, $(3.2\to 3)$, (-1.1\to -1), …
* **디코드** 는 $(1/(s_b\cdot s_\text{glob}))$를 곱해 근사 복원.
  → 이때 **모든 스텝을 fprop/bprop 모두 16×16 스케일** 로 공유하면 **같은 FP4 표현** 을 재사용하게 되어 **체인룰 일관성** 을 보장. (근거: §4.3, Fig.14)  

> **SR 미니 예시(스칼라 g=1.03, FP4 이웃={1,1.5})** : $(P(1)=\frac{1.5-1.03}{0.5}=0.94)$, $(P(1.5)=0.06)$ → $(\mathbb{E}[\text{SR}(g)]=1.03)$ (무편향). **그래디언트에만 SR** 을 적용해야 전체 수렴이 안정. (근거: §4.4, Appx E.3)  

---

## 8) 성능·정확도 맥락(필수 수치)

* **수렴 곡선(12B@10T):**  안정 구간 **상대 손실 <1%** , 말기(lr decay) **~1.5%** . (근거: Fig.2) 
* **다운스트림:**  전영역 **FP8 동급** , 코드만 약간 열세. (근거: Fig.3/Tab.2) 
* **NVFP4 vs MXFP4:**  NVFP4가 **학습 행태 일관 개선** , binade 손실 회피. (근거: §2 요약, §5 Fig.6)  
* **HW 이점(Blackwell):**  **FP4 수학 처리량 2×(GB200)/3×(GB300) vs FP8** , **메모리 ~1/2** . (근거: Tab.1) 

---

## 9) 한 눈에 보는 “선형층 컴퓨트 플로우” (그림 대응)

* 모든 GEMM 입력은 **NVFP4로 양자화** , Wgrad 입력에는 **RHT**  적용, 그래디언트는 **SR** , 가중치/옵티 상태는 **FP32 유지** . (근거: Fig.5, §4.1–4.4)  

---

## 10) (부록) 수식 요약

* **전역+블록 스케일:** 
 $$
  \widehat{T}=\operatorname{FP4}!\left(\underbrace{S_b} * {\text{E4M3}}\cdot\underbrace{s * \text{glob}}_{\text{FP32}}\cdot T\right),\quad
  S_b=\operatorname{diag}({s_b^{(k)}})
  $$
  (블록=16요소; 가중치에는 16×16 2D 스케일) (근거: §2, §4.3)  

* **SR(그래디언트):** 
  $$
  \operatorname{SR}(v)=
  \begin{cases}
  a, & \text{w.p. } \dfrac{b-v}{b-a}
  b, & \text{w.p. } \dfrac{v-a}{b-a}
  \end{cases}\quad (a\le v\le b)
  $$
  (근거: §4.4) 

> **정리:**  **NVFP4(전역 FP32 + 블록 E4M3) × 2D 가중치 스케일(16×16) × Wgrad-RHT(16) × Grad-SR × 말단 혼합정밀** 의 **순차 파이프라인** 이 **수치 일관성·바이어스·아웃라이어** 를 동시에 다루며, **대규모(12B·10T)** 에서도 **FP8 동급 정확도** 를 실증한다. (근거: §2–§5) 


## 프롬프트 1.3.2 ('비밀 병기' 식별)

```
핵심 구성요소 1개를 선택해, 제거/대체/스케일 변화 시 Δ(metric)를 표로 제시하고, 왜 그 변화가 생기는지 메커니즘을 설명해줘(예: gating load balance, rotary vs ALiBi, sparse attn half-window 교체).
```

# 프롬프트 1.3.2 — ‘비밀 병기’ 식별 (NVFP4 형식)

**결론 압축:**  핵심 구성요소로 **NVFP4(블록 16, E4M3 블록 스케일 + FP32 텐서 스케일)** 을 선택. 같은 세팅에서 **MXFP4로 대체** 하면 **최종 검증 손실 격차가 ~+1.0%p 악화(1.5%→2.5%)** , NVFP4 수준을 맞추려면 **토큰 +36% (1.0T→1.36T)** 가 필요하다. 메커니즘은 **블록 16·E4M3·FP32 2단계 스케일링** 으로 **동적범위 손실(한 binade)과 포화/제로화** 를 줄이고 **블록 내 ≥6.25% 값을 준-FP8 정밀로 보존** 하기 때문. (근거: Fig.6, §2)   

---

## Δ(metric) — NVFP4를 MXFP4로 대체했을 때

| 구성(한 개만 변경)       | 비교 조건         | 최종 검증 손실 상대오차 (vs BF16, %) |     토큰 예산 |     Δ vs NVFP4 |
| ----------------- | ------------- | -------------------------: | --------: | -------------: |
| **NVFP4 (기준)**     | 8B, 1T tokens |                  **~1.5%**  |     1.00T |              — |
| **MXFP4로 대체**      | 동일            |                  **~2.5%**  |     1.00T |  **+1.0%p**  악화 |
| **MXFP4 + 토큰 증액**  | 손실을 NVFP4와 맞춤 |                (NVFP4와 동급) | **1.36T**  | **+36%**  토큰 필요 |

* 수치 출처: **Fig.6** —NVFP4는 **~1.5%** , MXFP4는 **~2.5%**  상대오차(좌측), MXFP4가 **1.36T(=+36%)** 에서 NVFP4(1.0T)와 **동일 손실** (우측). (근거: Fig.6 설명/결론)  

> 해석: 같은 하드웨어·스루풋 가정 시 **+36% 토큰 = 시간/연산량도 비례 증가**  → “훈련 시간 상당 증가”로 귀결. (근거: Fig.6 코멘트) 

---

## 왜 이런 Δ가 생기는가? — 메커니즘(수치·구조)

1. **블록 크기 32→16 축소(동적범위 국소화)** 
   NVFP4는 블록 16으로 블록 내 값 분산을 낮춰 FP4 표현구간에 더 잘 적합시킨다. MXFP4(블록 32)는 같은 스케일 한계에서 **포화/제로화**  위험이 커진다. (근거: §2) 

2. **스케일 표현: UE8M0(거듭제곱) → E4M3(유효 맨티사)** 
   MXFP4는 **UE8M0(2^k)**  반올림으로 **한 binade(±4, ±6)**  손실 가능. NVFP4는 **E4M3 블록 스케일 + FP32 텐서 스케일** 로 정밀 스케일링을 제공, **스케일 반올림 손실을 회피** . (근거: §2, Appx B.4 요지) 

3. **준-FP8 보존 비율:** 
   NVFP4는 블록(16)마다 **최대값(amax)** 을 **E4M3** 로 인코딩해 **블록 내 ≥6.25% 값** 을 **준-FP8 정밀** 로 유지, 나머지는 FP4로 저장한다.
   [
   \text{rate}_{\text{near-FP8}}=\frac{1}{16}=6.25%\ (\text{per block})
   ]
   (근거: §2 Fig.1 설명) 

> 요약: **NVFP4 = (블록 16) × (E4M3 블록 스케일) × (FP32 텐서 스케일)** 의 2단계 마이크로스케일링 → **동적범위/정밀도 손실 최소화**  → **더 적은 토큰으로 동일 손실**  달성. 반대로 **MXFP4는 스케일 반올림·블록 크기 탓에 손실 증가 → 토큰 보강 필요** . (근거: Fig.6, §2)  

---

## (보너스) 같은 ‘비밀 병기’ 맥락의 다른 축 2개 — 정성 Δ

* **정밀 전환(말기 8.2T 이후 FPROP만 BF16)** : **상대오차 1.5%→0.5%** 로 회복, 전체 연산의 **~6%만 고정밀** 이라 오버헤드 작음. (근거: Fig.7) 
* **Hadamard 적용 위치/크기** : **Wgrad에만**  적용이 이득, **Fprop/Dgrad**  적용은 악화; **d=16** 이 **d=4보다 우수** , **d=128과 유사** (비용-효과 균형). (근거: Fig.11, Fig.12)  

> 전체 맥락: NVFP4 형식을 ‘핵’으로, **2D 가중치 스케일(16×16)·Wgrad-RHT(d=16)·Grad-SR·말기 일부 BF16** 을 결합해야 12B·10T 스케일에서 **안정 수렴** 과 **FP8 근접 손실** 을 달성. (근거: Fig.4 요약/권고) 


## 프롬프트 1.4.1 (핵심 결과 분석)

```
'Experiments' 또는 'Results'의 표/그림을 포함한 주요 결과를 분석해 줘. 핵심 성능 지표는 무엇인가? 어떤 벤치마크에서 보고되었는가? 저자들이 성공 증거로 가장 강조하는 결과를 요약해 줘.
```

# 프롬프트 1.4.1 — 핵심 결과 분석

**결론 압축:**  12B 하이브리드(Mamba-Transformer) 모델을 **10T tokens** 로 **NVFP4**  정밀도에서 프리트레이닝한 결과, **검증 손실은 FP8 대비 안정 구간 <1%** , 말기(lr decay)에도 **~1.5% 상대 오차** 만 유지되며, **다운스트림 정확도는 대부분 동급** (MMLU-Pro 62.58 vs 62.62 등)이다. 또한 **MXFP4 대비 같은 손실에 필요한 토큰이 −36%** (1.36T→1.0T)로 **훈련 효율** 이 높다. 말기에 **forward만 BF16로 전환(8.2T 이후)** 하면 손실 격차가 **1.5%→0.5%** 로 줄며, **고정밀 연산은 전체의 ~6%** 만 차지한다. (근거: Fig.2/3/6/7, Tab.2)     

---

## 핵심 성능 지표 (Experiments/Results의 포인트)

* **프리트레이닝 스케일:**  **12B params**  × **10T tokens** , FP8 baseline과 비교 평가. (근거: §3) 
* **검증 손실 곡선:**  NVFP4가 FP8을 **전 구간에서 근접 추종** , **안정 구간 <1%** , lr decay 시 **~1.5%** 까지 확대; **8T 토큰에서 기울기 변화는 lr decay** , **9T 토큰 근방 점프는 데이터 블렌드 변경**  영향. (근거: Fig.2) 
* **다운스트림 정확도:**  MMLU-Pro, MMLU, GSM8K-CoT, MATH, Global-MMLU, MGSM, HumanEval+, MBPP+, ARC-C, HellaSwag 등 **다영역 태스크에서 FP8 동급** (코딩 계열만 소폭 열세). (근거: Fig.3, Tab.2)  
* **형식 비교:**  **NVFP4 vs MXFP4** 에서 **MXFP4는 상대오차 ≈2.5% vs NVFP4 1.5%** , **동일 손실 달성에 +36% 토큰(1.36T)**  필요. (근거: Fig.6) 
* **말기 정밀 전환:**  **8.2T** 에서 **forward만 BF16** 으로 전환 시 **상대오차 1.5%→0.5%**  회복, **총 연산의 ~6%만 고정밀** (최종 18% 구간의 약 1/3). (근거: Fig.7, Appx D)  
* **필수 기법의 기여:**  **SR(grad), Wgrad-RHT, 2D weight scaling, 일부 BF16 층**  등 각 요소를 제거하면 **수렴 악화** (아블레이션). (근거: Fig.4) 

---

## 벤치마크 & 주요 수치 (Tab.2 발췌)

| 그룹  | 벤치마크                  | 지표     | FP8 (%) | NVFP4 (%) | Δ(NV−FP8, pp) |
| --- | --------------------- | ------ | ------: | --------: | ------------: |
| 종합  | **MMLU-Pro (5-shot)**  | acc    |   62.62 | **62.58**  |         −0.04 |
| 종합  | MMLU                  | acc    |   77.36 | **76.57**  |         −0.79 |
| 수리  | **GSM8K-CoT**          | acc    |   89.08 | **92.27**  |     **+3.19**  |
| 수리  | MATH                  | acc    |   83.32 | **81.48**  |         −1.84 |
| 다국어 | **Global-MMLU**        | acc    |   74.00 | **74.94**  |     **+0.94**  |
| 다국어 | **MGSM**               | acc    |   81.87 | **85.53**  |     **+3.66**  |
| 코드  | HumanEval+            | pass@1 |   59.93 | **57.43**  |         −2.50 |
| 코드  | MBPP+                 | pass@1 |   59.11 | **55.91**  |         −3.20 |
| 상식  | ARC-C                 | acc    |   91.81 | **91.81**  |         +0.00 |
| 상식  | HellaSwag             | acc    |   83.83 | **83.09**  |         −0.74 |

(근거: Tab.2)    

> 해석: **언어·수학·다국어**  영역에서는 **동급 혹은 일부 우위** (예: GSM8K +3.19pp, MGSM +3.66pp)이며, **코딩 계열(HE+, MBPP+)은 소폭 열세** —저자들은 **평가 노이즈/체크포인트 선택**  가능성을 언급. (근거: §3 해설) 

---

## NVFP4 vs MXFP4 — 토큰 효율 비교

* **동일한 8B 설정** 에서 **NVFP4 1.0T** 가 도달한 손실에 **MXFP4는 1.36T(+36%)**  필요 → **시간·연산량·에너지**  비용 증가로 직결. (근거: Fig.6) 
* **상대오차 기준:**  MXFP4 **~2.5%** , NVFP4 **~1.5%** 로 NVFP4가 **수렴 품질 우수** . (근거: Fig.6) 

---

## 말기 정밀 전환(“소량 고정밀”)의 효과

* **전환 시점:**  **8.2T tokens** 에서 **NVFP4→BF16** (forward만)로 전환. (근거: Fig.7) 
* **효과:**  상대오차 **1.5%→0.5%** , **고정밀 연산 비중 ~6%** (전체의 극히 일부)로 **런타임 영향 최소화** . (근거: Fig.7/Appx D)  

---

## 저자들이 강조하는 “성공 증거”

1. **장기 토큰 지평(10T)** 에서도 **손실 안정** ·**FP8 근접 추종** (<1%~1.5%), **데이터/스케줄 전환 지점 설명 포함** . (근거: Fig.2) 
2. **다운스트림 광범위 동급성** (언어·수학·다국어·상식·코드) — **대표치는 MMLU-Pro 62.58 vs 62.62** . (근거: Fig.3, Tab.2)  
3. **형식 간 효율성 검증**  — **NVFP4가 MXFP4 대비 +36% 토큰 절약** 으로 **동일 손실**  → **시간/비용 절감** . (근거: Fig.6) 
4. **레시피 구성요소의 필요성**  — **SR(grad), RHT(Wgrad), 2D W-scaling, 일부 BF16 층** 을 모두 포함해야 **발산 없이 수렴** (제거 시 악화). (근거: Fig.4) 

> 보조 맥락: Blackwell Tensor Cores는 **FP4 수학 처리량 2×(GB200)/3×(GB300)** , **메모리 ~1/2** (vs FP8)를 제공하므로, 위 수렴·정확도 결과는 **HW 이점의 실효화** 를 뒷받침. (근거: Tab.1) 

---

**요약:**  본 논문의 실험 결과는 **NVFP4가 대규모(12B, 10T) 프리트레이닝에서 FP8 동급 정확도** 를 유지하면서도, **형식 선택만으로도 훈련 효율(토큰·시간) 이득** 을 제공함을 보인다. 또한 **소량의 말기 고정밀 전환** 으로 **손실 격차를 0.5% 수준** 까지 줄일 수 있어 **실무적 트레이드오프** 가 명확하다. (근거: Fig.2/3/6/7, Tab.2)    


## 프롬프트 1.4.2 (비판적 비교)

```
제안된 방법론은 논문에서 언급된 주요 베이스라인 및 SOTA 모델들과 비교하여 어떤 성능을 보이는가? 우월성 주장을 가장 강력하게 뒷받침하는 특정 비교 지점을 식별해 줘. 반대로, 능가하지 못했거나 개선이 미미했던 결과가 있다면 이유를 정리해 줘.
```


**결론 압축:**  NVFP4는 **FP8 기준선** 과 **동일 아키텍처·데이터·스케줄** 에서 **검증 손실 <1% (안정 구간), 말기 ~1.5%**  수준으로 거의 추종하고(근거: Fig.2) , **다운스트림도 대체로 동급** 이며 일부 과제(예: GSM8K, MGSM)는 **오히려 +3.2~+3.7pp** 로 우세하다(평가 BF16)(근거: Tab.2) . 반면 **코딩 계열** (HE+, MBPP+)은 **−2.5~−3.2pp** 로 미달한다(근거: Tab.2) . 같은 FP4 계열 비교에서는 **MXFP4 대비 NVFP4가 손실 1.5% vs 2.5%** , MXFP4가 **동일 손실에 +36% 토큰(1.36T)** 을 요구하여 **효율 우위** 를 보인다(근거: Fig.6) .

---

## SOTA/베이스라인 대비 정량 비교

### 1) NVFP4 vs FP8 (동일 12B, 10T 프리트레인, 평가 BF16)

| 벤치마크 (세팅)             |     지표 |   FP8 |     NVFP4 | Δ(NV−FP8, pp) |
| --------------------- | -----: | ----: | --------: | ------------: |
| **MMLU-Pro (5-shot)**  |    acc | 62.62 | **62.58**  |         −0.04 |
| **MMLU**               |    acc | 77.36 | **76.57**  |         −0.79 |
| **GSM8K-CoT**          |    acc | 89.08 | **92.27**  |     **+3.19**  |
| **MATH**               |    acc | 83.32 | **81.48**  |         −1.84 |
| **Global-MMLU**        |    acc | 74.00 | **74.94**  |     **+0.94**  |
| **MGSM**               |    acc | 81.87 | **85.53**  |     **+3.66**  |
| **HumanEval+**         | pass@1 | 59.93 | **57.43**  |         −2.50 |
| **MBPP+**              | pass@1 | 59.11 | **55.91**  |         −3.20 |
| **ARC-C**              |    acc | 91.81 | **91.81**  |         +0.00 |
| **HellaSwag**          |    acc | 83.83 | **83.09**  |         −0.74 |

(근거: Tab.2·평가 BF16)  

**핵심 포인트:**  손실 곡선은 **안정 구간 <1%** , **lr decay 구간 ~1.5%**  오차로 FP8을 밀착 추종하며(근거: Fig.2) , **다양한 영역(지식, 수학, 상식, 다국어)** 에서 **대체로 동급** 임을 그림/표로 반복 확인한다(근거: Fig.3, Tab.2) .

### 2) NVFP4 vs MXFP4 (동일 8B, 1T~1.5T 프리트레인)

* **상대 손실(대 BF16):**  **NVFP4 ≈ 1.5%** , **MXFP4 ≈ 2.5%**  (1T 기준). (근거: Fig.6a) 
* **토큰 효율:**  **MXFP4가 NVFP4 손실을 맞추려면 +36% 토큰(1.36T vs 1.0T)**  필요 → **훈련 시간·연산량·에너지 증가** 로 직결. (근거: Fig.6b) 

> **보조 근거(하드웨어 함의):**  Blackwell에서 **FP4 연산처리량 2×(GB200)/3×(GB300)** , 메모리 **~1/2** (vs FP8) → 손실 동급일 때 **시스템적으로**  FP4의 성능/비용 이점 잠재(논문은 런타임·시스템 최적화는 범위 밖으로 명시). (근거: Tab.1; Intro 명시)  

---

## “우월성” 주장을 가장 강하게 뒷받침하는 비교 지점

1. **토큰 효율 (NVFP4 vs MXFP4):**  **+36% 토큰**  차이는 **동일 손실 달성 비용** 을 직접적으로 비교한 강력한 근거. 모델·데이터·스케줄을 동일하게 맞춘 **형식 간 통제 실험** 이다. (근거: Fig.6) 
2. **장기 수렴 안정성 (12B, 10T):**  **안정 구간 <1%, 말기 ~1.5%**  오차로 **FP8 추종** —**다단계 데이터 블렌드/학습률 전환**  이벤트를 명시적으로 주석해 인과를 밝힘. (근거: Fig.2) 
3. **다운스트림 동급성:**  **MMLU-Pro 62.58 vs 62.62** , **ARC-C 동률 91.81**  등 **대표 지표 동급** , 수학·다국어 일부 **우세(+0.9~+3.7pp)** . (근거: Tab.2)   

---

## 능가하지 못했거나 개선이 미미했던 지점 (및 가능한 이유)

* **코딩 계열 저하(HE+, MBPP+):**  **−2.5~−3.2pp** . 저자들은 **평가 노이즈/체크포인트 선택 영향**  가능성 제기(최종 체크포인트에서 MBPP+ 하락). (근거: Tab.2, 본문 해설)  
* **MATH 소폭 열세:**  **−1.84pp** —수학 장기 추론에서의 **미세한 정밀도 손실**  또는 **말기 정밀·스케일 민감성**  가능. (근거: Tab.2) 
* **절대적 SOTA 경쟁(모델 간) 비교의 부재:**  본 논문은 **동일 아키텍처(하이브리드 Mamba-Transformer)**  내 **정밀도 형식/레시피 비교** 에 초점. 외부 거대 SOTA(모델·데이터 상이)와의 **정면 성능 대결 표는 제공하지 않음** . (근거: §3 개요) 
* **시스템 측정 공백:**  **실제 학습 시간/에너지/비용** 의 **정량 측정(예: TFLOP-util, $/1M tok, kWh/1M tok)** 은 본 보고서 범위를 넘어선다고 명시—**형식·알고리즘** 에 집중. (근거: §1 요약) 

---

## 맥락 보강: “레시피” 요소의 필요성 (내적 베이스라인 대비)

* **아블레이션:**  **Grad-SR, Wgrad-RHT, 2D W-scaling, 일부 BF16 층**  중 어느 하나라도 제거 시 **손실 악화** —**10T**  지평에서 안정 수렴에 **각 요소가 필수** . (근거: Fig.4) 
* **SR 적용 대상:**  **그래디언트에만**  유효(다른 텐서엔 역효과) → **편향 완화**  핵심. (근거: §4.4, Fig.10)  
* **RHT 위치/크기:**  **Wgrad 전용** , **d=16** 이 계산비용·정확도 균형 최적에 가까움. (근거: §4.2, Fig.11–12)  
* **2D 가중치 스케일:**  fprop/bprop **양자 불일치→체인룰 위반** 을 완화, **손실 개선** . (근거: §4.3)  

---

## 종합 해석

**강한 주장(지지 근거):** “**FP8 동급 정확도** 를 **FP4 정밀도** 로 달성”이라는 주장은 **동일 세팅 비교(12B·10T)** 의 **손실·다운스트림 수치** 와 **MXFP4 대비 토큰 효율 +36%** 가 함께 지지한다. (근거: Fig.2/3/6, Tab.2)   
* **한계(해석 주의):**  **코딩 태스크 열세** , **외부 SOTA와의 직접 교차 비교 부재** , **시스템 수준 이득(시간/비용/에너지)** 의 **실측 부재** 는 향후 검증 대상이다. 그럼에도 **하드웨어 이론상 이점** 과 **형식 간 통제 실험 결과** 는 **FP4 실전화** 의 유망성을 보여준다. (근거: Tab.2, Tab.1)  

> 요약: **NVFP4 레시피** 는 **FP8 대비 정확도 손실 없이(or 근소) FP4로 훈련** 을 가능케 하며, **동일 FP4 계열(MXFP4)**  대비 **토큰·시간 효율 우위** 를 분명히 보인다. (근거: Fig.2/6, Tab.2)   


## 프롬프트 1.5.1 (언급된 한계와 잠재적 한계)

```
저자들이 명시적으로 인정한 한계/약점/실패 사례는 무엇인가? 분석을 바탕으로 잠재적 한계(강한 가정, 확장성, 연산 비용, 일반화 한계, 사회적 영향 등)는 무엇이라고 보나?
```


**결론 압축:** 저자들은 **모든 선형층을 FP4로 양자화하면 초반 수렴이 발산** 하고, **일부 층(≈마지막 8블록, ~15%)은 BF16 유지** ·**SR(gradient 한정)** ·**2D weight scaling** ·**Wgrad 전용 Hadamard**  등 **복수 기법을 “세트”로 써야 안정** 하다고 인정한다(수치: 손실격차 안정구간 <1%, 말기 ~1.5%; 8.2T에서 **forward만 BF16** 로 바꾸면 **1.5%→0.5%**  회복, 고정밀 연산 ≈**6%** )(근거: Fig.2, Fig.7, Fig.8/E.1)   . 또한 **시스템/런타임 평가는 범위 밖** 이라 **실제 속도·에너지·비용 이득** 은 보고되지 않았다(근거: §1 끝) .

---

## 1) 저자들이 **명시적으로 인정** 한 한계/약점

* **Base FP4만으론 발산** : “추가 기법 없이 전층 FP4”는 **초기 수렴 발산**  → **마지막 블록 일부 BF16 유지** 가 **핵심** (예: 마지막 4블록 BF16 시 안정) (근거: Appx E.1/E.2, Fig.9)  .

* **SR 적용 범위의 제약** : **SR을 gradient에만 적용** 해야 안정(**12B는 Dgrad·Wgrad 둘 다**  필요). **활성/가중치에 SR** 은 **발산 유발** (nearest보다 오차↑) (근거: §4.4, Appx E.3, Fig.10)   .

* **Chain rule 불일치 위험** : 전·후향에서 **스케일/변환 축이 달라 동일 가중치가 ‘다른 양자화 표현’** 이 되어 **체인룰 위반**  가능 → 이를 **2D weight scaling(16×16)** 으로 완화 필요 (근거: §4.3)  .

* **Forward 경로가 손실 격차의 주원인** : **8.2T** 부터 **forward만 BF16** 으로 전환 시 **상대오차 1.5%→0.5%** , 고정밀 연산 **≈6%** 로 오버헤드 작음 (근거: Fig.7, Appx D) .

* **코딩 태스크 일부 저하** : **HumanEval+/MBPP+ −2.5~−3.2pp** , 저자들은 **평가 노이즈/체크포인트 선택**  가능성 언급 (근거: Fig.3·Tab.2 해설, Tab.2)  .

* **범위 제한(시스템/런타임)** : 본 보고서는 **알고리즘·방법론 중심** 으로, **런타임 효율·시스템 최적화는 다루지 않음**  (근거: §1 끝) .

* **미완 과제(미래 작업)** : (i) **전 선형층 FP4 양자화** 로의 **레시피 정제** , (ii) **남은 BF16 층 축소** , (iii) **주의·통신 경로** 로의 확장, (iv) **더 큰 모델/더 긴 토큰 지평/MoE**  검증, (v) **사후 단계(post-training) 활용**  (근거: Conclusions) .

* **형식별 스케일링 법칙 미정** : **NVFP4 vs MXFP4** 의 **파라미터/토큰 지평별 scaling law** 는 **추가 연구 필요**  (근거: Fig.6 코멘트) .

---

## 2) **잠재적 한계** (분석 기반)

* **“부분 고정밀 의존”의 구조적 제약** : 현재 **≈15%** (마지막 **8블록** )을 **BF16** 로 유지하여 안정화(12B) → **완전 FP4** 를 위해선 **층 선택 기준** (예: **Wgrad 오차 지표** )의 자동화가 필요 (근거: Fig.6 설정, Appx E.2 코멘트)  .

* **아키텍처 일반화** : 본 검증은 **Mamba-Transformer 하이브리드 12B/10T** 에 집중. **MoE/순수 트랜스포머/다른 옵티마이저** 로의 일반화는 **미확인** (저자도 향후 계획에 명시) (근거: §3 설정, Conclusions)  .

* **엔드투엔드 속도 이득 불확실성** : **FP4는 수학 처리량 2×(GB200)/3×(GB300), 메모리 1/2(FP8 대비)** 이지만, **주의(Attention)는 디코딩에서 메모리 바운드** 라 **텐서코어 이점의 전달** 이 제한될 수 있음 → **주의/통신 경로 FP4**  확장이 필요한 이유 (근거: Tab.1; 외부 배경 Hydragen §2.2–2.3)   .

* **평가 변동성/체크포인트 민감성** : **MBPP+**  등 **코딩 벤치마크는 변동성** 이 커 **미세 격차** 가 발생 가능(저자 자인) → **평가 프로토콜/CHKP 샘플링**  정교화 필요 (근거: Fig.3 해설) .

* **레시피 복잡도와 구현 리스크** : **SR(gradient 한정)·2D scaling·RHT(Wgrad 전용)·부분 BF16**  등 **다수 기법의 조합** 이 필요 → **프레임워크/커널**  미비 시 **재현 난이도↑** (저자도 Transformer Engine 업데이트 제공) (근거: Fig.4, 코드 공지)  .

* **형식 선택의 민감성** : **MXFP4** 는 **동일 손실 달성에 +36% 토큰(1.36T vs 1.0T)**  필요 → **형식/스케일 선택 오설계 시 훈련 비용 급증** (스케일링 법칙 미정 포함) (근거: Fig.6) .

---

## 3) 맥락 수치(참고)

* **손실 격차 프로파일** : 안정구간 **<1%** , 말기 **~1.5%** (lr decay 구간). **8.2T** 에 **forward만 BF16**  전환 시 **0.5%** 로 회복, **고정밀 연산 ≈6%**  (근거: Fig.2, Fig.7)  .

* **하드웨어 상한(이론치)** : **FP4 연산량**  **2×/3×(GB200/GB300)** , **메모리 ~1/2(FP8)**  — *하지만 본 보고서에서 엔드투엔드 시간·에너지·$/1M tok 미보고* (근거: Tab.1, §1 끝)  .

---

### 한 줄 정리

**NVFP4는 ‘정확도-안정성’에서 FP8에 근접** 하지만, **완전 FP4** 를 위해선 **층별 고정밀 의존도 축소** , **forward 경로 민감성 해소** , **주의/통신 경로 확장** , **형식별 스케일링 법칙 정립** 과 **시스템 지표(시간·에너지·비용)**  검증이 남아 있다(근거: Fig.2/6/7, §4.3, Conclusions)     .



## 프롬프트 1.5.2 (미래 연구 궤적)

```
저자들이 제안하는 향후 연구 방향은 무엇인가? 한계에 비추어 합리적인 다음 단계나 대안적 방향을 제안해 줘.
```


**결론 압축:**  저자들은 **(i)**  형식별 성능 특성화·스케일링 법칙, **(ii)**  **전 선형층 FP4**  달성(잔여 BF16 축소), **(iii)**  **주의·통신 경로로 NVFP4 확장** , **(iv)**  **사후 단계(post-training)**  적용, **(v)**  **더 큰 모델/더 긴 토큰 지평/MoE**  검증을 공식 로드맵으로 제시한다(근거: Conclusions) . 또한 **MXFP4와의 스케일링 법칙**  비교(파라미터·토큰 지평 다양화)가 필요하다고 못박는다(근거: Fig.6 코멘트) . 이를 바탕으로 다음 단계로는 **층 선택의 자동화(오차 지표 기반)** , **forward-only 정밀 전환 시점 최적화** , **시스템 지표(TTFT/TPOT/토큰/s·kWh/1M tok) 공개** , **주의/통신 경로 FP4 커널화** 가 합리적이다(근거: Fig.7·Appx E.2/3·Tab.1·서문 범위 한정)    .

---

## 저자가 제시한 **공식 향후 연구** 

* **형식 간 특성화 & 스케일링 법칙:**  NVFP4의 프리트레이닝 성능을 **다른 형식들과 체계적으로 비교** , **파라미터 수·토큰 지평** 에 따른 **스케일링 법칙** 을 수립(예: 8B/12B/≥70B, 1T~10T+) (근거: Fig.6 코멘트) .
* **전 선형층 FP4 수렴:**  **모든 선형층을 FP4로 양자화** 해도 수렴성 저하가 없도록 **방법론 정제** , **잔여 고정밀(BF16) 층 축소** (현재 마지막 8블록≈15%) (근거: Conclusions; Fig.6 설정)  .
* **주의·통신 경로 확장:**  NVFP4를 **Attention**  및 **통신 경로** 까지 확장(현재는 주로 GEMM 경로) (근거: Conclusions) .
* **사후 단계(post-training) 적용:**  **PTQ/FT/SFT/RLHF**  등 **post-training 시나리오** 에서 NVFP4 활용 탐색 (근거: Conclusions) .
* **스케일 확장 검증:**  **더 큰 모델** , **더 긴 토큰 지평** , **MoE 아키텍처** 로 평가 확대 (근거: Conclusions) .
* **툴체인 지원:**  **Transformer Engine** 에 **NVFP4 학습 지원**  추가(엔지니어링 경로 확보) (근거: Conclusions) .

---

## 한계에 비춘 **합리적 다음 단계(제안)** 

1. **층 선택의 자동화(완전 FP4 목표)** 

   * 근거: **마지막 블록(출력 근접) 층이 FP4에 민감** 하며, **Wgrad 양자화 오차 지표** 가 **고정밀 유지 층 선별** 에 유용할 수 있음(저자 제안) (근거: Appx E.2) .
   * 제안: **온라인 Wgrad 오차(예: L2/amax/zero-rate) 임계 기반** 으로 **BF16↔FP4 동적 게이팅** · **커리큘럼 정밀 스케줄** 을 학습과 함께 최적화.

2. **Forward-only 정밀 전환 스케줄 최적화** 

   * 근거: **손실 격차의 주원인은 forward 경로** 이며, **8.2T에서 forward만 BF16**  전환 시 **상대오차 1.5%→0.5%** , **고정밀 연산≈6%** 에 불과 (근거: Fig.7) .
   * 제안: **학습률 감쇠 시작·데이터 블렌드 전환**  등 **체인지포인트 탐지** 로 **전환 시점/구간** 을 **적응형** 으로 결정(예: EMA-loss slope, grad-norm 드리프트).

3. **주의(Attention)·통신 경로 FP4 커널화 & 시스템 지표 보고** 

   * 근거: **FP4는 FP8 대비 수학 처리량 2×(GB200)/3×(GB300)** , **메모리 ~1/2** 로 **GEMM 비중이 큰 구간** 에서 가속 잠재 (근거: Tab.1) . 보고서는 **런타임·시스템 최적화는 범위 밖** 이라고 명시 (근거: §Intro/Scope) .
   * 제안: **FlashAttn/FlashFFTConv 계열과 결합** 한 **FP4-friendly 어텐션/통신** 을 구현하고, **TTFT(ms), TPOT(ms/tok), Throughput(tok/s), $/1M tok, kWh/1M tok(PUE)** 를 **동일 배치·컨텍스트 길이** 에서 공개 비교.

4. **형식·스케일링 법칙의 정량 모델링** 

   * 근거: **NVFP4 vs MXFP4** 에서 같은 손실에 **+36% 토큰(1.36T vs 1.0T)**  차이 → **형식 선택의 비용 민감성**  큼 (근거: Fig.6) .
   * 제안: $(\mathcal{L}(N_{\text{tok}},P;\ \text{format}))$에 대한 **형식별 scaling law** (토큰·파라미터·데이터 품질)를 회귀로 적합해 **최적 형식·토큰 예산** 을 사전 설계.

5. **SR/2D-scaling/RHT의 적용 경계 정밀화** 

   * 근거: **SR은 gradient에만 유효** , **forward/가중치 SR은 발산 유발** (nearest보다 오차↑); **2D weight scaling** 은 **체인룰 불일치**  완화; **RHT는 Wgrad 전용** 이 안정 (근거: §4.3–4.4, Appx E.3/E.4)   .
   * 제안: **텐서별 SR 정책 서쳐** (grad만·dgrad/wgrad 분리), **2D 스케일 granularity(8×16/16×16) 튜닝** , **RHT 크기 d=16/32** 의 **compute-accuracy Pareto** 를 아키텍처별로 표준화.

6. **스케일 업 & 아키텍처 일반화** 

   * 근거: 향후 **더 큰 모델·더 긴 토큰·MoE** 로 검증 예정 (근거: Conclusions) .
   * 제안: **70B Dense** , **64×(A=8) MoE** , **≥20T 토큰** 에서 **손실 격차(<1% 목표)**  유지 여부와 **FP4가 Token-optimal Regime** 에 미치는 영향 정량화.

7. **사후 단계(Post-training) 활용 프로토콜** 

   * 근거: **post-training**  시나리오 탐색 계획 (근거: Conclusions) .
   * 제안: **LoRA/DoRA** ·**SFT/RLHF** 에서 **NVFP4 미세학습**  안정 조건(학습률·clip·scale 업데이트 정책)과 **정확도-비용 곡선** 을 공개.

---

## 참고: 현재 확인된 제약과 동기(요약 수치)

* **Forward 경로가 손실 격차의 주원인**  → **8.2T에서 forward만 BF16**  전환 시 **1.5%→0.5%** , **고정밀 연산 ≈6%**  (근거: Fig.7) .
* **층 민감도 존재** : **마지막 블록 유지 시 수렴** , **초기 블록만 유지 시 불충분**  → **오차 지표로 층 선택**  가능성 (근거: Appx E.2)  .
* **형식 효율 차이** : **MXFP4가 NVFP4 손실을 맞추려면 +36% 토큰**  필요 (근거: Fig.6) .
* **하드웨어 잠재** : **FP4 연산 처리량 2×(GB200)/3×(GB300)** , **메모리 ~1/2(FP8 대비)**  (근거: Tab.1) .
* **범위 한정** : 본 보고서는 **알고리즘 중심** 으로 **런타임/시스템 최적화는 미포함**  (근거: §Intro/Scope) .

> 한 줄 정리: **저자 로드맵(형식·층·경로·스케일·사후·스케일업)**  + **우리가 제안한 자동화·스케줄링·시스템지표·커널화** 를 결합하면, **완전 FP4(전층) 수렴** 과 **엔드투엔드 비용-정확도 Pareto** 를 실제 프로덕션에서 증명할 수 있다(근거: Conclusions/Fig.6/Fig.7/Tab.1)    .


