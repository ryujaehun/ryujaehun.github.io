---
categories:
- paper-review
- with-gpt
date: "2024-11-25"
title: 'NACL: AGeneral and Effective KV Cache Eviction Framework for LLMs at Inference
  Time'
---

[논문 링크](https://arxiv.org/abs/2408.03675)

# Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘



### **논문 상세 분석**

#### **제목과 범위**
이 논문은 **대규모 언어 모델(LLMs)**의 추론 과정에서 **KV 캐시 관리**를 최적화하여 메모리 사용량을 줄이고 성능을 유지하기 위한 **NACL**이라는 프레임워크를 제안합니다. 이 프레임워크는 **프록시 토큰 제거(Proxy-Tokens Eviction)**와 **랜덤 제거(Random Eviction)**를 결합하여 긴 문맥과 짧은 문맥 작업에서 높은 성능을 유지하면서 메모리 사용량을 효과적으로 줄이는 것을 목표로 합니다.

---

### **논문의 강점과 독창성**

1. **혁신적인 프레임워크:**
   - NACL은 **프록시 토큰 제거**를 통해 중요한 정보를 전략적으로 식별하고, **랜덤 제거**를 통해 다양성을 확보하여 토큰 유지의 정확성과 강건성을 동시에 개선합니다.

2. **효율성과 확장성:**
   - KV 캐시 메모리를 **최대 5배** 줄이면서도 95% 이상의 성능을 유지해, 제한된 하드웨어에서 긴 문맥 작업을 수행할 수 있게 만듭니다.

3. **전역 최적화 접근 방식:**
   - 기존의 탐욕적 제거 방식과 달리, NACL은 **인코딩 단계**에서 전역 최적화를 수행하여 계산 복잡도를 \(O(p+T)\)에서 \(O(1)\)로 줄입니다.

4. **작업 성능 개선:**
   - NACL은 짧은 문맥과 긴 문맥 작업 모두에서 기존 방식보다 우수한 성능을 보이며, 높은 범용성과 강건성을 입증합니다.

5. **FlashAttention과의 호환성:**
   - 현대적인 메모리 효율 메커니즘(예: FlashAttention)을 통합하여, 운영 오버헤드를 줄이고 실용성을 높였습니다.

---

### **핵심 알고리즘: 예시와 설명**

#### **핵심 개념: KV 캐시 제거**
- LLM은 토큰에 대한 중간 계산을 저장하기 위해 KV 캐시를 사용하지만, 이로 인해 과도한 메모리 사용이 발생합니다. NACL은 덜 중요한 토큰을 선택적으로 제거하여 메모리 사용을 최적화합니다.

#### **예시 입력**
- **입력 텍스트:** "The quick brown fox jumps over the lazy dog in a distant meadow."
- **캐시 예산:** 원래 크기의 20%.

#### **NACL의 처리 단계**

1. **인코딩 단계 (프록시 토큰 제거):**
   - **프록시 선택:** "lazy dog", "distant meadow"와 같은 중요한 프레이즈를 프록시 토큰으로 식별.
   - **어텐션 점수 계산:** 프록시 토큰을 기준으로 전체 토큰의 중요도를 계산.
   - **토큰 유지:** 점수가 높은 토큰만 KV 캐시에 유지.

2. **랜덤 제거 (생성 단계):**
   - **샘플링 분포 생성:** 정규화된 어텐션 점수를 기반으로 토큰을 랜덤하게 선택.
   - **다양성 확보:** 서로 다른 헤드와 레이어에서 다양한 토큰을 유지하여 문맥 정보를 풍부하게 유지.

#### **결과**
"lazy dog", "fox", "meadow"와 같은 중요한 토큰은 유지되며, "the"나 "in"과 같은 불필요한 토큰은 제거됩니다. 이를 통해 메모리 사용을 줄이면서도 중요한 문맥 정보를 잃지 않습니다.

---

### **논문의 한계점**

1. **프록시 토큰 의존성:**
   - 프록시 토큰 제거의 효과는 프록시 토큰을 얼마나 잘 식별하느냐에 따라 크게 좌우됩니다. 적절히 선택하지 못하면 성능 저하가 발생할 수 있습니다.

2. **랜덤 제거의 민감성:**
   - 랜덤 제거는 강건성을 향상시키지만, 과도한 랜덤성은 중요한 토큰의 손실로 이어질 수 있습니다. 특히 캐시 예산이 엄격한 경우에 문제가 될 수 있습니다.

3. **확장성 문제:**
   - 초장문(128k 토큰 이상)에 대한 테스트가 충분하지 않아 해당 시나리오에서의 성능은 불확실합니다.

4. **응용 범위 제한:**
   - 추론 효율성을 위해 설계되었으나, 파인튜닝이나 도메인별 학습 문제는 다루지 않습니다.

---

### **기존 기법과 비교**

- **H2O:** 어텐션 점수 합계만을 사용해 탐욕적으로 토큰을 제거하지만, 어텐션 편향 문제가 있음.
- **MSRNN:** 인접한 문맥만 강조하여 더 넓은 의존성을 간과.
- **NACL:** 어텐션 기반과 확률적 접근 방식을 결합해 정확성과 메모리 절약 측면에서 우수한 성과를 냄.

| **방법** | **메모리 절약** | **성능 저하 (%)** |
| -------- | --------------- | ----------------- |
| H2O      | 4배             | ~4%               |
| MSRNN    | 4배             | ~3%               |
| **NACL** | **5배**         | **<1%**           |

---

### **주요 통찰 및 영향**

1. **효율적인 제거:** 
   - NACL은 인코딩 단계에서 한 번의 제거만으로 계산 오버헤드를 최소화하고, 토큰별 제거 방식보다 더 효과적입니다.

2. **핵심 토큰 유지:** 
   - 제한된 메모리에서도 중요한 토큰을 유지하여, 긴 문맥 작업에서도 정확한 추론을 가능하게 합니다.

3. **랜덤성의 도입:** 
   - 랜덤 제거는 어텐션 기반 방법의 편향 문제를 해결하며, 강건성과 적응성을 동시에 확보합니다.

---

### **최종 의견**

NACL은 KV 캐시 관리에서 효율성과 성능, 메모리 소비의 균형을 유지하는 혁신적인 접근 방식을 제시합니다. 초장문 작업 및 프록시 선택의 자동화 개선 등 추가 연구가 필요하지만, 이 혼합 접근법은 LLM 추론 최적화의 새로운 기준을 세우는 데 기여할 것입니다.

# Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘



### **Core Algorithm of the Paper: Detailed Explanation and Example**

#### **Understanding the Algorithm**
The NACL framework optimizes the **KV cache eviction** process by combining two key methods:
1. **Proxy-Tokens Eviction**: Focuses on identifying and retaining crucial tokens based on attention scores relative to selected proxy tokens.
2. **Random Eviction**: Adds randomness to the eviction process, improving the robustness of token retention by preserving diverse information across layers and attention heads.

The algorithm is divided into **two phases**:
- **Encoding Phase**: Processes the input text and performs a one-shot eviction of less important tokens.
- **Generation Phase**: Handles token-by-token generation while periodically applying the eviction strategy.

---

### **Step-by-Step Example**
#### **Scenario**
We have a sentence:
- **Input Text**: "The quick brown fox jumps over the lazy dog in a distant meadow."
- **KV Cache Budget**: Reduced to 30% of the original size.

---

### **Phase 1: Encoding Phase (Proxy-Tokens Eviction)**

1. **Input Processing:**
   - The model computes **attention scores** for all tokens. These scores represent how much attention each token receives in relation to others.

2. **Proxy Token Selection:**
   - Proxy tokens are selected from the input text. For instance, if the task involves understanding the sentence's key entities, **"lazy dog"** and **"distant meadow"** might be chosen as proxy tokens.

3. **Token Importance Scoring:**
   - The importance of each token is calculated based on its relationship to the proxy tokens. For example:
     - Token **"fox"** might score high because it’s central to the sentence's meaning.
     - Tokens like **"the"** and **"in"** might score low as they are less meaningful.

4. **Eviction Decision:**
   - Tokens with the lowest scores are evicted from the KV cache. For example:
     - Retained Tokens: **"quick," "fox," "lazy dog," "meadow."**
     - Evicted Tokens: **"the," "jumps," "over," "in."**

---

### **Phase 2: Generation Phase (Random Eviction)**

1. **Token-by-Token Processing:**
   - As the model generates new tokens (e.g., during text generation), the KV cache is updated.

2. **Random Sampling for Diversity:**
   - To ensure robustness, **random eviction** is applied alongside attention-based eviction:
     - A small portion of tokens is randomly retained, regardless of their scores.
     - This ensures that important but low-scoring tokens (e.g., rare words) are not lost.

3. **Periodic Eviction:**
   - The eviction strategy is applied periodically during generation to manage memory usage.

---

### **Illustration of the Algorithm**
#### Before Eviction:
- **KV Cache Contains**: ["The," "quick," "brown," "fox," "jumps," "over," "the," "lazy," "dog," "in," "a," "distant," "meadow."]

#### After Proxy-Tokens Eviction:
- **Retained**: ["quick," "fox," "lazy," "dog," "meadow."]
- **Evicted**: ["The," "jumps," "over," "in," "a."]

#### After Random Eviction During Generation:
- Random sampling ensures tokens like **"brown"** or **"distant"** (that might not have high scores) are occasionally retained for better context.

---

### **Summary of the Process**
1. **Proxy-Tokens Eviction:**
   - Uses specific tokens (proxy tokens) to evaluate the importance of others.
   - Retains tokens crucial for the task based on attention scores.

2. **Random Eviction:**
   - Adds randomness to avoid over-reliance on biased scores.
   - Ensures diversity and robustness in token retention.

3. **Hybrid Approach:**
   - Combines deterministic and probabilistic methods for optimal memory usage without significant performance loss.

---

### **Translated Explanation in Korean**

#### **논문의 핵심 알고리즘: 자세한 설명과 예시**

#### **알고리즘 이해**
NACL 프레임워크는 **KV 캐시 제거**를 최적화하기 위해 두 가지 주요 방법을 결합합니다.
1. **프록시 토큰 제거(Proxy-Tokens Eviction)**:
   - 선택된 프록시 토큰과의 관계를 기반으로 중요한 토큰을 식별하고 유지합니다.
2. **랜덤 제거(Random Eviction)**:
   - 제거 과정에 무작위성을 추가하여 레이어와 어텐션 헤드 전반에 걸쳐 다양한 정보를 유지합니다.

알고리즘은 **두 단계**로 나뉩니다.
- **인코딩 단계**: 입력 텍스트를 처리하고 덜 중요한 토큰을 한 번에 제거합니다.
- **생성 단계**: 토큰을 하나씩 생성하면서 주기적으로 제거 전략을 적용합니다.

---

### **예시 단계별 설명**
#### **상황**
다음과 같은 문장이 있습니다.
- **입력 텍스트**: "The quick brown fox jumps over the lazy dog in a distant meadow."
- **KV 캐시 예산**: 원래 크기의 30%로 줄임.

---

### **1단계: 인코딩 단계 (프록시 토큰 제거)**

1. **입력 처리:**
   - 모델이 모든 토큰의 **어텐션 점수**를 계산합니다. 이 점수는 각 토큰이 다른 토큰과 얼마나 관련 있는지를 나타냅니다.

2. **프록시 토큰 선택:**
   - 입력 텍스트에서 프록시 토큰을 선택합니다. 예를 들어, 문장에서 주요 엔티티를 이해하는 작업이라면 **"lazy dog"**와 **"distant meadow"**가 선택될 수 있습니다.

3. **토큰 중요도 점수 계산:**
   - 각 토큰의 중요도는 프록시 토큰과의 관계를 기반으로 계산됩니다. 예를 들어:
     - **"fox"**: 문장의 의미에서 핵심적이므로 높은 점수를 받음.
     - **"the"** 및 **"in"**: 덜 중요하므로 낮은 점수를 받음.

4. **제거 결정:**
   - 점수가 가장 낮은 토큰은 KV 캐시에서 제거됩니다. 예를 들어:
     - **유지된 토큰**: **"quick," "fox," "lazy dog," "meadow."**
     - **제거된 토큰**: **"the," "jumps," "over," "in."**

---

### **2단계: 생성 단계 (랜덤 제거)**

1. **토큰별 처리:**
   - 모델이 새 토큰을 생성함에 따라 KV 캐시가 갱신됩니다.

2. **다양성 확보를 위한 랜덤 샘플링:**
   - 강건성을 확보하기 위해 **랜덤 제거**가 어텐션 기반 제거와 함께 적용됩니다.
     - 일부 토큰은 점수와 관계없이 랜덤으로 유지됩니다.
     - 이를 통해 낮은 점수를 받았지만 중요한 토큰(예: 희귀 단어)이 손실되지 않도록 보장합니다.

3. **주기적 제거:**
   - 메모리 사용량을 관리하기 위해 주기적으로 제거 전략이 적용됩니다.

---

### **알고리즘 예시**
#### 제거 전:
- **KV 캐시 포함 토큰**: ["The," "quick," "brown," "fox," "jumps," "over," "the," "lazy," "dog," "in," "a," "distant," "meadow."]

#### 프록시 토큰 제거 후:
- **유지됨**: ["quick," "fox," "lazy," "dog," "meadow."]
- **제거됨**: ["The," "jumps," "over," "in," "a."]

#### 생성 단계에서 랜덤 제거 적용 후:
- 랜덤 샘플링을 통해 **"brown"** 또는 **"distant"**와 같은 중요도는 낮지만 의미 있는 토큰이 유지됩니다.

---

### **요약**
1. **프록시 토큰 제거**:
   - 특정 토큰(프록시 토큰)을 사용하여 다른 토큰의 중요도를 평가합니다.
   - 작업에 중요한 토큰을 어텐션 점수에 따라 유지합니다.

2. **랜덤 제거**:
   - 편향된 점수에 과도하게 의존하지 않도록 무작위성을 추가합니다.
   - 다양성과 강건성을 동시에 확보합니다.

3. **혼합 접근법**:
   - 결정론적 방식과 확률적 방식을 결합하여 메모리 사용을 최적화하면서 성능 손실을 최소화합니다.


# Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘



### **논문에서 제시한 결과 및 주요 성과**

#### **1. 결과 요약**

1. **KV 캐시 감소 효과:**
   - NACL은 **최대 5배의 KV 캐시 메모리 감소**를 이루었으며, 이 과정에서 모델 성능의 95% 이상을 유지했습니다.
   - 메모리 사용량을 대폭 줄이면서도, 긴 문맥 작업과 짧은 문맥 작업 모두에서 기존 방법론보다 우수한 성능을 보여주었습니다.

2. **작업 성능 비교 (짧은 문맥):**
   - NACL은 다양한 짧은 문맥 작업에서 기존 방법론(H2O, MSRNN 등)보다 **3~4% 높은 성능**을 기록했습니다.
   - 특히, 성능 감소율이 다른 방법론에 비해 훨씬 적었으며, **거의 전체 캐시를 사용하는 경우와 유사한 성능**을 보였습니다.

3. **작업 성능 비교 (긴 문맥):**
   - 긴 문맥 작업에서도 NACL은 캐시 크기를 줄이는 설정(10~30% 예산)에서도 **기존 방법론보다 평균적으로 2~5% 높은 성능**을 보였습니다.
   - 특정 데이터셋(예: QMSum, HotpotQA)에서는, 캐시를 전혀 줄이지 않은 모델보다 **더 나은 성능**을 기록하기도 했습니다.

4. **랜덤 제거와 프록시 토큰 제거의 효과:**
   - **랜덤 제거(Random Eviction)**: 다양한 문맥 정보를 보존해 성능을 약 **9% 이상** 개선.
   - **프록시 토큰 제거(Proxy-Tokens Eviction)**: 중요한 토큰의 정확한 유지로 약 **28%의 성능 개선**을 제공.

---

### **다른 방법론과의 비교**

#### **1. 기존 방법론과의 성능 차별성**

| **방법**  | **메모리 감소율** | **성능 감소율** | **주요 문제점**                                |
| --------- | ----------------- | --------------- | ---------------------------------------------- |
| **H2O**   | 4배               | 4%              | 어텐션 점수에 기반해 초기/최근 토큰에 편향적.  |
| **MSRNN** | 4배               | 3%              | 현재 토큰 중심의 계산으로 전체 문맥 유지 부족. |
| **NACL**  | **5배**           | **<1%**         | 프록시와 랜덤 결합으로 강건성과 다양성 확보.   |

- H2O와 MSRNN은 어텐션 점수 기반의 단순한 제거 방식으로 인해, 긴 문맥에서 중요한 중간 토큰을 놓치거나, 특정 토큰에 편향된 결과를 보임.
- 반면, NACL은 프록시 토큰과 랜덤 제거를 결합해, 성능 저하를 최소화하며 더 효과적으로 중요한 정보를 보존합니다.

---

#### **2. NACL이 우수한 결과를 도출한 이유 (논문이 제시하는 근거)**

1. **프록시 토큰 기반 제거(Proxy-Tokens Eviction):**
   - 기존의 H2O나 MSRNN과 달리, 특정 "프록시 토큰"을 사용하여 다른 토큰의 중요도를 계산합니다.
   - 프록시 토큰은 문맥적으로 중요한 토큰으로 구성되며, 어텐션 점수를 정규화해 과도한 편향을 방지합니다.
   - 이는 긴 문맥에서 **중간 위치의 중요한 토큰**을 더 잘 보존하도록 돕습니다.

2. **랜덤 제거(Random Eviction):**
   - 어텐션 점수 기반 방법의 단점을 보완하기 위해 랜덤 샘플링을 추가했습니다.
   - 랜덤성은 **저평가된 중요한 토큰**이 선택될 가능성을 높이며, 다양한 문맥 정보를 유지합니다.

3. **전역 최적화(Global Optimization):**
   - 기존 방법론은 단순 탐욕적(그리디) 방식으로, 매 단계마다 토큰을 제거했으나, NACL은 전역적인 최적화를 통해 한 번의 계산으로 최적의 제거 전략을 적용합니다.
   - 이 방식은 긴 문맥에서도 계산 효율성을 극대화합니다.

4. **헤드별 다양성 보장(Head-wise Diversity):**
   - 각 어텐션 헤드가 서로 다른 제거 전략을 사용하여, 특정 레이어에 지나치게 의존하지 않도록 보장합니다.
   - 이로 인해 모델이 다양한 차원의 정보를 보존할 수 있습니다.

---

### **NACL이 제시한 결과의 차별성을 가능케 한 핵심 요인**

#### **1. 프록시 토큰 제거**
- 프록시 토큰 제거는 어텐션 점수를 기반으로 하지만, 중요한 토큰을 결정할 때 특정 기준을 추가로 제공합니다. 
- 예를 들어, 질문 응답(task-oriented) 작업에서는 프록시 토큰이 질문 끝부분에서 선택되며, 이는 **질문과 관련된 토큰을 더 잘 보존**합니다.

#### **2. 랜덤 제거의 도입**
- 기존 방법론은 어텐션 점수만을 사용하여 중요한 토큰을 선택했으나, 이는 장문의 경우 평탄화된 어텐션 분포로 인해 신뢰성이 낮아질 수 있습니다.
- 랜덤 제거는 이러한 문제를 보완하며, 문맥 정보의 다양성을 유지합니다.

#### **3. 전역 최적화**
- 기존의 단계별 제거와 달리, 한 번의 계산으로 모든 토큰을 고려해 최적의 제거를 수행함으로써 성능과 계산 속도 모두에서 큰 개선을 이루었습니다.

---

### **나의 의견**

1. **랜덤 제거의 가치**
   - 어텐션 기반 토큰 제거 방식은 특정 조건에서는 중요한 정보를 놓칠 가능성이 높습니다. 랜덤 제거는 이를 보완하며, **다양한 문맥 정보를 유지**해 예기치 못한 손실을 방지합니다. 이는 특히 긴 문맥 작업에서 효과적입니다.

2. **프록시 토큰의 활용**
   - 프록시 토큰은 작업과 문맥에 따라 중요성을 설정하는 매우 합리적인 방식입니다. 그러나 이 방식은 **프록시 토큰을 선택하는 기준**에 따라 성능이 크게 달라질 수 있습니다. 이 기준을 모델이 자동으로 학습하도록 발전시킬 여지가 있습니다.

3. **결과의 설득력**
   - 논문의 결과는 잘 설계된 실험과 비교를 통해 매우 설득력 있게 제시됩니다. 특히, 짧은 문맥 작업뿐만 아니라 긴 문맥 작업에서도 안정적으로 성능을 유지한다는 점이 인상적입니다.

---

### **결론**

NACL은 기존 KV 캐시 제거 방법론의 한계를 효과적으로 해결하며, 메모리 효율성과 성능을 동시에 개선했습니다. 특히, **프록시 토큰 제거**와 **랜덤 제거**의 조합은 편향을 최소화하고 강건성을 높이는 데 결정적인 역할을 했습니다. 앞으로 **프록시 토큰 선택의 자동화**와 **초장문(ultra-long text)**에 대한 추가 연구가 이루어진다면 더욱 강력한 프레임워크로 발전할 수 있을 것입니다.

# Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니? 



### **논문에서 제시된 방법을 구현하기 위한 요구 사항 및 계산**

#### **1. 추가 구현 요구 사항**

#### **a. 프록시 토큰 제거**
- **추가 작업:**
  - **프록시 토큰 선택:** 입력 문맥에 따라 동적으로 프록시 토큰을 식별하는 로직 추가. 예를 들어, 작업 특성에 따라 질문 끝부분 또는 문서의 주요 키워드를 프록시 토큰으로 선택.
  - **어텐션 점수 정규화:** 프록시 토큰과 관련된 어텐션 점수를 계산하고, 이를 통해 덜 중요한 토큰을 식별하여 제거.
- **통합 포인트:**
  - 어텐션 매트릭스를 활용하여 새로운 "중요도 점수 계산" 메커니즘을 KV 캐시 관리 시스템에 통합.

#### **b. 랜덤 제거**
- **추가 작업:**
  - **랜덤 샘플링 프로세스:** 어텐션 점수 분포와 통합된 랜덤 샘플링 프로세스를 생성.
  - **헤드 및 레이어 수준의 무작위화:** 다양한 토큰 유지 전략을 위한 무작위화 메커니즘 추가.
- **통합 포인트:**
  - 생성 단계에서 주기적으로 랜덤 제거를 적용하도록 제거 정책 확장.

#### **c. 인코딩 단계에서의 단일 최적화**
- **추가 작업:**
  - 인코딩 단계에서 한 번의 계산으로 최적의 제거 전략을 수행하는 단일 패스 글로벌 최적화 전략 구현.
  - FlashAttention-2와 같은 메모리 효율 라이브러리를 활용하여 어텐션 점수를 효율적으로 재계산.
- **통합 포인트:**
  - 인코딩 단계 계산 방식을 최적화 로직에 맞게 조정.

---

### **2. 컴퓨팅 자원 요구 사항**

#### **a. 메모리 오버헤드**
- **프록시 토큰 제거:**
  - 어텐션 점수 계산은 다음을 저장하기 위해 추가 메모리가 필요:
    - 프록시 토큰 인덱스.
    - 각 레이어의 어텐션 점수 매트릭스.
  - 예상 메모리 사용량 증가는 다음과 같음:
    \[
    O(L \times H \times T_{proxy})
    \]
    여기서 \(L\)은 레이어 수, \(H\)는 어텐션 헤드 수, \(T_{proxy}\)는 프록시 토큰 수(전체 토큰의 약 10~20%).

- **랜덤 제거:**
  - 샘플링 분포를 위한 추가 메모리가 필요하지만, 프록시 토큰과 비교하면 미미한 수준임.

#### **b. 계산 오버헤드**
1. **프록시 토큰 제거:**
   - \(L\)개의 레이어와 \(H\)개의 어텐션 헤드에 대해, 어텐션 점수 감소 계산의 복잡도는 다음과 같음:
     \[
     O(L \times H \times T_{input} \times T_{proxy})
     \]
     여기서 \(T_{input}\)은 입력 토큰 수. 만약 \(T_{proxy} = 0.1 \times T_{input}\)라면, 기존 어텐션 점수 계산 대비 약 10%의 추가 계산 발생.

2. **랜덤 제거:**
   - 어텐션 헤드와 레이어별로 소프트맥스 정규화와 랜덤 샘플링 수행:
     \[
     O(L \times H \times T_{input})
     \]
   - 약 5~10%의 오버헤드 추가.

#### **c. GPU 활용 및 시간**
- **단일 노드 환경 (예: NVIDIA A100):**
  - 프록시 기반 계산 및 인코딩 최적화로 인해 4k 토큰 입력 시 **10~15%의 추가 계산 시간** 소요 예상.
  - 생성 단계에서의 랜덤 샘플링은 **5% 미만의 미미한 영향**을 미침.

---

### **3. 구현 노력 추정**

| **작업**              | **노력 수준** | **상세 설명**                                                  |
| --------------------- | ------------- | -------------------------------------------------------------- |
| 프록시 토큰 선택 로직 | 중간 (2~3주)  | 작업 메타데이터 또는 규칙에 기반해 프록시 토큰 선택 로직 구현. |
| 어텐션 점수 축소      | 중간 (2주)    | FlashAttention 등을 활용한 효율적 점수 계산 구현.              |
| 랜덤 제거 로직        | 낮음 (1~2주)  | 어텐션 헤드와 레이어별 샘플링 로직 추가.                       |
| 글로벌 최적화         | 중간 (2~3주)  | 단일 계산 기반 최적의 제거 정책 구현.                          |
| 테스트 및 통합        | 높음 (4~6주)  | 기존 파이프라인과의 통합 및 E2E(end-to-end) 테스트 수행.       |

**총 구현 소요 시간:** LLM 파이프라인 및 GPU 최적화에 익숙한 팀 기준, 약 **10~14주**.

---

### **4. 실제 활용 가능성**

#### **도전 과제:**
1. **프록시 토큰 선택:**
   - 작업별로 프록시 토큰을 선택하는 로직이 필요하며, 이를 일반화하기 위해 상당한 실험이 요구될 수 있음.
   - 프록시 토큰 선택을 임베딩 기반 자동화로 발전시키는 작업이 추가로 필요할 수 있음.

2. **초장문(ultra-long context)에 대한 확장성:**
   - 32k 토큰을 초과하는 시퀀스에서는 GPU 메모리 요구량이 제한 요소가 될 수 있음.

3. **기존 파이프라인과의 호환성:**
   - 하깅페이스(Hugging Face) 또는 OpenAI 시스템 등의 기존 KV 캐시 시스템을 수정해야 할 가능성이 있음.

#### **기대 효과:**
1. **효율성 향상:**
   - 메모리 사용량을 최대 5배 줄이고, 성능 저하를 최소화.
   - 긴 문맥 처리에서 고비용 하드웨어 업그레이드의 필요성을 줄임.

2. **이식성:**
   - 적절한 엔지니어링 노력으로 다양한 추론 프레임워크에 적응 가능.

---

### **결론**

NACL을 실제 시스템에 구현하려면 프록시 토큰 로직과 글로벌 최적화를 위한 일부 엔지니어링 노력이 필요합니다. 계산 비용은 비교적 적게 증가(인코딩 단계에서 약 15~20%)하며, 메모리 절약과 확장성 향상을 고려할 때 투자 가치가 충분합니다.

# Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘



### **논문의 입력 데이터와 추론 과정에 대한 설명**

#### **입력 데이터**
이 논문에서 다루는 입력 데이터는 **텍스트 시퀀스**로, 대규모 언어 모델(LLM)이 처리할 긴 문맥과 짧은 문맥 작업을 포함합니다.

##### **예시**
- **짧은 문맥 작업**:
  - 입력: "What is the capital of France?"
  - 작업: 질문에 대한 답변 생성.
  
- **긴 문맥 작업**:
  - 입력: 32,000 토큰 이상의 긴 문서.
    - 예: 연구 논문, 기술 문서 또는 책의 일부.
  - 작업: 요약, 특정 질문에 대한 응답 생성, 또는 문서의 특정 섹션 탐색.

---

### **추론 과정**

#### **1. 인코딩 단계**
- **입력 데이터 처리**:
  - 입력된 텍스트는 토큰화되어 시퀀스 형태로 변환됩니다.
    - 예: "The quick brown fox jumps over the lazy dog" → \([t_1, t_2, ..., t_n]\)
    - \(t_i\): 각 단어의 임베딩 벡터.

- **어텐션 점수 계산**:
  - 각 토큰의 **Query (Q)**, **Key (K)**, **Value (V)** 벡터를 계산:
    \[
    Q = X \cdot W_Q, \, K = X \cdot W_K, \, V = X \cdot W_V
    \]
    \(X\): 입력 토큰 임베딩, \(W_Q, W_K, W_V\): 가중치 행렬.

  - 어텐션 점수 \(A\)를 계산:
    \[
    A = \text{Softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)
    \]

- **프록시 토큰 기반 중요도 평가**:
  - 특정 작업과 관련된 **프록시 토큰**을 선택 (예: 질문의 끝부분).
  - 선택된 프록시 토큰에 기반해 중요도 점수 계산:
    \[
    F_{\text{score}} = \sum_{t \in \text{Proxy Tokens}} \text{Softmax}(A_{t, *})
    \]
    \(A_{t, *}\): 프록시 토큰과 모든 토큰 간의 어텐션 점수.

- **KV 캐시 초기화 및 토큰 제거**:
  - 계산된 중요도 점수를 기준으로 중요하지 않은 토큰을 KV 캐시에서 제거:
    \[
    K, V \rightarrow K_{\text{filtered}}, V_{\text{filtered}}
    \]
  - 제거는 한 번의 최적화 단계로 수행.

---

#### **2. 생성 단계**
- **토큰-by-토큰 생성**:
  - 새로운 토큰이 생성될 때마다 KV 캐시에 추가:
    \[
    K_t = [K_{t-1}, Q_t], \, V_t = [V_{t-1}, V_t]
    \]

- **랜덤 제거 적용**:
  - 주기적으로 랜덤 샘플링을 통해 일부 토큰을 유지:
    - 확률 분포:
      \[
      P_{\text{random}} = \text{Softmax}(F_{\text{random}})
      \]
    - 샘플링된 토큰 유지:
      \[
      K_{\text{random}}, V_{\text{random}}
      \]

- **출력 생성**:
  - 유지된 KV 캐시와 새로운 입력을 사용해 다음 토큰을 생성:
    \[
    P(y_t | y_{<t}, X) = \text{Softmax}(W \cdot h_t)
    \]
    \(h_t\): 현재 상태 벡터, \(W\): 출력 가중치 행렬.

---

### **모델 아키텍처 구성**

#### **1. 구성 요소**
- **Transformer 기반 LLM**:
  - 입력 임베딩 → 다중 헤드 어텐션 → 피드포워드 네트워크 → 출력.
  
- **KV 캐시 관리**:
  - 각 레이어와 어텐션 헤드에서 KV 캐시를 유지:
    \[
    \text{KV Cache} = \{(K_1, V_1), (K_2, V_2), ..., (K_L, V_L)\}
    \]

#### **2. 주요 연산**
- **다중 헤드 어텐션**:
  - \(H\)개의 헤드에서 어텐션 계산:
    \[
    \text{Attention}_h = \text{Softmax}\left(\frac{Q_h \cdot K_h^T}{\sqrt{d_k}}\right) \cdot V_h
    \]
  - 모든 헤드의 결과를 합침:
    \[
    \text{Output} = \text{Concat}(\text{Attention}_1, ..., \text{Attention}_H) \cdot W_O
    \]

- **피드포워드 네트워크 (FFN)**:
  - 각 토큰 임베딩을 두 개의 선형 변환과 활성화 함수로 처리:
    \[
    \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
    \]

---

### **메모리 요구량 및 컴퓨팅 비용**

#### **1. 메모리 요구량**
- **KV 캐시**:
  - 레이어 수 \(L\), 어텐션 헤드 수 \(H\), 토큰 길이 \(T\), 임베딩 차원 \(d\):
    \[
    \text{Memory} = O(L \cdot H \cdot T \cdot d)
    \]
  - 긴 문맥(예: 32k 토큰)에서는 \(>50 \, \text{GB}\) 메모리 소요.

- **프록시 및 랜덤 제거 오버헤드**:
  - 프록시 토큰의 어텐션 점수 계산:
    \[
    O(L \cdot H \cdot T_{\text{proxy}} \cdot T)
    \]
  - 랜덤 샘플링:
    \[
    O(L \cdot H \cdot T)
    \]

#### **2. 컴퓨팅 비용**
- **어텐션 계산**:
  \[
  O(L \cdot H \cdot T^2 \cdot d)
  \]
  - \(T^2\)에 비례하여 비용 증가.
  - NACL은 프록시 및 랜덤 제거로 \(T_{\text{proxy}} \ll T\)인 경우 비용 감소 가능.

- **최적화 연산**:
  - 한 번의 글로벌 최적화로 계산 단계 수를 줄여 기존 \(O(p+T)\)에서 \(O(1)\)로 단축.

---

### **결론**

#### **추론 과정 요약**
1. 입력 데이터를 토큰화하고 어텐션 점수를 계산.
2. 프록시 토큰을 기반으로 중요도를 평가해 덜 중요한 토큰 제거.
3. 생성 단계에서는 랜덤 샘플링을 추가해 다양성을 보장.
4. 최적화된 KV 캐시를 사용해 결과를 생성.

#### **모델 및 자원 요구량**
- 메모리 사용량은 긴 문맥에서 \(O(L \cdot H \cdot T \cdot d)\)로 높지만, NACL의 제거 정책이 이를 대폭 절감.
- 컴퓨팅 자원은 프록시와 랜덤 샘플링의 추가 연산을 포함해 약 10~15% 증가.

NACL의 효율적인 제거 메커니즘은 긴 문맥 처리의 주요 병목을 해결하고, 계산 및 메모리 비용을 최적화하여 실질적으로 활용 가능한 솔루션을 제공합니다.

# Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘



### **논문의 한계를 극복하기 위한 연구 방향 및 접근법**

NACL이 KV 캐시 최적화를 통해 메모리 절감과 성능 유지를 성공적으로 달성했지만, 논문에서는 몇 가지 한계를 언급하며 이를 해결하기 위한 추가 연구가 필요함을 시사합니다. 이러한 한계를 극복하기 위한 연구 흐름은 다음과 같은 방식으로 나눌 수 있습니다.

---

### **1. 프록시 토큰 의존성 문제**
#### **한계**
- 프록시 토큰의 선택이 모델 성능에 결정적 영향을 미침.
- 작업마다 다른 프록시 토큰 선택 기준이 필요하며, 이를 수동으로 정의하는 방식은 비효율적이고 일반화가 어려움.

#### **해결 방향**
1. **프록시 토큰 자동 선택 알고리즘 개발**
   - **어텐션 기반 선택**:
     - 모델의 어텐션 메커니즘을 활용해 입력 토큰 중 중요도가 높은 토큰을 자동으로 선택.
     - 기존 프록시 토큰 선택 방식을 개선하여 작업 특화 토큰뿐만 아니라 문맥 전반의 정보를 고려.
   - **학습 기반 접근법**:
     - 프록시 토큰 선택 과정을 학습 가능한 모듈로 설계.
     - 입력 데이터에 대해 중요한 토큰을 선택하는 **프록시 예측 모델**을 추가 학습.

2. **임베딩 유사성 활용**
   - 프록시 토큰을 선택할 때, 입력 데이터의 임베딩 공간에서 중심점(cluster center)을 찾는 방식 도입.
   - 주요 클러스터에 속하는 토큰을 프록시로 설정해 범용성을 높임.

3. **다중 프록시 토큰 전략**
   - 단일 프록시 토큰 집합에 의존하지 않고, **다중 프록시 집합**을 도입하여 작업의 다양성과 일반화를 보장.
   - 예: 질문 응답 작업에서는 질문의 시작과 끝부분을 모두 고려.

---

### **2. 초장문(Ultra-Long Context) 처리**
#### **한계**
- NACL은 128k 이상의 초장문 시나리오에서는 테스트되지 않았으며, 긴 문맥에서 프록시 토큰과 랜덤 제거가 효율적이지 않을 수 있음.
- 메모리 요구량이 여전히 GPU 용량을 초과할 가능성이 존재.

#### **해결 방향**
1. **동적 캐시 할당**
   - 입력 토큰의 중요도에 따라 KV 캐시 공간을 **동적으로 재분배**.
   - 중요도가 낮은 레이어 또는 헤드에 할당된 캐시를 줄이고, 더 중요한 헤드에 자원을 집중.

2. **분할 및 윈도잉(Windowing)**
   - 긴 시퀀스를 일정 길이로 분할한 후, 슬라이딩 윈도우 방식으로 처리.
   - 각 윈도우의 출력을 메모리 효율적으로 연결하여 전체 문맥을 유지.

3. **압축된 KV 캐시 활용**
   - 중요한 정보를 유지하면서 KV 캐시를 더 작게 압축:
     - **양자화(Quantization)**: KV 캐시의 부동소수점 표현을 정수로 변환하여 메모리 사용량 절감.
     - **스파스 매트릭스(Sparse Matrix)**: 중요도가 낮은 토큰의 캐시 항목을 제거해 스파스 구조를 도입.

4. **모듈화된 메모리 은닉층 추가**
   - Transformer의 **Recurrence Mechanism**을 적용하여 중요한 정보를 캐시에 저장하지 않고 별도의 메모리 모듈에 유지.
   - 예: Reformer나 Longformer에서 사용되는 메커니즘을 결합.

---

### **3. 랜덤 제거의 신뢰성 문제**
#### **한계**
- 랜덤 제거는 토큰 중요도 평가와 무관하게 일부 토큰을 선택하므로, 중요한 정보를 잃을 가능성이 존재.
- 랜덤성이 과도하게 적용되면 모델의 성능이 불안정해질 수 있음.

#### **해결 방향**
1. **랜덤 샘플링 정교화**
   - **가중 랜덤 샘플링**:
     - 어텐션 점수와 무작위성을 결합한 **가중치 기반 샘플링** 도입.
     - 중요도가 높은 토큰은 선택 확률을 높이고, 낮은 토큰은 낮은 확률로 선택.
   - 예:
     \[
     P(t) = \alpha \cdot F_{\text{score}}(t) + (1 - \alpha) \cdot P_{\text{random}}
     \]
     여기서 \(\alpha\)는 가중치를 조정하는 하이퍼파라미터.

2. **다중 랜덤 제거 전략**
   - 레이어별 또는 헤드별로 서로 다른 랜덤 제거 정책을 사용하여 정보의 다양성을 극대화.
   - 레이어 간의 상호보완적인 랜덤 샘플링 전략 설계.

---

### **4. 평가 지표의 한계**
#### **한계**
- 현재 평가 방식은 주로 퍼플렉시티(perplexity)와 정확도를 기반으로 하며, 실제 응용 시나리오(긴 문맥 이해, 질문 응답 등)를 충분히 반영하지 못함.

#### **해결 방향**
1. **실제 작업 기반 평가**
   - 긴 문맥에서 실제 작업(예: 논문 요약, 문서 질의응답)에 대한 평가를 강화.
   - 사용 사례 기반 지표 설계:
     - 처리 속도 (inference latency).
     - 메모리 사용량 대비 성능.
     - 문맥 보존률.

2. **다양한 입력 길이 테스트**
   - 짧은 문맥(1k~2k 토큰)과 긴 문맥(32k+ 토큰)을 모두 포함하는 **다양한 입력 데이터셋**에서 테스트 수행.
   - 입력 길이에 따른 성능 스케일링 분석.

---

### **5. 프레임워크 확장**
#### **한계**
- NACL은 추론 단계에서만 적용 가능하며, 학습 과정(파인튜닝 또는 사전 학습)에서는 활용되지 않음.

#### **해결 방향**
1. **학습 과정 통합**
   - KV 캐시 제거 전략을 사전 학습 또는 파인튜닝 과정에 통합하여 **학습된 제거 정책**을 적용.
   - 학습 과정에서 프록시 토큰 선택 및 랜덤 제거 전략의 최적화를 함께 수행.

2. **모델 구조 개선**
   - NACL을 통합한 새로운 Transformer 아키텍처 설계:
     - 어텐션 계산 단계에서 제거 메커니즘을 직접 내장.
     - 프록시 토큰과 랜덤 샘플링 모듈을 Transformer 내부에 포함.

---

### **결론**

NACL의 한계를 극복하기 위해 제안된 연구 흐름은 다음과 같은 주요 방향으로 요약할 수 있습니다:
1. **프록시 토큰 선택 자동화**를 통해 일반화와 효율성을 향상.
2. **초장문 처리 기술**(분할, 압축)을 활용하여 긴 문맥에서의 성능 보장.
3. **랜덤 제거의 정교화**로 신뢰성과 다양성을 동시 확보.
4. **평가 지표 개선**을 통해 실제 응용 시나리오에서의 유용성을 강조.
5. **학습 과정 통합 및 모델 구조 개선**으로 추론뿐만 아니라 학습에서도 활용 가능성 확대.

이러한 접근법은 NACL의 실용성을 높이고, 더 넓은 응용 범위로 확장할 수 있는 기반을 제공합니다.