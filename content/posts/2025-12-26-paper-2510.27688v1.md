---
title: "[논문리뷰]: Continuous Autoregressive Language Models"
date: "2025-12-26"
categories:
  - paper-review
  - with-gpt5.2
tags:
  - 2510.27688v1
  - calm
  - continuous-autoregressive
  - likelihood-free
  - energy-based-model
  - vector-language-model
  - semantic-bandwidth
  - scaling-law
  - inference-efficiency
  - autoencoder
  - brierlm
cover: https://discuss.pytorch.kr/uploads/default/original/2X/d/d1107e24375fae20a5f3a4733826880ce5d817db.png
---
[논문 링크](https://arxiv.org/abs/2510.27688v1)

## CALM: 토큰-by-토큰 병목을 “연속 벡터-by-벡터”로 우회하는 Likelihood-Free 언어모델링


**CALM** (Continuous Autoregressive Language Models) 은 (K=4) (tokens/step) 단위로 토큰을 연속 잠재벡터로 압축한 뒤 next-vector를 AR로 생성해, **유사한 품질(BrierLM)** 근처에서 학습 FLOPs와 추론 FLOPs/token을 동시에 낮추는 “성능–컴퓨트 프런티어 이동”을 주장한다. (근거: §7.2, Tab.1)

---

## 핵심 아이디어

* 현대 LLM은 여전히 **next-token** AR이라 시퀀스 길이 증가에 따라 계산비용이 누적되는 병목이 남아있다고 문제를 정의한다. (근거: §1)
* subword 토크나이저가 길이를 줄여온 것처럼, 다음 효율 축을 “생성 단위(토큰) 당 정보량” 증가로 본다. (근거: §1)
* 하지만 이산 토큰은 vocab이 **약 32,000–256,000 entries** 범위이고 토큰 정보량이 **15–18 bits/token** 수준이라 phrase-level 정보 확장에 한계가 있다고 주장한다. (근거: §1)
* phrase 수준을 vocab로 흡수하면 vocab이 **지수적으로 증가**해 softmax가 병목이 된다고 정리한다. (근거: §1)
* 해결책으로, (K) (tokens) 를 1개의 연속 벡터로 압축해 시퀀스 길이를 **(T \to T/K)** 로 줄이고 AR step 수를 **(K\times)** 감소시키는 축을 제안한다. (근거: §1, Fig.1)

---

## 배경: 그들이 해결한 문제

* “모델 용량(Params/Compute)은 커졌는데, 저정보량 토큰을 1개씩 예측하는 과제 자체는 정체되어 throughput을 제한한다”는 **capacity–task mismatch** 를 문제로 제기한다. (근거: §1)
* 연속공간으로 생성 단위를 바꾸면 finite vocabulary가 없어 표준 softmax 기반 **명시적 likelihood** 가 성립하지 않으며, 이 때문에 학습·평가·샘플링 도구가 비어있었다고 정리한다. (근거: §1)
* 기존 연속 생성 접근 중 diffusion/flow는 반복 샘플링이 필요해 추론 효율 목표와 충돌하는 축이 있다고 대비한다. (근거: §6.2)

---

## 새로운 접근법: CALM (Continuous Autoregressive Language Models)

**CALM** 은 “토큰을 연속 latent로 압축한 뒤 next-vector를 AR로 예측”하는 프레임워크로 제시된다. (근거: §1, Fig.1)

구성요소는 크게 4개다.

1. **고정밀 오토인코더** 로 (K) (tokens) → $(z_i\in\mathbb{R}^l)$ (vector) 압축 및 복원을 수행한다. (근거: §1)
2. backbone Transformer가 (z_{1:i-1}) 로부터 hidden $(h_{i-1})$ 를 만들고, generative head가 $(p(z_i\mid h_{i-1}))$ 에서 샘플링한다. (근거: §3.2)
3. likelihood 없이도 비교 가능한 평가로 **BrierLM** 을 제안한다. (근거: §4.2)
4. likelihood-free 조건에서도 온도 샘플링을 구현하는 알고리즘을 제안한다. (근거: §5.1, Alg.1, Appx.A.1)

---

## 작동 원리: 구체적인 예시로 살펴보기

### Step 0. 용어/변수 정의

* $(x_{1:T})$ : 길이 (T) (tokens) 의 이산 토큰 시퀀스. (근거: Fig.1)
* (K) : chunk size (tokens/step). (근거: §2)
* (L = T/K) : latent 시퀀스 길이 (vectors). (근거: §1, Fig.1)
* $(z_i\in\mathbb{R}^l)$ : i번째 chunk의 latent vector (dims). (근거: §2)

$$
L=\frac{T}{K}
$$
(근거: §1, Fig.1)

### Step 1. 토큰을 chunk로 묶고 인코딩한다

* 입력 $(x_{1:T})$ 를 (K) (tokens) 단위 chunk로 분할해 $(x^{(i)})$ 를 만든다. (근거: §1, Fig.1)
* 오토인코더 encoder는 $(x^{(i)})$ 를 보고 posterior $(q(z_i\mid x^{(i)}))$ 를 출력하며, $(z_i)$ 를 샘플링한다. (근거: §3.3.2)

### Step 2. latent 시퀀스를 AR로 예측한다

* backbone Transformer는 $(z_{1:i-1})$ 로부터 hidden $(h_{i-1})$ 를 만든다. (근거: §3.2)
* generative head는 $(z_i\sim p(\cdot\mid h_{i-1}))$ 를 샘플링한다. (근거: §3.2)
* diffusion/flow처럼 “수십~수백 번의 반복 평가”가 필요한 head는 효율 목표와 충돌한다고 정리하며 single-step 성격의 head를 강조한다. (근거: §1, §3.2)

### Step 3. 디코딩해 다시 토큰으로 복원한다

* decoder는 생성된 $(z_i)$ 를 (K) (tokens) 로 복원해 출력한다. (근거: §1)
* 이 과정에서 AR step 수는 토큰-AR 대비 (K\times) 줄어든다. (근거: §1, Fig.1)

### Toy example (매우 단순화)

* 가정: (K=3) (tokens/step), (l=2) (dims), 입력 토큰이 `A B C D E F` (tokens) 라고 하자. (근거: Fig.1의 “chunk→vector” 개념)
* encoder가 `A B C` → $(z_1\in\mathbb{R}^2)$, `D E F` → $(z_2\in\mathbb{R}^2)$ 로 압축한다. (근거: §3.3.2)
* 생성 시에는 $(z_1)$ 을 조건으로 head가 $(z_2)$ 를 한 번에 샘플링하고, decoder가 $(z_2)$ 를 `D E F` 로 복원한다. (근거: §3.2, §1)
* 결과적으로 한 step에서 3 토큰을 내보내므로 step 수가 $(3\times)$ 감소한다. (근거: §1, Fig.1)

### 학습 손실(핵심만)

* generative head는 likelihood 대신 strictly proper scoring rule인 energy score 기반 손실로 학습한다. (근거: §3.3.1–§3.3.2)
* 구현에서 head 샘플 수 (N=8) (samples/step), 타깃 샘플 수 (M=100) (targets/step) 를 사용한다. (근거: §3.3.2)

---

## 성능 검증: 주요 결과

### 실험 설정(요약)

* 학습 데이터는 Pile-uncopyrighted 약 **230B tokens** 로 서술된다. (근거: §7.1)
* 평가는 WikiText-103 기반 언어모델링으로 수행된다. (근거: §7.1)
* 오토인코더는 **75M Params** , **30k steps** , batch size **512k tokens/step** 으로 학습했다고 보고한다. (근거: §7.1)
* CALM 본체는 **250k steps** , batch size **2M tokens/step** 으로 학습했다고 보고한다. (근거: §7.1)

### 메인 테이블: Transformer vs CALM (K=4)

논문은 Tab.1을 primary 결과로 제시하며, 오토인코더 오버헤드까지 포함한 FLOPs를 보고한다. (근거: Tab.1, §7.1)

| Model         | Params (M Params) | Train FLOPs (×1e20 FLOPs) | Infer FLOPs/token (×1e8 FLOPs/token) | BrierLM (score, ↑) |
| ------------- | ----------------: | ------------------------: | -----------------------------------: | -----------------: |
| Transformer-S |               281 |                       6.6 |                                  4.4 |               6.05 |
| CALM-M (K=4)  |               371 |                       3.7 |                                  2.9 |               5.72 |
| Transformer-L |               849 |                      22.5 |                                 15.0 |               8.98 |
| CALM-XL (K=4) |              1820 |                      19.5 |                                  9.4 |               8.53 |

(근거: Tab.1)

* 저자들은 CALM-M (371M Params) 이 Transformer-S (281M Params) 와 **유사한 품질** 근처에서 **Train FLOPs 44% 감소(6.6→3.7×1e20)** 및 **Infer FLOPs/token 34% 감소(4.4→2.9×1e8)** 를 달성했다고 강조한다. (근거: §7.2, Tab.1)
* 동시에 Tab.1의 절대값 비교에서는 Transformer가 더 높은 BrierLM을 보이는 구간도 존재한다(예: 6.05 > 5.72, 8.98 > 8.53). (근거: Tab.1)

### “새 축” K의 효과

* (K=2) 는 비용이 “대략 절반”으로 줄고 성능 저하는 제한적이라고 서술된다. (근거: Fig.4, §7.2)
* (K=4) 는 baseline 성능–컴퓨트 프런티어를 “surpass”한다고 요약된다. (근거: Fig.4, §7.2)
* (K=8) 은 성능이 감소하며 capacity limitation 가능성이 언급된다. (근거: Fig.4, §7.2)

### ‘비밀 병기’ ablation: 오토인코더 정규화 패키지

Tab.2는 오토인코더 정규화( KL clipping + DropToken + DropLatent )가 downstream BrierLM을 좌우함을 보여준다. (근거: Tab.2, §7.3)

| AE 설정                                          | BrierLM (score, ↑) | Δ vs Full (score) |
| ------------------------------------------------ | -----------------: | ----------------: |
| Full (KL + KL clipping + DropToken + DropLatent) |               4.70 |             +0.00 |
| Recon-only (정규화 없음)                         |               3.99 |             -0.71 |
| KL만(naive VAE)                                  |               3.48 |             -1.22 |
| Dropouts 제거(KL + KL clipping만)                |               4.13 |             -0.57 |
| DropLatent 제거                                  |               4.55 |             -0.15 |
| DropToken 제거                                   |               4.46 |             -0.24 |

(근거: Tab.2)

* naive variational objective에서는 **128 dims 중 71 dims** 가 collapse했다고 보고한다. (근거: §7.3)
* 오토인코더 학습에서는 $(\beta=0.001)$ (unitless), KL floor $(\lambda_{KL}=0.5)$ (unitless), DropToken/DropLatent 확률 $(p=0.15)$ (rate) 를 사용한다. (근거: §2)
* 기본 설정에서 $(K=4)$ (tokens), $(l=128)$ (dims) 이며 $(\sigma\approx 0.3)$ (std-dev) 에서도 token-level accuracy **>99.9%** 를 유지한다고 보고한다. (근거: §2)

### Head 비교: diffusion / flow / energy

* diffusion은 “valid results”를 위해 많은 iteration이 필요하다고 정리된다. (근거: Fig.9, §7.5)
* flow(midpoint)는 **2 steps** 에서 decent quality, **4 steps** 에서 near-optimal이라고 보고한다. (근거: Fig.9, §7.5)
* energy-based head는 iterative decoding 없이도 최고 성능을 준다는 서술로 제시된다. (근거: Fig.9, §7.5)

---

## 우리의 관점: 강점, 한계, 그리고 이 연구가 중요한 이유

### 강점

* **토큰 병목을 “생성 단위”의 관점에서 직접 건드린다** 는 점이 논문 전체의 가장 명확한 차별점이다. (근거: §1, Fig.1)
* 오토인코더를 “크게 키우는 것”보다 **latent를 안정화(KL clipping)하고 예측 오차에 견디게(dropout)** 만드는 것이 downstream 성능에 더 큰 영향을 준다는 ablation은 설계 인사이트가 크다. (근거: Tab.2, §7.3)
* 평가 지표로 BrierLM을 제안하고, 샘플만으로 unbiased estimator를 구성해 likelihood-free 평가를 가능하게 한다. (근거: §4.2)

### 한계(논문 결과가 드러내는 현실적 제약)

* Tab.1에서 CALM은 “절대 성능 SOTA”를 단정하기보다, **성능–컴퓨트 프런티어 이동** 형태로 강점을 제시한다. (근거: §7.2, Tab.1)
* (K) 를 크게 키우면 (K=8) 에서 성능이 떨어지고 capacity limitation 가능성이 언급되어, “무제한 K 확대”가 쉽지 않음을 시사한다. (근거: Fig.4, §7.2)
* head 학습은 energy score의 지수 $(\alpha)$ 등에 민감하며, 특정 $(\alpha)$ 에서 실패 사례가 보고된다. (근거: Tab.4)
* 평가와 실험이 WikiText-103 중심으로 제시되어, 광범위 벤치마크(MMLU/GSM8K 등)에서의 일반화는 이 논문만으로는 판단할 수 없다. (근거: §7.1)

### 왜 중요한가(해석)

* (해석) 모델을 더 키우는 스케일링만으로는 해결하기 어려운 “생성 step 병목”을, **표현 단위와 학습/평가 도구체계** 자체를 바꿔 풀려는 시도라는 점에서 연구 궤적의 방향성을 제시한다.
* (해석) 특히 (K) 를 “세 번째 스케일링 축”으로 정식화하려는 관점은, 향후 inference-time 최적화(throughput/latency)와 학습-time 최적화(cost)의 공통 언어가 될 수 있다.

---

## 다음 단계는?: 앞으로의 길

* 저자들은 오토인코더가 재구성 중심이라 semantic structure가 약하다는 점을 한계로 들며, 의미론적으로 정렬된 latent 및 context-aware / autoregressive 오토인코더를 다음 단계로 제안한다. (근거: §8, §2)
* 또한 backbone+lightweight head 분리형 대신 end-to-end generative Transformer 같은 통합 설계를 탐색할 여지를 언급한다. (근거: §8)
* rejection sampling 기반의 “정확한 온도 샘플링”은 오버헤드 가능성이 있어, 더 가벼운 다양성 제어 휴리스틱(예: input noise scale 조절, loss 수정)을 연구 방향으로 제시한다. (근거: §8)
* Params/data에 더해 (K) (tokens/step) 를 포함한 스케일링 법칙을 수립하는 것이 중요한 오픈 문제로 언급된다. (근거: §8)
* log-prob/KL 접근이 없는 조건에서 RL(policy optimization)과 distillation을 sample-based로 재정식화하는 과제가 남아있다고 명시한다. (근거: §8)




### 토글을 클릭하면 논문에 대한 자세한 LLM 질의응답 내용을 확인할 수 있습니다.

<details markdown="block">
<summary>▶️<strong>클릭하여 펼치기</strong></summary>




## 프롬프트 1.1.1 (연구의 공백)

```
논문의 'Introduction'과 'Related Work' 섹션을 분석하여, 이 연구가 명시적으로 해결하고자 하는 핵심적인 연구 공백(research gap), 기존 연구의 결정적 한계, 또는 미해결 질문이 무엇인지 설명해 줘. 저자들이 설명하는, 이 논문 출판 시점의 '최신 기술(state of the art)'은 어떤 상태였는지 요약해 줘.
```


## 연구의 공백 (Introduction + Related Work 기반)

### TL;DR

* 저자들이 보는 핵심 공백은, **LLM의 “스케일링 축”이 파라미터/데이터에 치우친 반면, 생성 단위(토큰)의 정보량은 정체되어** 토큰 단위 AR 생성이 병목이 된다는 점이다. (근거: §1)
* 이를 해결하려면 “토큰→연속 벡터”로 생성 단위를 바꾸어 **한 스텝의 semantic bandwidth를 키우는 축**이 필요하나, 연속공간에서는 softmax 기반의 명시적 likelihood가 성립하지 않아 **학습·평가·샘플링 도구가 비어 있었다**고 정리한다. (근거: §1)

---

### 1) 저자들이 명시하는 SOTA의 상태 (출판 시점 관점)

* SOTA LLM은 “토큰-by-토큰” **순차적(auto-regressive) 생성**에 기반하며, 계산비용이 시퀀스 길이에 따라 증가해 장문 생성/긴 컨텍스트 처리에서 병목이 된다고 서술한다. (근거: §1)
* 현대 LLM의 서브워드 토크나이저는 시퀀스 길이를 줄여 효율을 올렸고, 이 성공이 “예측 단위의 정보 밀도 증가”를 다음 효율 축으로 시사한다고 주장한다. (근거: §1)
* 다만 저자들은 **이산(discrete) 표현은 근본 한계에 도달**했다고 명시하며, 현대 LLM의 vocabulary가 대략 **32,000–256,000 entries** 범위이고 토큰 정보량이 **15–18 bits/token** 수준이라고 수치로 제시한다. (근거: §1)
* “구(phrase) 수준”의 정보량을 토큰에 담으려면 vocabulary가 **지수적으로(exponential) 커져 softmax가 비현실적 병목**이 된다고 한계를 못 박는다. (근거: §1)

---

### 2) 핵심 연구 공백: “생성 단위의 정보량”을 키우는 방법론의 부재

#### 2.1 이산 토큰 패러다임의 결정적 한계(저자 주장)

* 저자들은 “모델 용량은 커졌지만, 저정보량 토큰을 1개씩 예측하는 과제 자체는 진화하지 않아 throughput을 제한한다”는 **capacity–task mismatch**를 제기한다. (근거: §1)
* 이 mismatch의 실무적 결과로, “긴 시퀀스일수록 계산비용이 증가”하는 AR 특성 때문에 **장문 생성과 긴 컨텍스트가 근본 병목**으로 남는다고 정리한다. (근거: §1)

#### 2.2 연속(continuous) 생성 단위로의 전환이 제안되었지만, LM 도구체계가 비어 있음

* 저자들은 K개 토큰을 1개 연속벡터로 압축하면 시퀀스 길이가 **T → T/K**로 줄어 **AR step 수가 K배 감소**한다고 도식으로 제시한다. (근거: §1, Fig.1)
* 그러나 연속공간으로 옮기면 finite vocabulary가 없어서 **표준 softmax로 “모든 가능한 결과”에 대한 명시적 분포를 계산할 수 없다는 문제**를 연구 공백의 핵심 제약으로 둔다. (근거: §1)
* 따라서 저자들이 보는 미해결 질문은, (i) 연속공간에서의 생성(학습), (ii) perplexity 없이의 평가, (iii) temperature sampling 같은 제어 생성이 “likelihood-free”로 어떻게 가능하냐는 도구체계 공백이다. (근거: §1)

---

### 3) Related Work가 드러내는 “기존 접근의 결정적 한계” 정리

| 연구 라인                          | 목표                     | 대표 접근(논문 서술 기준)                                         | 결정적 한계(저자 관점)                                                                                                                           |
| ---------------------------------- | ------------------------ | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| Prompt/Text compression            | 긴 입력을 짧은 표현으로  | attention 변형/재구성 목표 등 (근거: §6.1)                        | “재구성 fidelity”에 치우쳐 **downstream 생성모델링을 위한 robust/smooth latent manifold**가 약하다고 대비시킴 (근거: §6.1)                       |
| 초고압축(초대비율)                 | 극단적 압축비 실증       | 최대 **1568×** 압축비까지 보고 (근거: §6.1)                       | 압축 자체의 가능성은 보였으나, 저자들은 **생성 안정성의 전제(robust latent)**가 핵심이라고 선을 긋는다 (근거: §6.1)                              |
| Continuous AR (비언어 도메인 중심) | 연속 벡터의 AR 생성      | 이미지/비디오/오디오에서 성과가 축적 (근거: §6.2)                 | 언어 LM에 그대로 이식하기엔 head/손실/입력구조의 안정성 이슈가 남는다고 “언어 특화 개선” 필요성을 명시 (근거: §6.2)                              |
| GIVT류(혼합가우시안)               | 벡터 분포 모델링         | GMM family로 target vector 분포 적합 (근거: §6.2)                 | 분포 family가 사전 정의되어 **표현력 제한**이 있다고 지적 (근거: §6.2)                                                                           |
| Diffusion head                     | 더 높은 표현력           | diffusion 기반 head로 벡터 분포 모델링 (근거: §6.2)               | **iterative sampling** 때문에 추론 효율이 떨어진다고 명시 (근거: §6.2)                                                                           |
| Parallel token prediction / NAT    | 순차 병목 완화           | NAT 번역이 “한 번에 문장 생성”을 목표 (근거: §6.2)                | 번역 같은 조건부 과제엔 유효하지만, open-ended 생성의 **multi-modality**에서 취약하다고 정리 (근거: §6.2)                                        |
| Hierarchical(semantic chunk)       | 큰 단위 의미 → 세부 토큰 | MegaByte는 block 예측 후 block 내부는 여전히 토큰 AR (근거: §6.2) | “토큰 내부 AR”이 남아 **완전한 step 감소가 아님**을 암시 (근거: §6.2)                                                                            |
| LCM(연속 문장 임베딩 AR)           | 연속 개념 단위 생성      | global이 sentence embedding을 AR로 예측 (근거: §6.2)              | SONAR autoencoder가 **computationally heavy & fragile**, diffusion 기반 생성이 **iterative inference bottleneck**이라고 문제를 제기 (근거: §6.2) |

---

### 4) 이 논문이 “공백”으로 규정한 미해결 질문(정확히 무엇이 비었나)

* 연속 벡터 예측으로 step 수를 줄이려면, “K tokens → 1 vector”를 **고정밀로 복원**할 수 있는 경량 autoencoder가 필요하다고 전제한다. (근거: §1)
* 저자들은 실제로 autoencoder 복원 정확도가 **99.9% 이상**이어야 downstream LM의 전제조건이 된다고 요약한다. (근거: Abstract)
* 그러나 likelihood가 없으니 “perplexity 같은 전통 LM 지표가 부적절”해지고, **likelihood-free 평가 지표**가 필요하다는 점을 공백으로 설정한다. (근거: §1)
* 또한 temperature sampling은 “확률분포 조작”에 의존하는데, 연속공간/likelihood-free 조건에서 이를 **원리적으로 대체할 샘플링 알고리즘**이 필요하다고 명시한다. (근거: §1)
* 마지막으로, diffusion/flow처럼 iterative sampling을 쓰면 “병목이 재도입”되므로, **single-step 생성**이 가능한 head가 중요하다는 판단을 관련연구 대비로 제시한다. (근거: §1, §6.2)

---

### 5) 한 문장으로 요약한 “연구 공백 정의”

* 저자들이 정의한 연구 공백은 “토큰 단위 이산 AR의 정보량 한계(32k–256k vocab, 15–18 bits/token)로 인해 step 기반 병목이 고착된 상황에서, 연속공간의 next-vector prediction을 **언어모델링에 안정적으로 적용하기 위한 likelihood-free 학습·평가·샘플링 체계가 부족했다**”는 것이다. (근거: §1, §6)


## 프롬프트 1.1.2 (핵심 가설)

```
이 논문의 중심 가설(central hypothesis) 또는 핵심 주장은 무엇인가? '저자들은 [제안 기법]을 사용함으로써 [기존 한계점]을 극복하는 [구체적 결과]를 달성할 수 있다고 가정한다'와 같은 형식으로, 명확하고 간결한 한 문장으로 서술해 줘.
```

저자들은 **CALM(Continuous Autoregressive Language Models)** 에서 “고정밀(autoencoder) 기반 K-token→1-vector 압축 + 연속 next-vector AR 예측 + likelihood-free 학습/평가/샘플링”을 사용함으로써 (근거: §1, Fig.1), 기존 이산 next-token AR의 저정보량 토큰(step) 병목과 softmax vocab 스케일 한계를 극복하고 (근거: §1), 토큰 복원 정확도 **≥ 99.9%** (근거: §2.1) 및 생성 step **K× 감소** (근거: §1, Fig.1)를 통해 강한 discrete baseline 수준 성능을 더 낮은 연산비용으로 달성할 수 있다고 가정한다 (근거: §1).


## 프롬프트 1.2.1 (독창성 식별)

```
논문 전체를 바탕으로, 가장 중요하고 독창적인 기여(contribution) 1~3가지를 구별되는 항목으로 나열해 줘. 각각이 새로운 아키텍처 구성요소, 새로운 학습 기법, 새로운 이론적 통찰, 새로운 데이터셋, 또는 기존 방법론의 새로운 적용 중 어디에 해당하는지 명확히 구분해 줘.
```

 아래는 논문 전체에서 저자들이 **“primary contributions”** 로 정리한 내용을 기준으로 추출·재구성한 핵심 기여 3가지다. (근거: §1)

## 1.2.1 독창성 식별

### 기여 1) CALM 패러다임 + 고정밀 연속 오토인코더 + Energy Transformer로 likelihood-free next-token modeling 정식화

* 저자들은 **CALM(Continuous Autoregressive Language Models)** 을 “한 번의 모델 step에서 K-1 개의 추가 토큰을 더 예측해(semantic bandwidth 증가) 학습/추론을 가속”하는 연속(latent) 기반 AR 언어모델 패러다임으로 제안한다. (근거: §1)
* 이를 위해 “state-of-the-art reconstruction ratio 99.9% ” 를 달성하는 **개선된 연속 오토인코더** 를 제시하며, 기존 연속 오토인코더 기준선 “99.7% ” 대비 향상을 수치로 보고한다. (근거: §1)
* 또한 연속 잠재공간에서의 “next-token distribution” 을 직접 likelihood로 두지 않고, **Energy Transformer 기반 EBM** 으로 likelihood-free 생성(head) 을 구성해 모델링한다고 명시한다. (근거: §1)
* 분류: **새로운 아키텍처 구성요소** (연속 오토인코더 + EBM head) + **새로운 학습/모델링 정식화** (likelihood-free next-token modeling). (근거: §1)

---

### 기여 2) likelihood-free 언어모델 평가를 위한 절차 + BrierLM 메트릭 제안 (NLL과의 정합성 주장)

* 저자들은 likelihood-free 모델을 “동등한 기준에서 평가”하기 위한 **평가 절차(evaluation procedure)** 와 **BrierLM 메트릭** 을 제안한다. (근거: §1)
* BrierLM은 “NLL이 정의 가능한 경우 NLL과 align” 하도록 설계되었다고 주장한다. (근거: §1)
* 분류: **새로운 이론적 통찰/평가 방법론** (likelihood-free setting에서의 proper scoring 기반 정량평가 정식화). (근거: §1)

---

### 기여 3) likelihood-free temperature sampling 알고리즘 (Bernoulli factory 기반) 및 “per-token / per-sequence non-uniform temperature” 가능성 제시

* 저자들은 likelihood-free 샘플링에서 “효과적으로 temperature” 를 적용하는 **temperature sampling 알고리즘** 을 제안하며, 핵심 도구로 **Bernoulli factory** 를 사용한다고 밝힌다. (근거: §1)
* 이 방식이 “generation quality 개선” 및 “per-sequence / per-token 비균일 temperature” 같은 확장된 샘플링 제어를 가능케 한다고 주장한다. (근거: §1)
* 분류: **기존 방법론의 새로운 적용** (temperature sampling을 likelihood-free 생성에 맞게 재구성) + **추론(디코딩) 알고리즘 기여**. (근거: §1)


## 프롬프트 1.2.2 (저자 관점에서의 강점)

```
저자들의 관점에서, 자신들의 접근법이 이전 방법들보다 우월한 이유는 무엇인가? 그들이 자신들의 연구가 지닌 독창성과 강점을 뒷받침하기 위해 사용하는 핵심 논거를 인용하거나 알기 쉽게 설명해 줘.
```

 저자들은 CALM이 기존 방법 대비 우월한 이유를 “**생성 단위의 정보량을 키우면서도 **(bold-space rule)** 추론 병목(softmax/iterative sampling)을 재도입하지 않는 end-to-end likelihood-free 체계**”로 정리한다. (근거: §1, Fig.1)

## 저자 관점의 강점과 핵심 논거

### 1) “스케일링 축”을 파라미터/데이터에서 “step당 정보량”으로 확장

* 저자들은 이산 토큰에서 phrase-level 정보를 담으려면 vocab이 **지수적(exponential)으로 증가**하고, 그 결과 최종 **softmax가 병목**이 된다고 주장한다. (근거: §1)
* CALM은 K개 토큰을 1개 연속 벡터로 압축해 시퀀스 길이를 **T → T/K (tokens)** 로 줄여, AR step 수를 **K× 감소**시키는 것이 “근본적 효율 개선”이라고 논증한다. (근거: §1, Fig.1)
* 이 설계는 정보량 증가를 “vocab 크기”가 아니라 “벡터 차원(=latent dimension)”을 키우는 방식으로 처리해 확장 경로가 더 스케일러블하다고 주장한다. (근거: §1)

$$
L=\frac{T}{K}\quad (\text{tokens})
$$
(근거: §1, Fig.1)

---

### 2) 고정밀 + 강인한(autoencoder) latent를 “선결 조건”으로 충족

* 저자들은 downstream 언어모델이 안정적으로 학습되려면 **high-fidelity reconstruction**이 전제이며, 이를 위해 “강력하면서도 경량(lightweight)”한 오토인코더를 기여로 전면에 둔다. (근거: §1)
* 오토인코더는 latent vector dropout **p=0.15 (rate)** 와 token masking dropout **p=0.15 (rate)** 를 통해 “작은 예측 오차에 robust한 중복(redundant) 표현”을 학습하도록 설계했다고 설명한다. (근거: §2)
* K=4 (tokens)에서 latent dimension **l=128 (dims)** 를 사용하고, latent posterior 표준편차가 **σ≈0.3 (std-dev)** 로 수렴해도 decoder가 token-level accuracy **>99.9% (accuracy)** 를 유지한다고 제시하며 “고정밀+강인성”을 강점으로 든다. (근거: §2)
* 또한 autoencoder를 과도하게 스케일해도 최종 BrierLM 개선이 크지 않았고, “상대적으로 modest한 데이터/경량 구조로 충분”하여 전체 시스템에서 **계산적으로 negligible** 하다고 주장한다. (근거: §7.3)

---

### 3) “연속 생성 head”에서 iterative decoding을 제거해 병목 재도입을 차단

* 저자들은 diffusion/flow 계열 head는 **iterative sampling**이 필요해 “추론 병목을 재도입”한다고 명시하고, 이를 피하기 위해 **Energy Transformer**를 채택했다고 주장한다. (근거: §1)
* Energy Transformer는 연속 벡터를 **single-step generation**으로 생성하도록 설계된 최근 아키텍처이며, 저자들은 “경험적으로 superior generation quality”를 보였다고 주장한다. (근거: §1)
* 실험 비교에서 diffusion은 “valid results”를 위해 많은 iteration이 필요한 반면, energy-based head는 **iterative decoding 필요를 완전히 제거**하면서도 성능 상한(ceiling)이 높다고 결론낸다. (근거: §7.4, Fig.8–9)

---

### 4) perplexity가 불가능한 setting에서 “엄밀한 비교”를 가능케 하는 BrierLM

* 저자들은 likelihood가 intractable한 CALM에서 Perplexity가 부적절하므로, 샘플만으로 평가 가능한 **BrierLM**을 제안하며 이것이 **strictly proper scoring rule**이라 “공정한 비교”를 보장한다고 주장한다. (근거: §1, §6.3)
* BrierLM은 모델 샘플만으로 **unbiased estimator**로 추정 가능하다고 설명하며, 이는 likelihood-free 평가 프로토콜로 적합하다고 주장한다. (근거: §4.2)
* 추가로, baseline AR 모델에서 cross-entropy와 BrierLM이 강하게 정렬되며 Pearson **-0.966 (corr)**, Spearman **-0.991 (corr)** 로 “신뢰 가능한 대체 지표”임을 실증했다고 제시한다. (근거: §4.2, Fig.3)

---

### 5) likelihood-free 조건에서도 “temperature sampling”을 원리적으로 복원

* 저자들은 기존 temperature sampling이 pre-softmax logits 조작처럼 “명시적 분포 접근”을 요구하므로, sampler만 제공하는 CALM에서는 호환되지 않는다고 명시한다. (근거: §5.1)
* 이를 해결하기 위해 rejection sampling 직관(반복 샘플링 ↔ 확률 거듭제곱)을 기반으로, 1/T를 정수부 **n=⌊1/T⌋ (unitless)** 와 소수부 **α=1/T−n (unitless)** 로 분해하는 **2-stage 알고리즘**을 제시한다. (근거: §5.1, Alg.1)
* 소수부(α)는 **Bernoulli Factory**를 이용해 P(x)^α를 시뮬레이트한다고 설명하며, Alg.1이 목표 분포 (P_T(x)\propto P(x)^{1/T}) 를 정확히 샘플링한다는 정리를 제시한다. (근거: §5.1, Thm.1, Appx.A.1)

---

## 저자들의 “우월성” 주장 요약 표

| 저자 주장 강점                | 핵심 논거(요지)                                                                                                                                  | 대비되는 기존 한계                                                                               |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |
| step 수 **K× 감소**           | K tokens→1 vector로 길이 T→T/K (tokens)라 AR step을 K× 줄여 효율이 “근본적으로” 개선된다고 주장. (근거: §1, Fig.1)                               | token 단위 AR은 긴 시퀀스에서 병목이 누적된다는 문제의식. (근거: §1)                             |
| **softmax 병목 회피**         | phrase-level 정보량을 vocab으로 해결하면 vocab이 exponential로 커져 softmax가 병목이라 주장. (근거: §1)                                          | discrete vocab 확장 경로가 계산적으로 untenable하다는 한계. (근거: §1)                           |
| **iterative decoding 제거**   | diffusion/flow는 iterative sampling이 필요해 병목 재도입, Energy Transformer는 single-step generation으로 이를 제거한다고 주장. (근거: §1, §7.4) | diffusion류 head는 추론 iteration 수가 비용을 지배. (근거: §7.4, Fig.9)                          |
| likelihood-free **평가**      | BrierLM은 strictly proper이고 샘플만으로 unbiased 추정 가능해, likelihood 없이도 “fair comparison” 가능하다고 주장. (근거: §1, §4.2, §6.3)       | Perplexity는 likelihood 필요, implicit generative model 평가는 어려웠다는 문제의식. (근거: §4.2) |
| likelihood-free **제어 생성** | temperature sampling을 black-box sampler만으로 정확히 구현(Alg.1, Thm.1)했다고 주장. (근거: §5.1, Alg.1, Thm.1)                                  | 기존 temperature는 logits/분포 접근이 필요해 CALM과 비호환. (근거: §5.1)                         |

---

### 6) 저자들이 제시하는 “결과로 뒷받침되는 강점” (performance–compute trade-off)

* 저자들은 표준 LM 벤치마크에서 CALM이 “ **superior performance–compute trade-off** ”를 보였다고 주장하며, 예시로 K=4 (tokens)에서 **강한 discrete baseline과 comparable performance**를 **significantly lower computational cost**로 달성했다고 서술한다. (근거: §1)

원하면 다음 단계로, 위 강점들이 실제 실험 섹션(§7.x)의 어떤 설정/모델 크기/훈련 스텝(steps)에서 성립하는지 “정량 비교 축”으로 재정리해 주겠다. (근거: §1, §7)


## 프롬프트 1.3.1 (알고리즘 단계별 설명)

```
핵심 알고리즘, 모델 아키텍처, 또는 주요 방법론을 단계별(step-by-step)로 설명해 줘. 독자는 AI 분야의 대학원생 수준이라고 가정해. 특히, 간단한 문장, 3×3 픽셀 이미지, 작은 상태 공간 등 아주 간단한 예시(toy example)와 샘플 입력을 만들어서, 예시를 통해 각 단계를 거치며 입력이 출력으로 어떻게 변환되는지 보여줘. 등장하는 모든 핵심 용어와 변수는 그 즉시 정의해 줘.
```

 CALM은 “K 토큰을 1 연속벡터로 압축한 뒤, next-token이 아니라 next-vector를 AR로 생성”하는 프레임워크이며, 이를 위해 (1) 고정밀·강인한 오토인코더, (2) single-step 연속 생성이 가능한 Energy Transformer 기반 generative head, (3) likelihood-free 평가(BrierLM)와 temperature sampling(Alg.1)을 하나의 체계로 묶는다. (근거: Fig.1, §2, §3, §4, §5)

---



## 2) Step-by-step: 오토인코더(연속 표현) 학습 (근거: §2)

### Step 2.1 **Chunking** : 토큰을 K개씩 묶어 입력 단위를 만든다 (근거: Fig.1)

* 이산 토큰 시퀀스 $(x_{1:T})$ 를 길이 K 토큰의 chunk로 분할해 $(x^{(i)} = x_{(i-1)K+1:iK})$ 를 만든다. (근거: Fig.1)

### Step 2.2 **Encoder** : chunk → 연속 잠재벡터 posterior를 출력 (근거: §2)

* 오토인코더는 chunk를 단일 점이 아니라 **조건부 Gaussian posterior** 로 매핑해 $(z_i \sim q(\cdot \mid x^{(i)}))$ 를 샘플링한다. (근거: §3.3.2)

### Step 2.3 **VAE-style regularization + KL clipping** : latent manifold를 “생성하기 쉬운 공간”으로 만든다 (근거: §2)

* 총 손실은 재구성 손실과 KL 정규화의 가중합 $(L_{total}=L_{ae}+\beta L_{KL})$ 이고, $(\beta=0.001)$ 로 설정한다. (근거: §2)
* posterior collapse를 완화하기 위해 per-dimension KL을 바닥값 $(\lambda_{KL}=0.5)$ 로 클리핑하는 $(L^{clip}_{KL}=\sum_i \max(\lambda_{KL}, L_{KL,i}))$ 를 사용한다. (근거: §2)

### Step 2.4 **Robustness dropout** : “downstream 예측 오차”를 견디는 중복 표현 학습 (근거: §2)

* latent vector dropout을 rate (p=0.15) 로 적용해, decoder가 일부 latent 손실에도 복원하도록 유도한다. (근거: §2)
* input token masking dropout을 rate (p=0.15) 로 적용해, chunk 의미를 latent에 담도록 유도한다. (근거: §2)
* 이 dropout들은 오토인코더 학습에서만 사용하고, 이후 CALM 학습/추론에서는 비활성화한다. (근거: §2)

### Step 2.5 **구체 설정(핵심 수치)** : K=4 토큰 → l=128 dims latent, σ≈0.3에서도 99.9%+ 복원 (근거: §2)

* 저자들은 chunk size (K=4) 토큰과 latent dimension (l=128) dims를 사용한다. (근거: §2)
* encoder posterior의 표준편차 $(\sigma_i)$ 가 약 $(\sigma \approx 0.3)$ (std-dev)로 수렴해도, decoder는 token-level accuracy (>99.9% ) 를 유지한다고 보고한다. (근거: §2)

---

## 3) Step-by-step: CALM(연속 AR) 학습 (Transformer + Generative Head) (근거: §3.2–§3.3)

### Step 3.1 **입력 변환** : 토큰 시퀀스를 latent 시퀀스로 치환 (근거: Fig.1, §1)

* Fig.1의 개념도처럼, 토큰 길이 T는 latent 길이 (T/K) 로 감소한다. (근거: Fig.1)

### Step 3.2 **Backbone Transformer** : 이전 latent들로 hidden state를 만든다 (근거: §3.2)

* hidden state는 $(h_{i-1}=\text{Transformer}(z_{1:i-1}))$ 이고, $(h_{i-1}\in \mathbb{R}^d)$ 로 정의한다. (근거: §3.2)

### Step 3.3 **Generative head** : (p(z_i\mid h_{i-1})) 에서 z를 “샘플링”한다 (근거: §3.2)

* generative head는 $(z_i\in \mathbb{R}^l)$ 를 $(z_i \sim p(\cdot\mid h_{i-1}))$ 로 샘플링하는 확률적 함수로 정의된다. (근거: §3.2)
* diffusion/flow 계열은 벡터 1개 생성에 “dozens or hundreds” 회 네트워크 평가가 필요해 효율 목표와 충돌한다고 명시하고, single-step 생성을 위해 Energy Transformer를 채택한다. (근거: §1, §3.2)

### Step 3.4 **학습 목적** : likelihood 대신 strictly proper scoring rule 기반 “energy loss”로 학습 (근거: §3.3.1–§3.3.2)

* 저자들은 generative head 학습을 strictly proper scoring rule 최적화로 정식화한다. (근거: §3.3.1)
* Energy score는 likelihood-free 이며, 샘플 간 거리로 예측 분포와 관측치의 정합성을 측정한다. (근거: §3.3.2)

$$
S(P,y)=\mathbb{E}_{x',x''\sim P}\left[\lVert x'-x''\rVert^\alpha\right]-2\mathbb{E}_{x\sim P}\left[\lVert x-y\rVert^\alpha\right],\ \alpha\in(0,2)
$$
(근거: §3.3.2)

* 실용 손실(energy loss)을 만들기 위해 step i에서 generative head로 N개 후보 (\tilde z_{i,1:N}) 를 샘플링하고, 오토인코더 posterior에서 M개 타깃 (z_{i,1:M}) 를 샘플링해 분산을 줄인다. (근거: §3.3.2)
* 구현에서 (N=8) (samples/step), (M=100) (targets/step) 를 사용한다. (근거: §3.3.2)

$$
\mathcal{L}*{energy}=\sum*{i=1}^{L}\left(\frac{2}{NM}\sum_{n=1}^{N}\sum_{m=1}^{M}\lVert z_{i,m}-\tilde z_{i,n}\rVert-\frac{1}{N(N-1)}\sum_{n\neq k}\lVert \tilde z_{i,n}-\tilde z_{i,k}\rVert\right)
$$
(근거: §3.3.2)

---

## 4) Step-by-step: 생성(inference) 시 “벡터 1개 = K 토큰” 생성 흐름 (근거: Fig.1, §3.2)

1. (초기) 이미 주어진 prefix 토큰을 K개씩 묶어 encoder로 latent prefix (z_{1:i-1}) 를 만든다. (근거: Fig.1)
2. backbone Transformer로 $(h_{i-1}=\text{Transformer}(z_{1:i-1}))$ 를 계산한다. (근거: §3.2)
3. generative head가 $(z_i\sim p(\cdot\mid h_{i-1}))$ 를 **1-step** 으로 샘플링한다. (근거: §3.2, §1)
4. decoder가 $(z_i)$ 를 K개 토큰 $(\hat x^{(i)})$ 로 복원한다. (근거: §1)
5. $(\hat x^{(i)})$ 를 출력하고, 다음 step으로 진행한다(이때 step 수는 토큰 AR 대비 K배 감소). (근거: Fig.1)

---

## 5) Toy example: “3 토큰 → 1 벡터 → 3 토큰”이 실제로 어떻게 굴러가나 (가상 예시)

가정: vocab이 8개 토큰 {A,B,C,D,E,F,G,H}, chunk size (K=3), latent dimension (l=2).
입력 시퀀스: `A B C | D E F | G H (pad)`.

### Step 5.1 Encode (chunk → z)

* Encoder가 첫 chunk `A B C`를 보고 posterior $(q(z\mid ABC)=\mathcal{N}(\mu,\sigma^2I))$ 를 출력한다고 하자.
* 예를 들어 $(\mu=(0.2, -1.1))$, $(\sigma=0.3)$ 이면, 샘플 $(z_1=\mu+\sigma\epsilon)$ ( $(\epsilon\sim\mathcal{N}(0,I))$ )로 $(z_1)$ 을 뽑는다.

### Step 5.2 Next-vector prediction (z prefix → z next)

* Backbone Transformer는 $(z_1)$ 를 입력으로 hidden $(h_1)$ 을 만든다.
* Generative head는 $(h_1)$ 조건에서 $(z_2)$ 를 “샘플링”한다(예: $(z_2=(1.0,0.1))$).

### Step 5.3 Decode (z → K tokens)

* Decoder는 $(z_2)$ 를 받아 3개 토큰을 복원한다(예: $(\hat x^{(2)}=)$ `D E F`).
* 즉, **한 번의 AR step에서 3 토큰이 동시에 출력**되는 것이고, 토큰 AR 대비 step 수가 3배 줄어든다.

이 toy 흐름이 Fig.1의 “The cat sat on the mat” 예시에서 보이는 token-by-token 대비 vector-by-vector 개념과 동일한 구조다. (근거: Fig.1)

---

## 6) (보너스) 평가와 제어 생성: likelihood-free 환경을 “사용 가능하게” 만드는 구성요소들

### 6.1 BrierLM 평가: 샘플만으로 계산 가능한 지표 (근거: §4.2)

* Brier score는 full distribution (P) 가 필요하지만, CALM에서는 두 번 샘플링으로 unbiased estimator를 구성한다. (근거: §4.2)

$$
\widehat{\text{Brier}}(P,y)=\mathbb{1}{x_1=y}+\mathbb{1}{x_2=y}-\mathbb{1}{x_1=x_2},\ \ x_1,x_2\sim P
$$
(근거: §4.2)

* BrierLM은 (n=1..4) 의 Brier-n을 기하평균하고 100을 곱해 0–100 스케일로 만든다. (근거: §4.2)

$$
\text{BrierLM}=100\cdot\left(\prod_{n=1}^{4}\text{Brier-}n\right)^{0.25}
$$
(근거: §4.2)

### 6.2 Temperature sampling (Alg.1): “분포를 몰라도” 정확 온도 샘플링 (근거: §5.1, Alg.1, Appx.A.1)

* 목표 분포는 $(P_T(x)\propto P(x)^{1/T})$ 이고, base sampler (S) 만 접근 가능한 상황을 다룬다. (근거: §5.1, Thm.1)

$$
P_T(x)=\frac{P(x)^{1/T}}{Z_T},\ \ Z_T=\sum_x P(x)^{1/T}
$$
(근거: §5.1, Thm.1)

* 핵심 분해는 $(n=\lfloor 1/T\rfloor)$ (unitless), $(\alpha=1/T-n)$ (unitless) 이며 2-stage rejection으로 구성된다. (근거: §5.1)
* Stage 1은 n번 샘플이 모두 동일할 때만 candidate를 통과시키는 반복 샘플링 기반 rejection이다. (근거: §5.1)
* Stage 2는 Bernoulli factory 아이디어로 $(P(x)^\alpha)$ 성공확률을 시뮬레이션하는 반복 절차이며, Alg.1에는 $(u\sim U(0,1))$ 와 $(u<\alpha/i)$ 조건으로 restart/continue가 명시된다. (근거: Alg.1)


## 프롬프트 1.3.2 ('비밀 병기' 식별)

```
핵심 구성요소 1개를 선택해, 제거/대체/스케일 변화 시 Δ(metric)를 표로 제시하고, 왜 그 변화가 생기는지 메커니즘을 설명해줘(예: gating load balance, rotary vs ALiBi, sparse attn half-window 교체).
```

 CALM의 “비밀 병기”는 **KL clipping + 이중 dropout(DropToken/DropLatent)** 로 **고정밀(high-fidelity) + 강인(robust) latent space** 를 강제하는 오토인코더 정규화 패키지다. (근거: §2, §7.3, Tab.2)

## 비밀 병기 선정: **KL clipping + DropToken + DropLatent**  (Autoencoder regularization package)

### 제거/대체/스케일 변화에 따른 Δ(BrierLM)

* 공통 metric은 **downstream LM 성능을 BrierLM (score)** 로 측정한다. (근거: §7.3, Tab.2)

| 실험 조작(오토인코더)         | 설정 변화                              |                 BrierLM (score) | Δ vs Full (score) | Evidence            |
| ----------------------------- | -------------------------------------- | ------------------------------: | ----------------: | ------------------- |
| **Full (기본)**               | LKL + LclipKL + DropToken + DropLatent |                            4.70 |             +0.00 | (근거: Tab.2)       |
| 대체: Recon-only AE           | (정규화 없음)                          |                            3.99 |             -0.71 | (근거: Tab.2)       |
| 제거: KL clipping             | LKL만(naive VAE)                       |                            3.48 |             -1.22 | (근거: Tab.2, §7.3) |
| 제거: Dropouts 전부           | LKL + LclipKL만                        |                            4.13 |             -0.57 | (근거: Tab.2, §7.3) |
| 제거: DropLatent              | LKL + LclipKL + DropToken              |                            4.55 |             -0.15 | (근거: Tab.2)       |
| 제거: DropToken               | LKL + LclipKL + DropLatent             |                            4.46 |             -0.24 | (근거: Tab.2)       |
| 스케일 변화: AE 레이어 2→4    | encoder/decoder layers=4 (layers)      | “유의미한 개선 없음” (≈0 score) |                ≈0 | (근거: §7.3)        |
| 스케일 변화: AE hidden dim ↑  | hidden dim=1024 (dims)                 | “유의미한 개선 없음” (≈0 score) |                ≈0 | (근거: §7.3)        |
| 스케일 변화: AE 학습 데이터 ↑ | dataset=100B tokens                    | “유의미한 개선 없음” (≈0 score) |                ≈0 | (근거: §7.3)        |

---

## 왜 이런 Δ가 발생하는가: 메커니즘(저자 논리)

### 1) KL clipping이 없으면 “posterior collapse → 노이즈 차원 증가 → downstream 학습 불안정”이 터진다. (근거: §2, §7.3)

* naive variational objective를 넣으면 latent **128 dims 중 71 dims** 가 prior로 collapse했다고 보고한다. (근거: §7.3)
* collapse된 차원은 reconstruction에 정보가 없고, 더 중요한 문제로 “pure noise dimensions” 가 downstream LM에 **chaotic signal** 로 작용해 학습을 destabilize한다고 설명한다. (근거: §2)
* KL clipping은 per-dimension KL을 **λKL=0.5 (unitless floor)** 로 하한 고정해 모든 차원이 reconstruction에 참여하도록 강제하며, collapse를 막아 성능을 회복시키는 “crucial remedy”로 서술된다. (근거: §2, §7.3)

### 2) DropLatent은 “예측 오차 내성”을, DropToken은 “semantic context 압축”을 각각 강제해 성능 이득이 직교(orthogonal)로 누적된다. (근거: §2, §7.3, Tab.2)

* DropLatent은 latent (z) 에 **p=0.15 (rate)** dropout을 걸어 decoder가 redundant representation을 학습하게 만들고, downstream generative model의 **minor prediction errors** 를 견디게 한다. (근거: §2)
* DropToken은 입력 토큰에 **p=0.15 (rate)** masking을 걸어, chunk 의미를 주변 문맥으로 “추론”하게 만들어 latent가 단순 인덱스 압축이 아니라 semantic context를 담도록 유도한다. (근거: §2)
* Tab.2에서 DropToken만 추가했을 때 **+0.42 score (4.13→4.55)** , DropLatent만 추가했을 때 **+0.33 score (4.13→4.46)** 로 둘 다 의미 있는 개선이며, 둘을 같이 쓰면 **4.70 score** 까지 상승해 누적 이득을 보인다. (근거: Tab.2)

### 3) “오토인코더를 크게 키우는 것”보다 “latent 공간을 잘 ‘다듬는 것’”이 downstream 성능을 좌우한다. (근거: §7.3)

* 저자들은 AE의 layers 증가(2→4), hidden dim 증가(→1024 dims), 학습 데이터 증가(→100B tokens) 모두가 **최종 BrierLM (score)** 을 유의미하게 올리지 못했다고 명시한다. (근거: §7.3)
* 따라서 “AE는 계산적으로 negligible component”가 될 수 있고, 시스템 관점에서는 AE 스케일링보다 **정규화 설계(KL clipping + dropout)** 가 지배적 레버라고 해석할 수 있다. (근거: §7.3)

---

## 한 줄 결론(비밀 병기의 정체)

* CALM에서 성능을 “당기는” 핵심은 **latent를 생성하기 쉬운(manifold-smooth) 공간으로 만들면서도 reconstruction을 무너지게 하지 않는** KL clipping과, **예측 오차·의미 정보** 를 동시에 견디게 하는 이중 dropout의 조합이다. (근거: §2, §7.3, Tab.2)


## 프롬프트 1.4.1 (핵심 결과 분석)

```
'Experiments' 또는 'Results'의 표/그림을 포함한 주요 결과를 분석해 줘. 핵심 성능 지표는 무엇인가? 어떤 벤치마크에서 보고되었는가? 저자들이 성공 증거로 가장 강조하는 결과를 요약해 줘.
```

 이 논문의 **실험 파트가 보여주는 핵심 성공 증거는 “동일한 언어모델링 품질(**BrierLM** ) 대비 학습/추론 FLOPs를 유의미하게 줄이면서도 스케일링이 유지된다”는 점**이다. (근거: §7.2).

---

## 1.4.1 핵심 결과 분석

### Key Numbers (요약)

* Params: 281M–849M (Transformer S/M/L), 371M–1.82B (CALM M/L/XL, K=4) (근거: Tab.1).
* Context: 2048 steps; CALM은 2048×K tokens로 환산 (예: K=4 → 8192 tokens) (근거: §7.1).
* Benchmark/Data: Pile-uncopyrighted 약 230B tokens로 학습, WikiText-103로 평가, Llama3 tokenizer 사용 (근거: §7.1).
* Metric: **BrierLM** = 100×(Brier-n들의 geometric mean), 그리고 Brier-n이 downstream 성능 추이를 잘 트래킹한다고 보고 (근거: §7.1/Fig.3).
* Train recipe(핵심): autoencoder 75M params, 30k steps, bs=512k tokens; 이후 CALM 250k steps, bs=2M tokens (근거: §7.1).

---

## 1) 핵심 성능 지표와 벤치마크

이 논문에서 “품질(quality)”의 1차 지표는 **BrierLM** 이고, “비용(cost)”은 **Train FLOPs(총량)** 및 **Infer FLOPs/token** 으로 제시된다. (근거: §7.1/Tab.1).
평가는 WikiText-103 기반 언어모델링이며, 학습 코퍼스는 Pile-uncopyrighted 약 230B tokens이다. (근거: §7.1).

---

## 2) 메인 결과: 성능–컴퓨트 프론티어(“SOTA 포지션”의 실험적 정의)

### 2.1 Table 1: Transformer vs CALM(K=4) 정면 비교

논문은 **Table 1** 을 “primary results”로 명시하며, autoencoder 오버헤드(75M params 및 인코딩/디코딩 FLOPs 포함)까지 포함된 #Params/FLOPs를 보고한다. (근거: §7.1/Tab.1).

| Model         | Params | Train FLOPs (×10^20 FLOPs) | Infer FLOPs/token (×10^8 FLOPs/token) | BrierLM |
| ------------- | -----: | -------------------------: | ------------------------------------: | ------: |
| Transformer-S |   281M |                        6.6 |                                   4.4 |    6.05 |
| Transformer-M |   465M |                       11.9 |                                   7.9 |    7.07 |
| Transformer-L |   849M |                       22.5 |                                  15.0 |    8.98 |
| CALM-M (K=4)  |   371M |                        3.7 |                                   2.9 |    5.72 |
| CALM-L (K=4)  |   735M |                        7.7 |                                   4.6 |    6.58 |
| CALM-XL (K=4) |  1.82B |                       19.5 |                                   9.4 |    8.53 |

(근거: Tab.1).

저자들이 가장 강조하는 “성공 증거”는 **CALM-M(371M)이 Transformer-S(281M)와 ‘comparable’한 BrierLM을 보이면서 Train FLOPs 44%↓, Infer FLOPs 34%↓** 라는 문장이다. (근거: §7.2).

### 2.2 Figure 4: 새로운 스케일링 축 = semantic bandwidth K

저자들은 K를 “성능–컴퓨트 landscape를 탐색하는 새로운 레버”로 두고, **K=1→2에서 “거의 비용이 절반”으로 줄고 성능 저하는 “marginal”, K=4에서 baseline frontier를 “surpass”, K=8은 성능 저하가 커지며 capacity limit 가능성**을 언급한다. (근거: Fig.4/§7.2).

---

## 3) 학습 동역학(저자 관점의 해석 근거)

Figure 5에서 저자들은 Transformer가 초기 급등 후 포화(saturate)되는 반면, **CALM-XL은 초반엔 느리지만 더 가파른(steeper) 학습 곡선으로 Transformer-L과의 격차를 줄인다**고 서술한다. (근거: Fig.5/§7.2).
그 원인으로 **CALM이 “단일 저정보(discrete token) 예측”이 아니라 “고차원 연속 벡터 분포”를 모델링해야 해서 초반이 느리다**는 과업 난이도 차이를 든다. (근거: Fig.5/§7.2).

---

## 4) 구성요소별(ablations) 핵심 결과

### 4.1 Autoencoder 정규화(“posterior collapse” 대응)

Table 2에서, **정규화 없음(BrierLM 3.99) 대비 KL만 쓰면 오히려 악화(3.48)** 되고, **KL clipping을 넣으면 4.13, 여기에 token/latent dropout을 조합하면 최고 4.70** 까지 개선된다. (근거: Tab.2).
저자들은 collapse의 구체 사례로 **128 latent dims 중 71 dims collapse**를 언급한다. (근거: §7.3).

### 4.2 Autoencoder 하이퍼파라미터(β_KL, latent dim, 데이터량)

Figure 6에서 **β_KL=0.001이 best**, β_KL을 키우면 성능이 떨어지며 **β_KL=0.1에서 BrierLM drop + accuracy가 ~99%로 감소**한다고 보고한다. (근거: Fig.6).
Figure 7에서 **latent dim은 128에서 peak**, autoencoder 스케일업은 BrierLM에 큰 도움을 못 주고, **autoencoder 학습 데이터를 15B→100B tokens로 늘려도 BrierLM 개선이 없다**고 보고한다. (근거: Fig.7).

### 4.3 Generative head 선택: Diffusion vs Flow vs Energy

Figure 8에서 **flow matching과 energy-based head가 diffusion을 성능 격차로 앞선다**고 보고하며, **flow는 초기 수렴이 빠르고, energy head는 더 높은 ceiling**을 가진다고 정리한다. (근거: Fig.8).
Figure 9에서 **diffusion은 “valid results”에 많은 iteration이 필요**한 반면, **flow(midpoint)는 2 steps에 decent quality, 4 steps에 near-optimal** 을 달성한다고 보고한다. (근거: Fig.9).
동시에 **energy-based head는 iterative decoding을 제거하면서도 최고 성능**을 준다고 주장한다. (근거: Fig.9/§7.5).

### 4.4 Energy loss 하이퍼(N, M, α): 비용-성능 트레이드오프를 “수치”로 고정

기본 설정은 **N=8, M=100** 이며, N·M 증가는 에너지 스코어 추정이 좋아지지만 비용이 늘어난다고 설명한다. (근거: §7.5/Tab.3).

* Table 3: N 증가 시 BrierLM이 **4.37→4.70** 으로 증가하고 cost ratio는 **0.82×→1.00×** 로 증가한다. (근거: Tab.3).
* Table 4: α는 **1.0에서 BrierLM 4.70이 최고**, α=0.75 또는 2.0은 “Fail” 로 표기된다. (근거: Tab.4).

### 4.5 입력 표현: Discrete vs Continuous

Table 5에서 generative model 입력을 **Discrete token encoding** 으로 둘 때 BrierLM **4.70**, **Continuous** 만 사용하면 **3.25** 로 크게 악화되며, **Both** 는 **4.40** 으로 중간값이다. (근거: Tab.5).

---

## 5) (부가) “컨트롤 가능한 생성” 실험 결과의 포지션

Figure 10에서 **batch size N 증가 및 temperature T 감소가 accuracy↑ / collision rate(다양성↓) 트레이드오프** 를 만든다고 보고한다. (근거: Fig.10).
Figure 11에서 **CALM은 T를 직접 조절하지 못하므로 N 튜닝으로 기존 Transformer의 temperature trajectory를 재현**할 수 있다고 주장하며, 예시로 **T=0.6 ≈ N≈100, T=0.5 ≈ N=200** 을 제시한다. (근거: Fig.11).

---

## 정리: 저자들이 “성공”으로 가장 강하게 주장하는 결과

1. **Table 1 + Figure 4** 로 “성능–컴퓨트 프론티어가 이동”했음을 주장한다. (근거: §7.2/Tab.1/Fig.4).
2. **Figure 5** 로 “학습이 느리지만 장기적으로 더 큰 개선”이라는 동역학적 근거를 제시한다. (근거: Fig.5).
3. **Table 2–5, Figure 6–9** 로 프레임워크의 실용적 설계 포인트(regularization, N/M/α, head choice, input encoding)를 수치로 고정한다. (근거: Tab.2–5/Fig.6–9).


## 프롬프트 1.4.2 (비판적 비교)

```
제안된 방법론은 논문에서 언급된 주요 베이스라인 및 SOTA 모델들과 비교하여 어떤 성능을 보이는가? 우월성 주장을 가장 강력하게 뒷받침하는 특정 비교 지점을 식별해 줘. 반대로, 능가하지 못했거나 개선이 미미했던 결과가 있다면 이유를 정리해 줘.
```

 저자들은 CALM이 **동일(또는 유사) 품질 근처에서 학습/추론 FLOPs를 유의미하게 낮춘다** 는 점을 “주요 베이스라인(Transformer) 대비 성능-비용 프런티어 개선”으로 제시한다. (근거: §7.2/Fig.4)

---

## 1) 비교 기준과 “승부처”가 되는 지점

* 이 논문은 (i) **표준 next-token Transformer** (discrete token, cross-entropy)와 (ii) **연속공간에서 다음 토큰(또는 K-step)을 예측** 하기 위한 **diffusion/flow/energy head** 류를 주요 비교축으로 둔다. (근거: §7.2/§7.5)
* 보고 지표는 **BrierLM** (Higher is better)로 명시되어 있어, 단순히 “수치가 크다/작다”의 방향성을 혼동하면 비교 해석이 틀어진다. (근거: Tab.5)

---

## 2) CALM vs Transformer (핵심 베이스라인) — 정량 비교

아래 표는 논문 Tab.1 수치를 재정리한 것이다. (근거: Tab.1)

| Model         | Params (M Params) | Train FLOPs (×1e20 FLOPs) | Infer FLOPs/token (×1e8 FLOPs/token) | BrierLM (↑) |
| ------------- | ----------------: | ------------------------: | -----------------------------------: | ----------: |
| Transformer-S |               281 |                       6.6 |                                  4.4 |        6.05 |
| CALM-M (K=4)  |               371 |                       3.7 |                                  2.9 |        5.72 |
| Transformer-L |               849 |                      22.5 |                                 15.0 |        8.98 |
| CALM-XL (K=4) |              1820 |                      19.5 |                                  9.4 |        8.53 |

### 가장 강력한 “우월성 주장” 지점(저자 관점)

* 저자들은 **CALM-M (K=4)** 가 **Transformer-S** 와 “비슷한 성능”을 내면서도 **학습 FLOPs 44% 감소**, **추론 FLOPs/token 34% 감소** 를 달성했다고 강조한다. (근거: §7.2/Tab.1)
* 더 큰 스케일에서도 CALM이 “Transformer와 유사한 스케일링 효율”을 보이며, 특히 **K=4** 설정에서 성능-컴퓨트 프런티어를 밀어낸다고 서술한다. (근거: §7.2/Fig.4)

### 반대로, “능가하지 못했거나 개선이 미미한” 지점(정직한 해석)

* Tab.1의 대표 쌍 비교에서 **BrierLM 절대값은 Transformer가 더 높다** (예: 6.05 vs 5.72, 8.98 vs 8.53). (근거: Tab.1)
* 따라서 이 논문의 강점은 “절대 성능 SOTA 갱신”이라기보다, **비용(Train/Infer FLOPs) 절감과 성능의 동시 최적화(프런티어 이동) 주장** 에 더 가깝다. (근거: §7.2/Fig.4)

---

## 3) K-step 예측(K=1/2/4/8)에서의 비교 — 어디서 이기고 어디서 진다

* 저자들은 **K=4** 가 Transformer baseline의 성능-컴퓨트 프런티어를 “넘어선다”고 강조한다. (근거: Fig.4/§7.2)
* 동시에 **K=2** 는 “대략 비용을 절반으로” 줄이면서 성능 저하는 “제한적”이라고 요약한다. (근거: Fig.4/§7.2)
* 반면 **K=8** 은 성능이 떨어지며, 저자들은 이를 “representation capacity가 충분치 않기 때문일 수 있다”고 설명한다. (근거: Fig.4/§7.2)
* **K=1** 은 본 논문의 핵심 설정(K>1) 대비 명확히 불리하며(continuous prediction을 더 잘해야 함), 최적화 여지가 남아 있다고 인정한다. (근거: §7.2)

---

## 4) “SOTA 계열” 생성 헤드 비교: diffusion / flow / energy

* 저자들은 CALM의 연속 예측을 위해 여러 생성 방식을 비교하고, **diffusion** 은 “유효한 토큰 샘플”을 얻으려면 반복 횟수가 많이 필요하다고 명시한다. (근거: §7.5)
* **flow matching** 은 중간 지점(midpoint)에서 “2-step이면 꽤 괜찮고, 4-step이면 거의 최적”이라는 관찰을 제시한다. (근거: §7.5)
* 최종적으로 저자들은 **energy-based head** 가 “diffusion의 높은 성능”과 “flow의 단순성”을 결합하고, 특히 **반복적 디코딩 없이 단일 forward로 샘플링** 가능하다는 점을 장점으로 둔다. (근거: §7.5)

---

## 5) 성능이 “안 나오는” 조건들: 입력 표현/스코어링 룰의 민감도

### (A) 입력을 continuous로 바꾸면 성능이 크게 하락

아래는 Tab.5의 입력 표현 ablation이다. (근거: Tab.5)

| Input type | BrierLM (↑) |
| ---------- | ----------: |
| Discrete   |        4.70 |
| Continuous |        3.25 |
| Combined   |        4.40 |

* **Continuous input** 이 **Discrete input** 대비 BrierLM이 크게 낮고(4.70 → 3.25), **Combined** 도 이득 없이 소폭 악화된다(4.70 → 4.40). (근거: Tab.5)

### (B) energy score의 exponent α 선택을 잘못하면 “학습 자체가 무너짐”

* 저자들은 α<1에서 학습이 “failed to train”이며, α=2에서 BrierLM이 0으로 붕괴하는 결과를 보고한다. (근거: Tab.5)

---

## 정리: 이 논문 비교의 핵심 결론(비판적)

* 가장 설득력 있는 비교 포인트는 **CALM이 비용(Train/Infer FLOPs)을 크게 줄이면서도 품질을 근접하게 유지** 한다는 “프런티어 이동” 주장이다. (근거: §7.2/Fig.4/Tab.1)
* 반대로, Tab.1의 정면 비교만 보면 **BrierLM 절대 성능은 Transformer가 더 높게** 나오는 구간이 존재한다. (근거: Tab.1)
* 성능 병목은 (i) **K=1 약세**, (ii) **K를 과도하게 키울 때(K=8) capacity 부족**, (iii) **continuous input/부적절한 α에서의 붕괴** 로 요약된다. (근거: §7.2/Fig.4/Tab.5)


## 프롬프트 1.5.1 (언급된 한계와 잠재적 한계)

```
저자들이 명시적으로 인정한 한계/약점/실패 사례는 무엇인가? 분석을 바탕으로 잠재적 한계(강한 가정, 확장성, 연산 비용, 일반화 한계, 사회적 영향 등)는 무엇이라고 보나?
```


## 1.5.1 언급된 한계와 잠재적 한계

### 결론 요약 (Risk Hotspots)

* **확률(로그확률) 비가용성** 으로 RL/증류/KL 기반 툴체인이 직접 적용 불가. (근거: §Future Work)
* **Temperature sampling** 은 이론적으로 정확하지만 rejection sampling 기반이라 **추론 오버헤드** 리스크가 구조적으로 존재. (근거: §5, §Future Work)
* **스케일/용량 한계 신호** : K=1 성능 격차, K=8에서 “capacity limitation” 언급 → 큰 K 를 그대로 밀기 어려움. (근거: §Future Work, Fig.4)
* **학습 안정성/하이퍼 민감도** : α<1.0 (unitless) 학습 실패, α=2.0 (unitless)에서 BrierLM 0.00 (unitless)로 붕괴. (근거: Tab.4)
* **입력 표현 병목** : latent vector 를 입력으로 쓰면 BrierLM 0.30→0.39 (unitless)로 악화, “semantic unpacking” 이 어렵다고 서술. (근거: Tab.5, §3)

---

### 저자들이 명시적으로 인정한 한계/약점/실패 사례

| 구분                                          | 관찰/진술                                                                                                                   |                                                          정량 신호 (단위) | 왜 문제인가 (요지)                                                                                                            |
| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------: | ----------------------------------------------------------------------------------------------------------------------------- |
| **툴체인 호환성**                             | log-probability, PMF, KL divergence 가 직접 계산 불가 → RL 정책최적화/증류가 “재정의 필요” 로 제시. (근거: §Future Work)    |                                                             (정량 미제공) | 현업 LLM 파이프라인에서 RLHF/증류/정책그래디언트는 핵심인데, CALM 은 “sample-only” 체계로 재구성해야 함. (근거: §Future Work) |
| **Temperature sampling 오버헤드**             | “provably exact” 알고리즘이 rejection sampling 에 의존 → inference overhead 가능성을 한계로 직접 언급. (근거: §Future Work) |                                                             (정량 미제공) | 제어 생성(다양성-정확도)에서 온도는 기본 기능인데, 정확 구현이 곧바로 latency 리스크로 연결될 수 있음. (근거: §Future Work)   |
| **K=1 성능 격차**                             | K=1 CALM 과 표준 Transformer 사이 “performance gap” 이 남아있다고 서술. (근거: §Future Work)                                |                                                             (정량 미제공) | “연속 도메인+likelihood-free” 전환 자체가 baseline 을 완전히 대체하지 못했음을 의미. (근거: §Future Work)                     |
| **큰 K 의 용량 한계**                         | Fig.4 에서 K=8 (tokens/step) 결과가 “capacity limitation” 으로 해석됨. (근거: Fig.4)                                        |                                                             (정량 미제공) | semantic bandwidth 를 키우면 step 수는 줄지만, 한 step 이 담아야 하는 의미량이 커져 모델 용량/표현이 병목. (근거: Fig.4)      |
| **에너지 스코어 지수 α 민감도**               | α=0.5 (unitless)는 “failed to train”, α=2.0 (unitless)는 BrierLM 0.00 (unitless). (근거: Tab.4)                             |                  BrierLM 0.00 (unitless) @ α=2.0 (unitless) (근거: Tab.4) | scoring rule 기반 학습이 특정 범위에서 불안정/붕괴 가능. (근거: Tab.4)                                                        |
| **Energy loss 샘플링 비용-성능 트레이드오프** | N, M 설정에 따라 BrierLM 0.30↔0.60 (unitless) 범위로 변화. (근거: Tab.3)                                                    | 예: BrierLM 0.30 (unitless) (근거: Tab.3) / 0.60 (unitless) (근거: Tab.3) | 학습 품질을 위해 N/M 을 키우면 (샘플 수 증가) 학습/추론 비용이 선형으로 늘어날 여지. (근거: Tab.3, §3)                        |
| **연속 입력( latent ) 사용 시 성능 저하**     | input 을 latent vector 로 두면 성능 악화(BrierLM 0.30→0.39 (unitless)) 및 “semantic unpack” 어려움 명시. (근거: Tab.5, §3)  |                                     BrierLM 0.39 (unitless) (근거: Tab.5) | “완전 연속화” 를 하면 Transformer 가 compact latent 에서 의미를 복원하기 어렵다는 병목. (근거: Tab.5, §3)                     |

---

### 잠재적 한계 (논문 근거 기반의 해석)

#### (A) Temperature sampling 의 “정확성 vs 비용” 구조적 긴장

정확 온도 샘플링의 기대 호출 수가 다음 형태로 제시됨. (근거: §5.2)

$$
\mathbb{E}[N_{\text{total}}] ;=; n ;+; \mathbf{1}[\alpha>0]\sum_{x\in\mathcal{X}}\frac{P(x)^{\frac{1}{T}-1}}{Z_T}
$$

* 여기서 $n=\lfloor 1/T \rfloor$ (unitless), $\alpha=1/T-n$ (unitless), $Z_T=\sum_x P(x)^{1/T}$ (unitless), $\mathcal{X}$ 는 “chunk outcome space” 로 $|\mathcal{X}|=|V|^K$ (unitless) 임이 명시됨. (근거: §5.2)
* Corollary 에서 **P 가 uniform (unitless)** 이면 기대 호출 수가 $|\mathcal{X}|^{1/T-1}$ (unitless) 로 단순화되는 형태가 제시됨. (근거: §5.2)
* 해석상, K (tokens/step) 가 커질수록 $|\mathcal{X}|=|V|^K$ (unitless) 가 지수적으로 커지므로, “온도 제어를 강하게 걸고 싶을수록” (즉 1/T (unitless) 증가) 호출 수가 폭증할 수 있는 리스크가 존재. (근거: §5.2 (해석))
* 저자들도 Appendix 에서 **low temperature** 에서 rejection sampling 이 비효율적이라 언급하고, 실험에서는 batch approximation 을 쓴다고 밝힘. (근거: Appx B.5)

#### (B) “샘플만 가능한 모델” 이 만드는 평가·학습 루프의 제약

* Perplexity 같은 likelihood 기반 지표는 계산 불가하다고 명시하며, 이를 대체하기 위해 BrierLM 을 도입. (근거: §4.1)
* BrierLM 과 cross-entropy 간 상관이 Pearson -0.966 (unitless), Spearman -0.991 (unitless) 로 보고되어 “대체 지표” 로서 설득력을 보완. (근거: Fig.3)
* 다만, 본질적으로 sample-only 평가이므로 (특히 long-horizon generation 품질) 기존의 logprob 기반 디버깅/캘리브레이션 워크플로와는 다른 실패 양상을 가질 수 있음. (근거: §4.2 (해석))

#### (C) 의미론적 latent space 품질이 전체 시스템의 상한을 결정할 가능성

* Future Work 에서 현 autoencoder 가 reconstruction 중심이고 semantic structure 가 약하다고 “key limitation” 으로 직접 지적. (근거: §Future Work)
* 또한 context-aware / autoregressive autoencoder 가 “more reliable reconstruction” 을 줄 수 있다고 제안. (근거: §Future Work)
* 해석상, decoder 가 “토큰 복원” 은 잘해도(예: token-level accuracy 99.9%+ (unitless)) latent 공간이 의미론적으로 정렬되지 않으면, generative head 가 작은 에너지 차이로도 큰 의미 변화(모달 붕괴/샘플 품질 불안정)를 낼 위험이 남음. (근거: §2.1, §Future Work (해석))

---

### 정리: 이 논문이 남긴 “기술 부채” 체크리스트

* **(1) 온도 제어** 를 latency budget 안에서 어떻게 구현할지. (근거: §5, Appx B.5, §Future Work)
* **(2) RL/증류/정책최적화** 를 sample-only 체계에서 어떻게 재정의할지. (근거: §Future Work)
* **(3) 큰 K (tokens/step)** 로 갈 때 생기는 capacity limitation 을 어떤 scaling law 로 다룰지. (근거: Fig.4, §Future Work)
* **(4) 학습 안정성** (α (unitless), N/M (samples/step)) 의 민감도를 이론/실험적으로 어떻게 완화할지. (근거: Tab.3, Tab.4)



## 프롬프트 1.5.2 (미래 연구 궤적)

```
저자들이 제안하는 향후 연구 방향은 무엇인가? 한계에 비추어 합리적인 다음 단계나 대안적 방향을 제안해 줘.
```

 저자들이 제시하는 미래 연구의 축은 (1) **Autoencoder** 의 의미론 강화, (2) **Generative model** 의 end-to-end 통합, (3) **Sampling** 의 저비용 다양성-정확도 제어, (4) **Scaling** 법칙에서 (K) (tokens/step) 를 3번째 축으로 정식화, (5) **Algorithmic toolkit** (RL/증류) 재정의로 요약된다. (근거: §8)

---

## 1.5.2 미래 연구 궤적

### 저자들이 제안하는 향후 연구 방향 (논문 명시)

| 축                      | 저자 제안(요지)                                                                                                                                                                                   | 직접적으로 겨냥한 한계                                                                        | 기대되는 효과(해석)                                                                                                 |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| **Autoencoder**         | 재구성 중심에서 벗어나 “의미론적으로 정렬된(latent proximity ≈ semantic similarity)” 잠재공간을 학습하고, context-aware / autoregressive AE로 견고성·복원 신뢰도를 높이기. (근거: §8)             | 현재 AE가 reconstruction에 치우쳐 semantic structure가 약하다고 명시. (근거: §8)              | downstream generative head가 “조금 틀린 (z)”에도 의미가 급변하지 않는, 더 매끈한 의미 공간을 제공. (근거: §8(해석)) |
| **Model**               | Transformer backbone + lightweight head의 분리형 설계 대신, 더 통합된 end-to-end generative Transformer를 탐색. (근거: §8)                                                                        | “경량 head”는 효율적이지만, 생성 모델링 capability의 상한이 있을 수 있음. (근거: §8)          | 표현력/샘플 품질 개선 가능성(효율 ↔ 품질 재트레이드오프). (근거: §8(해석))                                          |
| **Objective**           | energy loss가 기반이지만, 다른 strictly proper scoring rule 또는 다른 continuous generative model을 조사. (근거: §8)                                                                              | scoring rule/학습 다이내믹/샘플 품질이 설계에 민감할 수 있음. (근거: §8)                      | 안정성·샘플 품질·수렴 특성 개선 여지. (근거: §8(해석))                                                              |
| **Sampling**            | “정확한(likelihood-free) temperature sampling” 은 rejection sampling 의존으로 inference overhead 가능 → 더 가벼운 휴리스틱(예: input noise scale 조절, modified loss로 미세조정) 탐색. (근거: §8) | controllable generation이 필요한데, provably exact 방법이 비싸질 수 있음. (근거: §8)          | latency/샘플 수를 줄이면서 diversity–fidelity trade-off를 실용적으로 제어. (근거: §8(해석))                         |
| **Scaling**             | 더 큰 모델이 더 높은 (K) (tokens/step) semantic bandwidth를 지탱할 수 있다는 가설을 검증하고, model size/data size에 더해 (K) 를 포함한 새로운 scaling law family를 수립. (근거: §8)              | (K) 가 커질수록 “한 스텝에 담는 의미량”이 커져 capacity bottleneck이 생길 수 있음. (근거: §8) | compute budget별 최적 (K) (tokens/step) 를 원리적으로 선택 가능. (근거: §8)                                         |
| **Algorithmic Toolkit** | RL의 policy optimization(보상 샘플의 log-prob 증가)과 distillation(KL 최소화)은 CALM에서 직접 계산 불가 → sample-based regime에서 재정식화가 핵심 오픈 문제. (근거: §8)                           | CALM은 full PMF/log-likelihood 접근이 불가하다는 구조적 제약. (근거: §8)                      | 실전 파이프라인(RLHF/증류/정책개선)과의 접합을 가능하게 하는 “연속-샘플 기반 학습 도구상자” 구축. (근거: §8(해석))  |

또한 본문에서 “context-aware autoencoder(이전 벡터를 조건으로 포함)”이 자연스러운 다음 단계라고 직접 언급한다. (근거: §2)

---

### 한계에 비추어 합리적인 다음 단계 / 대안적 방향 (제안)

아래는 위 “저자 로드맵”을 실제 연구 플랜으로 연결하기 위한, 기술적으로 자연스러운 확장 아이디어들이다. (제안)

#### A) **의미론적 AE** 를 “정렬 가능한 공간”으로 만들기

* 재구성 손실만으로는 latent가 “압축-복원”에는 최적이지만 “거리 구조”를 보장하지 못하므로, latent 거리와 의미 유사도를 직접 결속시키는 보조 목적(contrastive / metric learning / triplet류)을 추가하는 방향이 합리적이다. (제안)
* context-aware/AR AE는 “복원 신뢰도”뿐 아니라, latent가 실제 언어의 국소적 변이를 반영하도록 만들어 downstream generative head의 학습 난이도를 낮출 가능성이 있다. (제안)

#### B) **end-to-end 통합 모델** 로 갈 때의 설계 포인트

* “AE로 압축 → 별도 head로 생성 → AE로 복원”의 2-stage 구조는 각 모듈의 오류가 누적될 수 있으므로, 통합 모델에서는 (i) latent 생성과 (ii) 복원 안정성을 동시에 보장하는 공동 학습(joint training) 레짐을 설계하는 것이 핵심이다. (제안)
* 특히 (K) (tokens/step) 가 커질수록 capacity bottleneck이 나타날 수 있으므로, “(K) 증가에 맞춘 latent dim (dims) / model width(depth) / regularization”의 동반 스케일링 규칙을 경험적으로 찾는 것이 우선순위가 높다. (제안)

#### C) **sampling 비용** 을 줄이기 위한 실용적 우회로

* 논문이 제안한 대로 input noise scale을 조절하거나 loss를 바꿔 미세조정하는 접근은 “rejection 기반 exactness”를 포기하는 대신, latency를 예산 내로 맞추는 엔지니어링 친화적 해법이 될 수 있다. (근거: §8)
* 추가로는, 동일 compute에서 “다중 샘플 후 선택(best-of-N)”을 온도 제어의 대용으로 쓰는 방식(이미 논문이 diversity–fidelity trade-off를 샘플 수와 연결해 논의)과 결합해, 서비스 제약 하에서의 정책을 체계화할 수 있다. (제안)

#### D) **sample-based RL/증류** 를 위한 대안 목적 함수

* log-prob/KL이 막히면, teacher/student의 “샘플 집합”만으로 정의 가능한 분포 거리(예: energy score류, MMD/Wasserstein 계열 추정치)로 distillation을 재정의하는 것이 자연스러운 대안이다. (제안)
* RL도 “보상 높은 샘플을 더 자주 내게” 하는 방향 자체는 유지하되, likelihood ratio 대신 샘플 기반 surrogate(예: 점수함수 추정, rank-based objective)로 정책 업데이트를 정의하는 접근이 유력하다. (제안)
* 이 축이 정리되면 CALM은 연구용 프레임워크를 넘어, 실제 LLM 학습 스택에 들어갈 수 있는 조건을 갖추게 된다. (제안)

---

### 요약: 논문이 열어둔 ‘다음 논문’의 핵심 질문 5개

1. latent 공간에서 “의미 거리”를 어떻게 정의/학습할 것인가. (근거: §8)
2. head 분리형 vs end-to-end 통합형 중 어느 쪽이 (K) (tokens/step) 확장에 유리한가. (근거: §8)
3. rejection 없이 diversity–fidelity를 제어하는 실용 샘플링 규칙은 무엇인가. (근거: §8)
4. 성능 $(f(\text{params}, \text{data}, K))$ 형태의 scaling law를 실제로 맞출 수 있는가. (근거: §8)
5. logprob/KL 없는 세계에서 RL·증류를 어떤 “샘플 기반 정식화”로 재구성할 것인가. (근거: §8)

</details>

