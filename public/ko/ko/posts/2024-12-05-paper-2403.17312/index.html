<!doctype html><html lang=ko-kr data-theme=claude><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching | Jaehun's Blog</title><meta name=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창성 독창적인 알고리즘 및 시스템 설계:
Sparse..."><meta name=keywords content="개발,프로그래밍,에세이"><meta name=author content><meta name=language content="ko"><meta name=robots content="index, follow"><link rel=canonical href=https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.17312/><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.17312/"><meta property="og:site_name" content="Jaehun's Blog"><meta property="og:title" content="ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"><meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창성 독창적인 알고리즘 및 시스템 설계:"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="ko"><meta property="article:published_time" content="2024-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-05T00:00:00+00:00"><meta property="og:image" content="https://jaehun.me/images/og-default.avif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehun.me/images/og-default.avif"><meta name=twitter:title content="ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"><meta name=twitter:description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창성 독창적인 알고리즘 및 시스템 설계:"><meta itemprop=name content="ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"><meta itemprop=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창성 독창적인 알고리즘 및 시스템 설계:"><meta itemprop=datePublished content="2024-12-05T00:00:00+00:00"><meta itemprop=dateModified content="2024-12-05T00:00:00+00:00"><meta itemprop=wordCount content="7088"><meta itemprop=image content="https://jaehun.me/images/og-default.avif"><meta itemprop=keywords content="Paper-Review,With-Gpt"><script>window.HUGO_SEARCH_CONFIG={searchIndexURL:'"/ko/index.json"'}</script><script>window.HUGO_GALLERY_CONFIG={justified_gallery:!1,lightbox:!1,justified:"{}",lightbox_options:"{}"}</script><link rel=stylesheet href=/css/compiled.min.9165cace5b80a5be5daad6ea96eb2abc20a18a3f41d7fed7f0335b689fdf7883.css integrity="sha256-kWXKzluApb5dqtbqlusqvCChij9B1/7X8DNbaJ/feIM=" crossorigin=anonymous><link rel=stylesheet href=/css/chroma.min.491df36221f739b5948a747a0351f100ee7ccaaf466d9f46288bf06de1d59123.css integrity="sha256-SR3zYiH3ObWUinR6A1HxAO58yq9GbZ9GKIvwbeHVkSM=" crossorigin=anonymous><script src=/js/main.ba969e49a7a874e6938fe689a7dcaa00cb64cb4d429c2cf98d09bde512a16c6d.js integrity="sha256-upaeSaeodOaTj+aJp9yqAMtky01CnCz5jQm95RKhbG0=" crossorigin=anonymous></script><script src=/js/gumshoe.polyfills.min.js></script><script src=/js/toc.70654aebf0b738a78ae2df95297cc10ebc6e590f3c942dd1325fa852f50038b2.js integrity="sha256-cGVK6/C3OKeK4t+VKXzBDrxuWQ88lC3RMl+oUvUAOLI=" crossorigin=anonymous defer></script><script src=/js/search.de3eba3540834efb8328121784bff4774e06362412ee41eee665f5b4f987f393.js integrity="sha256-3j66NUCDTvuDKBIXhL/0d04GNiQS7kHu5mX1tPmH85M=" crossorigin=anonymous defer></script><script src=/js/dock.e02f422517e001d9fb6bea1e9e8b0e89e9d2e486ad4641f275bb3e36ce2ae79f.js integrity="sha256-4C9CJRfgAdn7a+oenosOienS5IatRkHydbs+Ns4q558=" crossorigin=anonymous defer></script><script>(function(){const e=localStorage.getItem("theme")||"system",n=localStorage.getItem("colorScheme")||"claude";document.documentElement.setAttribute("data-theme",n);function t(){e==="dark"||e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}t(),e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",t)})()</script></head><body class="bg-background text-foreground min-h-screen antialiased"><div id=reading-progress-container class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out" data-height=3 data-smooth-scroll=true data-hide-on-complete=false><div class="reading-progress-bg w-full"></div><div id=reading-progress-bar class="from-primary to-primary/80 reading-progress-bar
transition-all duration-150 ease-out
absolute top-0 left-0 w-0 bg-gradient-to-r"></div></div><script>(function(){"use strict";const t=document.getElementById("reading-progress-container"),o=document.getElementById("reading-progress-bar");if(!t||!o)return;const a={smoothScroll:t.dataset.smoothScroll==="true",hideOnComplete:t.dataset.hideOnComplete==="true"};let n=!0,l=null;function r(){const t=window.pageYOffset||document.documentElement.scrollTop,n=document.documentElement.scrollHeight,s=window.innerHeight,e=n-s;return e<=0?0:Math.min(Math.max(t/e*100,0),100)}function c(){const e=r();o.style.width=e+"%",a.hideOnComplete&&e>=99.5?n&&(t.style.opacity="0",n=!1):n||(t.style.opacity="1",n=!0)}let s=!1;function e(){s||(requestAnimationFrame(()=>{c(),s=!1}),s=!0)}function i(){window.addEventListener("scroll",e,{passive:!0}),window.addEventListener("resize",e,{passive:!0}),document.addEventListener("visibilitychange",()=>{document.hidden||e()}),e(),window.addEventListener("beforeunload",()=>{window.removeEventListener("scroll",e),window.removeEventListener("resize",e)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script><header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6"><div class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm"><div class="hidden w-full items-center md:flex"><div class="flex items-center"><a href=/ko/ class="flex h-10 w-10 items-center justify-center overflow-hidden rounded-full transition-transform duration-200 hover:scale-105" aria-label="Jaehun's Blog"><img src=/images/logo.svg alt="Jaehun's Blog" class="h-full w-full object-cover"></a></div><nav class="mx-8 flex flex-1 items-center justify-center"><div class="flex items-center space-x-1"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></div></nav><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div><div class="flex w-full items-center justify-between md:hidden"><div class=relative><button id=mobile-menu-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200" data-dropdown-type=mobile-menu aria-label=메뉴 aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="메뉴"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button><div id=mobile-menu class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden" data-dropdown-type=mobile-menu role=menu aria-labelledby=mobile-menu-toggle><nav class="flex flex-col"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></nav></div></div><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div></div></header><main class="mx-auto max-w-4xl px-4 py-6"><nav class="breadcrumb mb-4 md:mb-6 py-1" aria-label=경로><ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1"><li class=flex-shrink-0><a href=/ko/ class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="홈"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11 2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">홈</span></a></li><li class="flex items-center gap-1 md:gap-2 min-w-0"><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span><div class="flex items-center gap-1 md:gap-2 min-w-0"><a href=/ko class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5A2 2 0 003 7z"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">Ko</span>
</a><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span>
<span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/></svg>
<span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</span></span></div></li></ol></nav><header class=mb-8><h1 class="text-foreground mb-4 text-3xl font-bold md:text-4xl">ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</h1></header><article class="prose prose-neutral dark:prose-invert max-w-none"><p><a href=https://arxiv.org/abs/2403.17312>논문 링크</a></p><h1 id=q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘>Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1><h3 id=논문의-강점과-독창성>논문의 강점과 독창성</h3><ol><li><p><strong>독창적인 알고리즘 및 시스템 설계</strong>:</p><ul><li><strong>Sparse Window Attention (SWA)</strong>: 이 알고리즘은 중요 토큰을 선택하여 메모리 사용량을 줄이는 동시에 정확도를 유지하는 독특한 접근 방식을 제안합니다.</li><li><strong>세 단계 동적 스케줄링</strong>: 메모리 캐싱과 재계산을 동적으로 조정하여 리소스 제약 하드웨어에서 최적의 성능을 달성합니다.</li></ul></li><li><p><strong>효율성 개선</strong>:</p><ul><li>ALISA는 FlexGen 대비 최대 3배, vLLM 대비 1.9배의 처리량을 달성했습니다.</li><li>메모리 사용량을 줄이고 처리 속도를 높이기 위해 KV 텐서를 INT8로 압축하는 최적화를 제안했습니다.</li></ul></li><li><p><strong>적용 범위</strong>:</p><ul><li>LLaMA, OPT, Pythia 등 다양한 LLM 모델에 적용 가능하며, 여러 NLP 작업에서 일관되게 높은 성능을 보여줍니다.</li></ul></li></ol><hr><h3 id=핵심-알고리즘-sparse-window-attention-swa>핵심 알고리즘: Sparse Window Attention (SWA)</h3><p><strong>예시 입력</strong>:</p><ul><li>입력 시퀀스: <code>["The", "capital", "of", "France", "is"]</code></li><li>SWA 알고리즘은 과거 토큰 중 중요한 토큰만 선택하여 계산에 포함합니다.</li></ul><p><strong>단계별 과정</strong>:</p><ol><li><p><strong>중요 토큰 식별</strong>:</p><ul><li>각 토큰의 attention weight를 기반으로 중요한 토큰을 선택합니다. 예: &ldquo;capital&rdquo;, &ldquo;France"가 높은 weight를 가지므로 선택.</li></ul></li><li><p><strong>Sparse 패턴 생성</strong>:</p><ul><li>선택된 토큰을 기반으로 locally static 패턴(최근 토큰)과 globally dynamic 패턴(중요 토큰)을 생성합니다.</li><li>예: <code>locally static: "is"</code>, <code>globally dynamic: "France", "capital"</code>.</li></ul></li><li><p><strong>계산 최적화</strong>:</p><ul><li>선택된 KV 텐서만 사용하여 attention 계산 수행.</li><li>메모리 사용량 감소 및 처리 속도 증가.</li></ul></li></ol><hr><h3 id=논문의-한계점>논문의 한계점</h3><ol><li><p><strong>KV Sparsity와 정확도 간 트레이드오프</strong>:</p><ul><li>SWA에서 80% 이상의 sparsity를 적용하면 일부 작업에서 정확도가 감소할 수 있습니다.</li></ul></li><li><p><strong>동적 스케줄링의 복잡성</strong>:</p><ul><li>동적 스케줄링의 최적화는 모델, 하드웨어, 작업에 따라 다르며, 사전 설정이 필요합니다.</li></ul></li><li><p><strong>특정 하드웨어 의존성</strong>:</p><ul><li>제안된 최적화는 GPU-CPU 간 메모리 대역폭에 크게 의존하며, 하드웨어 제약이 있는 환경에서는 효과가 제한될 수 있습니다.</li></ul></li><li><p><strong>추가 계산 비용</strong>:</p><ul><li>Sparse Window Attention의 sparse 패턴 계산과 동적 스케줄링은 추가 계산 오버헤드를 초래할 수 있습니다.</li></ul></li></ol><hr><p>이 논문은 메모리 제약 환경에서 대규모 언어 모델 추론을 최적화하는 데 있어 획기적인 아이디어와 실질적인 성능 개선을 제안하지만, 알고리즘의 효율성은 사용 사례와 하드웨어 환경에 따라 달라질 수 있습니다.</p><h1 id=q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘>Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1><h3 id=핵심-알고리즘-sparse-window-attention-swa-1>핵심 알고리즘: Sparse Window Attention (SWA)</h3><p>Sparse Window Attention(SWA)는 <strong>토큰의 중요도를 기반으로 메모리 사용을 줄이고 효율성을 높이는 알고리즘</strong>입니다. 중요한 토큰만 선택적으로 처리하여 <strong>KV 캐싱의 메모리 사용량을 줄이는 동시에 성능을 유지</strong>합니다. 이 과정은 다음과 같은 단계로 나뉩니다.</p><hr><h3 id=예시-입력><strong>예시 입력</strong></h3><ol><li>입력 문장: <code>"The capital of France is Paris."</code></li><li>현재까지 처리된 토큰: <code>["The", "capital", "of", "France", "is"]</code></li><li>새로 예측해야 할 토큰: <code>"Paris"</code></li></ol><hr><h3 id=1단계-중요-토큰-식별><strong>1단계: 중요 토큰 식별</strong></h3><p>SWA는 각 토큰의 <strong>attention weight</strong>를 분석하여 중요도를 판단합니다.</p><ul><li><p><strong>입력 데이터</strong>:</p><ul><li>Query(Q): 현재 예측 토큰 <code>"Paris"</code>의 query 벡터.</li><li>Key(K): 이전 모든 토큰의 key 벡터.</li><li>Value(V): 이전 모든 토큰의 value 벡터.</li></ul></li><li><p><strong>Attention 계산</strong>:
[
AW(Q, K) = \text{Softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}}\right)
]
여기서 (AW(Q, K))는 각 토큰에 대한 attention weight입니다. 예를 들어:</p><ul><li><code>"capital"</code>: 0.4</li><li><code>"France"</code>: 0.35</li><li>나머지 토큰: 0.1 이하 (중요도가 낮음).</li></ul></li><li><p><strong>결론</strong>:</p><ul><li>중요도가 높은 <code>"capital"</code>과 <code>"France"</code>는 선택.</li><li>중요도가 낮은 <code>"The"</code>, <code>"of"</code>, <code>"is"</code>는 계산에서 제외(압축).</li></ul></li></ul><hr><h3 id=2단계-sparse-패턴-생성><strong>2단계: Sparse 패턴 생성</strong></h3><p>SWA는 <strong>local static</strong>과 <strong>global dynamic</strong> 패턴을 결합하여 메모리 사용을 줄입니다.</p><ol><li><p><strong>Locally Static 토큰</strong>:</p><ul><li>최근 몇 개의 토큰을 항상 유지.</li><li>예를 들어, <code>"is"</code>는 최신 토큰이므로 선택.</li></ul></li><li><p><strong>Globally Dynamic 토큰</strong>:</p><ul><li>전체 토큰 중 중요도가 높은 토큰을 선택.</li><li>예: <code>"capital"</code>과 <code>"France"</code>는 높은 attention weight로 인해 선택.</li></ul></li><li><p><strong>최종 Sparse 패턴</strong>:</p><ul><li>선택된 토큰: <code>["capital", "France", "is"]</code></li><li>삭제된 토큰: <code>["The", "of"]</code></li></ul></li></ol><hr><h3 id=3단계-sparse-계산><strong>3단계: Sparse 계산</strong></h3><p>선택된 토큰으로 attention 계산을 수행하여 <strong>메모리와 연산 비용을 줄임</strong>.</p><ol><li><p><strong>Sparse KV 텐서 생성</strong>:</p><ul><li>선택된 토큰만 포함한 작은 (K, V) 텐서를 생성.</li><li>메모리 사용량 감소.</li></ul></li><li><p><strong>Attention 결과 계산</strong>:</p><ul><li>선택된 Sparse (K, V)에 대해:
[
\text{Output} = \text{Softmax}(Q \cdot K_{\text{sparse}}^T) \cdot V_{\text{sparse}}
]</li><li>위 결과로 새 토큰 <code>"Paris"</code>를 생성.</li></ul></li></ol><hr><h3 id=4단계-캐싱-및-업데이트><strong>4단계: 캐싱 및 업데이트</strong></h3><ol><li>선택된 중요 토큰의 (K, V)를 GPU 또는 CPU 메모리에 캐싱.</li><li>새로 생성된 토큰 <code>"Paris"</code>의 (K, V)를 업데이트.</li><li>이전의 중요하지 않은 토큰은 삭제하여 메모리를 효율적으로 사용.</li></ol><hr><h3 id=알고리즘-정리><strong>알고리즘 정리</strong></h3><ol><li><strong>Sparse 토큰 선택</strong>:<ul><li>중요 토큰과 최신 토큰만 선택.</li></ul></li><li><strong>KV 텐서 크기 축소</strong>:<ul><li>Sparse (K, V) 텐서로 연산을 줄임.</li></ul></li><li><strong>동적 메모리 관리</strong>:<ul><li>중요도가 낮은 토큰의 데이터는 CPU로 이동하거나 삭제.</li></ul></li><li><strong>효율적인 연산</strong>:<ul><li>Sparse한 구조로도 원래 Attention 성능을 유지.</li></ul></li></ol><hr><h3 id=swa의-주요-장점><strong>SWA의 주요 장점</strong></h3><ol><li><strong>효율성</strong>:<ul><li>메모리 사용량과 계산량을 크게 줄임.</li><li>예를 들어, 80% sparsity를 적용하면 정확도 손실 없이 연산 성능이 향상.</li></ul></li><li><strong>유연성</strong>:<ul><li>다양한 LLM에 적용 가능.</li></ul></li><li><strong>정확도 유지</strong>:<ul><li>Sparse Attention에서도 Dense Attention과 유사한 정확도 유지.</li></ul></li></ol><hr><p>SWA는 메모리와 연산 효율성을 극대화하면서도 정확도 저하를 최소화한 획기적인 알고리즘입니다. <strong>중요 토큰 선택</strong>과 <strong>Sparse 패턴 생성</strong>의 핵심 아이디어는 LLM 추론 최적화에 큰 기여를 합니다.</p><h1 id=q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘>Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1><h3 id=논문에서-제시한-결과와-다른-방법론-대비-특출난-점>논문에서 제시한 결과와 다른 방법론 대비 특출난 점</h3><h4 id=논문의-주요-결과><strong>논문의 주요 결과</strong></h4><ol><li><p><strong>성능 향상</strong>:</p><ul><li>ALISA는 <strong>FlexGen</strong> 대비 <strong>최대 3배</strong>, <strong>vLLM</strong> 대비 <strong>최대 1.9배의 처리량 증가</strong>를 달성.</li><li>특히, <strong>메모리 제약 시스템</strong>에서 장시간 추론이 가능하며, GPU와 CPU 간 메모리 전송을 효율적으로 관리.</li></ul></li><li><p><strong>정확도 유지</strong>:</p><ul><li>SWA를 통해 <strong>최대 80% KV Sparsity</strong>에서도 Dense Attention과 유사한 정확도를 유지.</li><li>기존 Sparse Attention 방법(예: Local Attention, Strided Attention) 대비 <strong>3~4배 높은 스파스성</strong>에서도 성능 유지.</li></ul></li><li><p><strong>메모리 사용 감소</strong>:</p><ul><li>Sparse Window Attention과 KV Tensor 압축(INT8)을 결합하여 메모리 사용량을 획기적으로 감소.</li><li>GPU 메모리 용량이 적은 환경에서도 더 큰 모델이나 긴 시퀀스를 처리 가능.</li></ul></li></ol><hr><h3 id=alisa가-특출난-이유><strong>ALISA가 특출난 이유</strong></h3><h4 id=1-sparse-window-attention-swa><strong>1. Sparse Window Attention (SWA)</strong></h4><ul><li>기존 Sparse Attention 기법(Local, Strided)은 중요 토큰을 선택하지 않고, 고정된 슬라이딩 윈도우나 일정한 간격의 토큰만 처리.</li><li><strong>SWA는 토큰별 중요도를 동적으로 계산</strong>하여, 중요 토큰만 포함하는 Sparse 패턴을 생성:<ul><li><strong>Locally Static 패턴</strong>: 가장 최근의 토큰을 항상 포함하여 순차적인 정보를 유지.</li><li><strong>Globally Dynamic 패턴</strong>: Attention Weight가 높은 중요한 토큰을 동적으로 선택.</li></ul></li><li>이러한 설계는 메모리 사용량을 줄이면서도 Dense Attention에 가까운 정확도를 유지.</li></ul><h4 id=2-동적-스케줄링><strong>2. 동적 스케줄링</strong></h4><ul><li>기존 방법(예: FlexGen)은 KV Tensor의 GPU와 CPU 간 전송을 <strong>정적</strong>으로 관리:<ul><li>특정 규칙에 따라 미리 정의된 토큰 수를 전송, 모든 단계에서 동일한 정책 적용.</li></ul></li><li>ALISA는 <strong>세 가지 단계의 동적 스케줄링</strong>을 통해 메모리와 계산을 효율적으로 관리:<ul><li><strong>Phase I</strong>: GPU 메모리 내에서 모든 KV Tensor를 캐싱.</li><li><strong>Phase II</strong>: GPU 메모리가 부족하면 중요 토큰만 CPU로 오프로드.</li><li><strong>Phase III</strong>: CPU로 오프로드된 토큰 중 일부를 삭제하고, 필요할 경우 GPU에서 재계산.</li></ul></li><li>이 접근 방식은 메모리 전송 오버헤드를 줄이고, GPU 메모리를 더 효율적으로 활용.</li></ul><h4 id=3-kv-tensor-압축-int8-quantization><strong>3. KV Tensor 압축 (INT8 Quantization)</strong></h4><ul><li>기존 LLM 최적화 기법은 주로 <strong>모델 가중치</strong>를 압축.</li><li>ALISA는 <strong>KV Tensor를 INT8로 압축</strong>하여 메모리 사용량과 전송 비용을 줄임:<ul><li>INT8 형식에서도 Sparse Attention의 정확도를 유지.</li><li>GPU와 CPU 간 데이터 전송 시 필요한 메모리 대역폭 감소.</li></ul></li></ul><h4 id=4-전반적인-통합-설계><strong>4. 전반적인 통합 설계</strong></h4><ul><li>ALISA는 <strong>알고리즘-시스템 간 협업</strong> 설계를 도입:<ul><li>Sparse Attention과 동적 스케줄링을 결합하여, 메모리 효율성과 계산 성능을 극대화.</li><li>이전 연구들이 알고리즘 또는 시스템 중 하나에만 초점을 맞췄던 것과 차별화됨.</li></ul></li></ul><hr><h3 id=alisa의-결과를-뒷받침하는-논문의-이유><strong>ALISA의 결과를 뒷받침하는 논문의 이유</strong></h3><ol><li><p><strong>Attention Weight의 고스파스성</strong>:</p><ul><li>논문은 LLM 추론 중 Attention Weight가 <strong>80~95%가량 sparse</strong>하다는 것을 실험적으로 보여줌.</li><li>이를 활용하여 메모리와 계산량을 줄이면서 성능을 유지할 수 있음을 입증.</li></ul></li><li><p><strong>KV Tensor의 불균등한 중요성</strong>:</p><ul><li>실험 결과, 특정 토큰(Key/Value Tensor)의 중요도가 높으며, 나머지는 무시 가능.</li><li>Sparse Window Attention이 이러한 중요성을 효과적으로 활용.</li></ul></li><li><p><strong>CPU와 GPU 간 전송 오버헤드 문제</strong>:</p><ul><li>기존 방법론(FlexGen, vLLM)은 CPU와 GPU 간 전송 오버헤드로 인해 처리량이 제한.</li><li>ALISA는 동적 스케줄링과 재계산을 결합하여 이 문제를 해결.</li></ul></li><li><p><strong>정확도 유지</strong>:</p><ul><li>기존 Sparse Attention(Local, Strided)은 Dense Attention과 유사한 결과를 내지 못했으나, SWA는 Dense Attention의 power-law 분포를 유지.</li></ul></li></ol><hr><h3 id=나의-분석-및-의견><strong>나의 분석 및 의견</strong></h3><h4 id=긍정적인-측면><strong>긍정적인 측면</strong></h4><ul><li>ALISA는 Sparse Attention과 동적 메모리 관리를 결합하여, 리소스 제약 환경에서 LLM 추론의 한계를 극복했습니다.</li><li>Sparse Window Attention은 단순히 메모리를 줄이는 것을 넘어, Dense Attention의 정확도 특성을 유지하도록 설계되었습니다.</li><li>동적 스케줄링은 이론적으로도 최적화 문제로 모델링되었으며, 실제로 효율성을 입증.</li></ul><h4 id=개선-가능성><strong>개선 가능성</strong></h4><ol><li><strong>Sparse Attention 오버헤드</strong>:<ul><li>Sparse Attention 생성 및 관리에는 추가 계산 비용이 들어감. 이를 줄이는 방법이 연구될 필요가 있음.</li></ul></li><li><strong>유니버설 적용성</strong>:<ul><li>특정 하드웨어 환경(GPU-CPU 대역폭)에 의존하기 때문에, 더 다양한 시스템에서의 적용 가능성을 분석해야 함.</li></ul></li><li><strong>추론 속도와 정확도 간 균형</strong>:<ul><li>80% Sparsity를 넘어가면 정확도 손실이 발생. 이를 보완할 수 있는 정교한 중요 토큰 선택 방법이 필요.</li></ul></li></ol><hr><p>결론적으로, ALISA는 <strong>Sparse Attention을 효율적으로 설계하고, 이를 동적 메모리 관리로 통합</strong>함으로써, 리소스 제약이 큰 시스템에서도 고성능 LLM 추론을 가능하게 한 획기적인 방법론입니다.</p><h1 id=q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘>Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1><h3 id=논문의-입력-데이터와-추론-과정><strong>논문의 입력 데이터와 추론 과정</strong></h3><p>ALISA 논문은 **대규모 언어 모델(LLM)**의 효율적인 추론을 목표로 하며, 입력 데이터와 추론 과정은 다음과 같이 진행됩니다.</p><hr><h4 id=1-입력-데이터><strong>1. 입력 데이터</strong></h4><ul><li><strong>입력 데이터 형식</strong>: 텍스트 토큰 시퀀스<ul><li>예: &ldquo;The capital of France is Paris.&rdquo;</li></ul></li><li><strong>처리 단계</strong>:<ol><li>입력 텍스트를 <strong>토큰화</strong>하여 벡터화.<ul><li>예: <code>["The", "capital", "of", "France", "is"] → [101, 245, 34, 456, 89]</code></li><li>각 토큰은 고차원 임베딩 벡터로 변환.</li></ul></li><li>임베딩된 벡터는 모델의 <strong>Transformer 레이어</strong>로 입력.</li></ol></li></ul><hr><h4 id=2-추론-과정><strong>2. 추론 과정</strong></h4><p>추론은 <strong>Autoregressive(자기회귀적)</strong> 방식으로 수행됩니다.</p><h5 id=1-prefilling-단계><strong>(1) Prefilling 단계</strong></h5><ul><li>입력 시퀀스를 한 번에 처리하여 초기 **Key (K)**와 <strong>Value (V)</strong> 텐서를 생성.</li><li>예: <code>["The", "capital", "of", "France", "is"]</code>가 입력되면, 모든 토큰에 대해 ( K, V )가 계산되어 캐싱됨.</li></ul><h5 id=2-decoding-단계><strong>(2) Decoding 단계</strong></h5><ul><li>새로운 토큰을 한 번에 하나씩 생성.</li><li>이전 토큰과 현재 토큰을 기반으로 ( Q, K, V )를 조합하여 다음 토큰을 예측:<ul><li>입력: <code>"Paris"</code>의 Query (( Q )).</li><li>캐싱된 ( K, V ): <code>"The", "capital", "of", "France", "is"</code>에 대한 정보.</li><li>계산: Attention Score와 Softmax를 통해 <code>"Paris"</code> 예측.</li></ul></li></ul><h5 id=3-sparse-window-attention-swa><strong>(3) Sparse Window Attention (SWA)</strong></h5><ul><li>캐싱된 ( K, V ) 중 <strong>중요한 토큰만 선택</strong>하여 Sparse Attention 계산:<ul><li><strong>Locally Static</strong>: 최근 토큰은 항상 포함.</li><li><strong>Globally Dynamic</strong>: Attention Weight가 높은 토큰만 추가로 포함.</li></ul></li><li>선택된 ( K, V )만으로 새로운 토큰 생성.</li><li>예: <code>"capital"</code>과 <code>"France"</code>가 중요한 토큰으로 선택, 나머지는 제외.</li></ul><hr><h3 id=모델-아키텍처의-구성><strong>모델 아키텍처의 구성</strong></h3><h4 id=1-transformer-아키텍처><strong>1. Transformer 아키텍처</strong></h4><p>ALISA는 표준 Transformer 기반 LLM을 활용하며, 주요 구성 요소는 다음과 같습니다:</p><ol><li><p><strong>입력 임베딩</strong>:</p><ul><li>텍스트 토큰을 고차원 벡터로 변환.</li><li>입력 크기: <code>[Batch Size, Sequence Length, Embedding Dim]</code>.</li></ul></li><li><p><strong>Self-Attention 레이어</strong>:</p><ul><li>주요 계산: ( Q, K, V ) 생성 및 Attention 계산.</li><li>Attention 계산:
[
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
]
여기서 ( Q ), ( K ), ( V )의 크기:<ul><li>( Q, K, V ): <code>[Batch Size, Sequence Length, Head Dim]</code></li></ul></li></ul></li><li><p><strong>Feed-Forward Network (FFN)</strong>:</p><ul><li>각 Attention 출력을 고차원으로 변환 후 다시 축소.</li><li>주요 계산:<ul><li>( \text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2 )</li><li>입력 및 출력 크기: <code>[Batch Size, Sequence Length, Model Dim]</code>.</li></ul></li></ul></li><li><p><strong>Layer Normalization 및 Skip Connection</strong>:</p><ul><li>각 Attention 및 FFN 뒤에 적용.</li></ul></li><li><p><strong>최종 출력</strong>:</p><ul><li>Linear Layer와 Softmax를 통해 다음 토큰 예측 확률 생성.</li></ul></li></ol><hr><h3 id=연산-및-메모리-요구량><strong>연산 및 메모리 요구량</strong></h3><h4 id=1-메모리-요구량><strong>1. 메모리 요구량</strong></h4><ol><li><p><strong>KV 텐서 저장</strong>:</p><ul><li>( K )와 ( V )는 시퀀스 길이에 따라 선형적으로 증가:<ul><li>크기: <code>[Batch Size, Sequence Length, Hidden Dim]</code></li></ul></li><li>메모리 사용량:
[
\text{메모리} = 4 \cdot (\text{Batch Size} \times \text{Sequence Length} \times \text{Hidden Dim})
]<ul><li>FP16 기준, 1개의 토큰당 약 4바이트 사용.</li></ul></li></ul></li><li><p><strong>모델 가중치</strong>:</p><ul><li>모델 크기(예: 13B 파라미터)와 GPU 메모리에 따라 고정.</li><li>FP16 기준, 약 52GB 사용.</li></ul></li><li><p><strong>Intermediate Tensors</strong>:</p><ul><li>Attention 계산 중 생성되는 중간 행렬:<ul><li>Softmax 결과, MatMul, Output 행렬.</li></ul></li></ul></li></ol><hr><h4 id=2-연산-요구량><strong>2. 연산 요구량</strong></h4><ol><li><p><strong>Self-Attention의 계산 복잡도</strong>:</p><ul><li>(\mathcal{O}(\text{Sequence Length}^2 \cdot \text{Hidden Dim})):<ul><li>일반적인 Dense Attention.</li></ul></li><li>SWA 사용 시 (\mathcal{O}(\text{Sparse Tokens} \cdot \text{Sequence Length} \cdot \text{Hidden Dim}))로 감소.<ul><li>Sparse Tokens 수에 따라 계산량 선형 감소.</li></ul></li></ul></li><li><p><strong>Feed-Forward Layer</strong>:</p><ul><li>(\mathcal{O}(\text{Sequence Length} \cdot \text{Hidden Dim}^2)).</li></ul></li><li><p><strong>전체 연산량</strong>:</p><ul><li>Dense Attention에서 SWA로 변환 시, 최대 80% 연산량 감소 가능.</li></ul></li></ol><hr><h3 id=alisa의-계산-및-메모리-최적화><strong>ALISA의 계산 및 메모리 최적화</strong></h3><ol><li><p><strong>SWA 도입</strong>:</p><ul><li>불필요한 ( K, V )를 제거하여 메모리와 연산량을 동시에 줄임.</li></ul></li><li><p><strong>동적 스케줄링</strong>:</p><ul><li>CPU와 GPU 간 메모리 전송을 최소화.</li><li>GPU에서 재계산으로 일부 오프로드 오버헤드 제거.</li></ul></li><li><p><strong>KV 텐서 압축</strong>:</p><ul><li>FP16을 INT8로 압축하여 메모리 사용량을 추가로 감소.</li></ul></li></ol><hr><p>결론적으로, ALISA는 입력 데이터를 효율적으로 처리하고 Sparse Attention 및 동적 메모리 관리 기법을 통해 LLM 추론에서의 메모리 및 연산 요구량을 크게 줄였습니다. 이러한 최적화는 대규모 모델에서도 메모리 제약 환경에서 고성능을 유지할 수 있도록 설계되었습니다.</p><h1 id=q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘>Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1><h3 id=alisa-논문의-한계와-이를-극복하기-위한-연구-흐름><strong>ALISA 논문의 한계와 이를 극복하기 위한 연구 흐름</strong></h3><p>ALISA 논문은 Sparse Attention 및 동적 메모리 관리로 LLM 추론을 최적화했지만, 몇 가지 한계점이 존재합니다. 이를 극복하기 위한 연구 흐름과 방법론을 아래에 정리했습니다.</p><hr><h3 id=1-kv-sparsity와-정확도-간의-트레이드오프><strong>1. KV Sparsity와 정확도 간의 트레이드오프</strong></h3><h4 id=한계점><strong>한계점</strong>:</h4><ul><li>SWA는 80% 이상의 sparsity에서 일부 작업의 정확도가 감소합니다.<ul><li>특히, 복잡한 맥락이나 긴 텍스트의 경우, 중요 토큰을 정확히 선택하지 못하면 정확도가 크게 떨어질 수 있음.</li></ul></li></ul><h4 id=연구-흐름><strong>연구 흐름</strong>:</h4><ol><li><p><strong>학습 기반 중요 토큰 선택</strong>:</p><ul><li>중요 토큰 선택을 사전에 학습된 모델 또는 추가적인 fine-tuning을 통해 자동화.</li><li>예: 중요도를 예측하는 lightweight sub-model 설계.</li></ul></li><li><p><strong>다중 sparsity 패턴 결합</strong>:</p><ul><li>SWA의 고정된 sparsity 패턴 대신 여러 패턴을 동적으로 결합.</li><li>예: Sliding Window, Strided Attention, Global Sparse 패턴을 조합하여 정확도 손실을 줄임.</li></ul></li><li><p><strong>Gradient-Aware Sparsity</strong>:</p><ul><li>Gradient를 활용하여 Attention Score 변화에 민감한 토큰만 선택.</li><li>중요 토큰 선택 과정에서 모델의 loss gradient를 참고하여 선택 정확도를 높임.</li></ul></li></ol><hr><h3 id=2-동적-스케줄링의-복잡성><strong>2. 동적 스케줄링의 복잡성</strong></h3><h4 id=한계점-1><strong>한계점</strong>:</h4><ul><li>ALISA의 동적 스케줄링은 CPU와 GPU 간의 메모리 전송 및 재계산을 효율적으로 관리하지만, 하드웨어 특성에 따라 최적화가 어렵고 설정이 복잡함.</li><li>특정 하드웨어(GPU-CPU 대역폭)에 의존하여 보편적이지 않음.</li></ul><h4 id=연구-흐름-1><strong>연구 흐름</strong>:</h4><ol><li><p><strong>스케줄링 자동화</strong>:</p><ul><li>Reinforcement Learning(RL)을 활용하여 최적의 메모리 스케줄링 정책을 자동으로 학습.</li><li>예: RL 알고리즘이 각 단계에서 GPU와 CPU의 메모리 상태를 분석하고 최적의 스케줄링 결정.</li></ul></li><li><p><strong>온디맨드 재계산 정책</strong>:</p><ul><li>재계산을 사전 설정하는 대신, 실시간으로 I/O 대역폭과 계산 비용을 비교하여 결정.</li><li>GPU의 부하 상태를 기반으로 캐싱 또는 재계산을 동적으로 전환.</li></ul></li><li><p><strong>다중 디바이스 확장</strong>:</p><ul><li>GPU-CPU 간 전송만 최적화하는 대신, 다중 GPU 또는 GPU-TPU 혼합 환경에 최적화된 스케줄링 연구.</li><li>분산 메모리 관리 기법 도입.</li></ul></li></ol><hr><h3 id=3-sparse-attention-생성의-추가-연산-오버헤드><strong>3. Sparse Attention 생성의 추가 연산 오버헤드</strong></h3><h4 id=한계점-2><strong>한계점</strong>:</h4><ul><li>SWA의 중요 토큰 선택 및 Sparse KV 텐서 생성 과정에서 추가 연산 오버헤드 발생.</li><li>Sparse Attention이 작은 모델에서 비효율적일 수 있음.</li></ul><h4 id=연구-흐름-2><strong>연구 흐름</strong>:</h4><ol><li><p><strong>Sparse Matrix Hardware 가속기</strong>:</p><ul><li>Sparse Matrix 연산에 특화된 하드웨어를 활용하여 SWA의 추가 연산 비용을 줄임.</li><li>예: NVIDIA의 Ampere 아키텍처에서 Sparse Tensor Core 사용.</li></ul></li><li><p><strong>Sparse 구조 최적화</strong>:</p><ul><li>Sparse 구조를 보다 규칙적으로 설계하여 GPU의 병렬 처리 효율성을 극대화.</li><li>예: Block Sparse Attention으로 변환.</li></ul></li><li><p><strong>Sparse Attention 사전 학습</strong>:</p><ul><li>모델 학습 단계에서 Sparse Attention을 미리 학습하여, 추론 시 추가 연산을 최소화.</li></ul></li></ol><hr><h3 id=4-특정-데이터와-작업에서-성능-저하><strong>4. 특정 데이터와 작업에서 성능 저하</strong></h3><h4 id=한계점-3><strong>한계점</strong>:</h4><ul><li>ALISA는 일반적으로 성능이 우수하지만, 특정 데이터셋(예: 복잡한 질의 응답)에서는 Dense Attention 대비 성능이 낮아질 수 있음.</li></ul><h4 id=연구-흐름-3><strong>연구 흐름</strong>:</h4><ol><li><p><strong>Task-Aware Sparsity</strong>:</p><ul><li>작업별로 최적의 Sparsity 전략을 선택.</li><li>예: 문장 생성 작업에서는 Local Sparsity 강화, 질문 응답 작업에서는 Global Sparsity 강화.</li></ul></li><li><p><strong>Hierarchical Sparse Attention</strong>:</p><ul><li>Attention을 계층적으로 나눠, 중요한 정보가 높은 레벨에서 집중되도록 설계.</li><li>예: 주요 토큰은 더 많은 Attention Score를 할당하고, 나머지는 가중치 감소.</li></ul></li><li><p><strong>Multimodal 데이터 적용</strong>:</p><ul><li>텍스트뿐만 아니라 이미지, 오디오 등 다중 모달 데이터에 Sparse Attention 기법 확장.</li><li>모달 간 상호작용 정보를 기반으로 중요 토큰을 선택.</li></ul></li></ol><hr><h3 id=5-kv-compression압축-기술의-한계><strong>5. KV Compression(압축) 기술의 한계</strong></h3><h4 id=한계점-4><strong>한계점</strong>:</h4><ul><li>KV 텐서를 INT8로 압축하면 메모리 사용량은 감소하지만, 일부 작업에서 정확도 손실 발생 가능.</li><li>압축률이 더 높은 기술(INT4 등) 적용 시 정확도 손실이 증가.</li></ul><h4 id=연구-흐름-4><strong>연구 흐름</strong>:</h4><ol><li><p><strong>Adaptive Precision Quantization</strong>:</p><ul><li>토큰별로 가중치 민감도를 분석하여, 중요한 텐서는 고정밀도(FP16), 덜 중요한 텐서는 저정밀도(INT8)로 압축.</li><li>예: 중요 토큰의 KV는 FP16, 덜 중요한 토큰은 INT8로 관리.</li></ul></li><li><p><strong>Lossless Compression Techniques</strong>:</p><ul><li>Entropy 기반 압축 기법을 활용하여 정보 손실 없이 메모리 사용량 감소.</li><li>예: KV 텐서의 정보를 Huffman Coding으로 압축.</li></ul></li><li><p><strong>Gradient-Aware Quantization</strong>:</p><ul><li>모델 학습 중 Gradient를 기준으로 양자화를 최적화하여 압축 시 정확도 손실 최소화.</li></ul></li></ol><hr><h3 id=6-정적-sparse-attention의-한계><strong>6. 정적 Sparse Attention의 한계</strong></h3><h4 id=한계점-5><strong>한계점</strong>:</h4><ul><li>SWA는 동적으로 중요 토큰을 선택하지만, 선택된 Sparse 패턴이 모든 작업에 동일하게 적용될 수는 없음.</li></ul><h4 id=연구-흐름-5><strong>연구 흐름</strong>:</h4><ol><li><p><strong>Dynamic Sparse Patterns</strong>:</p><ul><li>추론 중 Attention Weight의 변화에 따라 Sparse 패턴을 실시간으로 수정.</li><li>예: 긴 시퀀스에서는 Local Sparsity 비율을 증가, 짧은 시퀀스에서는 Global Sparsity 비율 증가.</li></ul></li><li><p><strong>Feedback-Driven Sparsity</strong>:</p><ul><li>이전 단계의 추론 결과를 기반으로 Sparse Attention 패턴을 업데이트.</li><li>예: 특정 작업에서 성능이 저하되면 Sparse 비율을 동적으로 조정.</li></ul></li><li><p><strong>Reinforcement Learning 기반 Sparse Pattern 최적화</strong>:</p><ul><li>RL을 통해 Sparse Attention 패턴을 실시간으로 학습하고 업데이트.</li></ul></li></ol><hr><h3 id=종합><strong>종합</strong></h3><p>ALISA의 한계를 극복하려면 Sparse Attention 기법의 유연성과 정확도를 개선하고, 하드웨어와 시스템 수준에서 효율성을 극대화하는 방향으로 연구가 진행되어야 합니다. 주요 연구 흐름은 <strong>학습 기반 중요 토큰 선택</strong>, <strong>동적 스케줄링 자동화</strong>, <strong>압축 기술의 정교화</strong>, <strong>Sparse Attention의 실시간 최적화</strong>로 요약됩니다. 이들은 LLM 추론 효율성을 더욱 높이는 데 기여할 수 있습니다.</p></article></main><footer class="mx-auto max-w-4xl px-4 py-8"><div class="px-6 py-6"><nav class=mb-6><div class="flex flex-wrap items-center justify-center gap-1"><a href=/index.xml class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="RSS Feed"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18.0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z"/></svg>
<span>RSS Feed</span></a></div></nav><div class="border-border mb-6 border-t"></div><div class="flex flex-col items-center justify-between gap-4 md:flex-row"><div class="text-muted-foreground text-sm"><p>&copy; 2025 Jaehun's Blog.
모든 권리 보유.</p></div><div class="flex items-center gap-3"><a href=https://github.com/ryujaehun target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=GitHub aria-label=GitHub><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.4 5.4.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
</a><a href=https://www.linkedin.com/in/jaehunryu/ target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Linkedin aria-label=Linkedin><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
</a><a href=mailto:jaehunryu@icloud.com class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Email aria-label=Email><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Email"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22.0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v10a2 2 0 002 2z"/></svg></a></div></div></div></footer><div id=dock class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0" role=toolbar aria-label="바로가기 도구 모음"><nav class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3"><button id=dock-back class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=뒤로 aria-label=뒤로>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m12 19-7-7 7-7"/><path d="M19 12H5"/></svg>
<span class="hidden sm:inline">뒤로</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-toc class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=목차 aria-label=목차>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg>
<span class="hidden sm:inline">목차</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-search class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2" title=검색 aria-label=검색>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg>
<span class="hidden md:inline">검색</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-top class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title="맨 위로" aria-label="맨 위로">
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m18 15-6-6-6 6"/></svg>
<span class="hidden sm:inline">맨 위로</span></button></nav></div><div id=toc-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300" role=dialog aria-modal=true aria-labelledby=toc-title></div><div id=toc-card class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2" role=dialog aria-modal=true aria-labelledby=toc-title><div class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md"><div class="border-border bg-muted/30 flex items-center justify-between border-b p-4"><div class="flex items-center gap-3"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg><h2 id=toc-title class="text-foreground text-lg font-semibold">목차</h2></div><button id=toc-close class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div class="max-h-96 overflow-y-auto p-4"><nav id=toc-nav class=toc-scrollbar aria-label="글 목차"><div id=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#논문의-강점과-독창성>논문의 강점과 독창성</a></li><li><a href=#핵심-알고리즘-sparse-window-attention-swa>핵심 알고리즘: Sparse Window Attention (SWA)</a></li><li><a href=#논문의-한계점>논문의 한계점</a></li></ul></li></ul><ul><li><ul><li><a href=#핵심-알고리즘-sparse-window-attention-swa-1>핵심 알고리즘: Sparse Window Attention (SWA)</a></li><li><a href=#예시-입력><strong>예시 입력</strong></a></li><li><a href=#1단계-중요-토큰-식별><strong>1단계: 중요 토큰 식별</strong></a></li><li><a href=#2단계-sparse-패턴-생성><strong>2단계: Sparse 패턴 생성</strong></a></li><li><a href=#3단계-sparse-계산><strong>3단계: Sparse 계산</strong></a></li><li><a href=#4단계-캐싱-및-업데이트><strong>4단계: 캐싱 및 업데이트</strong></a></li><li><a href=#알고리즘-정리><strong>알고리즘 정리</strong></a></li><li><a href=#swa의-주요-장점><strong>SWA의 주요 장점</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#논문에서-제시한-결과와-다른-방법론-대비-특출난-점>논문에서 제시한 결과와 다른 방법론 대비 특출난 점</a><ul><li><a href=#논문의-주요-결과><strong>논문의 주요 결과</strong></a></li></ul></li><li><a href=#alisa가-특출난-이유><strong>ALISA가 특출난 이유</strong></a><ul><li><a href=#1-sparse-window-attention-swa><strong>1. Sparse Window Attention (SWA)</strong></a></li><li><a href=#2-동적-스케줄링><strong>2. 동적 스케줄링</strong></a></li><li><a href=#3-kv-tensor-압축-int8-quantization><strong>3. KV Tensor 압축 (INT8 Quantization)</strong></a></li><li><a href=#4-전반적인-통합-설계><strong>4. 전반적인 통합 설계</strong></a></li></ul></li><li><a href=#alisa의-결과를-뒷받침하는-논문의-이유><strong>ALISA의 결과를 뒷받침하는 논문의 이유</strong></a></li><li><a href=#나의-분석-및-의견><strong>나의 분석 및 의견</strong></a><ul><li><a href=#긍정적인-측면><strong>긍정적인 측면</strong></a></li><li><a href=#개선-가능성><strong>개선 가능성</strong></a></li></ul></li></ul></li></ul><ul><li><ul><li><a href=#논문의-입력-데이터와-추론-과정><strong>논문의 입력 데이터와 추론 과정</strong></a><ul><li><a href=#1-입력-데이터><strong>1. 입력 데이터</strong></a></li><li><a href=#2-추론-과정><strong>2. 추론 과정</strong></a></li></ul></li><li><a href=#모델-아키텍처의-구성><strong>모델 아키텍처의 구성</strong></a><ul><li><a href=#1-transformer-아키텍처><strong>1. Transformer 아키텍처</strong></a></li></ul></li><li><a href=#연산-및-메모리-요구량><strong>연산 및 메모리 요구량</strong></a><ul><li><a href=#1-메모리-요구량><strong>1. 메모리 요구량</strong></a></li><li><a href=#2-연산-요구량><strong>2. 연산 요구량</strong></a></li></ul></li><li><a href=#alisa의-계산-및-메모리-최적화><strong>ALISA의 계산 및 메모리 최적화</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#alisa-논문의-한계와-이를-극복하기-위한-연구-흐름><strong>ALISA 논문의 한계와 이를 극복하기 위한 연구 흐름</strong></a></li><li><a href=#1-kv-sparsity와-정확도-간의-트레이드오프><strong>1. KV Sparsity와 정확도 간의 트레이드오프</strong></a><ul><li><a href=#한계점><strong>한계점</strong>:</a></li><li><a href=#연구-흐름><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#2-동적-스케줄링의-복잡성><strong>2. 동적 스케줄링의 복잡성</strong></a><ul><li><a href=#한계점-1><strong>한계점</strong>:</a></li><li><a href=#연구-흐름-1><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#3-sparse-attention-생성의-추가-연산-오버헤드><strong>3. Sparse Attention 생성의 추가 연산 오버헤드</strong></a><ul><li><a href=#한계점-2><strong>한계점</strong>:</a></li><li><a href=#연구-흐름-2><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#4-특정-데이터와-작업에서-성능-저하><strong>4. 특정 데이터와 작업에서 성능 저하</strong></a><ul><li><a href=#한계점-3><strong>한계점</strong>:</a></li><li><a href=#연구-흐름-3><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#5-kv-compression압축-기술의-한계><strong>5. KV Compression(압축) 기술의 한계</strong></a><ul><li><a href=#한계점-4><strong>한계점</strong>:</a></li><li><a href=#연구-흐름-4><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#6-정적-sparse-attention의-한계><strong>6. 정적 Sparse Attention의 한계</strong></a><ul><li><a href=#한계점-5><strong>한계점</strong>:</a></li><li><a href=#연구-흐름-5><strong>연구 흐름</strong>:</a></li></ul></li><li><a href=#종합><strong>종합</strong></a></li></ul></li></ul></nav></div></nav></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground text-center text-xs"><span>제목을 클릭하면 해당 위치로 이동합니다</span></div></div></div></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>function initKaTeX(){if(typeof renderMathInElement=="undefined"){setTimeout(initKaTeX,100);return}var e=[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"}];renderMathInElement(document.body,{delimiters:e,throwOnError:!1,errorColor:"#cc0000",fleqn:!1,leqno:!1,trust:!1})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",initKaTeX):initKaTeX()</script><div id=search-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div><div id=search-modal class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300"><div class="border-border flex items-center gap-3 border-b p-4"><div class="text-muted-foreground h-5 w-5 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><div class="relative flex-1"><button id=search-clear class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200" title=지우기 aria-label=지우기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg>
</button>
<input id=search-input type=text placeholder="게시글 검색..." class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none" autocomplete=off spellcheck=false></div><button id=search-close class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div id=search-results class="max-h-96 overflow-y-auto"><div id=search-empty class="flex flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 시작</h3><p class="text-muted-foreground text-sm">검색어를 입력하세요</p></div><div id=search-loading class="flex hidden items-center justify-center py-8"><div class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div><span class=text-muted-foreground>검색 중...</span></div><div id=search-no-results class="flex hidden flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 결과 없음</h3><p class="text-muted-foreground text-sm">다른 검색어로 시도해보세요</p></div><div id=search-results-list class=hidden><div id=search-stats class="text-muted-foreground border-border border-b px-4 py-3 text-sm" data-template="%d개의 결과 찾음"></div><div id=search-items class="divide-border divide-y"></div></div></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground flex items-center justify-between text-xs"><div class="flex items-center gap-2 md:gap-4"><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↑↓</kbd>
<span class="hidden sm:inline">이동</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↵</kbd>
<span class="hidden sm:inline">선택</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">ESC</kbd>
<span class="hidden sm:inline">닫기</span></div></div><div class="search-hint-desktop flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">⌘K</kbd>
<span>단축키</span></div></div></div></div></body></html>