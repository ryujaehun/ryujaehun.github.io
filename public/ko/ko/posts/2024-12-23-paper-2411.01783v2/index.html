<!doctype html><html lang=ko-kr data-theme=claude><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Context Parallelism for Scalable Million-Token Inference | Jaehun's Blog</title><meta name=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 강점 및 독창적 기여 이 논문은 **Context Parallelism..."><meta name=keywords content="개발,프로그래밍,에세이"><meta name=author content><meta name=language content="ko"><meta name=robots content="index, follow"><link rel=canonical href=https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.01783v2/><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.01783v2/"><meta property="og:site_name" content="Jaehun's Blog"><meta property="og:title" content="Context Parallelism for Scalable Million-Token Inference"><meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 강점 및 독창적 기여 이 논문은 **Context Parallelism (CP)**라는 기법을 통해 대규모 언어 모델(LLM)에서의 긴 문맥 길이(최대 1M 토큰) 추론 효율성을 대폭 향상시키는 방법을 제안합니다. 주요 강점과 독창적인 기여는 다음과 같습니다:"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="ko"><meta property="article:published_time" content="2024-12-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-23T00:00:00+00:00"><meta property="og:image" content="https://jaehun.me/images/og-default.avif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehun.me/images/og-default.avif"><meta name=twitter:title content="Context Parallelism for Scalable Million-Token Inference"><meta name=twitter:description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 강점 및 독창적 기여 이 논문은 **Context Parallelism (CP)**라는 기법을 통해 대규모 언어 모델(LLM)에서의 긴 문맥 길이(최대 1M 토큰) 추론 효율성을 대폭 향상시키는 방법을 제안합니다. 주요 강점과 독창적인 기여는 다음과 같습니다:"><meta itemprop=name content="Context Parallelism for Scalable Million-Token Inference"><meta itemprop=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 강점 및 독창적 기여 이 논문은 **Context Parallelism (CP)**라는 기법을 통해 대규모 언어 모델(LLM)에서의 긴 문맥 길이(최대 1M 토큰) 추론 효율성을 대폭 향상시키는 방법을 제안합니다. 주요 강점과 독창적인 기여는 다음과 같습니다:"><meta itemprop=datePublished content="2024-12-23T00:00:00+00:00"><meta itemprop=dateModified content="2024-12-23T00:00:00+00:00"><meta itemprop=wordCount content="11527"><meta itemprop=image content="https://jaehun.me/images/og-default.avif"><meta itemprop=keywords content="Paper-Review,With-Gpt"><script>window.HUGO_SEARCH_CONFIG={searchIndexURL:'"/ko/index.json"'}</script><script>window.HUGO_GALLERY_CONFIG={justified_gallery:!1,lightbox:!1,justified:"{}",lightbox_options:"{}"}</script><link rel=stylesheet href=/css/compiled.min.9165cace5b80a5be5daad6ea96eb2abc20a18a3f41d7fed7f0335b689fdf7883.css integrity="sha256-kWXKzluApb5dqtbqlusqvCChij9B1/7X8DNbaJ/feIM=" crossorigin=anonymous><link rel=stylesheet href=/css/chroma.min.491df36221f739b5948a747a0351f100ee7ccaaf466d9f46288bf06de1d59123.css integrity="sha256-SR3zYiH3ObWUinR6A1HxAO58yq9GbZ9GKIvwbeHVkSM=" crossorigin=anonymous><script src=/js/main.ba969e49a7a874e6938fe689a7dcaa00cb64cb4d429c2cf98d09bde512a16c6d.js integrity="sha256-upaeSaeodOaTj+aJp9yqAMtky01CnCz5jQm95RKhbG0=" crossorigin=anonymous></script><script src=/js/gumshoe.polyfills.min.js></script><script src=/js/toc.70654aebf0b738a78ae2df95297cc10ebc6e590f3c942dd1325fa852f50038b2.js integrity="sha256-cGVK6/C3OKeK4t+VKXzBDrxuWQ88lC3RMl+oUvUAOLI=" crossorigin=anonymous defer></script><script src=/js/search.de3eba3540834efb8328121784bff4774e06362412ee41eee665f5b4f987f393.js integrity="sha256-3j66NUCDTvuDKBIXhL/0d04GNiQS7kHu5mX1tPmH85M=" crossorigin=anonymous defer></script><script src=/js/dock.e02f422517e001d9fb6bea1e9e8b0e89e9d2e486ad4641f275bb3e36ce2ae79f.js integrity="sha256-4C9CJRfgAdn7a+oenosOienS5IatRkHydbs+Ns4q558=" crossorigin=anonymous defer></script><script>(function(){const e=localStorage.getItem("theme")||"system",n=localStorage.getItem("colorScheme")||"claude";document.documentElement.setAttribute("data-theme",n);function t(){e==="dark"||e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}t(),e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",t)})()</script></head><body class="bg-background text-foreground min-h-screen antialiased"><div id=reading-progress-container class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out" data-height=3 data-smooth-scroll=true data-hide-on-complete=false><div class="reading-progress-bg w-full"></div><div id=reading-progress-bar class="from-primary to-primary/80 reading-progress-bar
transition-all duration-150 ease-out
absolute top-0 left-0 w-0 bg-gradient-to-r"></div></div><script>(function(){"use strict";const t=document.getElementById("reading-progress-container"),o=document.getElementById("reading-progress-bar");if(!t||!o)return;const a={smoothScroll:t.dataset.smoothScroll==="true",hideOnComplete:t.dataset.hideOnComplete==="true"};let n=!0,l=null;function r(){const t=window.pageYOffset||document.documentElement.scrollTop,n=document.documentElement.scrollHeight,s=window.innerHeight,e=n-s;return e<=0?0:Math.min(Math.max(t/e*100,0),100)}function c(){const e=r();o.style.width=e+"%",a.hideOnComplete&&e>=99.5?n&&(t.style.opacity="0",n=!1):n||(t.style.opacity="1",n=!0)}let s=!1;function e(){s||(requestAnimationFrame(()=>{c(),s=!1}),s=!0)}function i(){window.addEventListener("scroll",e,{passive:!0}),window.addEventListener("resize",e,{passive:!0}),document.addEventListener("visibilitychange",()=>{document.hidden||e()}),e(),window.addEventListener("beforeunload",()=>{window.removeEventListener("scroll",e),window.removeEventListener("resize",e)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script><header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6"><div class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm"><div class="hidden w-full items-center md:flex"><div class="flex items-center"><a href=/ko/ class="flex h-10 w-10 items-center justify-center overflow-hidden rounded-full transition-transform duration-200 hover:scale-105" aria-label="Jaehun's Blog"><img src=/images/logo.svg alt="Jaehun's Blog" class="h-full w-full object-cover"></a></div><nav class="mx-8 flex flex-1 items-center justify-center"><div class="flex items-center space-x-1"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></div></nav><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div><div class="flex w-full items-center justify-between md:hidden"><div class=relative><button id=mobile-menu-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200" data-dropdown-type=mobile-menu aria-label=메뉴 aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="메뉴"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button><div id=mobile-menu class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden" data-dropdown-type=mobile-menu role=menu aria-labelledby=mobile-menu-toggle><nav class="flex flex-col"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></nav></div></div><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div></div></header><main class="mx-auto max-w-4xl px-4 py-6"><nav class="breadcrumb mb-4 md:mb-6 py-1" aria-label=경로><ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1"><li class=flex-shrink-0><a href=/ko/ class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="홈"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11 2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">홈</span></a></li><li class="flex items-center gap-1 md:gap-2 min-w-0"><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span><div class="flex items-center gap-1 md:gap-2 min-w-0"><a href=/ko class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5A2 2 0 003 7z"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">Ko</span>
</a><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span>
<span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/></svg>
<span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">Context Parallelism for Scalable Million-Token Inference</span></span></div></li></ol></nav><header class=mb-8><h1 class="text-foreground mb-4 text-3xl font-bold md:text-4xl">Context Parallelism for Scalable Million-Token Inference</h1></header><article class="prose prose-neutral dark:prose-invert max-w-none"><p><a href=https://arxiv.org/abs/2411.01783v2>논문 링크</a></p><h1 id=q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘>Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1><h3 id=강점-및-독창적-기여>강점 및 독창적 기여</h3><p>이 논문은 **Context Parallelism (CP)**라는 기법을 통해 대규모 언어 모델(LLM)에서의 긴 문맥 길이(최대 1M 토큰) 추론 효율성을 대폭 향상시키는 방법을 제안합니다. 주요 강점과 독창적인 기여는 다음과 같습니다:</p><ol><li><p><strong>긴 문맥 길이에서의 추론 효율성</strong>:</p><ul><li>기존의 방법보다 <strong>93% 병렬화 효율성</strong>과 <strong>63% FLOPS 활용률</strong>을 달성하며, 1M 토큰의 문맥을 128개의 H100 GPU로 77초 만에 처리할 수 있습니다.</li></ul></li><li><p><strong>혁신적인 Ring Attention 알고리즘 변형</strong>:</p><ul><li>두 가지 변형(<code>Pass-KV</code>, <code>Pass-Q</code>)을 도입하여 다양한 사용 사례에 적합한 최적의 성능을 제공하며, 실시간 추론에서의 지연 시간을 줄이는 데 기여했습니다.</li></ul></li><li><p><strong>실제 데이터 센터 환경에서의 스케일링 검증</strong>:</p><ul><li>RDMA 및 TCP 기반 상용 데이터 센터 네트워크 환경에서 모두 확장성을 검증하여, 상용 시스템에서의 활용 가능성을 입증했습니다.</li></ul></li><li><p><strong>멀티턴 대화 시나리오 지원</strong>:</p><ul><li><strong>Persistent KV Cache</strong>를 활용해 대화 히스토리를 유지하면서도 빠른 응답을 제공하는 구조를 제안했습니다.</li></ul></li><li><p><strong>계산 및 메모리 부하 균형 최적화</strong>:</p><ul><li>토큰과 KV 캐시를 균형 있게 샤딩하는 새로운 알고리즘을 제안하여, 요청의 길이에 관계없이 성능을 안정적으로 유지했습니다.</li></ul></li></ol><hr><h3 id=핵심-알고리즘-설명-ring-attention-pass-kv와-pass-q>핵심 알고리즘 설명: Ring Attention (<code>Pass-KV</code>와 <code>Pass-Q</code>)</h3><p>논문의 핵심은 Ring Attention의 두 가지 변형(<code>Pass-KV</code>와 <code>Pass-Q</code>)으로, 각각의 동작 과정을 예시를 통해 설명하겠습니다.</p><h4 id=예시-입력>예시 입력:</h4><ul><li><strong>문맥 길이</strong> ( T = 128K ), <strong>기존 캐시된 토큰</strong> ( P = 89600 ), <strong>KV 캐시 누락률</strong> ( T / (T + P) = 12.5% ).</li></ul><h4 id=pass-kv-과정><code>Pass-KV</code> 과정:</h4><ol><li><strong>데이터 준비</strong>:<ul><li>각 GPU에서 KV 쌍(Key, Value)을 처리하도록 데이터를 균등하게 분할.</li></ul></li><li><strong>통신</strong>:<ul><li>GPU 사이에서 KV 데이터를 링 구조로 전송하며 병렬적으로 <code>Query</code>를 계산.</li></ul></li><li><strong>계산</strong>:<ul><li>각 <code>Query</code>는 해당 GPU에서 처리된 KV 쌍과의 Attention 값을 계산.</li></ul></li><li><strong>결과 병합</strong>:<ul><li>부분적으로 계산된 Attention 결과를 병합하여 최종 출력을 생성.</li></ul></li></ol><h4 id=pass-q-과정><code>Pass-Q</code> 과정:</h4><ol><li><strong>데이터 준비</strong>:<ul><li><code>Query</code> 데이터를 GPU 간 분산.</li></ul></li><li><strong>통신</strong>:<ul><li>GPU 간 <code>Query</code> 데이터를 전송하며, 고정된 KV 캐시를 사용.</li></ul></li><li><strong>계산</strong>:<ul><li>각 GPU에서 <code>Query</code>와 KV 캐시를 사용해 부분 Attention 결과를 계산.</li></ul></li><li><strong>결과 병합</strong>:<ul><li>다른 GPU에서 계산된 결과를 All-to-All 통신을 통해 병합.</li></ul></li></ol><h4 id=선택-기준>선택 기준:</h4><ul><li>( T / (T + P) > 5% )일 경우 <code>Pass-KV</code>가 효율적.</li><li>그 이하에서는 <code>Pass-Q</code>가 더 낮은 지연 시간을 제공.</li></ul><hr><h3 id=논문의-한계>논문의 한계</h3><ol><li><p><strong>KV 캐시 누락률에 따른 비효율성</strong>:</p><ul><li>KV 캐시 누락률이 증가할수록 <code>Pass-KV</code>와 <code>Pass-Q</code>의 성능 차이가 감소.</li><li>매우 높은 누락률에서 추가적인 최적화가 필요.</li></ul></li><li><p><strong>디코딩 단계에서의 성능 저하</strong>:</p><ul><li><code>Pass-Q</code>는 디코딩 성능에서 병렬성 확장이 제한적이며, 배치 패딩과 통신 오버헤드로 인해 지연 시간이 증가.</li></ul></li><li><p><strong>특정 하드웨어 종속성</strong>:</p><ul><li>제안된 최적화는 H100 GPU 및 NVLink 기반 환경에 최적화되어 있어, 다른 하드웨어 플랫폼에서의 성능 보장이 부족.</li></ul></li><li><p><strong>초장문 문맥에서의 효율성 저하</strong>:</p><ul><li>1M 토큰 이상의 문맥 길이에서 정확한 Attention 계산이 비효율적일 수 있음. 향후 근사적 Attention 기법과의 통합 필요.</li></ul></li></ol><hr><h3 id=결론>결론</h3><p>이 논문은 긴 문맥 추론에서의 LLM 성능 병목을 획기적으로 해결할 수 있는 중요한 시스템 최적화 기법을 제안했습니다. 그러나, 초장문 문맥의 효율성과 다양한 하드웨어 플랫폼에서의 보편성을 강화하기 위한 추가 연구가 필요합니다.</p><h1 id=q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘>Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1><h3 id=논문의-핵심-알고리즘-ring-attention-pass-kv와-pass-q>논문의 핵심 알고리즘: Ring Attention (<code>Pass-KV</code>와 <code>Pass-Q</code>)</h3><p>논문에서는 긴 문맥을 처리할 때 효율적인 <strong>Ring Attention</strong> 알고리즘의 두 가지 변형(<code>Pass-KV</code>와 <code>Pass-Q</code>)을 제안했습니다. 이를 이해하기 쉽게 예시와 함께 설명하고, 중요한 원리를 단계별로 정리하겠습니다.</p><hr><h3 id=알고리즘의-목표>알고리즘의 목표</h3><p>긴 문맥에서의 추론은 다음 문제를 해결해야 합니다:</p><ol><li><strong>계산 복잡도</strong>: 문맥 길이에 따라 계산량이 증가.</li><li><strong>통신 병목</strong>: 여러 GPU가 데이터를 교환할 때 발생하는 대역폭 한계.</li><li><strong>부하 불균형</strong>: GPU 간 계산 및 메모리 부하의 비대칭.</li></ol><p><code>Pass-KV</code>와 <code>Pass-Q</code> 알고리즘은 각기 다른 상황에서 효율적으로 계산 및 통신을 수행하도록 설계되었습니다.</p><hr><h3 id=알고리즘-과정-예시>알고리즘 과정 예시</h3><h4 id=예시-입력-1>예시 입력</h4><ol><li><strong>문맥 길이 (( T ))</strong>: 128,000 토큰.</li><li><strong>기존 캐시된 토큰 (( P ))</strong>: 89,600 토큰.</li><li><strong>KV 캐시 누락률 (( T / (T + P) ))</strong>: 약 12.5%.</li><li><strong>GPU 개수</strong>: 4개.</li></ol><h4 id=1-pass-kv-알고리즘>1. <code>Pass-KV</code> 알고리즘</h4><ul><li><strong>적용 조건</strong>: KV 캐시 누락률이 높거나, 새로 추가된 토큰 ( T )이 상대적으로 긴 경우.</li><li><strong>핵심 아이디어</strong>: KV(Key-Value) 데이터만 이동시키고, Query는 로컬에서 처리하여 통신 크기를 줄임.</li></ul><h5 id=과정>과정:</h5><ol><li><strong>데이터 준비</strong>:<ul><li>각 GPU는 전체 토큰을 일정 부분으로 나누어 ( P + T ) 크기의 KV 데이터를 저장.</li></ul></li><li><strong>링 구조 통신</strong>:<ul><li>각 GPU는 KV 데이터를 순차적으로 이웃 GPU로 전송(<code>SendRecv</code>).</li><li>전송 중에는 병렬적으로 로컬 Query와 받은 KV 데이터를 사용해 Attention 계산 수행.</li></ul></li><li><strong>계산</strong>:<ul><li>GPU별로 입력된 Query와 KV의 부분적 Attention 값을 계산.</li></ul></li><li><strong>결과 병합</strong>:<ul><li>GPU들이 계산한 부분적 Attention 값을 병합해 최종 결과 생성.</li></ul></li></ol><h5 id=장점><strong>장점</strong>:</h5><ul><li>KV 캐시 크기가 적을수록 전송되는 데이터 크기가 줄어들어 효율적.</li><li>계산과 통신이 동시에 이루어지므로 오버헤드가 감소.</li></ul><hr><h4 id=2-pass-q-알고리즘>2. <code>Pass-Q</code> 알고리즘</h4><ul><li><strong>적용 조건</strong>: KV 캐시 누락률이 낮거나, 새로 추가된 토큰 ( T )이 짧은 경우.</li><li><strong>핵심 아이디어</strong>: Query 데이터를 GPU 간 이동시켜, 고정된 KV 캐시를 활용.</li></ul><h5 id=과정-1>과정:</h5><ol><li><strong>데이터 준비</strong>:<ul><li>각 GPU는 전체 ( P ) 크기의 KV 캐시 데이터를 로컬에 저장.</li><li>Query 데이터를 ( T / \text{GPU 개수} )로 나누어 분산.</li></ul></li><li><strong>링 구조 통신</strong>:<ul><li>GPU는 Query 데이터를 이웃 GPU로 전송(<code>SendRecv</code>).</li><li>각 GPU는 로컬 KV 캐시를 사용하여 이동한 Query와 Attention 계산 수행.</li></ul></li><li><strong>계산</strong>:<ul><li>GPU는 각 Query 조각에 대해 고정된 KV 캐시와의 부분적 Attention 값을 계산.</li></ul></li><li><strong>결과 병합</strong>:<ul><li>다른 GPU에서 계산된 Attention 결과를 <code>All-to-All</code> 통신으로 병합.</li></ul></li></ol><h5 id=장점-1><strong>장점</strong>:</h5><ul><li>Query 데이터 크기가 작으므로 통신 비용 감소.</li><li>KV 캐시가 고정되어, 여러 토큰의 반복 계산을 피할 수 있음.</li></ul><hr><h3 id=주요-차이점-비교>주요 차이점 비교</h3><table><thead><tr><th>특성</th><th><code>Pass-KV</code></th><th><code>Pass-Q</code></th></tr></thead><tbody><tr><td><strong>이동 데이터</strong></td><td>KV (Key-Value 데이터)</td><td>Query 데이터</td></tr><tr><td><strong>적용 상황</strong></td><td>KV 캐시 누락률 높음, ( T > P )</td><td>KV 캐시 누락률 낮음, ( T &lt; P )</td></tr><tr><td><strong>통신 크기</strong></td><td>KV 크기 (( N_{KV} ))</td><td>Query 크기 (( T ))</td></tr><tr><td><strong>병목 문제</strong></td><td>통신 오버헤드 줄임</td><td><code>All-to-All</code> 단계의 병목 발생</td></tr><tr><td><strong>결과 병합 방식</strong></td><td>로컬에서 병합</td><td><code>All-to-All</code> 통신 필요</td></tr></tbody></table><hr><h3 id=알고리즘-요약>알고리즘 요약</h3><ol><li><strong>Ring 구조</strong>: GPU가 원형으로 연결되어 데이터를 교환하며 동시에 계산.</li><li><strong>동적 선택</strong>:<ul><li>**<code>Pass-KV</code>**는 긴 새 입력 (( T ))이 있는 경우 유리.</li><li>**<code>Pass-Q</code>**는 KV 캐시 누락률이 낮고, 많은 캐시를 재활용할 수 있는 경우 유리.</li></ul></li><li><strong>결과 병합</strong>:<ul><li><code>Pass-KV</code>는 로컬에서 병합하여 통신 오버헤드 감소.</li><li><code>Pass-Q</code>는 <code>All-to-All</code> 통신으로 모든 결과를 병합.</li></ul></li></ol><hr><h3 id=핵심-정리>핵심 정리</h3><p><code>Pass-KV</code>와 <code>Pass-Q</code>는 상황에 맞게 데이터를 분산 및 통신하여 긴 문맥의 추론에서 효율성을 극대화합니다. <strong>링 구조를 활용한 통신 병렬화</strong>와 <strong>동적 알고리즘 선택</strong>이 주요한 혁신이며, 이를 통해 긴 문맥 추론의 병목을 효과적으로 해소합니다.</p><h1 id=q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘>Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1><h3 id=논문-결과와-특출난-점-분석>논문 결과와 특출난 점 분석</h3><p>이 논문은 **Context Parallelism (CP)**을 활용하여 긴 문맥 길이를 처리하는 데 있어 기존 방법론에 비해 <strong>지연 시간 감소</strong>, <strong>효율적 확장성</strong>, 그리고 <strong>하드웨어 리소스 최적화</strong>를 달성했습니다. 구체적인 결과와 다른 방법론 대비 특출난 점을 정리하면 다음과 같습니다:</p><hr><h3 id=1-논문-결과-요약>1. <strong>논문 결과 요약</strong></h3><h4 id=1-긴-문맥-길이-처리-성능>(1) <strong>긴 문맥 길이 처리 성능</strong></h4><ul><li><strong>1M 토큰 문맥</strong>을 <strong>128개 H100 GPU</strong>를 활용하여 <strong>77초</strong>만에 처리.</li><li><strong>128K 토큰 문맥</strong>을 4개의 GPU에서 <strong>3.8초</strong>만에 처리.</li><li>병렬화 효율성: <strong>93%</strong>, FLOPS 활용률: <strong>63%</strong>.</li></ul><h4 id=2-pass-kv와-pass-q-성능-비교>(2) <strong>Pass-KV와 Pass-Q 성능 비교</strong></h4><ul><li><strong>Persistent KV Cache Hit Rate</strong>(캐시 적중률)에 따라 최적의 방법 선택:<ul><li>캐시 적중률이 <strong>5% 이하</strong>: <code>Pass-Q</code>가 더 낮은 지연 시간을 보임.</li><li>캐시 적중률이 <strong>5% 이상</strong>: <code>Pass-KV</code>가 더 나은 성능을 보임.</li></ul></li><li>캐시 누락률이 **12.5%**인 경우 <code>Pass-KV</code>가 선택되어 전체 성능 최적화.</li></ul><h4 id=3-확장성>(3) <strong>확장성</strong></h4><ul><li>GPU 노드 수를 늘릴수록 지연 시간이 선형적으로 감소.<ul><li>예: <strong>1노드 대비 8노드 사용 시</strong> 지연 시간이 약 1/8로 감소.</li></ul></li><li><strong>128K 토큰 문맥</strong>의 경우:<ul><li><strong>Context Parallelism (CP)</strong>: 8개 노드에서 <strong>5.85초</strong>.</li><li><strong>Tensor Parallelism (TP)</strong>: 유사 조건에서 지연 시간이 <strong>100% 이상 증가</strong>.</li></ul></li></ul><hr><h3 id=2-특출난-점>2. <strong>특출난 점</strong></h3><h4 id=1-긴-문맥-추론에서의-선형-확장성>(1) <strong>긴 문맥 추론에서의 선형 확장성</strong></h4><ul><li>기존 **Tensor Parallelism (TP)**은 통신 병목으로 인해 노드 수가 증가할수록 확장성이 제한됨.</li><li>**Context Parallelism (CP)**은 링 구조와 효율적인 통신(작은 메시지 크기)으로 확장성을 극대화.<ul><li>CP는 통신 크기를 줄이면서도 <strong>계산과 통신을 동시에 수행</strong>하여 병렬화 효율성을 높임.</li></ul></li></ul><h4 id=2-효율적인-ring-attention-알고리즘>(2) <strong>효율적인 Ring Attention 알고리즘</strong></h4><ul><li><code>Pass-KV</code>와 <code>Pass-Q</code>라는 두 가지 변형을 도입하여, 상황에 맞는 최적의 계산/통신 방식을 선택.</li><li>특히, <strong>Persistent KV Cache</strong>를 활용하여 중복 계산을 방지하고, 멀티턴 대화에서도 효율적으로 작동.</li></ul><h4 id=3-상용-데이터-센터에서-검증된-성능>(3) <strong>상용 데이터 센터에서 검증된 성능</strong></h4><ul><li>RDMA(400Gb/s)와 TCP/IP(100Gb/s) 기반 네트워크 환경 모두에서 유사한 확장성과 성능을 입증.</li><li>이는 실질적인 상용 시스템에서도 적용 가능성을 보임.</li></ul><hr><h3 id=3-이러한-결과를-도출하게-한-주요-방법론>3. <strong>이러한 결과를 도출하게 한 주요 방법론</strong></h3><h4 id=1-ring-attention-pass-kv와-pass-q>(1) <strong>Ring Attention (Pass-KV와 Pass-Q)</strong></h4><ul><li><strong>Pass-KV</strong>: KV(Key-Value) 데이터를 전송하여, 대규모 Query 데이터를 로컬에서 처리.</li><li><strong>Pass-Q</strong>: Query 데이터를 전송하여 고정된 KV 캐시를 재활용.</li><li>동적 알고리즘 선택(Heuristic)으로 상황에 맞는 방식을 채택하여 성능 최적화.</li></ul><h4 id=2-계산-및-통신-부하-균형>(2) <strong>계산 및 통신 부하 균형</strong></h4><ul><li>GPU별로 토큰과 KV 데이터를 <strong>동일한 양으로 분배</strong>하여 부하 불균형 문제를 해결.</li><li>특히, 긴 문맥에서도 메모리 초과(OOM) 문제를 방지하도록 설계.</li></ul><h4 id=3-persistent-kv-cache>(3) <strong>Persistent KV Cache</strong></h4><ul><li>기존 캐시 데이터를 활용해 중복 계산을 줄이고, 다중 요청 처리 시 성능을 유지.</li><li>캐시 적중률에 따라 효율적으로 Ring Attention 알고리즘을 전환.</li></ul><hr><h3 id=4-논문에서-제시하는-이유와-나의-생각>4. <strong>논문에서 제시하는 이유와 나의 생각</strong></h3><h4 id=논문에서-제시하는-이유>논문에서 제시하는 이유:</h4><ol><li><p><strong>통신 병목 감소</strong>: 기존 TP의 경우 KV 데이터의 전체 크기를 전송해야 하며, 이는 대규모 노드 간 통신에서 병목을 유발.</p><ul><li>CP는 메시지 크기를 줄이고, 통신과 계산을 동시에 수행하여 병목을 해결.</li></ul></li><li><p><strong>계산 효율성</strong>: 기존 TP는 Query와 KV 데이터를 동시에 처리하지만, CP는 이를 분리하여 각 GPU의 계산량을 줄임.</p></li><li><p><strong>멀티턴 대화 지원</strong>: KV 캐시를 지속적으로 유지하여, 대화 히스토리를 효율적으로 처리.</p></li></ol><hr><h4 id=나의-생각>나의 생각:</h4><ol><li><p><strong>선형 확장성</strong>은 LLM 추론에서 가장 중요한 요소 중 하나이며, CP는 이 문제를 뛰어나게 해결.</p><ul><li>특히, KV 캐시를 기반으로 한 <code>Pass-KV</code>와 <code>Pass-Q</code>의 동적 선택은 실용적이고 효율적.</li></ul></li><li><p><strong>실제 하드웨어 기반 검증</strong>은 상용화 가능성을 크게 높이는 중요한 기여.</p><ul><li>대부분의 논문이 이상적인 환경에서 평가되지만, 이 논문은 RDMA와 TCP 네트워크에서 검증함.</li></ul></li><li><p><strong>한계점 보완 필요</strong>:</p><ul><li>초장문(1M 이상)에서는 정확한 Attention보다 근사적 Attention이 필요할 가능성.</li><li>디코딩 단계에서의 성능 저하 문제를 해결하기 위한 추가 연구가 요구됨.</li></ul></li></ol><hr><h3 id=결론-1>결론</h3><p>이 논문은 긴 문맥 처리에서의 병목 문제를 극복하는 데 있어 시스템 수준의 최적화를 성공적으로 구현했습니다. <strong>Context Parallelism과 Ring Attention 알고리즘</strong>은 이 분야에서 새로운 표준을 제시할 잠재력을 갖고 있으며, 실제 응용 가능성 측면에서 매우 뛰어난 결과를 보여줍니다.</p><h1 id=q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘>Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1><h3 id=논문의-입력-데이터와-추론-과정>논문의 입력 데이터와 추론 과정</h3><h4 id=예시-긴-문맥에서의-입력-데이터와-추론-과정>예시: 긴 문맥에서의 입력 데이터와 추론 과정</h4><ul><li><strong>모델</strong>: Llama3 405B (파라미터 크기: 405B, 최대 문맥 길이: 1M 토큰).</li><li><strong>입력 데이터</strong>:<ul><li>문맥 길이((T)) = 128K (128,000 토큰).</li><li>기존 캐시된 KV 길이((P)) = 89,600 (89,600 토큰).</li><li>캐시 누락률 = (T / (T + P)) = 약 12.5%.</li></ul></li><li><strong>배치 크기 ((B))</strong>: 1.</li></ul><h4 id=추론-과정>추론 과정</h4><ol><li><p><strong>입력 준비 (Prefill)</strong>:</p><ul><li>첫 번째 입력 (T=128K)는 문맥 내 모든 토큰이 서로 영향을 미치는 <strong>causal attention</strong> 연산을 필요로 함.</li><li>KV(Key, Value) 캐시는 (T+P = 128K+89.6K)로 저장됨.</li></ul></li><li><p><strong>Attention 계산</strong>:</p><ul><li>입력된 Query ((Q))는 KV 캐시((K), (V))와 상호작용하여 Attention 결과를 계산.</li><li>Attention 연산은:
[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
]
여기서 (d)는 Query와 Key의 차원.</li></ul></li><li><p><strong>Pass-KV 또는 Pass-Q의 적용</strong>:</p><ul><li><strong>Pass-KV</strong>: KV 데이터를 GPU 간 순차적으로 전송하며, Query는 로컬에서 처리.</li><li><strong>Pass-Q</strong>: Query 데이터를 전송하며, 고정된 KV 캐시를 활용해 효율적으로 계산.</li></ul></li><li><p><strong>결과 병합</strong>:</p><ul><li>GPU 간 계산된 Partial Attention 결과를 병합하여 최종 출력 생성.</li></ul></li><li><p><strong>디코딩 단계</strong>:</p><ul><li>Prefill 이후 각 토큰은 자동회귀 방식으로 생성.</li><li>새 토큰은 이전의 KV 캐시와 상호작용하며, 한 번에 하나씩 생성됨.</li></ul></li></ol><hr><h3 id=모델-아키텍처-구성>모델 아키텍처 구성</h3><h4 id=llama3-405b-모델-구성>Llama3 405B 모델 구성</h4><ul><li><strong>파라미터 수</strong>: 405B.</li><li><strong>레이어 수</strong>: 126.</li><li><strong>모델 차원 ((D))</strong>: 16,384.</li><li><strong>Feed Forward 네트워크 (FFN)</strong> 차원: 53,248.</li><li><strong>Attention Heads</strong>:<ul><li>Query Head ((N_H)): 128.</li><li>Key/Value Head ((N_{KV})): 8.</li></ul></li></ul><h4 id=주요-연산>주요 연산</h4><ol><li><p><strong>Self-Attention</strong>:</p><ul><li>(Q, K, V) 텐서 생성:
[
Q, K, V = XW_Q, XW_K, XW_V
]
((W_Q, W_K, W_V)는 Query, Key, Value의 가중치 행렬).</li><li>Attention 출력 계산:
[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
]</li></ul></li><li><p><strong>Feed Forward 네트워크 (FFN)</strong>:</p><ul><li>두 개의 완전연결 레이어와 활성화 함수(ReLU 또는 GELU)로 구성:
[
\text{FFN}(X) = \text{ReLU}(XW_1 + b_1)W_2 + b_2
]</li></ul></li><li><p><strong>Layer Normalization</strong>:</p><ul><li>각 레이어의 입력을 정규화하여 학습 안정성 제공.</li></ul></li><li><p><strong>Residual Connection</strong>:</p><ul><li>각 모듈의 출력을 입력과 더해 안정성을 높임:
[
\text{Output} = X + \text{FFN}(\text{Attention}(Q, K, V))
]</li></ul></li></ol><hr><h3 id=메모리-요구량과-컴퓨팅-복잡도>메모리 요구량과 컴퓨팅 복잡도</h3><h4 id=1-메모리-요구량>(1) <strong>메모리 요구량</strong></h4><ul><li><p><strong>KV 캐시</strong>:</p><ul><li>(T + P = 128K + 89.6K = 217.6K) 토큰.</li><li>(K, V) 텐서 크기:
[
\text{Size}(K) = \text{Size}(V) = (T + P) \times N_{KV} \times (D / N_H)
]<ul><li>(N_{KV} = 8), (N_H = 128), (D = 16,384).</li><li>(K)와 (V)의 메모리 크기는 약 (2.76 \text{GB}).</li></ul></li></ul></li><li><p><strong>GPU당 메모리</strong>:</p><ul><li>(N_{\text{GPU}} = 128)인 경우, 각 GPU는 약 (96\text{GB})의 HBM 메모리를 활용.</li></ul></li></ul><h4 id=2-컴퓨팅-복잡도>(2) <strong>컴퓨팅 복잡도</strong></h4><ol><li><p><strong>Self-Attention</strong>:</p><ul><li>Attention 연산은 문맥 길이에 대해 **O((T^2 \cdot D))**의 복잡도를 가짐.</li><li>예를 들어, (T = 128K), (D = 16,384)일 때 약 (2.7 \times 10^{12}) FLOPs.</li></ul></li><li><p><strong>Feed Forward 네트워크</strong>:</p><ul><li>FFN 연산은 **O((T \cdot D^2))**의 복잡도를 가짐.</li><li>(T = 128K), (D = 16,384)일 때 약 (1.1 \times 10^{12}) FLOPs.</li></ul></li><li><p><strong>총 FLOPs</strong>:</p><ul><li>전체 모델 추론 복잡도는 Attention과 FFN의 합으로 결정:
[
\text{Total FLOPs} = 4.9 \times 10^{18}
]</li></ul></li></ol><h4 id=3-flops-활용률>(3) <strong>FLOPS 활용률</strong></h4><ul><li><strong>1M 문맥 길이</strong>에서 128개의 H100 GPU를 활용:<ul><li>GPU당 약 (502 \text{TF/sec}) 달성.</li><li>병렬화 효율성: (93%).</li></ul></li></ul><hr><h3 id=결론-2>결론</h3><ul><li><strong>입력 데이터와 추론 과정</strong>: 문맥 길이와 캐시된 데이터에 따라 <code>Pass-KV</code>와 <code>Pass-Q</code>를 동적으로 선택하며, 이를 통해 메모리 요구량과 계산 복잡도를 효율적으로 관리.</li><li><strong>모델 아키텍처 구성</strong>: Self-Attention과 FFN 모듈로 구성되며, Llama3 405B 모델은 대규모 파라미터를 기반으로 높은 표현력을 가짐.</li><li><strong>연산 및 메모리 최적화</strong>: KV 캐시를 분산 저장하고 링 구조를 활용하여 계산과 통신을 병렬화, 긴 문맥에서도 확장성과 성능을 동시에 달성.</li></ul><h1 id=q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘>Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1><h3 id=이-논문의-한계를-극복하기-위한-연구-흐름>이 논문의 한계를 극복하기 위한 연구 흐름</h3><p>논문에서 제시한 **Context Parallelism (CP)**과 <strong>Ring Attention</strong>은 긴 문맥 처리에서 탁월한 성능을 보여주지만, 몇 가지 한계가 존재합니다. 이를 극복하기 위해 활용될 수 있는 연구 흐름은 다음과 같습니다:</p><hr><h3 id=1-kv-캐시-효율성-개선>1. <strong>KV 캐시 효율성 개선</strong></h3><h4 id=한계>한계:</h4><ul><li><strong>KV 캐시 누락률이 높을 때(<code>Pass-KV</code>)</strong>, 통신 비용이 증가하여 성능이 저하될 수 있음.</li><li>메모리 사용량이 커질수록 캐시의 효율적 관리가 어려워짐.</li></ul><h4 id=연구-흐름>연구 흐름:</h4><ol><li><p><strong>KV 캐시 압축</strong>:</p><ul><li><strong>양자화(Quantization)</strong>:<ul><li>KV 캐시를 INT4, FP8과 같은 저정밀 형식으로 변환하여 메모리 사용량 감소.</li><li>예: KVQuant(2024)에서 3-bit 또는 INT4 형식으로 최대 4배의 메모리 절약을 달성.</li></ul></li><li><strong>압축 알고리즘</strong>:<ul><li>반복 사용되지 않는 KV 데이터를 제거하거나 적응적으로 압축.</li></ul></li></ul></li><li><p><strong>동적 캐시 관리</strong>:</p><ul><li><strong>가장 자주 사용되는 캐시 유지</strong>(LRU, Least Recently Used 방식).</li><li>사용자 맥락에 따라 중요도가 낮은 데이터를 캐시에서 제거.</li></ul></li><li><p><strong>Sparse Attention 활용</strong>:</p><ul><li>필요하지 않은 KV를 제거하여 연산과 메모리 비용을 줄임.</li><li>예: Longformer, BigBird 등에서 사용된 로컬 및 전역 토큰 Attention 방식을 결합.</li></ul></li></ol><hr><h3 id=2-초장문-문맥에서-효율성-문제>2. <strong>초장문 문맥에서 효율성 문제</strong></h3><h4 id=한계-1>한계:</h4><ul><li>문맥 길이가 1M 이상으로 증가하면 Attention 연산이 **O((T^2))**의 계산 복잡도를 가지며, 성능이 급격히 저하될 가능성.</li><li>모든 과거 토큰에 대한 Attention은 대부분 불필요한 계산일 수 있음.</li></ul><h4 id=연구-흐름-1>연구 흐름:</h4><ol><li><p><strong>근사 Attention(Approximate Attention)</strong>:</p><ul><li><strong>기억 기반 Retrieval</strong>:<ul><li>전체 문맥을 유지하는 대신, 중요 토큰만 선택하여 Attention 수행.</li><li>예: Reformer(2020)에서 **Locally Sensitive Hashing (LSH)**를 사용해 관련성 높은 토큰만 검색.</li></ul></li><li><strong>Chunking 및 Sliding Window</strong>:<ul><li>긴 문맥을 작은 청크로 분리하고 필요한 청크 간의 상호작용만 계산.</li></ul></li></ul></li><li><p><strong>히스토리 요약(Summarization)</strong>:</p><ul><li>긴 문맥의 과거 히스토리를 요약하여 중요한 정보만 유지.</li><li>요약 정보를 새로운 입력으로 제공하여 문맥 길이를 동적으로 조절.</li></ul></li><li><p><strong>Hierarchical Attention</strong>:</p><ul><li>로컬 Attention과 전역 Attention을 계층적으로 결합.</li><li>예: Perceiver 모델이 이러한 구조를 활용하여 효율성을 극대화.</li></ul></li></ol><hr><h3 id=3-디코딩-단계-성능-최적화>3. <strong>디코딩 단계 성능 최적화</strong></h3><h4 id=한계-2>한계:</h4><ul><li>디코딩 과정에서 토큰이 순차적으로 생성되므로, 병렬화 효과가 감소.</li><li><code>Pass-Q</code> 방식에서는 <code>All-to-All</code> 통신이 병목으로 작용.</li></ul><h4 id=연구-흐름-2>연구 흐름:</h4><ol><li><p><strong>병렬 디코딩(Parallel Decoding)</strong>:</p><ul><li><strong>Prefix Smoothing</strong>:<ul><li>Auto-regressive 디코딩에서 모든 토큰의 히스토리를 동적으로 예측하여 병렬화.</li><li>예: Flash Decoding(2024)에서 이러한 기술 활용.</li></ul></li></ul></li><li><p><strong>계층적 KV 캐시 분산</strong>:</p><ul><li>KV 캐시를 GPU 내에서 계층적으로 배치하여 병목을 줄임.</li><li>예: Mooncake(2024)에서 <strong>KVCentric Disaggregated Architecture</strong>를 제안.</li></ul></li><li><p><strong>캐시 공유 및 효율적 업데이트</strong>:</p><ul><li>디코딩 중 새로 생성되는 KV 데이터만 캐시에 추가하고 불필요한 캐시를 공유.</li></ul></li></ol><hr><h3 id=4-하드웨어-의존성-완화>4. <strong>하드웨어 의존성 완화</strong></h3><h4 id=한계-3>한계:</h4><ul><li>CP는 H100 GPU와 NVLink를 활용한 환경에서 최적화되어 있어, 다른 하드웨어 환경에서는 성능이 저하될 가능성.</li></ul><h4 id=연구-흐름-3>연구 흐름:</h4><ol><li><p><strong>모듈식 하드웨어 추상화</strong>:</p><ul><li>CPU, GPU, TPU와 같은 다양한 하드웨어에서 사용할 수 있는 하드웨어 독립적 알고리즘 설계.</li><li>예: PyTorch/XLA, TensorFlow와 같은 하드웨어 가속기 지원 라이브러리와 통합.</li></ul></li><li><p><strong>새로운 하드웨어 아키텍처 활용</strong>:</p><ul><li><strong>전용 LLM 하드웨어</strong>:<ul><li>GPU가 아닌 AI 특화 가속기를 사용해 모델 병렬화를 더 효율적으로 수행.</li></ul></li><li><strong>메모리 대역폭 최적화</strong>:<ul><li>RDMA 네트워크가 아닌 새로운 메모리 액세스 기법 도입.</li></ul></li></ul></li><li><p><strong>저비용 환경 최적화</strong>:</p><ul><li>TCP 기반 상용 데이터센터에서 대역폭 병목을 줄이기 위한 압축 기반 통신 설계.</li></ul></li></ol><hr><h3 id=5-상용-애플리케이션-통합>5. <strong>상용 애플리케이션 통합</strong></h3><h4 id=한계-4>한계:</h4><ul><li>멀티턴 대화에서의 실시간 반응 속도 개선 및 비용 절감 방안 필요.</li></ul><h4 id=연구-흐름-4>연구 흐름:</h4><ol><li><p><strong>멀티턴 히스토리 최적화</strong>:</p><ul><li>각 대화 히스토리의 중요도 평가 후, <strong>요약된 정보만 캐시</strong>.</li><li>히스토리 데이터를 <strong>순차적으로 축소</strong>하여 새로운 입력과 결합.</li></ul></li><li><p><strong>사용자 맞춤형 최적화</strong>:</p><ul><li>사용자 별로 적합한 문맥 길이 및 캐시 정책을 설정.</li><li>중요 문맥 정보만 선택적으로 모델에 제공.</li></ul></li><li><p><strong>추론-디코딩 분리</strong>:</p><ul><li>추론 과정과 디코딩 과정을 분리하여 독립적으로 최적화.</li><li>예: DistServe(2024)에서 추론과 디코딩을 분리하여 Goodput을 최적화.</li></ul></li></ol><hr><h3 id=종합>종합</h3><p>이 논문의 한계를 극복하기 위해서는 <strong>KV 캐시 효율성</strong>, <strong>초장문 문맥 처리</strong>, <strong>디코딩 병렬화</strong>, 그리고 <strong>하드웨어 독립성</strong>을 중심으로 연구가 필요합니다. 이러한 방향은 LLM의 추론 비용을 줄이고, 긴 문맥에서도 실시간 성능을 유지하며, 상용 시스템에서도 쉽게 적용 가능하도록 설계되는 흐름을 보이고 있습니다.</p><h1 id=q---논문에서-제안한-pass-kv와-pass-q-알고리즘의-선택-기준이-되는-heuristic-모델은-실제로-어떻게-구현되었으며-이를-다른-상황에-적용하려면-어떤-추가적인-조정이-필요한가>Q : 논문에서 제안한 Pass-KV와 Pass-Q 알고리즘의 선택 기준이 되는 Heuristic 모델은 실제로 어떻게 구현되었으며, 이를 다른 상황에 적용하려면 어떤 추가적인 조정이 필요한가?</h1><h3 id=논문에서-제안한-pass-kv와-pass-q-알고리즘의-선택-기준-heuristic-모델>논문에서 제안한 Pass-KV와 Pass-Q 알고리즘의 선택 기준 (Heuristic 모델)</h3><p>논문에서 <strong>Pass-KV</strong>와 <strong>Pass-Q</strong> 알고리즘은 <strong>Persistent KV Cache Hit Rate</strong>(캐시 적중률)과 <strong>문맥 길이</strong>에 따라 동적으로 선택됩니다. 이 선택을 수행하는 Heuristic 모델은 아래와 같은 수식을 기반으로 설계되었습니다:</p><h4 id=1-heuristic-모델의-기본-수식>1. Heuristic 모델의 기본 수식</h4><p>알고리즘은 다음 두 조건을 평가하여 <strong>Pass-KV</strong>와 <strong>Pass-Q</strong>를 선택합니다:</p><ol><li><p><strong>조건 1</strong>:
[
T \geq \frac{N \cdot C \cdot N_{KV} \cdot e}{2 \cdot N_H \cdot BW}
]</p><ul><li>(T): 새로운 입력 토큰 수.</li><li>(N): 노드 수.</li><li>(C): GPU의 계산 성능(테라플롭스).</li><li>(N_{KV}): Key/Value Head 수.</li><li>(N_H): Query Head 수.</li><li>(BW): 네트워크 대역폭.</li><li>(e): 데이터 타입의 크기(바이트).</li></ul></li><li><p><strong>조건 2</strong>:
[
\frac{T}{T + P} \geq 2 \cdot \frac{N_{KV}}{N_H} - \frac{4 \cdot T \cdot BW}{N \cdot C \cdot e}
]</p><ul><li>(P): 기존 캐시된 KV 길이.</li><li>(T/(T + P)): 캐시 누락률.</li></ul></li></ol><p>위 두 조건 중 하나라도 만족하면 <strong>Pass-KV</strong>가 선택되고, 만족하지 않으면 <strong>Pass-Q</strong>가 선택됩니다.</p><hr><h3 id=heuristic-모델-구현-과정>Heuristic 모델 구현 과정</h3><ol><li><p><strong>동적 계산</strong>:</p><ul><li>모델은 입력 데이터의 (T)와 (P), 하드웨어 사양(예: GPU FLOPS, 네트워크 대역폭)을 기반으로 조건을 평가.</li><li>캐시 적중률((T/(T + P)))이 높을 경우 <code>Pass-KV</code>, 적중률이 낮을 경우 <code>Pass-Q</code>를 선택.</li></ul></li><li><p><strong>정적 임계값 계산</strong>:</p><ul><li>(N), (C), (BW), (N_{KV}), (N_H), (e)와 같은 하드웨어 및 모델 파라미터를 기반으로 조건의 임계값을 미리 계산.</li><li>예를 들어, <strong>128K 문맥 길이</strong>에서 (N=4), (P=89.6K), 캐시 누락률이 5%일 경우 <code>Pass-KV</code>가 더 나은 성능을 보이는 것으로 분석됨.</li></ul></li><li><p><strong>경험적 조정</strong>:</p><ul><li>논문에서는 조건의 이론적 계산 외에도, 실제 실험 데이터를 기반으로 최적의 임계값을 경험적으로 조정.</li></ul></li></ol><hr><h3 id=다른-상황에-적용하기-위한-추가-조정>다른 상황에 적용하기 위한 추가 조정</h3><ol><li><p><strong>하드웨어 사양 변화</strong>:</p><ul><li>GPU 성능((C))이나 네트워크 대역폭((BW))이 달라질 경우:<ul><li>조건식에서 (C)와 (BW)를 기반으로 새 임계값 계산 필요.</li><li>네트워크가 TCP 기반이라면 낮은 (BW)에 따른 추가 최적화가 필요.</li></ul></li></ul></li><li><p><strong>모델 아키텍처 차이</strong>:</p><ul><li>다른 모델에서 **Query Head((N_H))**와 <strong>Key/Value Head((N_{KV}))</strong> 비율이 다르다면:<ul><li>조건식의 (N_{KV}/N_H) 항목을 해당 모델에 맞게 조정.</li></ul></li></ul></li><li><p><strong>문맥 길이 및 캐시 정책 변화</strong>:</p><ul><li>문맥 길이((T + P))가 증가하거나 캐시 정책이 변경되면:<ul><li>캐시 누락률((T/(T + P)))이 변하므로, 새로운 조건 계산이 필요.</li><li>캐시의 크기를 제한하거나 동적 압축 정책을 추가하여 조건 재설계.</li></ul></li></ul></li><li><p><strong>근사 Attention 도입</strong>:</p><ul><li>긴 문맥에서 근사 Attention(예: Sparse Attention, Retrieval 기반 Attention)을 사용하는 경우:<ul><li>KV 캐시 크기가 줄어들고 통신 크기도 감소하므로, 조건식에서 (P)와 (T)의 비율을 재평가.</li></ul></li></ul></li><li><p><strong>다중 사용자 시나리오</strong>:</p><ul><li>다중 사용자가 같은 모델을 공유하는 환경에서는:<ul><li>KV 캐시가 사용자 간에 분산될 수 있으므로, 캐시 적중률 분석을 사용자별로 세분화.</li></ul></li></ul></li></ol><hr><h3 id=추가적인-통찰>추가적인 통찰</h3><ul><li>이 Heuristic 모델은 <strong>정적 조건</strong>과 <strong>동적 평가</strong>를 결합하여 효율성을 극대화합니다.</li><li>다른 상황에서도 <strong>하드웨어 사양</strong>, <strong>모델 구조</strong>, <strong>문맥 길이</strong>만 명확히 정의되면 쉽게 확장 가능.</li><li>그러나 <strong>All-to-All 통신 비용</strong>이나 <strong>디코딩 병목</strong> 같은 문제는 추가적인 최적화 연구가 필요합니다. 이를 해결하기 위해 <strong>통신 비용 예측 모델</strong>이나 <strong>캐시 데이터 분산 최적화</strong>를 도입할 수 있습니다.</li></ul><h1 id=q--context-parallelism이-기존-tensor-parallelism과-비교해-긴-문맥-처리에서-더-높은-병렬화-효율성을-제공하는-이유는-무엇이며-이-접근법이-디코딩-단계에서-병목을-완전히-해결하지-못한-이유는-무엇인가>Q : Context Parallelism이 기존 Tensor Parallelism과 비교해 긴 문맥 처리에서 더 높은 병렬화 효율성을 제공하는 이유는 무엇이며, 이 접근법이 디코딩 단계에서 병목을 완전히 해결하지 못한 이유는 무엇인가?</h1><h3 id=context-parallelismcp이-긴-문맥-처리에서-tensor-parallelismtp-대비-높은-병렬화-효율성을-제공하는-이유>Context Parallelism(CP)이 긴 문맥 처리에서 Tensor Parallelism(TP) 대비 높은 병렬화 효율성을 제공하는 이유</h3><p>**Tensor Parallelism(TP)**과 **Context Parallelism(CP)**의 주요 차이는 데이터 및 작업 분배 방식에 있습니다. CP가 긴 문맥에서 더 높은 병렬화 효율성을 제공하는 이유는 다음과 같습니다.</p><hr><h3 id=1-통신-비용-감소>1. <strong>통신 비용 감소</strong></h3><h4 id=tensor-parallelismtp의-한계>Tensor Parallelism(TP)의 한계:</h4><ul><li>TP는 <strong>모델의 가중치</strong>를 여러 GPU에 분산하고, 각 레이어의 연산 후 <strong>AllReduce</strong>를 통해 통신합니다.</li><li>긴 문맥 길이((T))에서:<ul><li><strong>전체 문맥 길이에 비례한 대규모 데이터 교환</strong> 발생.</li><li>특히, TP는 <strong>선형 레이어</strong>에서 각 노드가 Query Head((N_H)) 전체를 처리하고, 결과를 동기화하므로 통신 비용이 높아집니다.</li></ul></li></ul><h4 id=context-parallelismcp의-장점>Context Parallelism(CP)의 장점:</h4><ul><li>CP는 **입력 토큰((T))**을 각 GPU에 분산하여 처리.</li><li>Attention 연산에서 필요한 **Query, Key, Value(QKV)**를 GPU 간 교환하지만, 교환 데이터 크기는 **KV Head((N_{KV}))**에 비례해 작음:<ul><li>(N_{KV} &lt; N_H) (예: Llama3 모델에서 (N_H = 128), (N_{KV} = 8)).</li><li>통신 크기가 약 (16\times) 작아짐.</li></ul></li></ul><hr><h3 id=2-계산-및-메모리-부하-균형>2. <strong>계산 및 메모리 부하 균형</strong></h3><h4 id=tensor-parallelismtp의-한계-1>Tensor Parallelism(TP)의 한계:</h4><ul><li>모든 토큰을 각 노드에서 처리하므로, 긴 문맥 길이에서는 <strong>각 GPU의 메모리 부담이 증가</strong>.</li><li>TP는 모델 가중치 크기를 분산하지만, 긴 문맥의 Key/Value 캐시는 각 GPU에 동일하게 저장해야 하므로 <strong>메모리 병목</strong>이 발생.</li></ul><h4 id=context-parallelismcp의-장점-1>Context Parallelism(CP)의 장점:</h4><ul><li><strong>KV 캐시를 GPU 간 분산 저장</strong>:<ul><li>(T + P) 길이의 KV 캐시를 GPU에 균등하게 분배해 각 GPU의 메모리 사용량 감소.</li></ul></li><li><strong>입력 토큰 분할</strong>로 계산 부하를 GPU 간 균형 있게 배분.</li></ul><hr><h3 id=3-병렬화-효율성>3. <strong>병렬화 효율성</strong></h3><h4 id=tensor-parallelismtp의-한계-2>Tensor Parallelism(TP)의 한계:</h4><ul><li>GPU 간 통신 시간이 증가하며, 노드가 많아질수록 병렬화 효율성이 급격히 감소.</li><li>특히, (N > 8)일 때 TP의 성능은 <strong>통신 병목</strong>으로 인해 감소하는 경향.</li></ul><h4 id=context-parallelismcp의-장점-2>Context Parallelism(CP)의 장점:</h4><ul><li>CP는 <strong>링 구조 통신</strong>을 통해 (N)개의 GPU가 순차적으로 데이터를 교환:<ul><li>통신은 연산과 겹치게 설계되어, <strong>통신 오버헤드가 숨겨짐</strong>.</li></ul></li><li>긴 문맥에서 CP는 선형 확장을 달성:<ul><li>(128K) 문맥에서 GPU 노드 수를 1→8로 늘리면, 지연 시간이 약 (1/8)로 감소.</li></ul></li></ul><hr><h3 id=cp가-디코딩-단계에서-병목을-완전히-해결하지-못한-이유>CP가 디코딩 단계에서 병목을 완전히 해결하지 못한 이유</h3><p><strong>디코딩 단계</strong>는 모델이 <strong>Auto-Regressive</strong> 방식으로 <strong>한 번에 하나의 토큰</strong>을 생성하므로, 고유한 병목 문제가 존재합니다.</p><hr><h3 id=1-auto-regressive의-본질적-순차성>1. <strong>Auto-Regressive의 본질적 순차성</strong></h3><ul><li>디코딩은 이전에 생성된 토큰을 기반으로 <strong>새 토큰을 하나씩 생성</strong>:<ul><li>각 단계에서 이전 토큰 정보를 필요로 하므로 <strong>병렬화에 한계</strong>.</li></ul></li><li>CP의 입력 토큰 분할 방식은 디코딩 단계에서 사용할 새로운 토큰((T=1))이 항상 GPU 간 통신을 유발.</li></ul><h4 id=결과>결과:</h4><ul><li>(T=1)인 디코딩 단계에서는 각 GPU의 계산량이 작아지고 통신 오버헤드가 상대적으로 더 커짐.</li></ul><hr><h3 id=2-all-to-all-통신의-병목>2. <strong>All-to-All 통신의 병목</strong></h3><ul><li><strong>Pass-Q</strong> 방식:<ul><li>Query 데이터를 각 GPU로 전송하여 계산 후 <strong>All-to-All</strong> 통신을 통해 Attention 결과를 병합.</li><li>All-to-All 통신은 GPU 간 전체 데이터 교환이 필요하므로 디코딩에서 병목을 초래.</li></ul></li><li>긴 문맥에서는 All-to-All 통신이 더 자주 발생하므로 병목이 심화.</li></ul><hr><h3 id=3-kv-캐시-갱신-문제>3. <strong>KV 캐시 갱신 문제</strong></h3><ul><li>디코딩 단계에서는 새 토큰이 생성될 때마다 <strong>KV 캐시를 갱신</strong>:<ul><li>갱신된 KV 데이터는 GPU 간 <strong>링 구조</strong>를 통해 전달되어야 함.</li><li>각 GPU의 KV 캐시 크기가 증가하면서 <strong>통신 및 메모리 오버헤드</strong> 발생.</li></ul></li></ul><hr><h3 id=디코딩-병목-해결을-위한-추가-방안>디코딩 병목 해결을 위한 추가 방안</h3><ol><li><p><strong>병렬 디코딩 기법</strong>:</p><ul><li>여러 토큰을 병렬로 생성하기 위해, <strong>Prefix Smoothing</strong>과 같은 예측 기반 디코딩 기법 도입.</li></ul></li><li><p><strong>KV 캐시 최적화</strong>:</p><ul><li>KV 캐시를 GPU 간 동적으로 재분배하거나, 중요성이 낮은 캐시 데이터를 제거.</li></ul></li><li><p><strong>근사 Attention 활용</strong>:</p><ul><li>과거 모든 토큰을 사용하는 대신, 중요 토큰만 선택하여 Attention 연산 수행.</li></ul></li></ol><hr><h3 id=결론-3>결론</h3><p>**Context Parallelism(CP)**는 긴 문맥 처리에서 <strong>통신 비용 감소</strong>, <strong>계산 부하 균형</strong>, <strong>효율적인 병렬화</strong>를 통해 Tensor Parallelism(TP) 대비 뛰어난 성능을 보입니다. 그러나 디코딩 단계에서의 병목은 Auto-Regressive 디코딩의 순차적 특성과 All-to-All 통신의 한계로 인해 완전히 해결되지 않았습니다. 이를 개선하려면 <strong>병렬 디코딩 기법</strong>, <strong>KV 캐시 최적화</strong>, 그리고 <strong>근사 Attention</strong> 도입과 같은 추가 연구가 필요합니다.</p><h1 id=q---kv-캐시-크기와-gpu-메모리-사용량-간의-균형을-최적화하기-위한-설계에서-ring-attention의-데이터-분산-및-통신-전략은-구체적으로-어떤-방식으로-구현되었는가>Q : KV 캐시 크기와 GPU 메모리 사용량 간의 균형을 최적화하기 위한 설계에서 Ring Attention의 데이터 분산 및 통신 전략은 구체적으로 어떤 방식으로 구현되었는가?</h1><h3 id=kv-캐시-크기와-gpu-메모리-사용량-간의-균형-최적화를-위한-ring-attention의-데이터-분산-및-통신-전략>KV 캐시 크기와 GPU 메모리 사용량 간의 균형 최적화를 위한 Ring Attention의 데이터 분산 및 통신 전략</h3><p>논문에서 제안된 <strong>Ring Attention</strong>은 긴 문맥 추론 시 발생하는 KV 캐시 크기와 GPU 메모리 사용량 간의 균형을 맞추기 위해 <strong>데이터 분산</strong>과 <strong>효율적인 통신 전략</strong>을 활용합니다. 이 접근 방식은 계산 부하와 메모리 사용량을 최적화하여 긴 문맥 추론의 병목을 해결합니다.</p><hr><h3 id=ring-attention의-핵심-설계>Ring Attention의 핵심 설계</h3><p>Ring Attention은 <strong>Pass-KV</strong>와 <strong>Pass-Q</strong>라는 두 가지 변형 알고리즘으로 구현됩니다. 두 방식 모두 <strong>링 구조 통신</strong>과 <strong>데이터 샤딩</strong>을 결합하여 GPU 메모리와 통신 비용을 최적화합니다.</p><hr><h3 id=1-데이터-분산-방식>1. <strong>데이터 분산 방식</strong></h3><h4 id=1-kv-캐시의-균등-분산>(1) KV 캐시의 균등 분산</h4><ul><li>긴 문맥에서는 KV 캐시 크기((P + T))가 선형적으로 증가하므로, 이를 GPU 간에 균등하게 분산하여 메모리 부하를 줄입니다.</li><li>입력 데이터((T))는 토큰 단위로 나누어 각 GPU에 분배되고, KV 캐시((P))는 다음과 같이 분산 저장됩니다:<ul><li><strong>Ring Partitioning</strong>:<ul><li>(T + P)의 전체 길이를 (N)개의 GPU로 나누고, 각 GPU는 두 개의 청크를 처리.</li><li>예: GPU (i)는 (C_i)와 (C_{2N-i-1})를 담당.</li></ul></li></ul></li></ul><h4 id=2-query와-kv의-독립적-샤딩>(2) Query와 KV의 독립적 샤딩</h4><ul><li><strong>Pass-KV</strong>:<ul><li>Query((Q))는 로컬에서 처리하며, KV(Key, Value) 데이터를 GPU 간 순차적으로 전송.</li></ul></li><li><strong>Pass-Q</strong>:<ul><li>KV 데이터는 로컬에 고정하고, Query 데이터를 GPU 간 전송.</li></ul></li><li>이를 통해, 데이터 전송량을 최소화하며 계산 부하를 분산.</li></ul><hr><h3 id=2-통신-전략>2. <strong>통신 전략</strong></h3><h4 id=1-링-구조-통신-ring-communication>(1) 링 구조 통신 (Ring Communication)</h4><ul><li>GPU 간 데이터 교환은 <strong>순환 구조</strong>(ring)를 통해 이루어집니다.</li><li>(N)개의 GPU가 서로 연결되어 데이터를 순차적으로 전달:<ol><li>GPU (i)는 Query 또는 KV 데이터를 GPU (i+1)로 전송.</li><li>데이터 전송과 Attention 계산을 병렬로 수행하여 통신 오버헤드를 숨김.</li><li>마지막 GPU까지 전송 후, 모든 GPU가 최종 Attention 값을 계산.</li></ol></li></ul><h4 id=2-all-to-all-통신-pass-q에서-사용>(2) All-to-All 통신 (Pass-Q에서 사용)</h4><ul><li><strong>Pass-Q</strong> 방식에서는 Query 데이터를 GPU 간 분산한 후, 계산된 Partial Attention 결과를 병합하기 위해 <strong>All-to-All 통신</strong>이 필요.</li><li>이 과정에서 각 GPU가 다른 GPU로부터 Partial Attention 값을 수집하여 최종 결과를 생성.</li></ul><h4 id=3-통신-오버헤드-최소화>(3) 통신 오버헤드 최소화</h4><ul><li>GPU 간 데이터 전송량은 (T)와 (N_{KV})에 따라 달라짐:<ul><li><strong>Pass-KV</strong>: KV 데이터 크기((T \times N_{KV})).</li><li><strong>Pass-Q</strong>: Query 데이터 크기((T \times N_H)).</li><li>일반적으로 (N_{KV} \ll N_H)이므로 Pass-KV가 긴 문맥에서 유리.</li></ul></li></ul><hr><h3 id=3-메모리-사용량-최적화>3. <strong>메모리 사용량 최적화</strong></h3><h4 id=1-kv-캐시-분산-저장>(1) KV 캐시 분산 저장</h4><ul><li>GPU 간 KV 캐시를 나누어 저장하여, 개별 GPU의 메모리 부하를 줄임.</li><li><strong>동적 부하 균형</strong>:<ul><li>각 GPU는 요청에 따라 Query 또는 KV 데이터를 처리하며, 긴 문맥에서도 메모리 초과(OOM) 방지.</li></ul></li></ul><h4 id=2-캐시-갱신-및-유지>(2) 캐시 갱신 및 유지</h4><ul><li>디코딩 과정에서 새로 생성된 KV 데이터는 GPU 간 분산하여 저장:<ul><li>필요 없는 캐시를 제거하거나, 중요도가 높은 KV 데이터를 우선 유지.</li></ul></li></ul><h4 id=3-양자화-및-데이터-압축>(3) 양자화 및 데이터 압축</h4><ul><li>FP8, INT4와 같은 저정밀 데이터 형식을 사용해 KV 캐시의 메모리 사용량을 줄임.</li></ul><hr><h3 id=4-ring-attention-구현-방식-요약>4. <strong>Ring Attention 구현 방식 요약</strong></h3><ol><li><p><strong>Pass-KV 알고리즘</strong>:</p><ul><li>GPU는 로컬에서 Query를 처리하고, KV 데이터를 링 구조로 순차 전송.</li><li>통신량이 (T \times N_{KV})로 제한되어 긴 문맥에서 효율적.</li></ul></li><li><p><strong>Pass-Q 알고리즘</strong>:</p><ul><li>GPU는 로컬에서 KV 데이터를 고정하고, Query 데이터를 링 구조로 전송.</li><li>통신량이 (T \times N_H)로 늘어나므로, 짧은 문맥 또는 캐시 적중률이 높은 상황에서 유리.</li></ul></li><li><p><strong>통신 오버랩</strong>:</p><ul><li>데이터 전송과 Attention 계산을 동시에 수행하여 통신 오버헤드를 최소화.</li></ul></li><li><p><strong>결과 병합</strong>:</p><ul><li>Partial Attention 결과를 병합하기 위해 All-to-All 또는 로컬 병합 연산 수행.</li></ul></li></ol><hr><h3 id=결론-4>결론</h3><p>Ring Attention의 데이터 분산 및 통신 전략은 긴 문맥 처리에서 GPU 메모리 사용량과 계산 부하를 효율적으로 관리하며, 통신 오버헤드를 최소화합니다. 이를 통해, 긴 문맥 추론에서 기존 방법론 대비 뛰어난 확장성과 효율성을 제공합니다. <strong>Pass-KV</strong>와 <strong>Pass-Q</strong>의 선택은 KV 캐시 크기와 문맥 길이에 따라 동적으로 조정되어, 각 상황에서 최적의 성능을 발휘합니다.</p></article></main><footer class="mx-auto max-w-4xl px-4 py-8"><div class="px-6 py-6"><nav class=mb-6><div class="flex flex-wrap items-center justify-center gap-1"><a href=/index.xml class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="RSS Feed"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18.0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z"/></svg>
<span>RSS Feed</span></a></div></nav><div class="border-border mb-6 border-t"></div><div class="flex flex-col items-center justify-between gap-4 md:flex-row"><div class="text-muted-foreground text-sm"><p>&copy; 2025 Jaehun's Blog.
모든 권리 보유.</p></div><div class="flex items-center gap-3"><a href=https://github.com/ryujaehun target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=GitHub aria-label=GitHub><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.4 5.4.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
</a><a href=https://www.linkedin.com/in/jaehunryu/ target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Linkedin aria-label=Linkedin><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
</a><a href=mailto:jaehunryu@icloud.com class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Email aria-label=Email><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Email"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22.0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v10a2 2 0 002 2z"/></svg></a></div></div></div></footer><div id=dock class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0" role=toolbar aria-label="바로가기 도구 모음"><nav class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3"><button id=dock-back class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=뒤로 aria-label=뒤로>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m12 19-7-7 7-7"/><path d="M19 12H5"/></svg>
<span class="hidden sm:inline">뒤로</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-toc class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=목차 aria-label=목차>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg>
<span class="hidden sm:inline">목차</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-search class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2" title=검색 aria-label=검색>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg>
<span class="hidden md:inline">검색</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-top class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title="맨 위로" aria-label="맨 위로">
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m18 15-6-6-6 6"/></svg>
<span class="hidden sm:inline">맨 위로</span></button></nav></div><div id=toc-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300" role=dialog aria-modal=true aria-labelledby=toc-title></div><div id=toc-card class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2" role=dialog aria-modal=true aria-labelledby=toc-title><div class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md"><div class="border-border bg-muted/30 flex items-center justify-between border-b p-4"><div class="flex items-center gap-3"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg><h2 id=toc-title class="text-foreground text-lg font-semibold">목차</h2></div><button id=toc-close class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div class="max-h-96 overflow-y-auto p-4"><nav id=toc-nav class=toc-scrollbar aria-label="글 목차"><div id=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#강점-및-독창적-기여>강점 및 독창적 기여</a></li><li><a href=#핵심-알고리즘-설명-ring-attention-pass-kv와-pass-q>핵심 알고리즘 설명: Ring Attention (<code>Pass-KV</code>와 <code>Pass-Q</code>)</a><ul><li><a href=#예시-입력>예시 입력:</a></li><li><a href=#pass-kv-과정><code>Pass-KV</code> 과정:</a></li><li><a href=#pass-q-과정><code>Pass-Q</code> 과정:</a></li><li><a href=#선택-기준>선택 기준:</a></li></ul></li><li><a href=#논문의-한계>논문의 한계</a></li><li><a href=#결론>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#논문의-핵심-알고리즘-ring-attention-pass-kv와-pass-q>논문의 핵심 알고리즘: Ring Attention (<code>Pass-KV</code>와 <code>Pass-Q</code>)</a></li><li><a href=#알고리즘의-목표>알고리즘의 목표</a></li><li><a href=#알고리즘-과정-예시>알고리즘 과정 예시</a><ul><li><a href=#예시-입력-1>예시 입력</a></li><li><a href=#1-pass-kv-알고리즘>1. <code>Pass-KV</code> 알고리즘</a></li><li><a href=#2-pass-q-알고리즘>2. <code>Pass-Q</code> 알고리즘</a></li></ul></li><li><a href=#주요-차이점-비교>주요 차이점 비교</a></li><li><a href=#알고리즘-요약>알고리즘 요약</a></li><li><a href=#핵심-정리>핵심 정리</a></li></ul></li></ul><ul><li><ul><li><a href=#논문-결과와-특출난-점-분석>논문 결과와 특출난 점 분석</a></li><li><a href=#1-논문-결과-요약>1. <strong>논문 결과 요약</strong></a><ul><li><a href=#1-긴-문맥-길이-처리-성능>(1) <strong>긴 문맥 길이 처리 성능</strong></a></li><li><a href=#2-pass-kv와-pass-q-성능-비교>(2) <strong>Pass-KV와 Pass-Q 성능 비교</strong></a></li><li><a href=#3-확장성>(3) <strong>확장성</strong></a></li></ul></li><li><a href=#2-특출난-점>2. <strong>특출난 점</strong></a><ul><li><a href=#1-긴-문맥-추론에서의-선형-확장성>(1) <strong>긴 문맥 추론에서의 선형 확장성</strong></a></li><li><a href=#2-효율적인-ring-attention-알고리즘>(2) <strong>효율적인 Ring Attention 알고리즘</strong></a></li><li><a href=#3-상용-데이터-센터에서-검증된-성능>(3) <strong>상용 데이터 센터에서 검증된 성능</strong></a></li></ul></li><li><a href=#3-이러한-결과를-도출하게-한-주요-방법론>3. <strong>이러한 결과를 도출하게 한 주요 방법론</strong></a><ul><li><a href=#1-ring-attention-pass-kv와-pass-q>(1) <strong>Ring Attention (Pass-KV와 Pass-Q)</strong></a></li><li><a href=#2-계산-및-통신-부하-균형>(2) <strong>계산 및 통신 부하 균형</strong></a></li><li><a href=#3-persistent-kv-cache>(3) <strong>Persistent KV Cache</strong></a></li></ul></li><li><a href=#4-논문에서-제시하는-이유와-나의-생각>4. <strong>논문에서 제시하는 이유와 나의 생각</strong></a><ul><li><a href=#논문에서-제시하는-이유>논문에서 제시하는 이유:</a></li><li><a href=#나의-생각>나의 생각:</a></li></ul></li><li><a href=#결론-1>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#논문의-입력-데이터와-추론-과정>논문의 입력 데이터와 추론 과정</a><ul><li><a href=#예시-긴-문맥에서의-입력-데이터와-추론-과정>예시: 긴 문맥에서의 입력 데이터와 추론 과정</a></li><li><a href=#추론-과정>추론 과정</a></li></ul></li><li><a href=#모델-아키텍처-구성>모델 아키텍처 구성</a><ul><li><a href=#llama3-405b-모델-구성>Llama3 405B 모델 구성</a></li><li><a href=#주요-연산>주요 연산</a></li></ul></li><li><a href=#메모리-요구량과-컴퓨팅-복잡도>메모리 요구량과 컴퓨팅 복잡도</a><ul><li><a href=#1-메모리-요구량>(1) <strong>메모리 요구량</strong></a></li><li><a href=#2-컴퓨팅-복잡도>(2) <strong>컴퓨팅 복잡도</strong></a></li><li><a href=#3-flops-활용률>(3) <strong>FLOPS 활용률</strong></a></li></ul></li><li><a href=#결론-2>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#이-논문의-한계를-극복하기-위한-연구-흐름>이 논문의 한계를 극복하기 위한 연구 흐름</a></li><li><a href=#1-kv-캐시-효율성-개선>1. <strong>KV 캐시 효율성 개선</strong></a><ul><li><a href=#한계>한계:</a></li><li><a href=#연구-흐름>연구 흐름:</a></li></ul></li><li><a href=#2-초장문-문맥에서-효율성-문제>2. <strong>초장문 문맥에서 효율성 문제</strong></a><ul><li><a href=#한계-1>한계:</a></li><li><a href=#연구-흐름-1>연구 흐름:</a></li></ul></li><li><a href=#3-디코딩-단계-성능-최적화>3. <strong>디코딩 단계 성능 최적화</strong></a><ul><li><a href=#한계-2>한계:</a></li><li><a href=#연구-흐름-2>연구 흐름:</a></li></ul></li><li><a href=#4-하드웨어-의존성-완화>4. <strong>하드웨어 의존성 완화</strong></a><ul><li><a href=#한계-3>한계:</a></li><li><a href=#연구-흐름-3>연구 흐름:</a></li></ul></li><li><a href=#5-상용-애플리케이션-통합>5. <strong>상용 애플리케이션 통합</strong></a><ul><li><a href=#한계-4>한계:</a></li><li><a href=#연구-흐름-4>연구 흐름:</a></li></ul></li><li><a href=#종합>종합</a></li></ul></li></ul><ul><li><ul><li><a href=#논문에서-제안한-pass-kv와-pass-q-알고리즘의-선택-기준-heuristic-모델>논문에서 제안한 Pass-KV와 Pass-Q 알고리즘의 선택 기준 (Heuristic 모델)</a><ul><li><a href=#1-heuristic-모델의-기본-수식>1. Heuristic 모델의 기본 수식</a></li></ul></li><li><a href=#heuristic-모델-구현-과정>Heuristic 모델 구현 과정</a></li><li><a href=#다른-상황에-적용하기-위한-추가-조정>다른 상황에 적용하기 위한 추가 조정</a></li><li><a href=#추가적인-통찰>추가적인 통찰</a></li></ul></li></ul><ul><li><ul><li><a href=#context-parallelismcp이-긴-문맥-처리에서-tensor-parallelismtp-대비-높은-병렬화-효율성을-제공하는-이유>Context Parallelism(CP)이 긴 문맥 처리에서 Tensor Parallelism(TP) 대비 높은 병렬화 효율성을 제공하는 이유</a></li><li><a href=#1-통신-비용-감소>1. <strong>통신 비용 감소</strong></a><ul><li><a href=#tensor-parallelismtp의-한계>Tensor Parallelism(TP)의 한계:</a></li><li><a href=#context-parallelismcp의-장점>Context Parallelism(CP)의 장점:</a></li></ul></li><li><a href=#2-계산-및-메모리-부하-균형>2. <strong>계산 및 메모리 부하 균형</strong></a><ul><li><a href=#tensor-parallelismtp의-한계-1>Tensor Parallelism(TP)의 한계:</a></li><li><a href=#context-parallelismcp의-장점-1>Context Parallelism(CP)의 장점:</a></li></ul></li><li><a href=#3-병렬화-효율성>3. <strong>병렬화 효율성</strong></a><ul><li><a href=#tensor-parallelismtp의-한계-2>Tensor Parallelism(TP)의 한계:</a></li><li><a href=#context-parallelismcp의-장점-2>Context Parallelism(CP)의 장점:</a></li></ul></li><li><a href=#cp가-디코딩-단계에서-병목을-완전히-해결하지-못한-이유>CP가 디코딩 단계에서 병목을 완전히 해결하지 못한 이유</a></li><li><a href=#1-auto-regressive의-본질적-순차성>1. <strong>Auto-Regressive의 본질적 순차성</strong></a><ul><li><a href=#결과>결과:</a></li></ul></li><li><a href=#2-all-to-all-통신의-병목>2. <strong>All-to-All 통신의 병목</strong></a></li><li><a href=#3-kv-캐시-갱신-문제>3. <strong>KV 캐시 갱신 문제</strong></a></li><li><a href=#디코딩-병목-해결을-위한-추가-방안>디코딩 병목 해결을 위한 추가 방안</a></li><li><a href=#결론-3>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#kv-캐시-크기와-gpu-메모리-사용량-간의-균형-최적화를-위한-ring-attention의-데이터-분산-및-통신-전략>KV 캐시 크기와 GPU 메모리 사용량 간의 균형 최적화를 위한 Ring Attention의 데이터 분산 및 통신 전략</a></li><li><a href=#ring-attention의-핵심-설계>Ring Attention의 핵심 설계</a></li><li><a href=#1-데이터-분산-방식>1. <strong>데이터 분산 방식</strong></a><ul><li><a href=#1-kv-캐시의-균등-분산>(1) KV 캐시의 균등 분산</a></li><li><a href=#2-query와-kv의-독립적-샤딩>(2) Query와 KV의 독립적 샤딩</a></li></ul></li><li><a href=#2-통신-전략>2. <strong>통신 전략</strong></a><ul><li><a href=#1-링-구조-통신-ring-communication>(1) 링 구조 통신 (Ring Communication)</a></li><li><a href=#2-all-to-all-통신-pass-q에서-사용>(2) All-to-All 통신 (Pass-Q에서 사용)</a></li><li><a href=#3-통신-오버헤드-최소화>(3) 통신 오버헤드 최소화</a></li></ul></li><li><a href=#3-메모리-사용량-최적화>3. <strong>메모리 사용량 최적화</strong></a><ul><li><a href=#1-kv-캐시-분산-저장>(1) KV 캐시 분산 저장</a></li><li><a href=#2-캐시-갱신-및-유지>(2) 캐시 갱신 및 유지</a></li><li><a href=#3-양자화-및-데이터-압축>(3) 양자화 및 데이터 압축</a></li></ul></li><li><a href=#4-ring-attention-구현-방식-요약>4. <strong>Ring Attention 구현 방식 요약</strong></a></li><li><a href=#결론-4>결론</a></li></ul></li></ul></nav></div></nav></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground text-center text-xs"><span>제목을 클릭하면 해당 위치로 이동합니다</span></div></div></div></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>function initKaTeX(){if(typeof renderMathInElement=="undefined"){setTimeout(initKaTeX,100);return}var e=[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"}];renderMathInElement(document.body,{delimiters:e,throwOnError:!1,errorColor:"#cc0000",fleqn:!1,leqno:!1,trust:!1})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",initKaTeX):initKaTeX()</script><div id=search-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div><div id=search-modal class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300"><div class="border-border flex items-center gap-3 border-b p-4"><div class="text-muted-foreground h-5 w-5 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><div class="relative flex-1"><button id=search-clear class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200" title=지우기 aria-label=지우기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg>
</button>
<input id=search-input type=text placeholder="게시글 검색..." class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none" autocomplete=off spellcheck=false></div><button id=search-close class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div id=search-results class="max-h-96 overflow-y-auto"><div id=search-empty class="flex flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 시작</h3><p class="text-muted-foreground text-sm">검색어를 입력하세요</p></div><div id=search-loading class="flex hidden items-center justify-center py-8"><div class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div><span class=text-muted-foreground>검색 중...</span></div><div id=search-no-results class="flex hidden flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 결과 없음</h3><p class="text-muted-foreground text-sm">다른 검색어로 시도해보세요</p></div><div id=search-results-list class=hidden><div id=search-stats class="text-muted-foreground border-border border-b px-4 py-3 text-sm" data-template="%d개의 결과 찾음"></div><div id=search-items class="divide-border divide-y"></div></div></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground flex items-center justify-between text-xs"><div class="flex items-center gap-2 md:gap-4"><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↑↓</kbd>
<span class="hidden sm:inline">이동</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↵</kbd>
<span class="hidden sm:inline">선택</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">ESC</kbd>
<span class="hidden sm:inline">닫기</span></div></div><div class="search-hint-desktop flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">⌘K</kbd>
<span>단축키</span></div></div></div></div></body></html>