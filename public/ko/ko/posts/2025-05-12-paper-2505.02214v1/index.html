<!doctype html><html lang=ko-kr data-theme=claude><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>An Empirical Study of Qwen3 Quantization | Jaehun's Blog</title><meta name=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 ✅ 결론 요약 Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit..."><meta name=keywords content="개발,프로그래밍,에세이"><meta name=author content><meta name=language content="ko"><meta name=robots content="index, follow"><link rel=canonical href=https://jaehun.me/ko/ko/posts/2025-05-12-paper-2505.02214v1/><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://jaehun.me/ko/ko/posts/2025-05-12-paper-2505.02214v1/"><meta property="og:site_name" content="Jaehun's Blog"><meta property="og:title" content="An Empirical Study of Qwen3 Quantization"><meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 ✅ 결론 요약 Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다."><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="ko"><meta property="article:published_time" content="2025-05-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-12T00:00:00+00:00"><meta property="og:image" content="https://jaehun.me/images/og-default.avif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehun.me/images/og-default.avif"><meta name=twitter:title content="An Empirical Study of Qwen3 Quantization"><meta name=twitter:description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 ✅ 결론 요약 Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다."><meta itemprop=name content="An Empirical Study of Qwen3 Quantization"><meta itemprop=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 ✅ 결론 요약 Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다."><meta itemprop=datePublished content="2025-05-12T00:00:00+00:00"><meta itemprop=dateModified content="2025-05-12T00:00:00+00:00"><meta itemprop=wordCount content="7058"><meta itemprop=image content="https://jaehun.me/images/og-default.avif"><meta itemprop=keywords content="Paper-Review,With-Gpt"><script>window.HUGO_SEARCH_CONFIG={searchIndexURL:'"/ko/index.json"'}</script><script>window.HUGO_GALLERY_CONFIG={justified_gallery:!1,lightbox:!1,justified:"{}",lightbox_options:"{}"}</script><link rel=stylesheet href=/css/compiled.min.9165cace5b80a5be5daad6ea96eb2abc20a18a3f41d7fed7f0335b689fdf7883.css integrity="sha256-kWXKzluApb5dqtbqlusqvCChij9B1/7X8DNbaJ/feIM=" crossorigin=anonymous><link rel=stylesheet href=/css/chroma.min.491df36221f739b5948a747a0351f100ee7ccaaf466d9f46288bf06de1d59123.css integrity="sha256-SR3zYiH3ObWUinR6A1HxAO58yq9GbZ9GKIvwbeHVkSM=" crossorigin=anonymous><script src=/js/main.ba969e49a7a874e6938fe689a7dcaa00cb64cb4d429c2cf98d09bde512a16c6d.js integrity="sha256-upaeSaeodOaTj+aJp9yqAMtky01CnCz5jQm95RKhbG0=" crossorigin=anonymous></script><script src=/js/gumshoe.polyfills.min.js></script><script src=/js/toc.70654aebf0b738a78ae2df95297cc10ebc6e590f3c942dd1325fa852f50038b2.js integrity="sha256-cGVK6/C3OKeK4t+VKXzBDrxuWQ88lC3RMl+oUvUAOLI=" crossorigin=anonymous defer></script><script src=/js/search.de3eba3540834efb8328121784bff4774e06362412ee41eee665f5b4f987f393.js integrity="sha256-3j66NUCDTvuDKBIXhL/0d04GNiQS7kHu5mX1tPmH85M=" crossorigin=anonymous defer></script><script src=/js/dock.e02f422517e001d9fb6bea1e9e8b0e89e9d2e486ad4641f275bb3e36ce2ae79f.js integrity="sha256-4C9CJRfgAdn7a+oenosOienS5IatRkHydbs+Ns4q558=" crossorigin=anonymous defer></script><script>(function(){const e=localStorage.getItem("theme")||"system",n=localStorage.getItem("colorScheme")||"claude";document.documentElement.setAttribute("data-theme",n);function t(){e==="dark"||e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}t(),e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",t)})()</script></head><body class="bg-background text-foreground min-h-screen antialiased"><div id=reading-progress-container class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out" data-height=3 data-smooth-scroll=true data-hide-on-complete=false><div class="reading-progress-bg w-full"></div><div id=reading-progress-bar class="from-primary to-primary/80 reading-progress-bar
transition-all duration-150 ease-out
absolute top-0 left-0 w-0 bg-gradient-to-r"></div></div><script>(function(){"use strict";const t=document.getElementById("reading-progress-container"),o=document.getElementById("reading-progress-bar");if(!t||!o)return;const a={smoothScroll:t.dataset.smoothScroll==="true",hideOnComplete:t.dataset.hideOnComplete==="true"};let n=!0,l=null;function r(){const t=window.pageYOffset||document.documentElement.scrollTop,n=document.documentElement.scrollHeight,s=window.innerHeight,e=n-s;return e<=0?0:Math.min(Math.max(t/e*100,0),100)}function c(){const e=r();o.style.width=e+"%",a.hideOnComplete&&e>=99.5?n&&(t.style.opacity="0",n=!1):n||(t.style.opacity="1",n=!0)}let s=!1;function e(){s||(requestAnimationFrame(()=>{c(),s=!1}),s=!0)}function i(){window.addEventListener("scroll",e,{passive:!0}),window.addEventListener("resize",e,{passive:!0}),document.addEventListener("visibilitychange",()=>{document.hidden||e()}),e(),window.addEventListener("beforeunload",()=>{window.removeEventListener("scroll",e),window.removeEventListener("resize",e)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script><header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6"><div class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm"><div class="hidden w-full items-center md:flex"><div class="flex items-center"><a href=/ko/ class="flex h-10 w-10 items-center justify-center overflow-hidden rounded-full transition-transform duration-200 hover:scale-105" aria-label="Jaehun's Blog"><img src=/images/logo.svg alt="Jaehun's Blog" class="h-full w-full object-cover"></a></div><nav class="mx-8 flex flex-1 items-center justify-center"><div class="flex items-center space-x-1"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></div></nav><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div><div class="flex w-full items-center justify-between md:hidden"><div class=relative><button id=mobile-menu-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200" data-dropdown-type=mobile-menu aria-label=메뉴 aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="메뉴"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button><div id=mobile-menu class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden" data-dropdown-type=mobile-menu role=menu aria-labelledby=mobile-menu-toggle><nav class="flex flex-col"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></nav></div></div><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div></div></header><main class="mx-auto max-w-4xl px-4 py-6"><nav class="breadcrumb mb-4 md:mb-6 py-1" aria-label=경로><ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1"><li class=flex-shrink-0><a href=/ko/ class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="홈"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11 2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">홈</span></a></li><li class="flex items-center gap-1 md:gap-2 min-w-0"><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span><div class="flex items-center gap-1 md:gap-2 min-w-0"><a href=/ko class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5A2 2 0 003 7z"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">Ko</span>
</a><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span>
<span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/></svg>
<span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">An Empirical Study of Qwen3 Quantization</span></span></div></li></ol></nav><header class=mb-8><h1 class="text-foreground mb-4 text-3xl font-bold md:text-4xl">An Empirical Study of Qwen3 Quantization</h1></header><article class="prose prose-neutral dark:prose-invert max-w-none"><p><a href=https://arxiv.org/abs/2505.02214v1>논문 링크</a></p><h1 id=q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘>Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1><h3 id=-결론-요약>✅ 결론 요약</h3><p>Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다.</p><hr><h2 id=1-논문의-핵심-강점-및-독창성>1. 논문의 핵심 강점 및 독창성</h2><table><thead><tr><th>강점 항목</th><th>상세 설명</th></tr></thead><tbody><tr><td><strong>종합적인 PTQ 비교</strong></td><td>Qwen3-0.6B<del>72B까지 다양한 크기 모델에 대해 RTN, GPTQ, AWQ, SmoothQuant, BiLLM 등 5가지 PTQ(Post-Training Quantization) 방법을 1</del>8bit 구간에서 정량적으로 평가</td></tr><tr><td><strong>다양한 평가 지표 적용</strong></td><td>PPL(WikiText2, C4), 0-shot reasoning(PIQA, ARC, HellaSwag 등), 5-shot MMLU</td></tr><tr><td><strong>AWQ vs GPTQ 성능비교</strong></td><td>4bit에서는 GPTQ가 MMLU 등에서 가장 낮은 성능 감소율 보임 (예: Qwen3-14B에서 FP16 대비 1% ↓)</td></tr><tr><td><strong>초저비트(1-3bit) 실험 포함</strong></td><td>대부분의 논문이 다루지 않는 2bit 및 binarization 방법(BiLLM) 실험도 포함</td></tr><tr><td><strong>LLaMA3와 비교 분석</strong></td><td>동일한 quantization 세팅에서도 Qwen3은 LLaMA3 대비 더 큰 성능 하락 → redundancy 차이 때문으로 분석</td></tr></tbody></table><hr><h2 id=2-핵심-알고리즘-설명-예시-중심>2. 핵심 알고리즘 설명 (예시 중심)</h2><h3 id=-대상-post-training-quantization-ptq>🧠 대상: <strong>Post-Training Quantization (PTQ)</strong></h3><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>정규화된 사전학습 weight와 activation을 사후적으로 quantize하여 float16 → low-bit(integer)로 압축</p></div></blockquote><h4 id=예시-입력>예시 입력</h4><p>Qwen3-4B 모델, 입력 시퀀스 길이 2048, task = PIQA (0-shot), Quantization = GPTQ, bit = 4</p><h4 id=전체-과정>전체 과정</h4><ol><li><p><strong>Calibration</strong></p><ul><li>128개의 샘플 (C4 데이터셋) 입력</li><li>Layer-wise로 weight의 범위 추정 (min/max), outlier 분석</li></ul></li><li><p><strong>Weight Quantization (GPTQ)</strong></p><ul><li><p>각 Linear Layer weight W → <code>int4 + scale + zero-point</code>로 변환</p></li><li><p>GPTQ는 weight의 중요도 기반으로 error를 최소화하는 방향으로 rounding</p></li><li><p>수식:</p><p>$$
W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
$$</p><p>단, error가 큰 축에 대해 extra correction 수행</p></li></ul></li><li><p><strong>Inference</strong></p><ul><li><p>Quantized Linear:</p><p>$$
y = (W_q \cdot x) + b_q
$$</p></li><li><p>계산은 INT4로, 결과는 float32로 복원됨</p></li></ul></li><li><p><strong>평가</strong></p><ul><li>PIQA 정확도 (GPTQ, 4bit, 4B 모델): 76.0 (FP16: 78.1 → 성능 감소 2.1%)</li></ul></li></ol><hr><h2 id=3-성능-비교-요약-4bit-기준>3. 성능 비교 요약 (4bit 기준)</h2><table><thead><tr><th>Model Size</th><th>GPTQ (4bit) MMLU</th><th>AWQ (4bit) MMLU</th><th>SmoothQuant (w4a8)</th><th>FP16</th></tr></thead><tbody><tr><td>0.6B</td><td>40.4</td><td>47.3</td><td>32.8</td><td>52.3</td></tr><tr><td>1.7B</td><td>53.2</td><td>57.5</td><td>47.9</td><td>61.0</td></tr><tr><td>4B</td><td>68.9</td><td>69.2</td><td>63.2</td><td>73.0</td></tr><tr><td>8B</td><td>72.7</td><td>73.8</td><td>61.6</td><td>76.7</td></tr><tr><td>14B</td><td>78.5</td><td>78.7</td><td>72.7</td><td>80.7</td></tr></tbody></table><p>➡ GPTQ와 AWQ는 대부분의 경우 성능을 잘 유지함. SmoothQuant는 activation quantization의 영향으로 하락 폭이 큼.</p><hr><h2 id=4-한계점>4. 한계점</h2><table><thead><tr><th>문제점</th><th>설명</th></tr></thead><tbody><tr><td><strong>Activation Quantization에 매우 민감</strong></td><td>w4a8 설정에서도 GPTQ/AWQ 대비 성능 저하가 큼 (e.g. 14B에서 SmoothQuant는 72.7 vs GPTQ 78.5)</td></tr><tr><td><strong>초저비트 환경에서 급격한 성능 하락</strong></td><td>2bit/1bit에서는 거의 usable하지 않은 수준의 PPL/accuracy (e.g. 4B-GPTQ: PPL 13.5 → 113x 증가)</td></tr><tr><td><strong>모델 크기에 따라 robust 성능 차이</strong></td><td>큰 모델일수록 quantization에 강함 (Redundancy로 noise absorbing), 작은 모델에서는 10% 이상 성능 감소</td></tr><tr><td><strong>Qwen3 특유의 민감성</strong></td><td>LLaMA3 대비 quantization noise에 훨씬 민감, 강한 pretraining이 오히려 overfitting된 표현으로 작용</td></tr></tbody></table><hr><h2 id=5-향후-연구-제안>5. 향후 연구 제안</h2><ul><li><p><strong>Rotation/Channel Reorder 기반 Quantization</strong></p><ul><li>예: SpinQuant [11], RPTQ [18] → redundant 표현 재배열로 정보 손실 완화</li></ul></li><li><p><strong>Outlier-aware Activation Quantization</strong></p><ul><li>Activation에서 발생하는 large magnitude token 대응 필요</li></ul></li><li><p><strong>Hybrid Bit-width Quantization</strong></p><ul><li>중요 레이어에만 6<del>8bit, 기타 2</del>3bit → latency, memory 절감과 성능 균형</li></ul></li></ul><h1 id=q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘>Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1><p>논문의 핵심 알고리즘은 <strong>Qwen3 LLM을 다양한 Post-Training Quantization(PTQ) 기법으로 1~8bit로 정밀도 축소하면서도, 최대한 성능 저하를 줄이는 것</strong>입니다. 그중에서도 주요 알고리즘은 다음의 3가지입니다:</p><hr><h2 id=-핵심-알고리즘-1-gptq-gradient-post-training-quantization>✅ 핵심 알고리즘 1: <strong>GPTQ (Gradient Post-Training Quantization)</strong></h2><h3 id=-요약>🌟 요약</h3><ul><li>GPTQ는 weight-only quantization 방식으로, 주어진 full-precision weight를 <strong>error-aware하게</strong> quantize함.</li><li>주요 특징은 <strong>Hessian approximation</strong>을 사용해 중요한 weight는 더 정밀하게 보존하고, 덜 중요한 weight만 과감히 양자화함.</li></ul><hr><h3 id=-예시-qwen3-4b-gptq-4bit-mlp-layer>🧪 예시: Qwen3-4B, GPTQ, 4bit, MLP Layer</h3><h4 id=1-원래의-weight-matrix>1. 원래의 weight matrix</h4><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">TEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-0 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-0 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-0><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=ln>1</span><span class=cl>W (FP32) = [
</span></span><span class=line><span class=ln>2</span><span class=cl>  [1.23, -0.74,  0.08],
</span></span><span class=line><span class=ln>3</span><span class=cl>  [-2.15,  0.63,  0.99],
</span></span><span class=line><span class=ln>4</span><span class=cl>  ...
</span></span><span class=line><span class=ln>5</span><span class=cl>]</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-0",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script><h4 id=2-calibration-데이터-통과>2. Calibration 데이터 통과</h4><ul><li><p>입력 데이터 128개 (from C4) 통과하여 Hessian 행렬 근사 생성:</p><p>$$
H \approx \frac{1}{N} \sum_{i=1}^{N} \nabla^2 \mathcal{L}(W_i)
$$</p></li><li><p>중요도가 높은 weight는 distortion이 덜 일어나게 조절.</p></li></ul><h4 id=3-quantization>3. Quantization</h4><ul><li><p>weight를 $W_q = \text{round}(W / \Delta) \cdot \Delta$ 로 변환</p></li><li><p>단, 중요한 weight는 rounding 전에 error compensation 적용:</p><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">TEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-1 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-1 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-1><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=ln>1</span><span class=cl>W_q = Quantize(W - E) + E</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-1",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script><p>여기서 $E$는 이전 양자화로 생긴 오차 누적값 (dequantized residual)</p></li></ul><h4 id=4-결과>4. 결과</h4><ul><li><p>Quantized weight (INT4)와 scale factor (float32)를 저장</p></li><li><p>추론 시:</p><p>$$
\hat{y} = \text{dequant}(W_q) \cdot x
$$</p></li></ul><hr><h2 id=-핵심-알고리즘-2-awq-activation-aware-weight-quantization>✅ 핵심 알고리즘 2: <strong>AWQ (Activation-aware Weight Quantization)</strong></h2><h3 id=-요약-1>🌟 요약</h3><ul><li>GPTQ보다 빠르고 lightweight한 방식</li><li>활성화의 dynamic range (outlier)에 따라 weight를 스케일링하여 quantization loss를 최소화</li></ul><hr><h3 id=-예시-qwen3-8b-awq-4bit>🧪 예시: Qwen3-8B, AWQ, 4bit</h3><h4 id=1-입력-활성화-분석>1. 입력 활성화 분석</h4><ul><li><p>특정 레이어의 activation을 관찰:</p><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">TEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-2 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-2 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-2><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=ln>1</span><span class=cl>x = [1.2, -5.3, 0.7, 10.1, -0.2]  # outlier 포함</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-2",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script></li></ul><h4 id=2-weight-scaling-factor-계산>2. Weight scaling factor 계산</h4><ul><li><p>outlier를 줄이기 위해 weight column마다 scaling factor $s$를 부여</p><p>$$
W&rsquo; = W \cdot s, \quad x&rsquo; = x / s
$$</p></li></ul><h4 id=3-weight-양자화>3. Weight 양자화</h4><ul><li>$W&rsquo;$를 int4로 양자화 → clipping 후 rounding</li></ul><h4 id=4-추론-시>4. 추론 시</h4><ul><li><p>원래대로 복원:</p><p>$$
y = (W_q \cdot x&rsquo;) \cdot s
$$</p><p>이 방식은 outlier를 보정하여 전체 dynamic range가 narrow해져 quantization이 용이함</p></li></ul><hr><h2 id=-핵심-알고리즘-3-billm-binarized-llm>✅ 핵심 알고리즘 3: <strong>BiLLM (Binarized LLM)</strong></h2><h3 id=-요약-2>🌟 요약</h3><ul><li>weight를 <strong>1bit</strong>로 양자화하여 극단적인 압축을 달성</li><li>${-1, +1}$ 만 사용</li><li>대신 residual error를 보완하는 구조가 필요</li></ul><hr><h3 id=-예시-qwen3-4b-billm-106bit>🧪 예시: Qwen3-4B, BiLLM, 1.06bit</h3><h4 id=1-weight-binarization>1. Weight binarization</h4><ul><li><p>각 weight matrix를 다음과 같이 binarize:</p><p>$$
W_b = \text{sign}(W)
$$</p></li></ul><h4 id=2-scaling-factor-학습>2. Scaling factor 학습</h4><ul><li><p>각 레이어마다 binarized weight와 원래의 weight 차이를 보정하기 위한 $\alpha$ 계산:</p><p>$$
W \approx \alpha \cdot W_b
$$</p></li></ul><h4 id=3-추론>3. 추론</h4><ul><li><p>추론 시에는 INT1과 scalar 곱만 수행:</p><p>$$
y = (\alpha \cdot W_b) \cdot x
$$</p></li></ul><hr><h2 id=-전체-알고리즘-흐름-요약>🔄 전체 알고리즘 흐름 요약</h2><pre class=mermaid>
  graph TD
A[FP16 Qwen3 Model] --&gt; B[Calibration with 128 samples]
B --&gt; C[Weight Importance Estimation (e.g., GPTQ, AWQ)]
C --&gt; D[Quantization (1~8bit)]
D --&gt; E[Store quantized weights + scale]
E --&gt; F[LLM Inference]
F --&gt; G[Eval on PIQA, MMLU, C4]
</pre><hr><h2 id=-비교-정리>📌 비교 정리</h2><table><thead><tr><th>방법</th><th>핵심 전략</th><th>비트폭</th><th>특징</th></tr></thead><tbody><tr><td>GPTQ</td><td>error-aware rounding (Hessian)</td><td>2–8bit</td><td>가장 정밀함, 느림</td></tr><tr><td>AWQ</td><td>activation-aware rescaling</td><td>2–8bit</td><td>빠르고 효과적</td></tr><tr><td>BiLLM</td><td>sign만 유지 + scale</td><td>~1bit</td><td>극한 압축, 성능 유지 도전적</td></tr></tbody></table><h1 id=q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘>Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1><h3 id=-결론-요약-1>✅ 결론 요약</h3><p>이 논문은 기존 PTQ(Post-Training Quantization) 기법들(GPTQ, AWQ, SmoothQuant, BiLLM 등)을 Qwen3에 광범위하게 적용하여 <strong>Qwen3가 특히 ultra-low-bit (≤3bit)</strong> 에서 성능 저하가 <strong>타 모델 대비 심각</strong>하다는 점을 정량적으로 보여줍니다. 특히:</p><ul><li><strong>4bit 이상에서는 AWQ와 GPTQ가 우수한 성능 보존</strong></li><li><strong>2bit/3bit에서는 Qwen3가 LLaMA3보다 훨씬 더 취약</strong></li><li><strong>BiLLM은 binarized (1bit 수준)임에도 일부 3bit보다 좋은 결과 제공</strong></li></ul><hr><h2 id=-주요-실험-결과-요약-qwen3-14b-기준>📊 주요 실험 결과 요약 (Qwen3-14B 기준)</h2><table><thead><tr><th>방법</th><th>비트폭 (W/A)</th><th>PPL (C4)</th><th>MMLU</th><th>특징</th></tr></thead><tbody><tr><td>FP16</td><td>16/16</td><td>9.68</td><td>80.7</td><td>full precision baseline</td></tr><tr><td>GPTQ</td><td>4/16</td><td>10.3</td><td>78.5</td><td><strong>성능 저하 ≈ 2.2%</strong></td></tr><tr><td>AWQ</td><td>4/16</td><td>10.3</td><td>78.7</td><td>GPTQ보다 근소히 우수</td></tr><tr><td>SmoothQuant</td><td>4/8</td><td>46.0</td><td>72.7</td><td><strong>활성화 양자화로 성능 하락 큼</strong></td></tr><tr><td>BiLLM</td><td>~1bit</td><td>20.4</td><td>39.9</td><td>3bit AWQ보다 우수, 이례적</td></tr></tbody></table><hr><h2 id=-특출난-점-논문이-보여주는-qwen3-specific한-현상>🌟 특출난 점: 논문이 보여주는 Qwen3-specific한 현상</h2><table><thead><tr><th>구분</th><th>설명</th></tr></thead><tbody><tr><td><strong>1. Quantization 민감도</strong></td><td>Qwen3는 다른 LLM(LLaMA3 등)보다 quantization에 훨씬 민감함</td></tr><tr><td><strong>2. 사전학습 품질의 역설</strong></td><td>더 정교하게 학습된 모델일수록 redundancy가 적고, quantization error를 흡수하지 못함</td></tr><tr><td><strong>3. BiLLM의 의미 있는 성능</strong></td><td>1bit 수준에서도 일부 3bit보다 좋은 성능은 기존 상식 깨뜨림</td></tr><tr><td><strong>4. PTQ 방법별 강건성 차이</strong></td><td>GPTQ는 전반적으로 최상위 성능, AWQ는 속도/정확도 균형, SmoothQuant는 작은 모델에 특히 불리함</td></tr></tbody></table><hr><h2 id=-논문이-제시하는-원인-정교한-사전학습--낮은-redundancy>🧠 논문이 제시하는 원인: &ldquo;정교한 사전학습 = 낮은 redundancy&rdquo;</h2><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>Qwen3는 LLaMA3 등보다 훨씬 더 많은 품질 관리된 데이터로 학습되어 있어, 표현들이 매우 압축적이고 정제됨 →
이로 인해 양자화 시 약간의 precision 손실도 성능에 <strong>치명적</strong>.</p></div></blockquote><p>논문에서는 아래와 같은 실증으로 이를 지지합니다:</p><ul><li>LLaMA3-8B (AWQ w3a16) → C4 PPL: 11.6</li><li>Qwen3-8B (AWQ w3a16) → C4 PPL: 23.8 (<strong>2배 이상 나쁨</strong>)</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>이는 고성능 LLM일수록 PTQ에 더 정교한 전략이 필요하다는 점을 강조</p></div></blockquote><hr><h2 id=-개인적-해석과-의견>🤔 개인적 해석과 의견</h2><table><thead><tr><th>항목</th><th>평가</th></tr></thead><tbody><tr><td><strong>문제의식의 명확성</strong></td><td>단순히 quantization을 적용하는 것이 아니라, 왜 성능이 나빠지는지 <strong>원인 분석까지 정량적으로 수행</strong>한 점에서 높은 평가를 받을 만함</td></tr><tr><td><strong>BiLLM 관련 결과</strong></td><td>일반적으로 1bit quantization은 usable하지 않다고 여겨지지만, BiLLM이 3bit보다 우수한 성능을 보이는 부분은 <strong>더 깊은 분석이 필요</strong>함. (구조 보정? ReLU clipping 전략?)</td></tr><tr><td><strong>데이터셋 구성의 통일성</strong></td><td>모든 PTQ 기법을 동일한 calibration 데이터(128 samples from C4), 동일한 평가셋에 적용하여 <strong>실험의 공정성 보장</strong></td></tr><tr><td><strong>제안의 부족</strong></td><td>다만 논문 자체는 <strong>새로운 PTQ 방법을 제안하지는 않음</strong>. 현존 기법의 평가 및 진단에 집중한 메타-연구로 보아야 함</td></tr></tbody></table><hr><h2 id=-향후-제안-논문내-제언>🔄 향후 제안 (논문+내 제언)</h2><table><thead><tr><th>제안 방식</th><th>설명</th></tr></thead><tbody><tr><td><strong>Reordering PTQ</strong></td><td>채널 순서 변경하여 outlier 제거 (예: RPTQ)</td></tr><tr><td><strong>Rotation Quantization</strong></td><td>SpinQuant처럼 rotation을 통한 표현 flattening</td></tr><tr><td><strong>Layer-aware Bit-width Tuning</strong></td><td>중요 레이어는 6bit, 덜 중요한 건 2bit 등 selective bit-width</td></tr></tbody></table><h1 id=q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘>Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1><p>이 논문 <strong>&ldquo;An Empirical Study of Qwen3 Quantization&rdquo;</strong> 의 한계는 크게 두 가지로 요약됩니다:</p><hr><h3 id=-핵심-한계-요약>✅ 핵심 한계 요약</h3><table><thead><tr><th>한계점</th><th>설명</th></tr></thead><tbody><tr><td><strong>1. Ultra-low-bit (≤3bit) 성능 급락</strong></td><td>Qwen3는 redundancy가 적은 고성능 모델이라서 2~3bit quantization에서 성능이 급격히 저하됨</td></tr><tr><td><strong>2. Activation Quantization에 취약</strong></td><td>SmoothQuant(w4a8, w4a4) 실험에서 MMLU 기준 10~25pt 이상 성능 하락 발생 → 특히 outlier activation에 민감</td></tr></tbody></table><hr><h2 id=-이를-극복하기-위한-주요-연구-흐름-3가지>🔍 이를 극복하기 위한 주요 연구 흐름 3가지</h2><hr><h3 id=1-rotation-based-quantization-eg-spinquant>1️⃣ <strong>Rotation-Based Quantization (e.g., SpinQuant)</strong></h3><table><thead><tr><th>핵심 아이디어</th><th>Quantization 전에 weight/activation 공간을 rotation하여 <strong>양자화 손실을 균등하게 분산</strong></th></tr></thead><tbody><tr><td>대표 논문</td><td>SpinQuant: LLM Quantization with Learned Rotations (arXiv:2405.16406, 2024)</td></tr><tr><td>기술 요약</td><td></td></tr></tbody></table><ul><li><p>Rotation matrix $R$ 학습 후:</p><p>$$
W&rsquo; = W \cdot R,\quad x&rsquo; = R^{-1} x
$$</p><ul><li>→ Outlier 값들이 압축되어 범위가 좁아짐</li><li>→ Quantization clipping이 적게 일어나고, reconstruction loss 감소 |</li></ul></li></ul><p>| 장점 | GPTQ 대비 더 정교한 표현 압축 가능 (특히 activation-aware GPTQ에 효과적) |
| 단점 | 학습 overhead와 rotation matrix 저장 부담 |</p><hr><h3 id=2-reordering-based-ptq-eg-rptq>2️⃣ <strong>Reordering-based PTQ (e.g., RPTQ)</strong></h3><p>| 핵심 아이디어 | activation 혹은 weight channel 순서를 <strong>정렬(정렬 기반 permutation)</strong> 하여 <strong>정보 밀도 향상</strong> |
| 대표 논문 | RPTQ: Reorder-based Post-Training Quantization for LLMs (arXiv:2304.01089) |
| 기술 요약 |</p><ul><li><p>활성화에서 큰 값이 집중된 채널을 먼저 정렬:</p><p>$$
W \Rightarrow \text{sort}(W, key=\text{activation variance})
$$</p></li><li><p>→ Quantizer가 표현해야 할 분포를 더 압축적으로 만들 수 있음 |
| 장점 | 특별한 추가 학습 없이 단순한 permutation으로 효과 |
| 적용 기대 | Qwen3의 outlier activation 문제에 특히 적합 |</p></li></ul><hr><h3 id=3-mixed-precision-quantization--layer-sensitivity-profiling>3️⃣ <strong>Mixed-Precision Quantization + Layer Sensitivity Profiling</strong></h3><p>| 핵심 아이디어 | Layer 별로 중요도를 분석해 <strong>중요한 레이어에는 6~8bit</strong>, 중요도가 낮은 레이어는 <strong>2~4bit</strong> 할당 |
| 대표 접근 | GPTQ + Hessian 기반 sensitivity 분석 |
| 구체 전략 |</p><ul><li>Qwen3-14B의 attention/output layer는 6bit 유지</li><li>feed-forward 일부와 embedding은 3bit로 축소</li><li>→ 전체 모델의 평균 비트수는 4bit 이하로 낮출 수 있음 |
| 장점 | 비트당 효율을 최적화하면서 성능 보존 |
| 관련 연구 | &ldquo;Quant-Noise&rdquo; (Fairseq), &ldquo;BitSplit&rdquo; quantization, &ldquo;DoReFa-Net&rdquo; |</li></ul><hr><h2 id=-전략별-비교-정리>📊 전략별 비교 정리</h2><table><thead><tr><th>전략명</th><th>대상</th><th>주 목적</th><th>성능 회복 기대</th><th>계산 비용</th><th>적용 난이도</th></tr></thead><tbody><tr><td><strong>SpinQuant</strong></td><td>Weight/Activation</td><td>분포 평탄화</td><td>★★★★☆</td><td>높음</td><td>중</td></tr><tr><td><strong>RPTQ</strong></td><td>Activation</td><td>outlier 제거</td><td>★★★★☆</td><td>낮음</td><td>낮음</td></tr><tr><td><strong>Mixed-Precision</strong></td><td>Layer-wise</td><td>중요도 반영</td><td>★★★★★</td><td>중</td><td>중</td></tr></tbody></table><hr><h2 id=-내-생각-qwen3에는-어떤-방법이-가장-적합한가>💡 내 생각: Qwen3에는 어떤 방법이 가장 적합한가?</h2><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>Rotation + Mixed Precision 조합</strong>이 가장 유력</p></div></blockquote><ul><li>이유: Qwen3은 redundancy가 매우 낮고, activation 값 분포가 넓음 → rotation으로 분포를 평탄화하고, 중요 레이어만 high-bit로 보호하는 전략이 실용적임</li><li>BiLLM처럼 binarization을 고려하는 것도 흥미롭지만, production-level에서는 mixed precision이 더 안정적임</li></ul><h1 id=q--qwen3-모델은-기존-llm들llama3-등에-비해-ultra-low-bit-quantization3bit-이하에서-왜-더-큰-성능-저하를-보이는가-이-현상이-사전학습-방식이나-표현-압축과-어떤-관련이-있는가>Q : Qwen3 모델은 기존 LLM들(LLaMA3 등)에 비해 ultra-low-bit quantization(3bit 이하)에서 왜 더 큰 성능 저하를 보이는가? 이 현상이 사전학습 방식이나 표현 압축과 어떤 관련이 있는가?</h1><h3 id=-결론부터-요약>✅ 결론부터 요약</h3><p>Qwen3는 <strong>표현 압축이 매우 정교하게 이뤄진 고성능 모델</strong>이기 때문에, ultra-low-bit (≤3bit) quantization에서 미세한 weight 오차에도 성능이 급격히 저하된다. 이는 <strong>redundancy(중복 표현)의 부족</strong> 때문이며, 이는 LLaMA3 같은 모델과의 가장 큰 차이점이다.</p><hr><h2 id=-1-성능-저하의-실측-증거>🔍 1. 성능 저하의 실측 증거</h2><table><thead><tr><th>모델</th><th>방법 (w3a16)</th><th>PPL(C4)</th><th>성능 비교</th></tr></thead><tbody><tr><td>LLaMA3-8B</td><td>AWQ</td><td>11.6</td><td>준수한 성능 유지</td></tr><tr><td>Qwen3-8B</td><td>AWQ</td><td>23.8</td><td><strong>2배 이상 악화</strong></td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>Qwen3는 3bit quantization에 <strong>더 민감하며</strong>, LLaMA3보다 PPL이 훨씬 나빠짐</p></div></blockquote><hr><h2 id=-2-사전학습-방식의-차이-표현-압축-vs-표현-중복>🧠 2. 사전학습 방식의 차이: 표현 압축 vs 표현 중복</h2><h3 id=qwen3>Qwen3:</h3><ul><li>대규모 고품질 데이터로 <strong>fine-tuning된 pretraining</strong></li><li>적은 수의 파라미터로도 높은 정확도를 내기 위해 <strong>불필요한 표현을 제거함</strong></li><li>즉, 하나의 neuron/weight에 <strong>더 많은 정보</strong>가 집중됨</li></ul><h3 id=llama3>LLaMA3:</h3><ul><li>상대적으로 <strong>다소 일반적인 pretraining 데이터</strong></li><li>표현 공간의 redundancy가 많아, 일부 손실에 대한 <strong>robustness 확보</strong></li></ul><p>➡ 결과적으로, Qwen3는 3bit 이하로 양자화될 때 <strong>한 weight의 손실이 곧 정보 손실</strong>로 이어짐</p><hr><h2 id=-3-표현-압축과-quantization-noise-간-상호작용>📐 3. 표현 압축과 quantization noise 간 상호작용</h2><ul><li><p>양자화는 weight를 아래와 같이 근사함:</p><p>$$
W_q = \text{round}\left( \frac{W}{\Delta} \right) \cdot \Delta
$$</p></li><li><p>정밀도가 낮을수록 (e.g., 3bit → 8개의 표현값만 존재) → <strong>quantization error</strong>가 증가</p></li><li><p>redundancy가 있다면 이 error는 다른 neuron에서 보완 가능</p></li><li><p>하지만 Qwen3는 이러한 보완 메커니즘이 부족</p></li></ul><p>➡ 결국, <strong>표현 압축이 뛰어난 모델ほど quantization에 취약</strong></p><hr><h2 id=-4-논문에서의-해석>🔄 4. 논문에서의 해석</h2><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>“Advanced pre-training techniques… tend to produce models with less parameter redundancy, consequently making them more sensitive to quantization-induced information loss.”</p></div></blockquote><ul><li>이는 단순한 실험 결과가 아니라, <strong>학습 방식의 진화에 따라 quantization 전략도 진화해야 한다</strong>는 시사점을 던짐</li></ul><hr><h2 id=-보완-가능성>🔧 보완 가능성</h2><ul><li>activation/channel rotation (SpinQuant)</li><li>attention block selective precision</li><li>pre-quantization distillation (loss-aware finetune)</li></ul><h1 id=q--gptq-awq-smoothquant-billm-등의-quantization-방법은-각각-어떤-방식으로-weight-또는-activation의-정보를-압축하고-qwen3의-구조적-특성에-대해-어떤-방식으로-대응하고-있는가>Q : GPTQ, AWQ, SmoothQuant, BiLLM 등의 quantization 방법은 각각 어떤 방식으로 weight 또는 activation의 정보를 압축하고, Qwen3의 구조적 특성에 대해 어떤 방식으로 대응하고 있는가?</h1><h3 id=-결론-요약-2>✅ 결론 요약</h3><p>GPTQ, AWQ, SmoothQuant, BiLLM은 서로 다른 방식으로 weight 또는 activation의 정보 손실을 줄이기 위한 기법이다.
이들은 Qwen3의 <strong>low-redundancy, high-precision</strong> 구조에 대해 다음과 같은 방식으로 대응하며, <strong>error 보상, outlier 처리, 이진화 등</strong> 각기 다른 전략을 취한다.</p><hr><h2 id=-방법별-정보-압축-전략-및-qwen3-대응-요약>📦 방법별 정보 압축 전략 및 Qwen3 대응 요약</h2><table><thead><tr><th>방법</th><th>압축 대상</th><th>핵심 전략</th><th>Qwen3 대응 방식</th><th>성능 요약 (Qwen3-14B, 4bit 기준)</th></tr></thead><tbody><tr><td><strong>GPTQ</strong></td><td>Weight</td><td>Gradient-aware rounding (Hessian 기반 error 최소화)</td><td>중요 weight의 정보 손실 억제</td><td>MMLU: <strong>78.5</strong></td></tr><tr><td><strong>AWQ</strong></td><td>Weight</td><td>Activation-aware weight scaling</td><td>outlier activation에 선제적 대응</td><td>MMLU: <strong>78.7</strong></td></tr><tr><td><strong>SmoothQuant</strong></td><td>Activation + Weight</td><td>Weight rescaling via activation statistics</td><td>Activation 범위 정규화로 quantization noise 저감</td><td>MMLU(w4a8): <strong>72.7</strong></td></tr><tr><td><strong>BiLLM</strong></td><td>Weight (1bit)</td><td>Binarization + residual scaling</td><td>극한 압축에서의 구조적 정규화</td><td>MMLU(~1bit): <strong>39.9</strong></td></tr></tbody></table><hr><h2 id=-1-gptq-gradient-post-training-quantization>🔍 1. GPTQ (Gradient Post-Training Quantization)</h2><h3 id=-핵심-메커니즘>⛏️ 핵심 메커니즘</h3><ul><li><p>각 weight의 영향력을 Hessian matrix로 근사하여, <strong>가장 손실이 적은 방향으로 rounding</strong></p></li><li><p>수식 기반:</p><p>$$
\min |W - W_q|_H = (W - W_q)^T H (W - W_q)
$$</p></li></ul><h3 id=-qwen3-대응>🎯 Qwen3 대응</h3><ul><li>Qwen3의 weight는 매우 민감하므로, 중요 weight를 정밀하게 보존하는 GPTQ는 <strong>고정밀 모델에 적합</strong></li><li>특히 4bit 환경에서 가장 안정적인 성능 유지</li></ul><hr><h2 id=-2-awq-activation-aware-weight-quantization>🔍 2. AWQ (Activation-aware Weight Quantization)</h2><h3 id=-핵심-메커니즘-1>⛏️ 핵심 메커니즘</h3><ul><li><p><strong>Activation 값의 분포(특히 outlier)</strong> 를 보고, 각 채널의 weight를 scaling하여 압축</p></li><li><p>수식:</p><p>$$
W&rsquo; = \frac{W}{s}, \quad x&rsquo; = x \cdot s \quad \text{(scale factor s는 activation 기반)}
$$</p></li></ul><h3 id=-qwen3-대응-1>🎯 Qwen3 대응</h3><ul><li>Qwen3는 <strong>activation outlier가 많음</strong> → weight만 정규화하면 quantization error가 커짐</li><li>이를 막기 위해 activation의 영향까지 고려 → <strong>activation noise 제어에 효과적</strong></li></ul><hr><h2 id=-3-smoothquant>🔍 3. SmoothQuant</h2><h3 id=-핵심-메커니즘-2>⛏️ 핵심 메커니즘</h3><ul><li>Activation의 variance를 기준으로 LayerNorm 이전에 weight rescaling 수행</li><li>목적: <strong>activation의 dynamic range를 줄여서 activation quantization 시 clipping 방지</strong></li></ul><h3 id=-qwen3-대응-2>🎯 Qwen3 대응</h3><ul><li><p>Qwen3에서 SmoothQuant는 w4a8 또는 w4a4 등 activation quantization 시 <strong>성능 급감</strong></p><ul><li>이유: Qwen3의 <strong>LayerNorm 및 GeLU 이후 activation 분포가 매우 wide</strong> → clipping에 매우 취약</li></ul></li></ul><hr><h2 id=-4-billm-binarized-llm>🔍 4. BiLLM (Binarized LLM)</h2><h3 id=-핵심-메커니즘-3>⛏️ 핵심 메커니즘</h3><ul><li><p>weight를 {+1, -1}로 양자화</p></li><li><p>단순 binarization이 아닌, residual 정보를 scaling factor로 유지하여 복원 가능성 향상:</p><p>$$
W \approx \alpha \cdot \text{sign}(W)
$$</p></li></ul><h3 id=-qwen3-대응-3>🎯 Qwen3 대응</h3><ul><li>일반적으로 고성능 모델에서 binarization은 불리하지만,</li><li>BiLLM은 일부 레이어만 binarize하거나, residual 정규화를 잘해서 3bit보다 좋은 성능을 낼 수 있음 (특히 Qwen3-32B에서)</li></ul><hr><h2 id=-실험-예시-qwen3-14b-4bit-weight-기준-mmlu-성능>📈 실험 예시 (Qwen3-14B, 4bit weight 기준 MMLU 성능)</h2><table><thead><tr><th>방법</th><th>MMLU (↑)</th><th>PPL (C4, ↓)</th><th>특징</th></tr></thead><tbody><tr><td>FP16</td><td>80.7</td><td>9.68</td><td>baseline</td></tr><tr><td>GPTQ</td><td>78.5</td><td>10.3</td><td>가장 안정적</td></tr><tr><td>AWQ</td><td>78.7</td><td>10.3</td><td>outlier 대응 탁월</td></tr><tr><td>SmoothQuant</td><td>72.7</td><td>46.0</td><td>activation 양자화에 민감</td></tr><tr><td>BiLLM (~1bit)</td><td>39.9</td><td>20.4</td><td>극한 조건에서도 의미 있는 성능</td></tr></tbody></table><hr><h2 id=-요약-정리>🔚 요약 정리</h2><table><thead><tr><th>방법</th><th>Qwen3 구조 대응 전략</th><th>장점</th><th>단점</th></tr></thead><tbody><tr><td>GPTQ</td><td>중요한 weight를 정밀하게 보존 (low redundancy 대응)</td><td>정확도 높음</td><td>느림, 메모리 사용 ↑</td></tr><tr><td>AWQ</td><td>activation outlier를 사전 scale로 정규화</td><td>빠르고 정확도도 좋음</td><td>극단적인 outlier에는 한계</td></tr><tr><td>SmoothQuant</td><td>activation range 조절</td><td>낮은 latency, 좋은 w8a8</td><td>activation clipping에 취약</td></tr><tr><td>BiLLM</td><td>극단적 quantization에 residual 보정</td><td>압축률 최고</td><td>성능 유지 어려움</td></tr></tbody></table><h1 id=q--논문에서-사용한-calibration-방식128개-c4-샘플-기반이-quantization-성능에-어떤-영향을-미치는가-calibration-데이터의-양이나-다양성을-조절할-경우-성능-향상-여지가-있는가>Q : 논문에서 사용한 calibration 방식(128개 C4 샘플 기반)이 quantization 성능에 어떤 영향을 미치는가? calibration 데이터의 양이나 다양성을 조절할 경우 성능 향상 여지가 있는가?</h1><h3 id=-결론-요약-3>✅ 결론 요약</h3><p>Qwen3 quantization에서 사용된 <strong>128개 C4 샘플 기반의 calibration</strong>은 <strong>상대적으로 적은 양</strong>이지만, 일반적인 PTQ 방식(GPTQ, AWQ 등)에서는 <strong>충분히 유효한 수준</strong>으로 판단된다. 다만, <strong>bit 수가 작아질수록</strong> 또는 <strong>activation quantization을 포함할수록</strong>, calibration 데이터의 <strong>양과 다양성</strong>이 성능에 <strong>큰 영향을 미칠 수 있다</strong>.</p><hr><h2 id=-1-논문에서-사용한-calibration-방식-요약>🔍 1. 논문에서 사용한 Calibration 방식 요약</h2><table><thead><tr><th>항목</th><th>세부 내용</th></tr></thead><tbody><tr><td>데이터셋</td><td><strong>C4 (Colossal Cleaned Common Crawl)</strong></td></tr><tr><td>샘플 수</td><td><strong>128개 샘플</strong></td></tr><tr><td>입력 길이</td><td>2048 토큰</td></tr><tr><td>목적</td><td>weight 분포, activation range 추정 (e.g., min/max, Hessian)</td></tr><tr><td>대상 방법</td><td>GPTQ, AWQ, SmoothQuant, BiLLM 등 모두 동일한 calibration 사용</td></tr></tbody></table><p>➡ 실험의 <strong>공정성을 보장</strong>하기 위해 모든 방법에 동일한 calibration 조건 적용</p><hr><h2 id=-2-calibration-데이터-양이-적을-때-영향>🧠 2. Calibration 데이터 양이 적을 때 영향</h2><h3 id=양이-적은-경우>양이 적은 경우:</h3><ul><li><strong>분포 추정 정확도 ↓</strong> → 특히 activation의 outlier 범위를 과소평가</li><li><strong>Hessian 근사 오차 ↑</strong> → GPTQ의 rounding precision 낮아짐</li><li><strong>채널별 scaling factor 오류 발생 가능</strong> (AWQ, SmoothQuant)</li></ul><h3 id=결과적으로>결과적으로:</h3><ul><li><strong>bit 수가 낮을수록 더 민감</strong>함
→ 8bit에서는 거의 영향 없음
→ 4bit 이하에서는 <strong>정확도 최대 5~10%까지 차이 가능</strong></li></ul><hr><h2 id=-실험적-근거-논문-외-gptq-공식-repo-기준>📈 실험적 근거 (논문 외 GPTQ 공식 repo 기준)</h2><ul><li>32개 샘플: 성능 불안정</li><li>128개 샘플: baseline 수준</li><li>512<del>1024개 샘플: 1</del>2pt 향상 가능 (특히 MMLU 등 reasoning task)</li></ul><hr><h2 id=-데이터-다양성의-중요성>🔁 데이터 다양성의 중요성</h2><table><thead><tr><th>조건</th><th>설명</th><th>영향</th></tr></thead><tbody><tr><td><strong>단일 도메인 텍스트</strong></td><td>뉴스/위키 위주 → narrow distribution</td><td>일부 layer 편향 발생 가능</td></tr><tr><td><strong>다양한 도메인/길이 포함</strong></td><td>code, dialogue, QA 혼합</td><td>LayerNorm, MLP 등 다양한 분포 추정 가능 → <strong>activation quantization에 특히 유리</strong></td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p>특히 SmoothQuant처럼 activation statistics에 기반한 방법은 <strong>다양성 확보가 핵심</strong></p></div></blockquote><hr><h2 id=-내-제안-실용적-개선-방법>💡 내 제안: 실용적 개선 방법</h2><table><thead><tr><th>전략</th><th>설명</th></tr></thead><tbody><tr><td><strong>샘플 수 증가 (e.g., 512~1024)</strong></td><td>대부분의 weight/activation 분포가 안정화됨</td></tr><tr><td><strong>stratified sampling</strong></td><td>다양한 토픽/domain/text style을 고르게 반영</td></tr><tr><td><strong>token-level calibration</strong></td><td>LayerNorm 및 RMS-based quantizer가 있는 경우, 입력 diversity를 높이는 게 중요</td></tr></tbody></table><hr><h2 id=-실험-설계-예시>🔬 실험 설계 예시</h2><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">PYTHON</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-4 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-4 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=ln>1</span><span class=cl><span class=c1># 다양한 calibration 비교 실험</span>
</span></span><span class=line><span class=ln>2</span><span class=cl><span class=k>for</span> <span class=n>n_samples</span> <span class=ow>in</span> <span class=p>[</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>1024</span><span class=p>]:</span>
</span></span><span class=line><span class=ln>3</span><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>sample_c4</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=n>n_samples</span><span class=p>)</span>
</span></span><span class=line><span class=ln>4</span><span class=cl>    <span class=n>quant_model</span> <span class=o>=</span> <span class=n>gptq_calibrate</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=ln>5</span><span class=cl>    <span class=n>eval_mmlu</span><span class=p>(</span><span class=n>quant_model</span><span class=p>)</span>  <span class=c1># accuracy 측정</span></span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-4",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script><p>➡ 대부분의 경우 <strong>128개 정도로 기본적인 안정성은 확보되나</strong>,
<strong>ultra-low-bit (&lt;4bit) 또는 activation quantization 적용 시</strong>에는 더 많은/다양한 데이터가 필요함</p></article></main><footer class="mx-auto max-w-4xl px-4 py-8"><div class="px-6 py-6"><nav class=mb-6><div class="flex flex-wrap items-center justify-center gap-1"><a href=/index.xml class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="RSS Feed"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18.0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z"/></svg>
<span>RSS Feed</span></a></div></nav><div class="border-border mb-6 border-t"></div><div class="flex flex-col items-center justify-between gap-4 md:flex-row"><div class="text-muted-foreground text-sm"><p>&copy; 2025 Jaehun's Blog.
모든 권리 보유.</p></div><div class="flex items-center gap-3"><a href=https://github.com/ryujaehun target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=GitHub aria-label=GitHub><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.4 5.4.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
</a><a href=https://www.linkedin.com/in/jaehunryu/ target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Linkedin aria-label=Linkedin><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
</a><a href=mailto:jaehunryu@icloud.com class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Email aria-label=Email><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Email"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22.0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v10a2 2 0 002 2z"/></svg></a></div></div></div></footer><div id=dock class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0" role=toolbar aria-label="바로가기 도구 모음"><nav class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3"><button id=dock-back class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=뒤로 aria-label=뒤로>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m12 19-7-7 7-7"/><path d="M19 12H5"/></svg>
<span class="hidden sm:inline">뒤로</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-toc class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=목차 aria-label=목차>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg>
<span class="hidden sm:inline">목차</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-search class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2" title=검색 aria-label=검색>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg>
<span class="hidden md:inline">검색</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-top class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title="맨 위로" aria-label="맨 위로">
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m18 15-6-6-6 6"/></svg>
<span class="hidden sm:inline">맨 위로</span></button></nav></div><div id=toc-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300" role=dialog aria-modal=true aria-labelledby=toc-title></div><div id=toc-card class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2" role=dialog aria-modal=true aria-labelledby=toc-title><div class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md"><div class="border-border bg-muted/30 flex items-center justify-between border-b p-4"><div class="flex items-center gap-3"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg><h2 id=toc-title class="text-foreground text-lg font-semibold">목차</h2></div><button id=toc-close class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div class="max-h-96 overflow-y-auto p-4"><nav id=toc-nav class=toc-scrollbar aria-label="글 목차"><div id=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#-결론-요약>✅ 결론 요약</a></li></ul></li><li><a href=#1-논문의-핵심-강점-및-독창성>1. 논문의 핵심 강점 및 독창성</a></li><li><a href=#2-핵심-알고리즘-설명-예시-중심>2. 핵심 알고리즘 설명 (예시 중심)</a><ul><li><a href=#-대상-post-training-quantization-ptq>🧠 대상: <strong>Post-Training Quantization (PTQ)</strong></a><ul><li><a href=#예시-입력>예시 입력</a></li><li><a href=#전체-과정>전체 과정</a></li></ul></li></ul></li><li><a href=#3-성능-비교-요약-4bit-기준>3. 성능 비교 요약 (4bit 기준)</a></li><li><a href=#4-한계점>4. 한계점</a></li><li><a href=#5-향후-연구-제안>5. 향후 연구 제안</a></li></ul><ul><li><a href=#-핵심-알고리즘-1-gptq-gradient-post-training-quantization>✅ 핵심 알고리즘 1: <strong>GPTQ (Gradient Post-Training Quantization)</strong></a><ul><li><a href=#-요약>🌟 요약</a></li><li><a href=#-예시-qwen3-4b-gptq-4bit-mlp-layer>🧪 예시: Qwen3-4B, GPTQ, 4bit, MLP Layer</a><ul><li><a href=#1-원래의-weight-matrix>1. 원래의 weight matrix</a></li><li><a href=#2-calibration-데이터-통과>2. Calibration 데이터 통과</a></li><li><a href=#3-quantization>3. Quantization</a></li><li><a href=#4-결과>4. 결과</a></li></ul></li></ul></li><li><a href=#-핵심-알고리즘-2-awq-activation-aware-weight-quantization>✅ 핵심 알고리즘 2: <strong>AWQ (Activation-aware Weight Quantization)</strong></a><ul><li><a href=#-요약-1>🌟 요약</a></li><li><a href=#-예시-qwen3-8b-awq-4bit>🧪 예시: Qwen3-8B, AWQ, 4bit</a><ul><li><a href=#1-입력-활성화-분석>1. 입력 활성화 분석</a></li><li><a href=#2-weight-scaling-factor-계산>2. Weight scaling factor 계산</a></li><li><a href=#3-weight-양자화>3. Weight 양자화</a></li><li><a href=#4-추론-시>4. 추론 시</a></li></ul></li></ul></li><li><a href=#-핵심-알고리즘-3-billm-binarized-llm>✅ 핵심 알고리즘 3: <strong>BiLLM (Binarized LLM)</strong></a><ul><li><a href=#-요약-2>🌟 요약</a></li><li><a href=#-예시-qwen3-4b-billm-106bit>🧪 예시: Qwen3-4B, BiLLM, 1.06bit</a><ul><li><a href=#1-weight-binarization>1. Weight binarization</a></li><li><a href=#2-scaling-factor-학습>2. Scaling factor 학습</a></li><li><a href=#3-추론>3. 추론</a></li></ul></li></ul></li><li><a href=#-전체-알고리즘-흐름-요약>🔄 전체 알고리즘 흐름 요약</a></li><li><a href=#-비교-정리>📌 비교 정리</a></li></ul><ul><li><ul><li><a href=#-결론-요약-1>✅ 결론 요약</a></li></ul></li><li><a href=#-주요-실험-결과-요약-qwen3-14b-기준>📊 주요 실험 결과 요약 (Qwen3-14B 기준)</a></li><li><a href=#-특출난-점-논문이-보여주는-qwen3-specific한-현상>🌟 특출난 점: 논문이 보여주는 Qwen3-specific한 현상</a></li><li><a href=#-논문이-제시하는-원인-정교한-사전학습--낮은-redundancy>🧠 논문이 제시하는 원인: &ldquo;정교한 사전학습 = 낮은 redundancy&rdquo;</a></li><li><a href=#-개인적-해석과-의견>🤔 개인적 해석과 의견</a></li><li><a href=#-향후-제안-논문내-제언>🔄 향후 제안 (논문+내 제언)</a></li></ul><ul><li><ul><li><a href=#-핵심-한계-요약>✅ 핵심 한계 요약</a></li></ul></li><li><a href=#-이를-극복하기-위한-주요-연구-흐름-3가지>🔍 이를 극복하기 위한 주요 연구 흐름 3가지</a><ul><li><a href=#1-rotation-based-quantization-eg-spinquant>1️⃣ <strong>Rotation-Based Quantization (e.g., SpinQuant)</strong></a></li><li><a href=#2-reordering-based-ptq-eg-rptq>2️⃣ <strong>Reordering-based PTQ (e.g., RPTQ)</strong></a></li><li><a href=#3-mixed-precision-quantization--layer-sensitivity-profiling>3️⃣ <strong>Mixed-Precision Quantization + Layer Sensitivity Profiling</strong></a></li></ul></li><li><a href=#-전략별-비교-정리>📊 전략별 비교 정리</a></li><li><a href=#-내-생각-qwen3에는-어떤-방법이-가장-적합한가>💡 내 생각: Qwen3에는 어떤 방법이 가장 적합한가?</a></li></ul><ul><li><ul><li><a href=#-결론부터-요약>✅ 결론부터 요약</a></li></ul></li><li><a href=#-1-성능-저하의-실측-증거>🔍 1. 성능 저하의 실측 증거</a></li><li><a href=#-2-사전학습-방식의-차이-표현-압축-vs-표현-중복>🧠 2. 사전학습 방식의 차이: 표현 압축 vs 표현 중복</a><ul><li><a href=#qwen3>Qwen3:</a></li><li><a href=#llama3>LLaMA3:</a></li></ul></li><li><a href=#-3-표현-압축과-quantization-noise-간-상호작용>📐 3. 표현 압축과 quantization noise 간 상호작용</a></li><li><a href=#-4-논문에서의-해석>🔄 4. 논문에서의 해석</a></li><li><a href=#-보완-가능성>🔧 보완 가능성</a></li></ul><ul><li><ul><li><a href=#-결론-요약-2>✅ 결론 요약</a></li></ul></li><li><a href=#-방법별-정보-압축-전략-및-qwen3-대응-요약>📦 방법별 정보 압축 전략 및 Qwen3 대응 요약</a></li><li><a href=#-1-gptq-gradient-post-training-quantization>🔍 1. GPTQ (Gradient Post-Training Quantization)</a><ul><li><a href=#-핵심-메커니즘>⛏️ 핵심 메커니즘</a></li><li><a href=#-qwen3-대응>🎯 Qwen3 대응</a></li></ul></li><li><a href=#-2-awq-activation-aware-weight-quantization>🔍 2. AWQ (Activation-aware Weight Quantization)</a><ul><li><a href=#-핵심-메커니즘-1>⛏️ 핵심 메커니즘</a></li><li><a href=#-qwen3-대응-1>🎯 Qwen3 대응</a></li></ul></li><li><a href=#-3-smoothquant>🔍 3. SmoothQuant</a><ul><li><a href=#-핵심-메커니즘-2>⛏️ 핵심 메커니즘</a></li><li><a href=#-qwen3-대응-2>🎯 Qwen3 대응</a></li></ul></li><li><a href=#-4-billm-binarized-llm>🔍 4. BiLLM (Binarized LLM)</a><ul><li><a href=#-핵심-메커니즘-3>⛏️ 핵심 메커니즘</a></li><li><a href=#-qwen3-대응-3>🎯 Qwen3 대응</a></li></ul></li><li><a href=#-실험-예시-qwen3-14b-4bit-weight-기준-mmlu-성능>📈 실험 예시 (Qwen3-14B, 4bit weight 기준 MMLU 성능)</a></li><li><a href=#-요약-정리>🔚 요약 정리</a></li></ul><ul><li><ul><li><a href=#-결론-요약-3>✅ 결론 요약</a></li></ul></li><li><a href=#-1-논문에서-사용한-calibration-방식-요약>🔍 1. 논문에서 사용한 Calibration 방식 요약</a></li><li><a href=#-2-calibration-데이터-양이-적을-때-영향>🧠 2. Calibration 데이터 양이 적을 때 영향</a><ul><li><a href=#양이-적은-경우>양이 적은 경우:</a></li><li><a href=#결과적으로>결과적으로:</a></li></ul></li><li><a href=#-실험적-근거-논문-외-gptq-공식-repo-기준>📈 실험적 근거 (논문 외 GPTQ 공식 repo 기준)</a></li><li><a href=#-데이터-다양성의-중요성>🔁 데이터 다양성의 중요성</a></li><li><a href=#-내-제안-실용적-개선-방법>💡 내 제안: 실용적 개선 방법</a></li><li><a href=#-실험-설계-예시>🔬 실험 설계 예시</a></li></ul></nav></div></nav></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground text-center text-xs"><span>제목을 클릭하면 해당 위치로 이동합니다</span></div></div></div></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>function initKaTeX(){if(typeof renderMathInElement=="undefined"){setTimeout(initKaTeX,100);return}var e=[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"}];renderMathInElement(document.body,{delimiters:e,throwOnError:!1,errorColor:"#cc0000",fleqn:!1,leqno:!1,trust:!1})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",initKaTeX):initKaTeX()</script><div id=search-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div><div id=search-modal class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300"><div class="border-border flex items-center gap-3 border-b p-4"><div class="text-muted-foreground h-5 w-5 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><div class="relative flex-1"><button id=search-clear class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200" title=지우기 aria-label=지우기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg>
</button>
<input id=search-input type=text placeholder="게시글 검색..." class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none" autocomplete=off spellcheck=false></div><button id=search-close class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div id=search-results class="max-h-96 overflow-y-auto"><div id=search-empty class="flex flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 시작</h3><p class="text-muted-foreground text-sm">검색어를 입력하세요</p></div><div id=search-loading class="flex hidden items-center justify-center py-8"><div class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div><span class=text-muted-foreground>검색 중...</span></div><div id=search-no-results class="flex hidden flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 결과 없음</h3><p class="text-muted-foreground text-sm">다른 검색어로 시도해보세요</p></div><div id=search-results-list class=hidden><div id=search-stats class="text-muted-foreground border-border border-b px-4 py-3 text-sm" data-template="%d개의 결과 찾음"></div><div id=search-items class="divide-border divide-y"></div></div></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground flex items-center justify-between text-xs"><div class="flex items-center gap-2 md:gap-4"><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↑↓</kbd>
<span class="hidden sm:inline">이동</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↵</kbd>
<span class="hidden sm:inline">선택</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">ESC</kbd>
<span class="hidden sm:inline">닫기</span></div></div><div class="search-hint-desktop flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">⌘K</kbd>
<span>단축키</span></div></div></div></div></body></html>