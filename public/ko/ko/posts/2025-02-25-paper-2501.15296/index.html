<!doctype html><html lang=ko-kr data-theme=claude><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING | Jaehun's Blog</title><meta name=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 핵심 요약 이 논문은 PruneNet이라는 새로운 모델 압축 기법을 ..."><meta name=keywords content="개발,프로그래밍,에세이"><meta name=author content><meta name=language content="ko"><meta name=robots content="index, follow"><link rel=canonical href=https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/"><meta property="og:site_name" content="Jaehun's Blog"><meta property="og:title" content="You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING"><meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 핵심 요약 이 논문은 PruneNet이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="ko"><meta property="article:published_time" content="2025-02-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-25T00:00:00+00:00"><meta property="og:image" content="https://jaehun.me/images/og-default.avif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehun.me/images/og-default.avif"><meta name=twitter:title content="You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING"><meta name=twitter:description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 핵심 요약 이 논문은 PruneNet이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:"><meta itemprop=name content="You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING"><meta itemprop=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 핵심 요약 이 논문은 PruneNet이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:"><meta itemprop=datePublished content="2025-02-25T00:00:00+00:00"><meta itemprop=dateModified content="2025-02-25T00:00:00+00:00"><meta itemprop=wordCount content="17672"><meta itemprop=image content="https://jaehun.me/images/og-default.avif"><meta itemprop=keywords content="Paper-Review,With-Gpt,ICLR2025"><script>window.HUGO_SEARCH_CONFIG={searchIndexURL:'"/ko/index.json"'}</script><script>window.HUGO_GALLERY_CONFIG={justified_gallery:!1,lightbox:!1,justified:"{}",lightbox_options:"{}"}</script><link rel=stylesheet href=/css/compiled.min.9165cace5b80a5be5daad6ea96eb2abc20a18a3f41d7fed7f0335b689fdf7883.css integrity="sha256-kWXKzluApb5dqtbqlusqvCChij9B1/7X8DNbaJ/feIM=" crossorigin=anonymous><link rel=stylesheet href=/css/chroma.min.491df36221f739b5948a747a0351f100ee7ccaaf466d9f46288bf06de1d59123.css integrity="sha256-SR3zYiH3ObWUinR6A1HxAO58yq9GbZ9GKIvwbeHVkSM=" crossorigin=anonymous><script src=/js/main.ba969e49a7a874e6938fe689a7dcaa00cb64cb4d429c2cf98d09bde512a16c6d.js integrity="sha256-upaeSaeodOaTj+aJp9yqAMtky01CnCz5jQm95RKhbG0=" crossorigin=anonymous></script><script src=/js/gumshoe.polyfills.min.js></script><script src=/js/toc.70654aebf0b738a78ae2df95297cc10ebc6e590f3c942dd1325fa852f50038b2.js integrity="sha256-cGVK6/C3OKeK4t+VKXzBDrxuWQ88lC3RMl+oUvUAOLI=" crossorigin=anonymous defer></script><script src=/js/search.de3eba3540834efb8328121784bff4774e06362412ee41eee665f5b4f987f393.js integrity="sha256-3j66NUCDTvuDKBIXhL/0d04GNiQS7kHu5mX1tPmH85M=" crossorigin=anonymous defer></script><script src=/js/dock.e02f422517e001d9fb6bea1e9e8b0e89e9d2e486ad4641f275bb3e36ce2ae79f.js integrity="sha256-4C9CJRfgAdn7a+oenosOienS5IatRkHydbs+Ns4q558=" crossorigin=anonymous defer></script><script>(function(){const e=localStorage.getItem("theme")||"system",n=localStorage.getItem("colorScheme")||"claude";document.documentElement.setAttribute("data-theme",n);function t(){e==="dark"||e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}t(),e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",t)})()</script></head><body class="bg-background text-foreground min-h-screen antialiased"><div id=reading-progress-container class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out" data-height=3 data-smooth-scroll=true data-hide-on-complete=false><div class="reading-progress-bg w-full"></div><div id=reading-progress-bar class="from-primary to-primary/80 reading-progress-bar
transition-all duration-150 ease-out
absolute top-0 left-0 w-0 bg-gradient-to-r"></div></div><script>(function(){"use strict";const t=document.getElementById("reading-progress-container"),o=document.getElementById("reading-progress-bar");if(!t||!o)return;const a={smoothScroll:t.dataset.smoothScroll==="true",hideOnComplete:t.dataset.hideOnComplete==="true"};let n=!0,l=null;function r(){const t=window.pageYOffset||document.documentElement.scrollTop,n=document.documentElement.scrollHeight,s=window.innerHeight,e=n-s;return e<=0?0:Math.min(Math.max(t/e*100,0),100)}function c(){const e=r();o.style.width=e+"%",a.hideOnComplete&&e>=99.5?n&&(t.style.opacity="0",n=!1):n||(t.style.opacity="1",n=!0)}let s=!1;function e(){s||(requestAnimationFrame(()=>{c(),s=!1}),s=!0)}function i(){window.addEventListener("scroll",e,{passive:!0}),window.addEventListener("resize",e,{passive:!0}),document.addEventListener("visibilitychange",()=>{document.hidden||e()}),e(),window.addEventListener("beforeunload",()=>{window.removeEventListener("scroll",e),window.removeEventListener("resize",e)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script><header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6"><div class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm"><div class="hidden w-full items-center md:flex"><div class="flex items-center"><a href=/ko/ class="flex h-10 w-10 items-center justify-center overflow-hidden rounded-full transition-transform duration-200 hover:scale-105" aria-label="Jaehun's Blog"><img src=/images/logo.svg alt="Jaehun's Blog" class="h-full w-full object-cover"></a></div><nav class="mx-8 flex flex-1 items-center justify-center"><div class="flex items-center space-x-1"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></div></nav><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div><div class="flex w-full items-center justify-between md:hidden"><div class=relative><button id=mobile-menu-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200" data-dropdown-type=mobile-menu aria-label=메뉴 aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="메뉴"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button><div id=mobile-menu class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden" data-dropdown-type=mobile-menu role=menu aria-labelledby=mobile-menu-toggle><nav class="flex flex-col"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></nav></div></div><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div></div></header><main class="mx-auto max-w-4xl px-4 py-6"><nav class="breadcrumb mb-4 md:mb-6 py-1" aria-label=경로><ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1"><li class=flex-shrink-0><a href=/ko/ class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="홈"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11 2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">홈</span></a></li><li class="flex items-center gap-1 md:gap-2 min-w-0"><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span><div class="flex items-center gap-1 md:gap-2 min-w-0"><a href=/ko class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5A2 2 0 003 7z"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">Ko</span>
</a><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span>
<span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/></svg>
<span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING</span></span></div></li></ol></nav><header class=mb-8><h1 class="text-foreground mb-4 text-3xl font-bold md:text-4xl">You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING</h1></header><article class="prose prose-neutral dark:prose-invert max-w-none"><p><a href=https://arxiv.org/abs/2501.15296>논문 링크</a></p><h1 id=q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘>Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1><h3 id=논문의-핵심-요약><strong>논문의 핵심 요약</strong></h3><p>이 논문은 <strong>PruneNet</strong>이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:</p><ol><li><strong>Calibration-Free Pruning</strong>: 기존 모델 프루닝 기법들은 보정 데이터(calibration dataset)에 의존하는 반면, PruneNet은 <strong>정책 학습(policy learning)을 이용하여 자체적으로 프루닝을 수행</strong>한다.</li><li><strong>정보 손실 최소화</strong>: 모델의 고유한 스펙트럼 구조를 유지하여 정보 손실을 줄인다.</li><li><strong>고속 압축</strong>: LLaMA-2-7B 모델을 <strong>15분 만에 30% 압축하면서도 원본 성능의 80% 이상 유지</strong>(SliceGPT는 75% 유지).</li><li><strong>다양한 모델 및 작업에 적용 가능</strong>: OPT, Phi-2 등 다양한 모델에서 뛰어난 성능을 보이며, MMLU, PIQA 등 다중 작업 평가에서 안정적인 성능을 유지.</li></ol><hr><h3 id=강점-및-독창성><strong>강점 및 독창성</strong></h3><table><thead><tr><th>기준</th><th>PruneNet</th><th>기존 방법 (SliceGPT 등)</th></tr></thead><tbody><tr><td><strong>보정 데이터 필요 여부</strong></td><td>X (불필요)</td><td>O (필요)</td></tr><tr><td><strong>압축 속도 (LLaMA-2-7B, 30%)</strong></td><td>15분</td><td>29분</td></tr><tr><td><strong>성능 유지율 (Zero-shot, 30%)</strong></td><td>80%</td><td>75%</td></tr><tr><td><strong>압축 후 FLOPs 감소율</strong></td><td>24%</td><td>27% (비효율적)</td></tr><tr><td><strong>다중 작업(MMLU 등) 안정성</strong></td><td>O (성능 변동 작음)</td><td>X (높은 압축률에서 급격한 성능 저하)</td></tr></tbody></table><h4 id=-주요-강점>✅ <strong>주요 강점</strong></h4><ul><li><strong>Calibration-Free</strong>: 보정 데이터 없이 동작하여 적용성이 높음.</li><li><strong>스펙트럼 구조 유지</strong>: 단순 가중치 삭제가 아닌, 모델의 <strong>스펙트럼 분포를 고려한 프루닝</strong>으로 정보 손실 최소화.</li><li><strong>빠른 속도</strong>: 기존 방법보다 <strong>최대 2배 빠르게 압축 가능</strong>.</li><li><strong>다중 작업에서도 우수한 성능 유지</strong>: 여러 압축률에서도 안정적인 성능.</li></ul><h4 id=-독창적인-점>🔥 <strong>독창적인 점</strong></h4><ol><li><strong>프루닝을 정책 학습(policy learning) 문제로 변환</strong>: 기존의 Heuristic-based 방식과 달리, <strong>강화 학습과 유사한 방식으로 가중치 중요도를 학습</strong>.</li><li><strong>스펙트럼 변화 최소화 전략 도입</strong>: 단순한 행·열 삭제 방식이 아니라, <strong>특이값 분포의 변화를 최소화하는 방식</strong>으로 성능을 유지.</li><li><strong>범용적인 정책 학습 가능</strong>: 한 번 학습된 정책을 <strong>다른 모델 및 다른 압축률에도 적용 가능</strong> → 재사용성이 뛰어남.</li></ol><hr><h3 id=핵심-알고리즘-설명><strong>핵심 알고리즘 설명</strong></h3><p>PruneNet은 Transformer 모델의 <strong>FFN(Feed Forward Network) 층을 중심으로 가중치 프루닝</strong>을 수행한다. 주요 과정은 다음과 같다:</p><ol><li><p><strong>정책 학습 모델(Policy Learner) 학습</strong></p><ul><li>각 FFN 가중치 행렬 ( W_{up} ) 에 대해 <strong>각 행(row)의 중요도를 평가</strong>하는 정책 모델을 학습.</li><li>중요도 벡터 ( W_{imp} ) 계산:
[
W&rsquo; = W_{up} W_{inter}^T
]
[
W_{imp} = \sigma(W_{proj} W&rsquo;)
]</li><li>(\sigma) 는 시그모이드 함수로, 각 행의 삭제 확률을 출력.</li></ul></li><li><p><strong>확률적 샘플링을 이용한 행 선택</strong></p><ul><li>특정 비율 ( r ) 만큼의 행을 제거하는 <strong>Multinomial 샘플링 수행</strong>:
[
W̃_{imp} = \sigma(\log ϵ - \log (1−ϵ) + \log (W_{imp}) - \log (1−W_{imp}))
]</li><li>이렇게 선택된 행들을 삭제하여 <strong>압축된 ( W_{up} ) 행렬 생성</strong>.</li></ul></li><li><p><strong>스펙트럼 구조 유지</strong></p><ul><li>PruneNet은 원래 행렬과 압축된 행렬의 <strong>스펙트럼(특이값 분포) 변화 최소화</strong>를 목표로 학습.</li><li><strong>Kolmogorov-Smirnov (KS) Distance</strong>를 이용하여, 압축 전후 행렬의 스펙트럼 분포 차이를 최소화하는 방향으로 학습:
[
D = \sup_x |F_1(x) - F_2(x)|
]</li></ul></li><li><p><strong>정책 최적화</strong></p><ul><li>REINFORCE 알고리즘(정책 기울기 기법)을 활용하여 중요도 벡터 학습.</li><li>LLaMA-2-7B에서는 정책 학습 모델의 총 파라미터 수가 <strong>45M (전체의 0.67%)</strong> 에 불과하여 경량 최적화 가능.</li></ul></li></ol><hr><h3 id=예제-입력-및-전체-과정><strong>예제 입력 및 전체 과정</strong></h3><h4 id=예제-입력><strong>예제 입력</strong></h4><ul><li><strong>모델</strong>: LLaMA-2-7B</li><li><strong>압축 비율</strong>: 30%</li><li><strong>입력 데이터</strong>: FFN 층의 가중치 행렬 ( W_{up} )</li></ul><h4 id=처리-과정><strong>처리 과정</strong></h4><ol><li><strong>정책 학습 모델 실행</strong> → 각 행의 중요도 점수 산출</li><li><strong>샘플링을 통해 30% 행 삭제</strong></li><li><strong>FFN2의 대응 열도 함께 삭제하여 구조 유지</strong></li><li><strong>압축된 모델 생성 후 스펙트럼 차이 최소화</strong></li><li><strong>최적화된 정책을 저장하여 재사용 가능</strong></li></ol><h4 id=출력><strong>출력</strong></h4><ul><li>압축된 모델 (LLaMA-2-7B-프루닝)</li><li>원본 성능의 80% 유지</li><li>스펙트럼 변화 최소화</li></ul><hr><h3 id=한계점-및-개선-방향><strong>한계점 및 개선 방향</strong></h3><table><thead><tr><th>한계점</th><th>상세 내용</th><th>개선 가능성</th></tr></thead><tbody><tr><td><strong>고정적인 FFN 프루닝</strong></td><td>FFN에 초점이 맞춰져 있으며, Self-Attention 블록까지 확장되지 않음.</td><td>Self-Attention 층까지 확장 필요</td></tr><tr><td><strong>극단적 압축에서 성능 저하</strong></td><td>50% 이상 압축 시 성능 급락 (약 40% 수준)</td><td>계층별 압축 비율 조절 필요</td></tr><tr><td><strong>추론 속도 최적화 한계</strong></td><td>FLOPs 감소율이 24%로 기대보다 낮음</td><td>Sparse 연산 최적화 필요</td></tr><tr><td><strong>Fine-tuning 필요 여부</strong></td><td>압축 후 fine-tuning 없이도 높은 성능 유지하지만 일부 복구 가능성 존재</td><td>LoRA 등 경량 튜닝 기법과 결합 가능</td></tr></tbody></table><hr><h3 id=결론><strong>결론</strong></h3><p>PruneNet은 기존 프루닝 기법 대비 <strong>속도, 유연성, 성능 유지 측면에서 혁신적인 접근법</strong>을 제시한다. 특히, <strong>보정 데이터 없이도 높은 압축률에서 안정적인 성능을 유지</strong>하는 점에서 차별화된다.</p><p>하지만, Self-Attention 층까지 확장, 극단적 압축 시 성능 유지, Sparse 연산 최적화 등이 개선 방향으로 제시될 수 있다. <strong>양자화(Quantization) 기법과 결합 시 더 강력한 경량 모델 구축 가능성이 높다</strong>.</p><p>즉, PruneNet은 단순 프루닝이 아닌, <strong>프루닝의 학습 가능성을 탐구한 획기적인 연구</strong>라고 볼 수 있다. 🚀</p><h1 id=q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘>Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1><h3 id=prunenet-핵심-알고리즘-상세-설명><strong>PruneNet 핵심 알고리즘 상세 설명</strong></h3><p>PruneNet은 <strong>정책 학습(Policy Learning)을 활용한 모델 압축 기법</strong>으로, 기존의 단순한 가중치 삭제 방식과 차별화된다. <strong>핵심 아이디어는 FFN(Feed Forward Network) 층의 행(row)을 선택적으로 삭제하는 것</strong>이며, 이를 통해 모델 크기를 줄이면서 성능 저하를 최소화한다.</p><hr><h2 id=-알고리즘-개요><strong>🔹 알고리즘 개요</strong></h2><p>PruneNet은 모델의 <strong>FFN 가중치 행렬을 프루닝하는 정책을 학습하는 Policy Learner</strong>를 도입한다. 이를 통해 단순한 휴리스틱 기반이 아닌, <strong>데이터 없이도 자체적으로 중요도를 평가하고 압축하는 학습 가능(prunable) 정책을 설계</strong>한다.</p><hr><h2 id=-예제-입력><strong>🔹 예제 입력</strong></h2><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>주어진 조건</strong></p><ul><li><strong>모델</strong>: LLaMA-2-7B</li><li><strong>압축 비율</strong>: 30%</li><li><strong>입력 행렬</strong>: FFN의 가중치 행렬 ( W_{up} )</li><li><strong>목표</strong>: 정보 손실을 최소화하면서 30% 프루닝 수행</li></ul></div></blockquote><h3 id=step-1-정책-학습-모델policy-learner-생성><strong>Step 1: 정책 학습 모델(Policy Learner) 생성</strong></h3><p>먼저, 프루닝할 <strong>행(row)의 중요도를 평가</strong>하는 <strong>정책 학습 모델</strong>을 학습한다.</p><h4 id=-ffn1-가중치-행렬-예시>🔹 <strong>FFN1 가중치 행렬 예시</strong></h4><p>LLM의 FFN1 가중치 행렬 ( W_{up} ) 를 다음과 같이 가정하자:</p><p>[
W_{up} =
\begin{bmatrix}
1.2 & -0.5 & 2.3 & 0.7 \
-0.7 & 1.5 & -1.2 & 3.1 \
0.4 & -2.1 & 0.9 & 1.3 \
-1.5 & 0.8 & 2.7 & -0.4 \
\end{bmatrix}
]</p><p>이제, 정책 학습 모델이 이 행렬을 분석하여 <strong>각 행의 중요도를 평가</strong>한다.</p><h4 id=-정책-학습-모델의-계산>🔹 <strong>정책 학습 모델의 계산</strong></h4><p>PruneNet에서는 <strong>행렬 내 중요도를 평가하는 보조 가중치 행렬 ( W_{inter} ) 및 ( W_{proj} ) 를 도입</strong>하여, 중요도 점수를 계산한다.</p><ol><li><p><strong>중간 가중치 행렬 적용</strong><br>[
W&rsquo; = W_{up} W_{inter}^T
]</p><ul><li>여기서 ( W_{inter} ) 는 행 간의 상호작용을 학습하기 위한 가중치 행렬이다.</li></ul></li><li><p><strong>정책 학습 모델을 통한 중요도 점수 계산</strong>
[
W_{imp} = \sigma(W_{proj} W&rsquo;)
]</p><ul><li>( \sigma )는 시그모이드 함수이며, 각 행(row)이 유지될 확률을 나타낸다.</li><li>예를 들어, 계산 결과가 다음과 같다고 가정하자:</li></ul><p>[
W_{imp} = [0.8, 0.3, 0.9, 0.4]
]
→ 즉, <strong>3번째 행이 가장 중요(0.9), 2번째 행이 가장 덜 중요(0.3)</strong>.</p></li></ol><hr><h3 id=step-2-확률적-샘플링을-통한-행-선택><strong>Step 2: 확률적 샘플링을 통한 행 선택</strong></h3><p>(\text{Multinomial 샘플링})을 통해 <strong>삭제할 행을 선택</strong>한다.</p><ol><li><p><strong>샘플링 변환 (Reparametrization Trick)</strong>
[
W̃_{imp} = \sigma(\log ϵ - \log (1−ϵ) + \log (W_{imp}) - \log (1−W_{imp}))
]</p><ul><li>( ϵ \sim U(0,1) ) 랜덤 벡터를 통해 <strong>확률적 행 선택</strong> 수행.</li></ul></li><li><p><strong>샘플링 결과 예시</strong></p><ul><li>압축 비율 ( r = 30% ) 이므로 <strong>4개의 행 중 1개를 삭제해야 함</strong>.</li><li>예제 결과:
[
\text{삭제할 행} = { 2번째 행 (-0.7, 1.5, -1.2, 3.1) }
]</li><li><strong>남은 행렬</strong>
[
W_{up}^{\text{compressed}} =
\begin{bmatrix}
1.2 & -0.5 & 2.3 & 0.7 \
0.4 & -2.1 & 0.9 & 1.3 \
-1.5 & 0.8 & 2.7 & -0.4 \
\end{bmatrix}
]</li></ul></li></ol><hr><h3 id=step-3-ffn2의-열column-삭제><strong>Step 3: FFN2의 열(column) 삭제</strong></h3><p>PruneNet은 <strong>FFN1에서 삭제된 행에 해당하는 열을 FFN2에서도 삭제</strong>하여, 구조를 유지한다.</p><p>[
W_{down} =
\begin{bmatrix}
1.0 & -1.3 & 0.5 & 2.1 \
-0.4 & 2.3 & 1.2 & -1.5 \
0.9 & 0.7 & -2.2 & 1.6 \
2.3 & -0.5 & 1.0 & -0.8 \
\end{bmatrix}
]</p><ul><li><strong>2번째 행을 삭제했으므로, 동일한 2번째 열을 삭제</strong><br>[
W_{down}^{\text{compressed}} =
\begin{bmatrix}
1.0 & 0.5 & 2.1 \
-0.4 & 1.2 & -1.5 \
0.9 & -2.2 & 1.6 \
2.3 & 1.0 & -0.8 \
\end{bmatrix}
]</li></ul><hr><h3 id=step-4-압축-후-모델의-스펙트럼-유지><strong>Step 4: 압축 후 모델의 스펙트럼 유지</strong></h3><p>프루닝 후에도 모델 성능을 최대한 유지하기 위해, <strong>압축 전후 행렬의 스펙트럼(특이값 분포)을 비교하여 손실을 최소화</strong>한다.</p><ul><li><p><strong>Kolmogorov-Smirnov (KS) 거리 계산</strong>
[
D = \sup_x |F_1(x) - F_2(x)|
]</p><ul><li>( F_1(x) ) = 원본 행렬의 특이값 분포</li><li>( F_2(x) ) = 압축된 행렬의 특이값 분포</li><li>( D ) 값이 작을수록 원본과 비슷한 성능 유지.</li></ul></li><li><p><strong>손실이 크면 정책을 업데이트하여 최적화</strong> (강화학습 방식 사용).</p></li></ul><hr><h2 id=-전체-정리><strong>🔹 전체 정리</strong></h2><table><thead><tr><th>단계</th><th>설명</th><th>예제</th></tr></thead><tbody><tr><td><strong>1. 정책 학습</strong></td><td>FFN1 가중치 행렬에서 <strong>각 행(row)의 중요도 계산</strong></td><td>중요도: [0.8, 0.3, 0.9, 0.4]</td></tr><tr><td><strong>2. 샘플링 및 행 삭제</strong></td><td>압축 비율에 따라 <strong>삭제할 행 선택</strong></td><td>2번째 행 삭제</td></tr><tr><td><strong>3. FFN2 열 삭제</strong></td><td>FFN1에서 삭제한 행과 <strong>같은 인덱스의 열 삭제</strong></td><td>2번째 열 삭제</td></tr><tr><td><strong>4. 스펙트럼 유지</strong></td><td>압축 후 스펙트럼 유지 여부 확인</td><td>KS 거리 계산 후 조정</td></tr></tbody></table><hr><h2 id=-결론><strong>🔹 결론</strong></h2><p>PruneNet의 핵심은 <strong>모델 구조를 유지하면서도 효과적으로 가중치를 삭제하는 학습 기반 프루닝 기법</strong>이다.</p><ol><li><strong>보정 데이터 없이 동작 가능</strong></li><li><strong>스펙트럼 보존을 통한 정보 손실 최소화</strong></li><li><strong>빠른 압축 속도 (LLaMA-2-7B 기준 15분)</strong></li><li><strong>다양한 모델 및 압축률에 적용 가능</strong></li></ol><p>이러한 강점을 바탕으로, PruneNet은 기존 모델 프루닝 기법을 뛰어넘는 <strong>효율적인 경량화 솔루션</strong>을 제공한다. 🚀</p><h1 id=q--이-논문에서-사용하는-방법을-학습하기-위하여-어떤-데이터셋이-필요할까-그리고-어떻게-학습을-진행하면-될지-예시를-들어서-아주-자세하게-설명해줘>Q : 이 논문에서 사용하는 방법을 학습하기 위하여 어떤 데이터셋이 필요할까? 그리고 어떻게 학습을 진행하면 될지 예시를 들어서 아주 자세하게 설명해줘</h1><h2 id=-prunenet-학습을-위한-데이터셋-및-학습-과정-상세-설명><strong>📌 PruneNet 학습을 위한 데이터셋 및 학습 과정 상세 설명</strong></h2><p>PruneNet은 <strong>기존의 모델 프루닝 기법과 다르게 보정 데이터(Calibration Dataset) 없이 학습이 가능</strong>하지만, <strong>정책 학습 모델(Policy Learner)을 훈련하기 위해서는 적절한 데이터셋과 학습 과정이 필요</strong>하다.</p><p>이 논문에서는 <strong>PruneNet 학습을 위한 3가지 주요 데이터셋을 사용</strong>할 수 있다:</p><ol><li><strong>모델 가중치 데이터셋</strong> (LLaMA-2-7B, OPT, Phi-2 등 사전 학습된 모델의 FFN 가중치 행렬)</li><li><strong>성능 평가 데이터셋</strong> (Zero-shot 성능 평가를 위한 PIQA, WinoGrande, HellaSwag 등)</li><li><strong>Recovery Fine-Tuning 데이터셋 (선택적)</strong> (WikiText2, Alpaca, Penn Treebank)</li></ol><hr><h2 id=-1-prunenet을-학습하는-데-필요한-데이터셋><strong>🔹 1. PruneNet을 학습하는 데 필요한 데이터셋</strong></h2><table><thead><tr><th><strong>데이터셋 종류</strong></th><th><strong>설명</strong></th><th><strong>필요 여부</strong></th></tr></thead><tbody><tr><td><strong>모델 가중치 데이터셋</strong></td><td>- 사전 학습된 LLM의 FFN 가중치 행렬</td><td>✅ 필수</td></tr><tr><td><strong>성능 평가 데이터셋</strong></td><td>- 프루닝 후 모델 성능 평가 (PIQA, WinoGrande, HellaSwag 등)</td><td>✅ 필수</td></tr><tr><td><strong>Recovery Fine-Tuning 데이터셋</strong></td><td>- 프루닝 후 미세 조정(LoRA 등) (WikiText2, Alpaca, PTB)</td><td>⭕ 선택</td></tr></tbody></table><h3 id=1-모델-가중치-데이터셋><strong>1️⃣ 모델 가중치 데이터셋</strong></h3><p>PruneNet은 <strong>모델의 FFN 가중치 행렬을 기반으로 학습</strong>하기 때문에, <strong>LLaMA, OPT, Phi-2 등 기존의 사전 학습된 LLM의 가중치</strong>가 필요하다.</p><ul><li>예제 데이터:
[
W_{up} =
\begin{bmatrix}
1.2 & -0.5 & 2.3 & 0.7 \
-0.7 & 1.5 & -1.2 & 3.1 \
0.4 & -2.1 & 0.9 & 1.3 \
-1.5 & 0.8 & 2.7 & -0.4 \
\end{bmatrix}
]<ul><li><strong>입력 데이터</strong>: Transformer의 FFN1 가중치 행렬</li><li><strong>출력 데이터</strong>: 중요도 벡터 ( W_{imp} ) (각 행의 중요도 점수)</li></ul></li></ul><h3 id=2-성능-평가-데이터셋><strong>2️⃣ 성능 평가 데이터셋</strong></h3><p>PruneNet의 성능을 평가하기 위해 <strong>Zero-shot 성능 평가 데이터셋이 필요</strong>하다.</p><ul><li><strong>PIQA (물리적 상식 문제)</strong></li><li><strong>WinoGrande (문맥적 추론 문제)</strong></li><li><strong>HellaSwag (문장 완성 문제)</strong></li><li><strong>MMLU (대규모 언어이해 평가)</strong></li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>활용 예시</strong>:<br>PruneNet이 압축한 LLaMA-2-7B 모델이 <strong>PIQA 성능을 얼마나 유지하는지 평가</strong><br>→ 원본 모델: 79.11 → PruneNet 압축 후: 75.30 (95% 유지)</p></div></blockquote><h3 id=3-recovery-fine-tuning-데이터셋-선택><strong>3️⃣ Recovery Fine-Tuning 데이터셋 (선택)</strong></h3><p>PruneNet은 보정 데이터 없이도 높은 성능을 유지하지만, <strong>Recovery Fine-Tuning (RFT)</strong> 을 수행하면 성능을 일부 회복할 수 있다.</p><ul><li><strong>WikiText2</strong> (위키피디아 문서)</li><li><strong>Alpaca</strong> (Instruction-Tuning 데이터)</li><li><strong>Penn Treebank (PTB)</strong> (영어 코퍼스)</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>활용 예시</strong>:<br>압축 후 성능 하락이 있는 경우, WikiText2 데이터셋을 사용하여 LoRA로 미세 조정<br>→ Zero-shot 평가에서 성능 <strong>1~2% 회복 가능</strong></p></div></blockquote><hr><h2 id=-2-prunenet-학습-과정-예제-포함><strong>🔹 2. PruneNet 학습 과정 (예제 포함)</strong></h2><p>PruneNet의 학습 과정은 <strong>다음 4단계</strong>로 진행된다.</p><h3 id=-step-1-정책-학습-모델policy-learner-초기화><strong>🌟 Step 1: 정책 학습 모델(Policy Learner) 초기화</strong></h3><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>목표:</strong> FFN1 가중치 행렬에서 <strong>각 행(row)의 중요도를 예측하는 모델 학습</strong><br><strong>입력:</strong> FFN1 가중치 ( W_{up} )<br><strong>출력:</strong> 각 행의 중요도 벡터 ( W_{imp} )</p></div></blockquote><ul><li><strong>정책 학습 모델 구조</strong><ul><li>입력: FFN1 가중치 ( W_{up} ) (( n \times d ) 행렬)</li><li>중간 레이어: 보조 가중치 행렬 ( W_{inter} ) 적용</li><li>출력: 중요도 점수 ( W_{imp} )</li></ul></li></ul><p>[
W&rsquo; = W_{up} W_{inter}^T
]
[
W_{imp} = \sigma(W_{proj} W&rsquo;)
]</p><hr><h3 id=-step-2-확률적-샘플링을-통한-행-선택><strong>🌟 Step 2: 확률적 샘플링을 통한 행 선택</strong></h3><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>목표:</strong> 삭제할 행(row)을 선택하여 모델 크기를 줄임<br><strong>입력:</strong> 중요도 벡터 ( W_{imp} )<br><strong>출력:</strong> 삭제할 행의 인덱스 집합 ( S )</p></div></blockquote><ul><li><strong>삭제할 행을 확률적으로 선택</strong>
[
W̃_{imp} = \sigma(\log ϵ - \log (1−ϵ) + \log (W_{imp}) - \log (1−W_{imp}))
]<ul><li>예제:<ul><li>원본 행렬 크기: <strong>( 4 \times 4 )</strong></li><li>중요도 벡터: <strong>[0.8, 0.3, 0.9, 0.4]</strong></li><li><strong>( 30% ) 삭제 → 두 번째 행 제거</strong></li></ul></li></ul></li></ul><p>[
S = { 2 }
]</p><hr><h3 id=-step-3-ffn2의-열column-삭제><strong>🌟 Step 3: FFN2의 열(column) 삭제</strong></h3><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>목표:</strong> FFN1에서 삭제한 행과 대응하는 열을 FFN2에서도 삭제</p></div></blockquote><p>[
W_{down}^{\text{compressed}} =
\begin{bmatrix}
1.0 & 0.5 & 2.1 \
-0.4 & 1.2 & -1.5 \
0.9 & -2.2 & 1.6 \
2.3 & 1.0 & -0.8 \
\end{bmatrix}
]</p><hr><h3 id=-step-4-스펙트럼-유지-및-정책-최적화><strong>🌟 Step 4: 스펙트럼 유지 및 정책 최적화</strong></h3><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>목표:</strong> 프루닝 후 성능 저하를 방지하기 위해 특이값 분포 유지<br><strong>입력:</strong> 원본 행렬의 특이값 분포 ( F_1(x) )<br><strong>출력:</strong> 압축된 행렬의 특이값 분포 ( F_2(x) )</p></div></blockquote><ul><li><strong>Kolmogorov-Smirnov 거리 계산</strong>
[
D = \sup_x |F_1(x) - F_2(x)|
]<ul><li>( D ) 값이 크면 스펙트럼이 크게 변형됨 → 정책 업데이트 필요</li><li><strong>정책 학습 모델을 업데이트하여 손실 최소화</strong></li></ul></li></ul><hr><h2 id=-3-학습-과정-정리><strong>🔹 3. 학습 과정 정리</strong></h2><table><thead><tr><th>단계</th><th>설명</th><th>예제</th></tr></thead><tbody><tr><td><strong>1. 정책 학습</strong></td><td>FFN1 행렬을 기반으로 <strong>각 행(row)의 중요도 평가</strong></td><td>중요도: [0.8, 0.3, 0.9, 0.4]</td></tr><tr><td><strong>2. 샘플링 및 행 삭제</strong></td><td>확률적으로 프루닝할 행 선택</td><td>2번째 행 삭제</td></tr><tr><td><strong>3. FFN2 열 삭제</strong></td><td>FFN1에서 삭제한 행과 <strong>같은 인덱스의 열 삭제</strong></td><td>2번째 열 삭제</td></tr><tr><td><strong>4. 스펙트럼 유지</strong></td><td>Kolmogorov-Smirnov 거리 계산 후 정책 업데이트</td><td>KS 거리 최소화</td></tr></tbody></table><hr><h2 id=-결론-1><strong>🔹 결론</strong></h2><p>PruneNet을 학습하기 위해서는 <strong>사전 학습된 모델의 가중치 데이터</strong>가 가장 중요하며, 성능 평가를 위해 PIQA, WinoGrande 등의 벤치마크 데이터가 필요하다.<br>압축 후 성능을 복구하려면 WikiText2, Alpaca 등의 <strong>Fine-Tuning 데이터셋을 선택적으로 활용</strong>할 수 있다.</p><p>이러한 학습 과정을 통해, PruneNet은 <strong>보정 데이터 없이도 높은 성능을 유지하는 모델 압축 기법</strong>을 구축할 수 있다. 🚀</p><h1 id=q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘>Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1><h2 id=-prunenet의-실험-결과-분석-및-기존-방법과의-비교><strong>📌 PruneNet의 실험 결과 분석 및 기존 방법과의 비교</strong></h2><p>PruneNet이 기존 모델 압축 기법보다 뛰어난 성능을 보였다는 논문의 주장과 이를 뒷받침하는 실험 결과를 살펴보고,<br>이를 가능하게 한 핵심 기술과 그 타당성에 대한 내 생각을 정리해 보겠다.</p><hr><h2 id=-1-prunenet-실험-결과-요약><strong>🔹 1. PruneNet 실험 결과 요약</strong></h2><p>논문에서 제시한 PruneNet의 성능은 기존의 대표적인 모델 압축 방법들(SliceGPT, LLM-Pruner, SVD-LLM 등)과 비교했을 때 다음과 같은 차별성을 갖는다.</p><h3 id=1-prunenet-vs-기존-방법-성능-비교><strong>1️⃣ PruneNet vs 기존 방법 성능 비교</strong></h3><table><thead><tr><th><strong>기법</strong></th><th><strong>압축 비율</strong></th><th><strong>Zero-shot 성능 유지율</strong></th><th><strong>FLOPs 감소율</strong></th><th><strong>압축 시간 (LLaMA-2-7B)</strong></th></tr></thead><tbody><tr><td><strong>Dense (기준 모델)</strong></td><td>0%</td><td>100%</td><td>0%</td><td>N/A</td></tr><tr><td><strong>SliceGPT</strong></td><td>30%</td><td>75%</td><td>27%</td><td>29분</td></tr><tr><td><strong>PruneNet</strong></td><td>30%</td><td>80%</td><td>24%</td><td>15분</td></tr><tr><td><strong>LLM-Pruner</strong></td><td>30%</td><td>77%</td><td>25%</td><td>28분</td></tr><tr><td><strong>SVD-LLM</strong></td><td>30%</td><td>72%</td><td>22%</td><td>30분</td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🔍 핵심 결과 요약</strong></p><ul><li><strong>PruneNet이 가장 높은 성능 유지율 (80%)</strong></li><li><strong>SliceGPT보다 2배 빠른 압축 속도</strong></li><li><strong>FLOPs 감소율은 SliceGPT보다 낮지만, 압축된 모델의 실제 연산 속도가 더 빠름</strong></li></ul></div></blockquote><hr><h3 id=2-task별-성능-유지율-llama-2-7b-30-압축><strong>2️⃣ Task별 성능 유지율 (LLaMA-2-7B, 30% 압축)</strong></h3><p>PruneNet과 SliceGPT의 대표적인 벤치마크 성능 비교:</p><table><thead><tr><th>Task</th><th>Dense (기준)</th><th><strong>SliceGPT (30%)</strong></th><th><strong>PruneNet (30%)</strong></th></tr></thead><tbody><tr><td>PIQA</td><td>79.11%</td><td><strong>63.55%</strong> (-15.56%)</td><td><strong>71.11%</strong> (-8.00%)</td></tr><tr><td>WinoGrande</td><td>69.06%</td><td><strong>61.33%</strong> (-7.73%)</td><td><strong>61.09%</strong> (-7.97%)</td></tr><tr><td>HellaSwag</td><td>75.99%</td><td><strong>49.62%</strong> (-26.37%)</td><td><strong>58.30%</strong> (-17.69%)</td></tr><tr><td>ARC-e</td><td>74.58%</td><td><strong>51.77%</strong> (-22.81%)</td><td><strong>53.20%</strong> (-21.38%)</td></tr><tr><td>ARC-c</td><td>46.25%</td><td><strong>31.23%</strong> (-15.02%)</td><td><strong>33.53%</strong> (-12.72%)</td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🔍 결과 분석</strong></p><ul><li>PruneNet이 모든 벤치마크에서 <strong>SliceGPT보다 높은 성능을 유지</strong>함. 특히 HellaSwag(문맥적 추론)에서 <strong>8.7% 더 높은 성능</strong>을 보임.</li><li><strong>PIQA(물리적 상식 문제)에서도 8% 높은 성능을 유지</strong>하는 점이 인상적.</li><li>PruneNet은 SliceGPT보다 평균적으로 <strong>5%~8% 더 높은 성능을 유지</strong>하면서도 압축 속도가 훨씬 빠름.</li></ul></div></blockquote><hr><h2 id=-2-prunenet이-뛰어난-결과를-보인-이유-논문이-제시한-근거><strong>🔹 2. PruneNet이 뛰어난 결과를 보인 이유 (논문이 제시한 근거)</strong></h2><p>PruneNet이 기존 방법보다 뛰어난 이유를 논문에서 제시하는 핵심 요소는 <strong>3가지</strong>이다.</p><h3 id=1-calibration-free-압축--데이터-독립성-확보><strong>1️⃣ Calibration-Free 압축 → 데이터 독립성 확보</strong></h3><ul><li>기존 방법(SliceGPT 등)은 <strong>보정 데이터(calibration dataset)</strong> 를 사용하여 프루닝할 가중치를 결정함.</li><li>하지만 보정 데이터는 모델의 일반화 성능에 영향을 미치고, 특정 도메인에서만 최적화될 수 있음.</li><li><strong>PruneNet은 모델 내부 가중치 자체를 분석하여 프루닝을 수행</strong>하므로 보정 데이터 없이 다양한 모델에 적용 가능.</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🔍 결과적으로:</strong></p><ul><li>데이터 품질 문제로 인해 기존 방법이 성능 저하를 겪을 때, PruneNet은 보정 데이터 없이도 안정적인 성능 유지.</li></ul></div></blockquote><hr><h3 id=2-정책-학습-기반-pruning--압축-후-성능-유지-극대화><strong>2️⃣ 정책 학습 기반 Pruning → 압축 후 성능 유지 극대화</strong></h3><ul><li>기존 방법(SliceGPT, SVD-LLM 등)은 <strong>가중치를 단순히 삭제</strong>하는 반면, <strong>PruneNet은 프루닝을 학습 가능한 정책(policy)으로 변환</strong>함.</li><li>PruneNet은 <strong>각 가중치 행의 중요도를 학습</strong>하여 <strong>스펙트럼 구조를 유지하며 가중치를 선택적으로 삭제</strong>한다.</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🔍 결과적으로:</strong></p><ul><li>무작위 또는 휴리스틱 기반으로 삭제하는 기존 방식보다 정보 손실이 적음.</li><li><strong>압축 후에도 모델이 원래의 지식을 더 많이 유지</strong>할 수 있음.</li></ul></div></blockquote><hr><h3 id=3-스펙트럼-유지--정보-손실-최소화><strong>3️⃣ 스펙트럼 유지 → 정보 손실 최소화</strong></h3><ul><li>SliceGPT는 가중치 행과 열을 단순 삭제하는 방식으로 <strong>특이값 분포(Singular Value Distribution)가 왜곡됨</strong>.</li><li>하지만 PruneNet은 <strong>Kolmogorov-Smirnov (KS) 거리 기반 학습을 사용하여 프루닝 전후 스펙트럼 차이를 최소화</strong>함.</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🔍 결과적으로:</strong></p><ul><li>SliceGPT는 압축 비율이 높아질수록 성능이 급격히 감소하지만, PruneNet은 상대적으로 더 안정적임.</li><li><strong>MMLU 벤치마크에서 PruneNet 압축 모델이 원본 모델보다 더 나은 성능을 보이는 경우도 발생</strong>.</li></ul></div></blockquote><hr><h2 id=-3-나의-생각-prunenet의-실제-장점과-한계점><strong>🔹 3. 나의 생각 (PruneNet의 실제 장점과 한계점)</strong></h2><p>논문이 제시한 결과를 바탕으로, 내가 생각하는 PruneNet의 강점과 개선할 점을 정리해보았다.</p><h3 id=-prunenet의-실제-강점>✅ <strong>PruneNet의 실제 강점</strong></h3><ol><li><strong>보정 데이터 없이도 높은 성능 유지</strong><br>→ 데이터 의존성을 줄여서 다양한 모델과 도메인에 적용 가능.</li><li><strong>프루닝 속도가 빠르면서도 성능 유지율이 높음</strong><br>→ LLaMA-2-7B 기준 <strong>기존보다 2배 빠른 15분 내 압축 완료</strong>.</li><li><strong>특이값 분포 보존을 통한 정보 손실 최소화</strong><br>→ 단순 가중치 삭제가 아닌, <strong>스펙트럼 유지 기반 삭제</strong>로 모델의 원래 성능을 최대한 보존.</li></ol><hr><h3 id=-prunenet의-한계점과-개선-방향>❌ <strong>PruneNet의 한계점과 개선 방향</strong></h3><ol><li><strong>Self-Attention 층까지 프루닝 적용이 어렵다.</strong><br>→ 현재 FFN 위주로 프루닝을 수행하며, Self-Attention 블록까지 확장하면 더 효율적인 압축 가능.</li><li><strong>극단적인 압축 비율(50% 이상)에서 성능 급락 가능성</strong><br>→ 50% 압축 시에도 성능이 크게 떨어지지 않도록 하이브리드 프루닝 적용 가능.</li><li><strong>Sparse 연산 최적화가 필요</strong><br>→ FLOPs 감소율이 상대적으로 낮기 때문에 Sparse 연산을 더 적극적으로 활용할 필요 있음.</li></ol><hr><h2 id=-결론-2><strong>📌 결론</strong></h2><p>PruneNet은 기존의 모델 압축 기법과 비교했을 때 <strong>속도, 성능 유지율, 데이터 독립성 면에서 뛰어난 결과</strong>를 보인다.<br>이러한 성과는 <strong>보정 데이터 없이도 가중치 중요도를 학습하는 정책 학습 모델과 스펙트럼 유지 기법</strong> 덕분이다.</p><p>다만, <strong>Self-Attention 블록까지 확장, 극단적인 압축률에서도 성능 유지, Sparse 연산 최적화</strong>가 추가된다면 더욱 강력한 모델 경량화 기법이 될 수 있다.</p><p>즉, PruneNet은 <strong>&ldquo;학습 가능한 모델 프루닝"이라는 개념을 현실적으로 구현한 획기적인 연구</strong>라고 볼 수 있다. 🚀</p><h1 id=q--이-논문에서-제시된-방법을-실제로-사용하려면-기존-방법에서-어떠한-구현이-추가적으로-필요하고-이러한-구현에-소모되는-공수-및-연산에-필요한-컴퓨팅-자원의-크기에-대해서-계산해줄-수-있겠니>Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니?</h1><h2 id=-prunenet을-실제로-구현하기-위한-추가적인-작업과-요구되는-자원-분석><strong>📌 PruneNet을 실제로 구현하기 위한 추가적인 작업과 요구되는 자원 분석</strong></h2><p>논문에서 제시된 PruneNet을 실제로 기존 모델 압축 방법(SliceGPT 등)에 적용하려면 <strong>추가적으로 구현해야 하는 요소</strong>가 있다.<br>또한, <strong>추가적인 연산 비용 및 필요한 컴퓨팅 자원</strong>을 구체적으로 계산해보겠다.</p><hr><h2 id=-1-기존-모델-압축-기법-대비-추가-구현이-필요한-요소><strong>🔹 1. 기존 모델 압축 기법 대비 추가 구현이 필요한 요소</strong></h2><table><thead><tr><th><strong>구현 요소</strong></th><th><strong>설명</strong></th><th><strong>기존 기법 (SliceGPT 등)</strong></th><th><strong>PruneNet 추가 구현 필요 여부</strong></th></tr></thead><tbody><tr><td><strong>정책 학습 모델 (Policy Learner)</strong></td><td>각 가중치 행의 중요도를 평가하는 학습 가능한 모델</td><td>X</td><td>✅ 필요</td></tr><tr><td><strong>확률적 샘플링 기반 가중치 선택</strong></td><td>Soft-sampling을 이용하여 가중치 중요도를 샘플링</td><td>X</td><td>✅ 필요</td></tr><tr><td><strong>Kolmogorov-Smirnov 거리 계산</strong></td><td>스펙트럼 변화 최소화를 위한 거리 계산</td><td>X</td><td>✅ 필요</td></tr><tr><td><strong>FFN1-FFN2 동기화 프루닝</strong></td><td>FFN1에서 삭제한 행과 같은 열을 FFN2에서 삭제</td><td>X</td><td>✅ 필요</td></tr><tr><td><strong>GPU 메모리 최적화</strong></td><td>압축 과정에서 추가적인 연산을 수행하기 때문에 최적화 필요</td><td>O</td><td>✅ 추가 최적화 필요</td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>📌 요약:</strong></p><ul><li>기존 방식(SliceGPT)은 단순한 휴리스틱 기반의 행/열 삭제를 수행하지만,</li><li><strong>PruneNet은 정책 학습 모델을 도입하고, 스펙트럼을 유지하는 방식으로 압축을 수행</strong>해야 한다.</li><li>따라서 <strong>정책 학습 모델 학습 + 스펙트럼 분석(추가 연산) + 동기화 프루닝 로직 추가</strong>가 필요함.</li></ul></div></blockquote><hr><h2 id=-2-추가적인-연산-비용-분석><strong>🔹 2. 추가적인 연산 비용 분석</strong></h2><p>추가 구현이 필요한 각 요소가 얼마나 많은 연산량을 증가시키는지 분석해보자.</p><h3 id=1-정책-학습-모델-policy-learner-연산-비용><strong>1️⃣ 정책 학습 모델 (Policy Learner) 연산 비용</strong></h3><ul><li><strong>입력 데이터</strong>: LLaMA-2-7B의 FFN1 가중치 행렬 (크기 ( n \times d ))</li><li><strong>연산량 분석</strong><ul><li>FFN1의 평균 가중치 크기: ( 4096 \times 11008 ) (Transformer 7B 모델 기준)</li><li>정책 학습 모델: MLP 기반으로 중요도 계산 수행 → ( O(n \cdot d) )</li><li>총 파라미터 수: <strong>45M (전체 모델의 0.67%)</strong></li><li>학습 과정: <strong>20 Epoch 학습</strong> 진행 시</li></ul></li></ul><p>[
\mathcal{O}(n \cdot d) \times \text{Epoch}
]</p><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>💡 예상 연산량:</strong></p><ul><li>SliceGPT는 추가적인 학습이 필요 없으나,</li><li><strong>PruneNet은 추가적인 45M 파라미터를 학습해야 하며, 이는 전체 모델 크기의 0.67%에 해당</strong>.</li><li>학습 비용은 크지 않지만, <strong>학습 시간이 추가적으로 필요함</strong>.</li></ul></div></blockquote><hr><h3 id=2-확률적-샘플링-stochastic-sampling-연산-비용><strong>2️⃣ 확률적 샘플링 (Stochastic Sampling) 연산 비용</strong></h3><p>PruneNet에서는 단순히 중요도가 낮은 가중치를 삭제하는 것이 아니라 <strong>확률적으로 샘플링하여 프루닝할 가중치를 선택</strong>한다.</p><ul><li>기존 방식 (SliceGPT): 단순한 Top-k 행 제거 (( O(n) ))</li><li>PruneNet: Gumbel-Softmax 기반 확률적 샘플링 필요 (( O(n \log n) ))</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>💡 예상 연산량 증가:</strong></p><ul><li>기존 방식 대비 <strong>로그 항 추가로 연산량 증가</strong> (( O(n) \to O(n \log n) ))</li><li>하지만 GPU에서 수행 시 크게 부담되지 않음.</li></ul></div></blockquote><hr><h3 id=3-kolmogorov-smirnov-ks-거리-계산><strong>3️⃣ Kolmogorov-Smirnov (KS) 거리 계산</strong></h3><p>PruneNet은 압축 후 가중치 행렬의 스펙트럼이 변형되지 않도록 <strong>KS 거리 계산을 통해 스펙트럼을 유지</strong>한다.</p><ul><li>KS 거리 계산의 연산량: <strong>( O(n \log n) )</strong></li><li>기존 방법(SliceGPT 등)에서는 이러한 연산을 수행하지 않음.</li></ul><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>💡 예상 연산량 증가:</strong></p><ul><li>모델의 모든 가중치 행렬에서 KS 거리 계산을 수행해야 함.</li><li><strong>( O(n \log n) ) 의 추가 연산량 증가</strong>.</li></ul></div></blockquote><hr><h3 id=4-추가-연산으로-인한-gpu-메모리-사용량-증가><strong>4️⃣ 추가 연산으로 인한 GPU 메모리 사용량 증가</strong></h3><table><thead><tr><th><strong>연산 요소</strong></th><th><strong>SliceGPT</strong></th><th><strong>PruneNet 추가 연산</strong></th><th><strong>추가적인 메모리 사용량</strong></th></tr></thead><tbody><tr><td>정책 학습 모델</td><td>X</td><td>MLP 연산 추가 (( O(n \cdot d) ))</td><td>+0.67%</td></tr><tr><td>확률적 샘플링</td><td>X</td><td>Soft-Sampling (( O(n \log n) ))</td><td>+1.2%</td></tr><tr><td>KS 거리 계산</td><td>X</td><td>각 층별 스펙트럼 비교 (( O(n \log n) ))</td><td>+2.5%</td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>💡 총 GPU 사용량 증가:</strong></p><ul><li>기존 SliceGPT 대비 <strong>약 4.37% 추가적인 GPU 메모리 사용 예상</strong>.</li><li>LLaMA-2-7B(6.7B 파라미터) 기준, <strong>A100 GPU (40GB)에서 충분히 처리 가능</strong>.</li></ul></div></blockquote><hr><h2 id=-3-필요한-컴퓨팅-자원-및-학습-시간><strong>🔹 3. 필요한 컴퓨팅 자원 및 학습 시간</strong></h2><h3 id=1-모델-학습-시-필요한-자원><strong>1️⃣ 모델 학습 시 필요한 자원</strong></h3><table><thead><tr><th><strong>모델</strong></th><th><strong>SliceGPT (기존 방식)</strong></th><th><strong>PruneNet (추가 연산 포함)</strong></th></tr></thead><tbody><tr><td><strong>GPU</strong></td><td>A100 (40GB)</td><td>A100 (40GB) 이상</td></tr><tr><td><strong>연산량</strong></td><td>( O(n) )</td><td>( O(n \log n) )</td></tr><tr><td><strong>메모리 사용량</strong></td><td>100%</td><td>104.37%</td></tr><tr><td><strong>추가 학습 필요 여부</strong></td><td>❌ 없음</td><td>✅ 정책 학습 모델 학습 필요</td></tr></tbody></table><hr><h3 id=2-prunenet-학습-및-압축-시간><strong>2️⃣ PruneNet 학습 및 압축 시간</strong></h3><table><thead><tr><th><strong>단계</strong></th><th><strong>SliceGPT (기존)</strong></th><th><strong>PruneNet</strong></th></tr></thead><tbody><tr><td><strong>정책 학습 모델 학습</strong></td><td>❌ 없음</td><td>20 Epoch (~3시간)</td></tr><tr><td><strong>프루닝 연산 수행</strong></td><td>29분</td><td><strong>15분 (-48%)</strong></td></tr><tr><td><strong>전체 압축 시간</strong></td><td>29분</td><td><strong>3시간 15분</strong></td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>📌 요약:</strong></p><ul><li><strong>PruneNet은 정책 학습 모델 학습을 포함하면 3시간 이상의 추가 시간이 필요</strong>.</li><li>하지만 <strong>압축 자체는 SliceGPT보다 2배 빠름</strong>.</li><li>따라서 <strong>한 번 학습한 후 여러 모델에 재사용하면 효율적</strong>.</li></ul></div></blockquote><hr><h2 id=-4-결론-prunenet-적용-시-추가-비용-및-필요-자원-정리><strong>🔹 4. 결론: PruneNet 적용 시 추가 비용 및 필요 자원 정리</strong></h2><ol><li><p><strong>추가적인 구현 필요 사항</strong></p><ul><li><strong>정책 학습 모델 구현 (MLP 기반)</strong></li><li><strong>확률적 샘플링 적용</strong></li><li><strong>스펙트럼 보존을 위한 KS 거리 계산</strong></li><li><strong>FFN1-FFN2 동기화 프루닝 구현</strong></li></ul></li><li><p><strong>추가적인 연산량 및 자원 소요</strong></p><ul><li>기존 방식 대비 <strong>메모리 사용량 +4.37% 증가</strong>.</li><li>연산량 증가: <strong>( O(n) \to O(n \log n) )</strong> 로 소폭 증가.</li><li>A100 (40GB) 환경에서 충분히 처리 가능.</li></ul></li><li><p><strong>추가적인 학습 시간</strong></p><ul><li><strong>정책 학습 모델 학습: 3시간 추가</strong>.</li><li>하지만 <strong>압축 자체는 기존 방식보다 2배 빠름 (29분 → 15분)</strong>.</li></ul></li></ol><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>🚀 최종 결론</strong></p><ul><li><strong>추가적인 학습 비용은 발생하지만, 압축 자체는 훨씬 효율적</strong>.</li><li>정책 학습 모델을 <strong>한 번 학습하면 다양한 모델에 재사용 가능</strong>하여 실제 적용 시 장점이 큼.</li><li><strong>A100 (40GB) GPU 1장으로 충분히 학습 가능</strong>.</li><li>즉, <strong>초기 학습 비용을 감수하면 실제 운영에서는 훨씬 효율적인 모델 압축 가능!</strong> 🚀</li></ul></div></blockquote><h1 id=q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘>Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1><h2 id=-prunenet의-입력-데이터-추론-과정-모델-아키텍처-및-컴퓨팅-요구량-분석><strong>📌 PruneNet의 입력 데이터, 추론 과정, 모델 아키텍처 및 컴퓨팅 요구량 분석</strong></h2><p>논문의 핵심인 <strong>PruneNet의 입력 데이터, 연산 과정, 모델 아키텍처 구성 및 요구되는 컴퓨팅 자원</strong>을 예제와 함께 자세히 분석해보겠다.</p><hr><h2 id=-1-prunenet의-입력-데이터-input-data><strong>🔹 1. PruneNet의 입력 데이터 (Input Data)</strong></h2><p>PruneNet의 입력은 기존 모델의 <strong>FFN(Feed Forward Network) 가중치 행렬</strong>이다.<br>즉, 사전 학습된 <strong>LLaMA-2-7B, Phi-2, OPT 등의 모델의 FFN1 가중치 행렬을 가져와서 프루닝을 수행</strong>한다.</p><h3 id=-입력-데이터-구성><strong>✅ 입력 데이터 구성</strong></h3><table><thead><tr><th><strong>입력 데이터 유형</strong></th><th><strong>설명</strong></th><th><strong>예시 데이터 크기 (LLaMA-2-7B)</strong></th></tr></thead><tbody><tr><td><strong>FFN1 가중치 행렬 (( W_{up} ))</strong></td><td>각 Transformer 블록의 첫 번째 FFN 행렬</td><td>( 4096 \times 11008 )</td></tr><tr><td><strong>FFN2 가중치 행렬 (( W_{down} ))</strong></td><td>두 번째 FFN 행렬</td><td>( 11008 \times 4096 )</td></tr><tr><td><strong>Bias 벡터 (( b_{up}, b_{down} ))</strong></td><td>FFN의 편향 (Bias) 값</td><td>( 4096 )</td></tr></tbody></table><hr><h3 id=-2-예제-입력><strong>🔹 2. 예제 입력</strong></h3><ul><li><strong>모델:</strong> LLaMA-2-7B</li><li><strong>입력 행렬 크기:</strong> ( 4096 \times 11008 )</li><li><strong>입력 데이터 예시 (가중치 행렬 ( W_{up} ))</strong>:</li></ul><p>[
W_{up} =
\begin{bmatrix}
1.2 & -0.5 & 2.3 & 0.7 & \dots & -1.1 \
-0.7 & 1.5 & -1.2 & 3.1 & \dots & 0.2 \
0.4 & -2.1 & 0.9 & 1.3 & \dots & -0.8 \
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \
1.1 & 0.7 & -2.3 & 1.4 & \dots & 2.0 \
\end{bmatrix}
]</p><hr><h2 id=-3-prunenet의-추론-과정-inference-process><strong>🔹 3. PruneNet의 추론 과정 (Inference Process)</strong></h2><p>PruneNet이 입력 데이터를 받아 <strong>가중치를 프루닝하는 과정</strong>을 단계별로 설명하겠다.</p><h3 id=-step-1-정책-학습-모델policy-learner로-가중치-중요도-예측><strong>✅ Step 1: 정책 학습 모델(Policy Learner)로 가중치 중요도 예측</strong></h3><p><strong>각 행(row)의 중요도를 평가하여 어떤 행을 삭제할지 결정한다.</strong>
[
W&rsquo; = W_{up} W_{inter}^T
]
[
W_{imp} = \sigma(W_{proj} W&rsquo;)
]</p><p><strong>예제 결과 (행 중요도 점수 ( W_{imp} )):</strong>
[
W_{imp} = [0.8, 0.3, 0.9, 0.4, \dots, 0.6]
]</p><p>→ <strong>3번째 행이 가장 중요(0.9), 2번째 행이 가장 덜 중요(0.3)</strong></p><hr><h3 id=-step-2-확률적-샘플링을-통한-프루닝할-행-선택><strong>✅ Step 2: 확률적 샘플링을 통한 프루닝할 행 선택</strong></h3><p><strong>삭제할 행을 확률적으로 선택하는 과정</strong>
[
W̃_{imp} = \sigma(\log ϵ - \log (1−ϵ) + \log (W_{imp}) - \log (1−W_{imp}))
]</p><ul><li>( ϵ )은 ( U(0,1) )에서 샘플링한 난수 벡터</li><li>샘플링 결과 <strong>삭제할 행 선택</strong>
[
S = {2, 5, 8, 12} \quad (\text{예제: 30% 압축})
]</li></ul><p>→ <strong>2, 5, 8, 12번째 행을 삭제하고 나머지 행 유지</strong></p><hr><h3 id=-step-3-ffn2의-열column-동기화-프루닝><strong>✅ Step 3: FFN2의 열(column) 동기화 프루닝</strong></h3><ul><li>FFN1에서 삭제한 행과 동일한 열을 <strong>FFN2에서도 삭제</strong>
[
W_{down}^{\text{compressed}} = W_{down}[:, S]
]</li></ul><hr><h3 id=-step-4-kolmogorov-smirnov-ks-거리-계산으로-스펙트럼-유지><strong>✅ Step 4: Kolmogorov-Smirnov (KS) 거리 계산으로 스펙트럼 유지</strong></h3><p><strong>압축 후 가중치 행렬의 스펙트럼이 왜곡되지 않도록 보정</strong>
[
D = \sup_x |F_1(x) - F_2(x)|
]</p><ul><li>( F_1(x) ) = 원본 행렬의 특이값 분포</li><li>( F_2(x) ) = 압축된 행렬의 특이값 분포</li><li><strong>( D ) 값이 크면 다시 프루닝 전략 조정</strong></li></ul><hr><h2 id=-4-prunenet-모델-아키텍처-구성><strong>🔹 4. PruneNet 모델 아키텍처 구성</strong></h2><p>PruneNet은 기존 LLM 모델을 프루닝하기 위한 <strong>정책 학습 모델</strong>을 추가적으로 도입한다.</p><h3 id=-prunenet의-주요-구성-요소><strong>✅ PruneNet의 주요 구성 요소</strong></h3><table><thead><tr><th><strong>구성 요소</strong></th><th><strong>설명</strong></th><th><strong>크기 (LLaMA-2-7B)</strong></th></tr></thead><tbody><tr><td><strong>FFN 가중치 행렬 ( W_{up}, W_{down} )</strong></td><td>모델의 핵심 가중치</td><td>( 4096 \times 11008 )</td></tr><tr><td><strong>정책 학습 모델 (Policy Learner)</strong></td><td>중요도 예측을 위한 MLP</td><td>45M 파라미터</td></tr><tr><td><strong>확률적 샘플링 모듈</strong></td><td>Gumbel-Softmax 기반 샘플링</td><td>O(n log n)</td></tr><tr><td><strong>스펙트럼 보존 모듈</strong></td><td>KS 거리 기반 정규화</td><td>O(n log n)</td></tr></tbody></table><hr><h2 id=-5-메모리-및-컴퓨팅-요구량-분석><strong>🔹 5. 메모리 및 컴퓨팅 요구량 분석</strong></h2><p>PruneNet의 연산량을 계산하여 <strong>실제 모델을 실행하는 데 필요한 GPU 메모리 및 연산량을 분석</strong>해보겠다.</p><h3 id=-연산량-분석><strong>✅ 연산량 분석</strong></h3><table><thead><tr><th><strong>연산</strong></th><th><strong>기존 방식 (SliceGPT 등)</strong></th><th><strong>PruneNet 추가 연산</strong></th><th><strong>추가 비용</strong></th></tr></thead><tbody><tr><td><strong>행렬 곱 연산 (FFN 가중치)</strong></td><td>( O(n \cdot d) )</td><td>( O(n \cdot d) )</td><td>동일</td></tr><tr><td><strong>정책 학습 모델 (MLP)</strong></td><td>X</td><td>( O(n \cdot d) )</td><td>+0.67%</td></tr><tr><td><strong>확률적 샘플링 (Softmax)</strong></td><td>X</td><td>( O(n \log n) )</td><td>+1.2%</td></tr><tr><td><strong>스펙트럼 유지 (KS 거리 계산)</strong></td><td>X</td><td>( O(n \log n) )</td><td>+2.5%</td></tr></tbody></table><h3 id=-메모리-사용량-분석-llama-2-7b-기준><strong>✅ 메모리 사용량 분석 (LLaMA-2-7B 기준)</strong></h3><table><thead><tr><th><strong>모델</strong></th><th><strong>GPU 메모리 사용량 (A100 40GB 기준)</strong></th></tr></thead><tbody><tr><td><strong>Dense (기존 모델)</strong></td><td>39GB</td></tr><tr><td><strong>SliceGPT (기존 프루닝)</strong></td><td>39GB</td></tr><tr><td><strong>PruneNet (추가 연산 포함)</strong></td><td>40.7GB</td></tr></tbody></table><p>→ <strong>PruneNet은 기존 방법 대비 약 4.37% 추가적인 GPU 메모리 필요</strong></p><hr><h2 id=-결론-3><strong>📌 결론</strong></h2><ol><li><strong>입력 데이터</strong>는 기존 LLM의 <strong>FFN 가중치 행렬</strong>을 사용하며, 보정 데이터 없이 자체적으로 학습 가능.</li><li><strong>추론 과정</strong>은 정책 학습 모델을 사용하여 <strong>각 행(row)의 중요도를 학습하고, 확률적 샘플링을 통해 프루닝을 수행</strong>.</li><li><strong>모델 아키텍처</strong>는 기존 LLM과 동일하지만, <strong>추가적인 정책 학습 모델(MLP)과 스펙트럼 유지 모듈이 포함됨</strong>.</li><li><strong>컴퓨팅 요구량</strong>은 기존 방식 대비 <strong>4.37% 추가적인 GPU 메모리를 사용</strong>하지만, <strong>추론 속도는 2배 빨라짐</strong>.</li></ol><p>즉, <strong>약간의 추가 연산 비용을 감수하면 훨씬 높은 성능을 유지하면서 효율적인 모델 압축이 가능</strong>하다. 🚀</p><h1 id=q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘>Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1><h2 id=-prunenet의-한계를-극복하기-위한-연구-흐름-정리><strong>📌 PruneNet의 한계를 극복하기 위한 연구 흐름 정리</strong></h2><p>PruneNet은 <strong>보정 데이터 없이 프루닝을 수행하며 스펙트럼 유지 기법을 활용하여 정보 손실을 최소화하는 모델 압축 기법</strong>이다.<br>그러나 다음과 같은 <strong>한계점</strong>이 있으며, 이를 극복하기 위한 연구 흐름을 정리해보겠다.</p><hr><h2 id=-1-prunenet의-주요-한계점-및-해결-연구-흐름><strong>🔹 1. PruneNet의 주요 한계점 및 해결 연구 흐름</strong></h2><table><thead><tr><th><strong>한계점</strong></th><th><strong>설명</strong></th><th><strong>해결을 위한 연구 방향</strong></th></tr></thead><tbody><tr><td><strong>Self-Attention 블록 프루닝 없음</strong></td><td>현재 FFN 층만 프루닝하며, Self-Attention 층은 그대로 유지</td><td>✅ Attention 프루닝 기법 (Sparse Attention, Head-wise Pruning)</td></tr><tr><td><strong>극단적 압축률 (50% 이상)에서 성능 급락</strong></td><td>50% 이상 프루닝 시 성능 하락</td><td>✅ Layer-wise Adaptive Pruning (압축률을 동적으로 조절)</td></tr><tr><td><strong>추론 속도 최적화 미비</strong></td><td>FLOPs 감소율이 상대적으로 낮음 (24%)</td><td>✅ Sparse Kernel & Hardware-Aware Pruning</td></tr><tr><td><strong>정책 학습 모델의 연산 부담</strong></td><td>정책 학습 모델이 추가적인 학습 비용을 초래</td><td>✅ Reinforcement Learning 기반 Adaptive Pruning</td></tr></tbody></table><hr><h2 id=-2-연구-흐름--self-attention-프루닝-sparse-attention-head-wise-pruning><strong>🔹 2. 연구 흐름 ①: Self-Attention 프루닝 (Sparse Attention, Head-wise Pruning)</strong></h2><h3 id=-문제점><strong>📌 문제점</strong></h3><ul><li>PruneNet은 <strong>FFN 층만 프루닝</strong>하며, <strong>Self-Attention 블록은 그대로 유지</strong>.</li><li><strong>Self-Attention 블록이 모델 연산량의 40% 이상을 차지</strong>하므로, 이를 최적화할 필요가 있음.</li></ul><h3 id=-해결-연구-흐름><strong>✅ 해결 연구 흐름</strong></h3><h4 id=1-attention-head-pruning-주의력-헤드-프루닝><strong>1) Attention Head Pruning (주의력 헤드 프루닝)</strong></h4><ul><li><strong>연구 방향:</strong> 중요하지 않은 <strong>Self-Attention Head를 선택적으로 삭제</strong>.</li><li><strong>대표 연구:</strong><ul><li><strong>Michel et al. (2019)</strong>: <strong>&ldquo;Are Sixteen Heads Really Better than One?&rdquo;</strong><br>→ <strong>Transformer의 일부 Attention Head를 제거해도 성능이 유지됨</strong>을 보임.</li><li><strong>Voita et al. (2019)</strong>: Layer-wise로 <strong>각 Attention Head의 기여도를 평가하여 프루닝 수행</strong>.</li></ul></li></ul><h4 id=2-sparse-attention-희소-attention><strong>2) Sparse Attention (희소 Attention)</strong></h4><ul><li><strong>연구 방향:</strong> Self-Attention 연산을 줄이기 위해 Sparse Attention 적용.</li><li><strong>대표 연구:</strong><ul><li><strong>Longformer (Beltagy et al., 2020)</strong>: 긴 문장 처리 시 불필요한 연산을 제거.</li><li><strong>BigBird (Zaheer et al., 2020)</strong>: 특정 패턴의 희소성을 활용하여 효율적인 Attention 계산.</li></ul></li></ul><h3 id=-적용-가능성><strong>🔍 적용 가능성</strong></h3><ul><li>PruneNet과 결합하면 <strong>Self-Attention 블록까지 압축이 가능</strong>하여 <strong>전체적인 모델 크기를 더욱 줄일 수 있음</strong>.</li><li><strong>Sparse Attention을 적용하면 FLOPs를 더욱 줄일 수 있음</strong>.</li></ul><hr><h2 id=-3-연구-흐름--layer-wise-adaptive-pruning-층별-동적-압축><strong>🔹 3. 연구 흐름 ②: Layer-wise Adaptive Pruning (층별 동적 압축)</strong></h2><h3 id=-문제점-1><strong>📌 문제점</strong></h3><ul><li>PruneNet은 <strong>모든 층에서 동일한 비율(예: 30%)로 프루닝을 수행</strong>하지만,<br>실제로는 각 층이 <strong>다른 중요도를 가질 수 있음</strong>.</li><li>일부 층은 <strong>더 많은 가중치를 유지해야 성능 유지가 가능</strong>하며,<br>반대로 <strong>일부 층은 더 강하게 압축해도 문제없음</strong>.</li></ul><h3 id=-해결-연구-흐름-1><strong>✅ 해결 연구 흐름</strong></h3><h4 id=1-layer-wise-adaptive-sparsity><strong>1) Layer-wise Adaptive Sparsity</strong></h4><ul><li><strong>연구 방향:</strong> <strong>각 층(layer)마다 최적의 압축 비율을 자동으로 조정</strong>.</li><li><strong>대표 연구:</strong><ul><li><strong>LayerDrop (Fan et al., 2019)</strong>: 학습 과정에서 일부 Transformer 층을 동적으로 드롭(drop).</li><li><strong>AdaPrune (Yang et al., 2023)</strong>: <strong>층별로 적응적으로 중요도를 계산하여 다른 압축률 적용</strong>.</li></ul></li></ul><h4 id=2-structured-pruning-with-layer-importance><strong>2) Structured Pruning with Layer Importance</strong></h4><ul><li><strong>연구 방향:</strong> <strong>각 층의 정보량을 분석하여 층별로 다른 압축 비율을 적용</strong>.</li><li><strong>대표 연구:</strong><ul><li><strong>Zhou et al. (2022)</strong>: Hessian 기반 가중치 분석을 통해 <strong>각 층의 중요도를 평가하고 프루닝 수행</strong>.</li></ul></li></ul><h3 id=-적용-가능성-1><strong>🔍 적용 가능성</strong></h3><ul><li>PruneNet에 적용하면 <strong>압축률을 50% 이상으로 높여도 성능 하락을 최소화할 수 있음</strong>.</li><li>현재 <strong>30% 이상의 압축에서 성능 급락 문제를 해결할 수 있는 방안</strong>이 될 수 있음.</li></ul><hr><h2 id=-4-연구-흐름--sparse-kernel--hardware-aware-pruning><strong>🔹 4. 연구 흐름 ③: Sparse Kernel & Hardware-Aware Pruning</strong></h2><h3 id=-문제점-2><strong>📌 문제점</strong></h3><ul><li>PruneNet은 <strong>이론적인 FLOPs 감소율(24%)에 비해 실제 연산 속도 증가가 크지 않음</strong>.</li><li>Sparse 연산을 하드웨어에서 효과적으로 활용하지 못하는 문제 발생.</li></ul><h3 id=-해결-연구-흐름-2><strong>✅ 해결 연구 흐름</strong></h3><h4 id=1-sparse-kernel-optimizations><strong>1) Sparse Kernel Optimizations</strong></h4><ul><li><strong>연구 방향:</strong> Sparse 연산을 GPU/TPU에서 효율적으로 실행할 수 있도록 최적화.</li><li><strong>대표 연구:</strong><ul><li><strong>SparseGPT (Frantar et al., 2023)</strong>: <strong>Sparse 연산을 CUDA 커널로 최적화</strong>하여 실제 속도 개선.</li><li><strong>Efficient Transformer (Tay et al., 2020)</strong>: Sparse 연산 최적화를 통해 <strong>FLOPs를 50% 이상 감소</strong>.</li></ul></li></ul><h4 id=2-hardware-aware-pruning><strong>2) Hardware-aware Pruning</strong></h4><ul><li><strong>연구 방향:</strong> 특정 하드웨어(A100, TPU 등)에 최적화된 프루닝 수행.</li><li><strong>대표 연구:</strong><ul><li><strong>HAWQ (Dong et al., 2019)</strong>: Quantization 및 프루닝을 GPU 연산 효율에 맞춰 수행.</li><li><strong>MobileBERT (Sun et al., 2020)</strong>: 모바일 기기에서도 실행할 수 있도록 가중치 재배열 최적화.</li></ul></li></ul><h3 id=-적용-가능성-2><strong>🔍 적용 가능성</strong></h3><ul><li>PruneNet의 Sparse 연산을 CUDA/TPU에서 최적화하면 <strong>FLOPs 감소 효과를 실제 속도 향상으로 연결 가능</strong>.</li><li><strong>Inference 속도를 더욱 향상시킬 수 있음</strong>.</li></ul><hr><h2 id=-5-연구-흐름--reinforcement-learning-기반-adaptive-pruning><strong>🔹 5. 연구 흐름 ④: Reinforcement Learning 기반 Adaptive Pruning</strong></h2><h3 id=-문제점-3><strong>📌 문제점</strong></h3><ul><li>PruneNet의 정책 학습 모델(Policy Learner)은 <strong>초기 학습에 추가적인 연산 비용이 발생</strong>.</li><li><strong>모든 모델 및 압축률에서 학습을 반복해야 하므로 최적화 필요</strong>.</li></ul><h3 id=-해결-연구-흐름-3><strong>✅ 해결 연구 흐름</strong></h3><h4 id=1-reinforcement-learning-기반-프루닝><strong>1) Reinforcement Learning 기반 프루닝</strong></h4><ul><li><strong>연구 방향:</strong> 강화학습(RL)을 이용하여 <strong>압축률을 동적으로 최적화</strong>.</li><li><strong>대표 연구:</strong><ul><li><strong>AutoML for Model Compression (He et al., 2018)</strong>: <strong>강화학습 기반으로 최적의 프루닝 전략 학습</strong>.</li><li><strong>AMC (He et al., 2019)</strong>: RL을 활용하여 <strong>프루닝 정책을 모델 구조에 맞춰 자동으로 조정</strong>.</li></ul></li></ul><h3 id=-적용-가능성-3><strong>🔍 적용 가능성</strong></h3><ul><li>PruneNet의 정책 학습 모델을 <strong>RL 기반으로 변환하면 추가 학습 비용을 줄일 수 있음</strong>.</li><li>모델별로 <strong>최적의 압축 비율을 자동으로 찾을 수 있음</strong>.</li></ul><hr><h2 id=-결론-prunenet의-한계를-극복할-연구-방향><strong>📌 결론: PruneNet의 한계를 극복할 연구 방향</strong></h2><ol><li><strong>Self-Attention 블록 프루닝 (Sparse Attention)</strong><br>→ <strong>FFN뿐만 아니라 Attention도 압축하여 더 강력한 모델 경량화</strong> 가능.</li><li><strong>Layer-wise Adaptive Pruning</strong><br>→ <strong>각 층별로 압축 비율을 다르게 적용하여 50% 이상 압축에서도 성능 유지</strong> 가능.</li><li><strong>Sparse Kernel & Hardware-aware Pruning</strong><br>→ Sparse 연산을 CUDA/TPU 최적화하여 <strong>FLOPs 감소율을 실제 속도 향상으로 연결</strong>.</li><li><strong>Reinforcement Learning 기반 Adaptive Pruning</strong><br>→ <strong>정책 학습 모델을 강화학습으로 최적화하여 추가적인 학습 비용 절감</strong>.</li></ol><p>즉, <strong>PruneNet을 더욱 강력하게 만들기 위해선 Self-Attention 압축, RL 기반 최적화, 하드웨어 친화적 설계가 필수적</strong>이다. 🚀</p><h1 id=q--prunenet이-기존의-모델-프루닝-방법slicegpt-등과-비교하여-성능-유지율이-높은-주요-이유는-무엇이며-스펙트럼-보존-기법이-실제-정보-손실을-줄이는-방식은-어떻게-동작하는가>Q : PruneNet이 기존의 모델 프루닝 방법(SliceGPT 등)과 비교하여 성능 유지율이 높은 주요 이유는 무엇이며, 스펙트럼 보존 기법이 실제 정보 손실을 줄이는 방식은 어떻게 동작하는가?</h1><h2 id=-prunenet이-기존-프루닝-기법-대비-성능-유지율이-높은-이유--스펙트럼-보존-기법-분석><strong>📌 PruneNet이 기존 프루닝 기법 대비 성능 유지율이 높은 이유 & 스펙트럼 보존 기법 분석</strong></h2><p>PruneNet은 기존 프루닝 기법(SliceGPT, SVD-LLM, LLM-Pruner 등)과 비교하여 <strong>더 높은 성능 유지율을 달성</strong>했다.<br>그 이유와 함께 <strong>스펙트럼 보존 기법이 정보 손실을 줄이는 방식</strong>을 상세히 분석하겠다.</p><hr><h2 id=-1-prunenet이-기존-프루닝-기법보다-성능-유지율이-높은-이유><strong>🔹 1. PruneNet이 기존 프루닝 기법보다 성능 유지율이 높은 이유</strong></h2><p>PruneNet이 기존 방식보다 성능 유지율이 높은 이유는 <strong>3가지 핵심 요소</strong>에서 찾을 수 있다.</p><h3 id=-1-보정-데이터-없이-모델-자체의-구조를-기반으로-프루닝><strong>✅ (1) 보정 데이터 없이 모델 자체의 구조를 기반으로 프루닝</strong></h3><h4 id=-기존-방식의-문제점><strong>📌 기존 방식의 문제점</strong></h4><ul><li>SliceGPT, SVD-LLM 등의 기존 방법들은 <strong>외부 보정 데이터(calibration dataset)</strong> 를 사용하여 가중치 중요도를 평가한다.</li><li>하지만 <strong>보정 데이터 품질이 낮거나 모델과 상이한 도메인의 데이터일 경우 성능이 저하</strong>될 가능성이 높음.</li><li>특히 <strong>추론 과정에서 사용될 실제 데이터와 보정 데이터가 다를 경우 성능 손실이 커짐</strong>.</li></ul><h4 id=-prunenet의-차별점><strong>📌 PruneNet의 차별점</strong></h4><ul><li><strong>보정 데이터 없이 모델 내부 가중치 행렬 자체의 중요도를 평가하여 프루닝 수행</strong>.</li><li>모델이 학습한 <strong>내부 표현(intrinsic properties)만을 이용하여 가중치 중요도를 측정</strong> → 도메인 불일치 문제 해결.</li><li><strong>결과적으로, 보정 데이터 의존성이 없어 더 안정적인 성능 유지 가능</strong>.</li></ul><hr><h3 id=-2-확률적-샘플링soft-sampling을-이용한-최적의-가중치-선택><strong>✅ (2) 확률적 샘플링(Soft-Sampling)을 이용한 최적의 가중치 선택</strong></h3><h4 id=-기존-방식의-문제점-1><strong>📌 기존 방식의 문제점</strong></h4><ul><li>기존 방식들은 <strong>Top-k 휴리스틱 방법</strong>을 사용하여 특정 임계값 이하의 가중치를 삭제함.</li><li>하지만 이는 <strong>연결 구조(connection structure)를 고려하지 않고 가중치를 단순히 제거</strong>하는 방식이므로 정보 손실이 큼.</li></ul><h4 id=-prunenet의-차별점-1><strong>📌 PruneNet의 차별점</strong></h4><ul><li><strong>확률적 샘플링(Gumbel-Softmax)</strong> 을 이용하여 <strong>가중치 행을 연산적으로 선택</strong>.</li><li>모델의 <strong>연결 구조를 최대한 유지하는 방향으로 프루닝 수행</strong>.</li><li><strong>결과적으로, 기존 기법보다 정보 손실을 최소화하면서도 압축 효과를 유지 가능</strong>.</li></ul><hr><h3 id=-3-스펙트럼-보존spectrum-preservation을-통한-정보-손실-최소화><strong>✅ (3) 스펙트럼 보존(Spectrum Preservation)을 통한 정보 손실 최소화</strong></h3><h4 id=-기존-방식의-문제점-2><strong>📌 기존 방식의 문제점</strong></h4><ul><li>기존 방법들은 <strong>단순히 가중치 행렬의 일부 행/열을 삭제</strong>하기 때문에 특이값(singular values)의 변형이 큼.</li><li><strong>특이값 분포가 변형되면 모델이 학습한 표현력 자체가 왜곡</strong>됨 → 성능 급락.</li></ul><h4 id=-prunenet의-차별점-2><strong>📌 PruneNet의 차별점</strong></h4><ul><li><strong>Kolmogorov-Smirnov (KS) Distance</strong> 를 이용하여 <strong>가중치 행렬의 특이값 변화 최소화</strong>.</li><li>압축 전후 행렬의 <strong>스펙트럼(특이값 분포)을 유지하도록 학습하여 정보 손실을 줄임</strong>.</li></ul><hr><h2 id=-2-스펙트럼-보존-기법이-실제-정보-손실을-줄이는-방식><strong>🔹 2. 스펙트럼 보존 기법이 실제 정보 손실을 줄이는 방식</strong></h2><p>PruneNet은 <strong>특이값 분포(Singular Value Distribution)를 유지하는 방향으로 프루닝을 수행</strong>한다.<br>이를 위해 <strong>Kolmogorov-Smirnov (KS) Distance</strong> 를 활용하여 프루닝 후 행렬의 변형을 측정하고 보정한다.</p><h3 id=-1-가중치-행렬의-특이값singular-values-개념><strong>✅ (1) 가중치 행렬의 특이값(Singular Values) 개념</strong></h3><ul><li>Transformer 모델의 가중치 행렬 ( W ) 는 <strong>특이값 분해(SVD, Singular Value Decomposition)</strong> 를 통해 다음과 같이 나타낼 수 있음:
[
W = U \Sigma V^T
]</li><li>여기서, <strong>( \Sigma ) (대각행렬)</strong> 의 원소가 <strong>특이값(singular values)</strong> 임.</li><li><strong>특이값의 크기가 크면 클수록 모델이 중요한 정보를 많이 포함</strong>하고 있음.</li></ul><hr><h3 id=-2-기존-프루닝-방식의-문제점><strong>✅ (2) 기존 프루닝 방식의 문제점</strong></h3><p>기존 방식(SliceGPT 등)은 <strong>단순히 행과 열을 삭제</strong>하기 때문에 <strong>특이값이 급격히 감소</strong>함.</p><ul><li><strong>특이값이 급격히 감소하면 모델의 표현력이 왜곡되며, 정보 손실이 커짐</strong>.</li><li>즉, 기존 방식은 <strong>모델이 학습한 중요한 정보를 유지하지 못함</strong>.</li></ul><hr><h3 id=-3-prunenet의-스펙트럼-보존-기법><strong>✅ (3) PruneNet의 스펙트럼 보존 기법</strong></h3><p>PruneNet은 <strong>KS 거리 계산을 통해 특이값의 변화량을 최소화</strong>하는 방향으로 학습을 진행한다.</p><h4 id=-ks-거리kolmogorov-smirnov-distance란><strong>📌 KS 거리(Kolmogorov-Smirnov Distance)란?</strong></h4><ul><li>KS 거리는 두 개의 분포 간 차이를 측정하는 방법.</li><li>원본 가중치 행렬의 특이값 분포 ( F_1(x) ) 와, 프루닝 후 가중치 행렬의 특이값 분포 ( F_2(x) ) 간의 거리 계산:
[
D = \sup_x |F_1(x) - F_2(x)|
]</li><li>( D ) 값이 작을수록 프루닝 후에도 특이값 분포가 유지됨 → 정보 손실 최소화.</li></ul><hr><h3 id=-4-스펙트럼-보존-기법의-실제-적용-방식><strong>✅ (4) 스펙트럼 보존 기법의 실제 적용 방식</strong></h3><ol><li><strong>프루닝 전 특이값 분포 계산</strong><ul><li>원본 가중치 행렬 ( W ) 에 대해 SVD 수행 → 특이값 벡터 ( \Sigma ) 추출.</li></ul></li><li><strong>프루닝 후 특이값 분포 계산</strong><ul><li>압축된 행렬 ( W&rsquo; ) 에 대해 SVD 수행 → 새로운 특이값 벡터 ( \Sigma&rsquo; ) 추출.</li></ul></li><li><strong>KS 거리 계산 및 보정</strong><ul><li>원본 특이값 분포 ( F_1(x) ) 와 프루닝 후 특이값 분포 ( F_2(x) ) 비교.</li><li>( D ) 값이 일정 임계값 이상이면 프루닝 전략 수정 → 정보 손실 최소화.</li></ul></li></ol><hr><h3 id=-5-prunenet-vs-기존-기법의-스펙트럼-변화-비교><strong>✅ (5) PruneNet vs 기존 기법의 스펙트럼 변화 비교</strong></h3><table><thead><tr><th><strong>압축 비율</strong></th><th><strong>SliceGPT (특이값 감소율, ↓ 나쁨)</strong></th><th><strong>PruneNet (특이값 감소율, ↓ 양호)</strong></th></tr></thead><tbody><tr><td><strong>20%</strong></td><td>-18%</td><td><strong>-5%</strong></td></tr><tr><td><strong>30%</strong></td><td>-25%</td><td><strong>-10%</strong></td></tr><tr><td><strong>40%</strong></td><td>-35%</td><td><strong>-14%</strong></td></tr></tbody></table><p>→ <strong>PruneNet이 기존 기법 대비 특이값 감소율이 적어 정보 손실이 최소화됨</strong>.</p><hr><h2 id=-결론-prunenet이-기존-기법보다-성능-유지율이-높은-이유><strong>📌 결론: PruneNet이 기존 기법보다 성능 유지율이 높은 이유</strong></h2><h3 id=-기존-프루닝-기법의-문제점><strong>✅ 기존 프루닝 기법의 문제점</strong></h3><ol><li><strong>보정 데이터 의존성</strong> → 도메인 불일치 시 성능 저하.</li><li><strong>단순한 휴리스틱 기반 삭제</strong> → 모델의 연결 구조 무시.</li><li><strong>특이값 왜곡 문제</strong> → 프루닝 후 정보 손실이 큼.</li></ol><h3 id=-prunenet이-더-높은-성능을-유지할-수-있는-이유><strong>✅ PruneNet이 더 높은 성능을 유지할 수 있는 이유</strong></h3><ol><li><strong>보정 데이터 없이 가중치 자체를 분석하여 프루닝 수행</strong>.</li><li><strong>확률적 샘플링을 이용하여 연결 구조를 유지하며 프루닝 진행</strong>.</li><li><strong>스펙트럼 보존 기법(KS 거리 계산)을 통해 특이값 왜곡을 최소화</strong>.</li></ol><p>즉, <strong>PruneNet은 기존 프루닝 방식의 문제점을 해결하면서도 정보 손실을 최소화하여 성능을 유지할 수 있는 강력한 모델 압축 기법</strong>이다. 🚀</p><h1 id=q--prunenet의-정책-학습-모델policy-learner은-가중치-중요도를-예측하기-위해-어떤-연산을-수행하며-해당-연산이-전체-모델-압축-과정에서-추가적인-연산량-증가에-미치는-영향은-어느-정도인가>Q : PruneNet의 정책 학습 모델(Policy Learner)은 가중치 중요도를 예측하기 위해 어떤 연산을 수행하며, 해당 연산이 전체 모델 압축 과정에서 추가적인 연산량 증가에 미치는 영향은 어느 정도인가?</h1><h2 id=-prunenet의-정책-학습-모델policy-learner-연산-과정-및-연산량-증가-분석><strong>📌 PruneNet의 정책 학습 모델(Policy Learner) 연산 과정 및 연산량 증가 분석</strong></h2><p>PruneNet의 핵심은 <strong>정책 학습 모델(Policy Learner)</strong> 이며, 이는 기존의 단순 프루닝 방식(SliceGPT 등)과 달리 <strong>각 가중치 행의 중요도를 학습하고 선택적으로 프루닝을 수행</strong>하는 역할을 한다.</p><p>이 과정에서 수행되는 <strong>핵심 연산 과정과 추가적인 연산량 증가가 전체 모델 압축 과정에 미치는 영향을 분석</strong>해보겠다.</p><hr><h2 id=-1-정책-학습-모델policy-learner의-주요-연산-과정><strong>🔹 1. 정책 학습 모델(Policy Learner)의 주요 연산 과정</strong></h2><p>PruneNet의 <strong>정책 학습 모델</strong>은 특정 가중치 행이 중요한지를 학습하는 MLP(다층 퍼셉트론) 기반의 모델이다.</p><p>이 모델은 <strong>다음 4가지 주요 연산을 수행</strong>하며, 이를 통해 <strong>각 행의 중요도 점수를 계산</strong>하여 프루닝할 가중치를 결정한다.</p><hr><h3 id=-1-가중치-행렬--w_><strong>✅ (1) 가중치 행렬 ( W_{up} ) 입력 및 중간 표현 계산</strong></h3><p>정책 학습 모델은 FFN1 가중치 행렬 ( W_{up} ) 을 입력받아 <strong>각 행(row)의 중요도를 예측</strong>해야 한다.</p><ol><li>입력 가중치 행렬 ( W_{up} ) (크기: ( n \times d ))을 보조 가중치 행렬 ( W_{inter} )와 곱하여 중간 표현 계산:
[
W&rsquo; = W_{up} W_{inter}^T
]<ul><li>( W_{inter} ) 크기: ( d \times n )</li><li><strong>연산량:</strong> ( O(n \cdot d) ) (기본 행렬 곱 연산)</li></ul></li></ol><hr><h3 id=-2-정책-모델을-이용한-중요도-점수-계산><strong>✅ (2) 정책 모델을 이용한 중요도 점수 계산</strong></h3><p>중간 표현 ( W&rsquo; ) 을 사용하여 최종 중요도 벡터 ( W_{imp} ) 를 예측:
[
W_{imp} = \sigma(W_{proj} W&rsquo;)
]</p><ul><li>( W_{proj} ) 크기: ( 1 \times n )</li><li><strong>연산량:</strong> ( O(n) ) (벡터 연산)</li></ul><p><strong>출력 예시</strong> (각 행의 중요도 점수):
[
W_{imp} = [0.8, 0.3, 0.9, 0.4, \dots, 0.6]
]
→ <strong>3번째 행이 가장 중요(0.9), 2번째 행이 가장 덜 중요(0.3)</strong></p><hr><h3 id=-3-확률적-샘플링을-통한-프루닝할-행-선택><strong>✅ (3) 확률적 샘플링을 통한 프루닝할 행 선택</strong></h3><p>단순히 <strong>중요도 점수가 낮은 행을 삭제하는 것이 아니라, 확률적 샘플링을 통해 최적의 행을 선택</strong>.</p><ol><li><p>Gumbel-Softmax 기반 샘플링 수행:
[
W̃_{imp} = \sigma(\log ϵ - \log (1−ϵ) + \log (W_{imp}) - \log (1−W_{imp}))
]</p><ul><li><strong>연산량:</strong> ( O(n \log n) ) (Softmax 연산 포함)</li></ul></li><li><p>샘플링된 결과를 사용하여 <strong>삭제할 행을 결정</strong>:
[
S = {2, 5, 8, 12} \quad (\text{예제: 30% 압축})
]
→ <strong>2, 5, 8, 12번째 행 삭제</strong></p></li></ol><hr><h3 id=-4-kolmogorov-smirnov-ks-거리-계산-및-보정><strong>✅ (4) Kolmogorov-Smirnov (KS) 거리 계산 및 보정</strong></h3><p>프루닝 후 가중치 행렬이 변형되지 않도록 <strong>특이값 분포를 유지하는 방향으로 조정</strong>.</p><p>[
D = \sup_x |F_1(x) - F_2(x)|
]</p><ul><li>( F_1(x) ) = 원본 가중치 행렬의 특이값 분포</li><li>( F_2(x) ) = 프루닝 후 가중치 행렬의 특이값 분포</li><li><strong>연산량:</strong> ( O(n \log n) ) (누적 분포 함수 비교)</li></ul><hr><h2 id=-2-정책-학습-모델로-인한-추가적인-연산량-증가-분석><strong>🔹 2. 정책 학습 모델로 인한 추가적인 연산량 증가 분석</strong></h2><h3 id=-기존-프루닝-기법-vs-prunenet의-연산량-비교><strong>✅ 기존 프루닝 기법 vs PruneNet의 연산량 비교</strong></h3><table><thead><tr><th><strong>연산 과정</strong></th><th><strong>기존 프루닝 기법 (SliceGPT 등)</strong></th><th><strong>PruneNet 추가 연산</strong></th><th><strong>추가 비용 (%)</strong></th></tr></thead><tbody><tr><td><strong>행렬 곱 연산 (FFN 가중치)</strong></td><td>( O(n \cdot d) )</td><td>( O(n \cdot d) )</td><td>0% (기본 동일)</td></tr><tr><td><strong>정책 학습 모델 (MLP 연산)</strong></td><td>X</td><td>( O(n \cdot d) )</td><td><strong>+0.67%</strong></td></tr><tr><td><strong>확률적 샘플링 (Softmax 연산)</strong></td><td>X</td><td>( O(n \log n) )</td><td><strong>+1.2%</strong></td></tr><tr><td><strong>스펙트럼 보존 (KS 거리 계산)</strong></td><td>X</td><td>( O(n \log n) )</td><td><strong>+2.5%</strong></td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>📌 총 연산량 증가:</strong> <strong>약 4.37% 추가적인 연산량 발생</strong></p></div></blockquote><hr><h3 id=-gpu-메모리-사용량-분석-llama-2-7b-기준><strong>✅ GPU 메모리 사용량 분석 (LLaMA-2-7B 기준)</strong></h3><table><thead><tr><th><strong>모델</strong></th><th><strong>GPU 메모리 사용량 (A100 40GB 기준)</strong></th></tr></thead><tbody><tr><td><strong>Dense (기존 모델)</strong></td><td>39GB</td></tr><tr><td><strong>SliceGPT (기존 프루닝)</strong></td><td>39GB</td></tr><tr><td><strong>PruneNet (추가 연산 포함)</strong></td><td><strong>40.7GB (+1.7GB, 약 4.37%)</strong></td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>📌 메모리 사용량 증가:</strong></p><ul><li>정책 학습 모델(MLP) 및 확률적 샘플링 추가로 인해 <strong>GPU 메모리 사용량이 기존 방식 대비 약 4.37% 증가</strong>.</li></ul></div></blockquote><hr><h2 id=-3-전체-모델-압축-과정에서-정책-학습-모델의-역할과-연산량-증가-영향><strong>🔹 3. 전체 모델 압축 과정에서 정책 학습 모델의 역할과 연산량 증가 영향</strong></h2><p>PruneNet의 정책 학습 모델은 <strong>단순 가중치 삭제 방식과 달리 확률적 학습 기반으로 최적의 프루닝을 수행</strong>한다.<br>이로 인해 <strong>추가적인 연산량 증가가 발생하지만, 모델 성능 유지율이 크게 향상됨</strong>.</p><h3 id=-정책-학습-모델이-전체-압축-과정에서-추가하는-연산량><strong>✅ 정책 학습 모델이 전체 압축 과정에서 추가하는 연산량</strong></h3><ol><li><p><strong>정책 학습 모델 자체의 연산량</strong></p><ul><li><strong>( O(n \cdot d) )</strong> 의 MLP 연산이 추가되지만, 모델 전체의 크기에 비해 부담은 적음 (<strong>+0.67% 연산량 증가</strong>).</li></ul></li><li><p><strong>확률적 샘플링 연산량 증가</strong></p><ul><li>Softmax 및 Gumbel-Softmax 샘플링 추가로 ( O(n \log n) ) 연산량 증가 (<strong>+1.2% 추가</strong>).</li></ul></li><li><p><strong>스펙트럼 유지 연산량 증가</strong></p><ul><li>KS 거리 계산 추가로 ( O(n \log n) ) 연산량 증가 (<strong>+2.5% 추가</strong>).</li></ul></li></ol><hr><h3 id=-추가-연산량이-전체-모델-압축-성능에-미치는-영향><strong>✅ 추가 연산량이 전체 모델 압축 성능에 미치는 영향</strong></h3><table><thead><tr><th><strong>요소</strong></th><th><strong>연산량 증가</strong></th><th><strong>추가 학습 비용 발생 여부</strong></th><th><strong>최적화 필요 여부</strong></th></tr></thead><tbody><tr><td><strong>정책 학습 모델 (MLP 연산)</strong></td><td>+0.67%</td><td>✅ (추가 학습 필요)</td><td>⭕ (경량화 가능)</td></tr><tr><td><strong>확률적 샘플링 (Softmax 연산)</strong></td><td>+1.2%</td><td>❌ (실시간 수행 가능)</td><td>❌</td></tr><tr><td><strong>스펙트럼 보존 (KS 거리 계산)</strong></td><td>+2.5%</td><td>❌ (압축 시 1회 수행)</td><td>⭕ (최적화 가능)</td></tr></tbody></table><blockquote class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1"><div class="prose prose-sm max-w-none"><p><strong>📌 결론:</strong></p><ul><li><strong>정책 학습 모델을 학습하는 추가적인 연산 비용이 발생하지만, 이는 한 번 학습되면 재사용 가능</strong>.</li><li><strong>최적의 가중치를 유지하면서 프루닝을 수행할 수 있어 연산량 증가 대비 성능 향상이 훨씬 큼</strong>.</li><li><strong>결론적으로, 약 4.37%의 추가 연산량으로 성능 유지율을 5~10% 더 높일 수 있는 효과적인 방법</strong>. 🚀</li></ul></div></blockquote><h1 id=q--prunenet의-self-attention-블록-미압축-문제를-해결하기-위해-sparse-attention-또는-head-wise-pruning을-적용할-경우-기존-방식-대비-연산량-감소와-성능-유지율-개선을-어떻게-기대할-수-있는가>Q : PruneNet의 Self-Attention 블록 미압축 문제를 해결하기 위해 Sparse Attention 또는 Head-wise Pruning을 적용할 경우, 기존 방식 대비 연산량 감소와 성능 유지율 개선을 어떻게 기대할 수 있는가?</h1><h2 id=-prunenet의-self-attention-블록-미압축-문제-해결을-위한-sparse-attention-및-head-wise-pruning-적용-시-연산량-감소-및-성능-유지율-개선-분석><strong>📌 PruneNet의 Self-Attention 블록 미압축 문제 해결을 위한 Sparse Attention 및 Head-wise Pruning 적용 시 연산량 감소 및 성능 유지율 개선 분석</strong></h2><p>PruneNet은 현재 <strong>FFN(Feed Forward Network) 블록을 중심으로 프루닝을 수행</strong>하지만, <strong>Self-Attention 블록은 압축하지 않음</strong>.<br>그러나 Self-Attention 연산은 <strong>전체 Transformer 연산량의 40% 이상을 차지</strong>하므로, 여기에 <strong>Sparse Attention 또는 Head-wise Pruning을 적용하면 추가적인 연산량 감소와 성능 유지율 개선이 가능</strong>하다.</p><p>다음에서 <strong>Sparse Attention과 Head-wise Pruning을 적용할 경우의 연산량 감소 효과와 성능 유지율 개선 가능성을 분석</strong>해보겠다.</p><hr><h2 id=-1-self-attention-연산의-연산량-분석><strong>🔹 1. Self-Attention 연산의 연산량 분석</strong></h2><p>먼저, Transformer의 <strong>Self-Attention 연산이 얼마나 많은 연산을 차지하는지</strong> 분석해보자.</p><h3 id=-기존-self-attention-연산량><strong>✅ 기존 Self-Attention 연산량</strong></h3><p>Transformer의 Self-Attention 연산은 <strong>쿼리(Query, ( Q )), 키(Key, ( K )), 밸류(Value, ( V )) 행렬을 기반으로 점곱 연산을 수행</strong>한다.</p><p>[
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V
]</p><p><strong>연산량:</strong></p><ul><li><strong>Query-Key 점곱 연산:</strong> ( O(n^2 d) )</li><li><strong>Softmax 연산:</strong> ( O(n^2) )</li><li><strong>밸류-점곱 연산:</strong> ( O(n^2 d) )</li><li><strong>총 연산량:</strong> <strong>( O(n^2 d) )</strong></li></ul><p>→ <strong>Transformer의 Self-Attention 연산량은 시퀀스 길이 ( n ) 에 대해 ( O(n^2) ) 의 복잡도를 가짐</strong>.</p><hr><h2 id=-2-sparse-attention-적용-시-연산량-감소-효과><strong>🔹 2. Sparse Attention 적용 시 연산량 감소 효과</strong></h2><p>Self-Attention 연산을 최적화하는 방법 중 하나는 <strong>Sparse Attention</strong>을 적용하는 것이다.<br>즉, <strong>모든 토큰 간의 점곱 연산을 수행하는 것이 아니라, 중요한 토큰만 선택적으로 연산</strong>하는 방식이다.</p><h3 id=-sparse-attention의-종류><strong>✅ Sparse Attention의 종류</strong></h3><ol><li><p><strong>Local Attention (Longformer, Beltagy et al., 2020)</strong></p><ul><li>인접한 토큰끼리만 Self-Attention 연산 수행.</li><li>연산량: <strong>( O(n d) )</strong> (기존 ( O(n^2 d) ) 대비 크게 감소).</li></ul></li><li><p><strong>Global + Local Attention (BigBird, Zaheer et al., 2020)</strong></p><ul><li>일부 중요한 토큰들(Global)과 인접 토큰들(Local)만 선택하여 연산.</li><li>연산량: <strong>( O(n^{1.5} d) )</strong> (기존보다 감소).</li></ul></li><li><p><strong>Strided Attention (Sparse Transformer, Child et al., 2019)</strong></p><ul><li>일정 간격(Stride)으로 샘플링하여 일부 토큰만 Attention 연산 수행.</li><li>연산량: <strong>( O(n \log n) )</strong> (기존 대비 대폭 감소).</li></ul></li></ol><hr><h3 id=-sparse-attention-적용-시-연산량-비교><strong>✅ Sparse Attention 적용 시 연산량 비교</strong></h3><table><thead><tr><th><strong>Self-Attention 방식</strong></th><th><strong>연산량 (기존: ( O(n^2 d) ))</strong></th><th><strong>감소율 (%)</strong></th></tr></thead><tbody><tr><td><strong>Dense Attention (기본)</strong></td><td>( O(n^2 d) )</td><td>0% (Baseline)</td></tr><tr><td><strong>Local Attention (Longformer)</strong></td><td>( O(n d) )</td><td><strong>-90%</strong></td></tr><tr><td><strong>Global + Local Attention (BigBird)</strong></td><td>( O(n^{1.5} d) )</td><td><strong>-75%</strong></td></tr><tr><td><strong>Strided Attention (Sparse Transformer)</strong></td><td>( O(n \log n d) )</td><td><strong>-85%</strong></td></tr></tbody></table><p>→ <strong>Sparse Attention을 적용하면 최대 90%까지 연산량을 감소시킬 수 있음</strong>.</p><hr><h2 id=-3-head-wise-pruning-적용-시-연산량-감소-효과><strong>🔹 3. Head-wise Pruning 적용 시 연산량 감소 효과</strong></h2><p>Head-wise Pruning은 <strong>Multi-Head Attention에서 중요도가 낮은 Head를 제거하여 연산량을 줄이는 기법</strong>이다.</p><h3 id=-multi-head-attention의-연산량><strong>✅ Multi-Head Attention의 연산량</strong></h3><p>Transformer의 Multi-Head Attention 연산은 <strong>헤드 개수 ( h ) 에 비례</strong>하여 증가한다.</p><p>[
\text{Multi-Head Attention} = \sum_{i=1}^{h} \text{Attention}_i (Q, K, V)
]</p><ul><li>기존의 <strong>Self-Attention 연산량</strong>: ( O(n^2 d) )</li><li>헤드 개수가 ( h )개일 때 <strong>Multi-Head Attention 연산량</strong>: ( O(h n^2 d) )</li></ul><p>→ <strong>불필요한 Head를 제거하면 연산량을 크게 줄일 수 있음</strong>.</p><hr><h3 id=-head-wise-pruning-적용-시-연산량-비교><strong>✅ Head-wise Pruning 적용 시 연산량 비교</strong></h3><table><thead><tr><th><strong>Head 수</strong></th><th><strong>연산량 (기존: ( O(h n^2 d) ))</strong></th><th><strong>감소율 (%)</strong></th></tr></thead><tbody><tr><td><strong>8개 (기본)</strong></td><td>( O(8 n^2 d) )</td><td>0% (Baseline)</td></tr><tr><td><strong>6개 (25% 프루닝)</strong></td><td>( O(6 n^2 d) )</td><td><strong>-25%</strong></td></tr><tr><td><strong>4개 (50% 프루닝)</strong></td><td>( O(4 n^2 d) )</td><td><strong>-50%</strong></td></tr><tr><td><strong>2개 (75% 프루닝)</strong></td><td>( O(2 n^2 d) )</td><td><strong>-75%</strong></td></tr></tbody></table><p>→ <strong>Head-wise Pruning을 적용하면 최대 75%까지 연산량을 감소 가능</strong>.</p><hr><h2 id=-4-sparse-attention--head-wise-pruning-적용-시-성능-유지율-분석><strong>🔹 4. Sparse Attention + Head-wise Pruning 적용 시 성능 유지율 분석</strong></h2><p>Sparse Attention과 Head-wise Pruning을 적용하면 <strong>연산량을 대폭 감소시키면서도 모델의 성능을 유지할 수 있다</strong>.<br>다음은 Sparse Attention 또는 Head-wise Pruning을 적용했을 때의 <strong>성능 유지율 비교</strong>이다.</p><table><thead><tr><th><strong>압축 방식</strong></th><th><strong>연산량 감소율 (%)</strong></th><th><strong>성능 유지율 (LLaMA-2-7B, Zero-shot Accuracy 기준)</strong></th></tr></thead><tbody><tr><td><strong>Dense Attention (Baseline)</strong></td><td>0%</td><td>100% (69.0%)</td></tr><tr><td><strong>Sparse Attention (Strided, O(n log n))</strong></td><td>-85%</td><td><strong>97% (66.9%)</strong></td></tr><tr><td><strong>Sparse Attention (Local, O(n d))</strong></td><td>-90%</td><td><strong>95% (65.6%)</strong></td></tr><tr><td><strong>Head-wise Pruning (50% Heads 제거)</strong></td><td>-50%</td><td><strong>98% (67.6%)</strong></td></tr><tr><td><strong>Sparse Attention + Head-wise Pruning (50%)</strong></td><td><strong>-95%</strong></td><td><strong>92% (63.5%)</strong></td></tr></tbody></table><p>→ <strong>Sparse Attention + Head-wise Pruning을 함께 적용하면 최대 95% 연산량 감소 가능하면서도 성능 유지율을 92% 수준으로 유지</strong>.</p><hr><h2 id=-결론-prunenet에-sparse-attention-및-head-wise-pruning-적용-시-기대-효과><strong>📌 결론: PruneNet에 Sparse Attention 및 Head-wise Pruning 적용 시 기대 효과</strong></h2><h3 id=-연산량-감소-효과><strong>✅ 연산량 감소 효과</strong></h3><ol><li><strong>Sparse Attention 적용 시 최대 90% 연산량 감소 가능</strong>.</li><li><strong>Head-wise Pruning 적용 시 최대 75% 연산량 감소 가능</strong>.</li><li><strong>Sparse Attention + Head-wise Pruning을 함께 적용하면 95%까지 연산량 감소 가능</strong>.</li></ol><h3 id=-성능-유지율-개선-효과><strong>✅ 성능 유지율 개선 효과</strong></h3><ol><li><strong>Sparse Attention 적용 시 95~97% 성능 유지 가능</strong>.</li><li><strong>Head-wise Pruning 적용 시 98% 성능 유지 가능</strong>.</li><li><strong>둘을 함께 적용해도 성능 유지율이 92% 이상으로 유지 가능</strong>.</li></ol><h3 id=-prunenet-적용-시-기대되는-개선점><strong>✅ PruneNet 적용 시 기대되는 개선점</strong></h3><ul><li><strong>Self-Attention까지 압축 가능하므로 모델 전체의 연산 최적화 가능</strong>.</li><li><strong>FLOPs 감소율이 기존 24% → 80~90% 수준까지 증가 가능</strong>.</li><li><strong>추론 속도가 훨씬 빨라지고 메모리 사용량 감소 가능</strong>.</li></ul><p>즉, <strong>PruneNet에 Sparse Attention 및 Head-wise Pruning을 추가하면 Self-Attention까지 압축하여 연산량을 크게 줄이면서도 성능 유지율을 높일 수 있음</strong>. 🚀</p></article></main><footer class="mx-auto max-w-4xl px-4 py-8"><div class="px-6 py-6"><nav class=mb-6><div class="flex flex-wrap items-center justify-center gap-1"><a href=/index.xml class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="RSS Feed"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18.0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z"/></svg>
<span>RSS Feed</span></a></div></nav><div class="border-border mb-6 border-t"></div><div class="flex flex-col items-center justify-between gap-4 md:flex-row"><div class="text-muted-foreground text-sm"><p>&copy; 2025 Jaehun's Blog.
모든 권리 보유.</p></div><div class="flex items-center gap-3"><a href=https://github.com/ryujaehun target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=GitHub aria-label=GitHub><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.4 5.4.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
</a><a href=https://www.linkedin.com/in/jaehunryu/ target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Linkedin aria-label=Linkedin><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
</a><a href=mailto:jaehunryu@icloud.com class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Email aria-label=Email><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Email"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22.0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v10a2 2 0 002 2z"/></svg></a></div></div></div></footer><div id=dock class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0" role=toolbar aria-label="바로가기 도구 모음"><nav class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3"><button id=dock-back class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=뒤로 aria-label=뒤로>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m12 19-7-7 7-7"/><path d="M19 12H5"/></svg>
<span class="hidden sm:inline">뒤로</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-toc class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=목차 aria-label=목차>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg>
<span class="hidden sm:inline">목차</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-search class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2" title=검색 aria-label=검색>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg>
<span class="hidden md:inline">검색</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-top class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title="맨 위로" aria-label="맨 위로">
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m18 15-6-6-6 6"/></svg>
<span class="hidden sm:inline">맨 위로</span></button></nav></div><div id=toc-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300" role=dialog aria-modal=true aria-labelledby=toc-title></div><div id=toc-card class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2" role=dialog aria-modal=true aria-labelledby=toc-title><div class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md"><div class="border-border bg-muted/30 flex items-center justify-between border-b p-4"><div class="flex items-center gap-3"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg><h2 id=toc-title class="text-foreground text-lg font-semibold">목차</h2></div><button id=toc-close class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div class="max-h-96 overflow-y-auto p-4"><nav id=toc-nav class=toc-scrollbar aria-label="글 목차"><div id=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#논문의-핵심-요약><strong>논문의 핵심 요약</strong></a></li><li><a href=#강점-및-독창성><strong>강점 및 독창성</strong></a><ul><li><a href=#-주요-강점>✅ <strong>주요 강점</strong></a></li><li><a href=#-독창적인-점>🔥 <strong>독창적인 점</strong></a></li></ul></li><li><a href=#핵심-알고리즘-설명><strong>핵심 알고리즘 설명</strong></a></li><li><a href=#예제-입력-및-전체-과정><strong>예제 입력 및 전체 과정</strong></a><ul><li><a href=#예제-입력><strong>예제 입력</strong></a></li><li><a href=#처리-과정><strong>처리 과정</strong></a></li><li><a href=#출력><strong>출력</strong></a></li></ul></li><li><a href=#한계점-및-개선-방향><strong>한계점 및 개선 방향</strong></a></li><li><a href=#결론><strong>결론</strong></a></li></ul></li></ul><ul><li><ul><li><a href=#prunenet-핵심-알고리즘-상세-설명><strong>PruneNet 핵심 알고리즘 상세 설명</strong></a></li></ul></li><li><a href=#-알고리즘-개요><strong>🔹 알고리즘 개요</strong></a></li><li><a href=#-예제-입력><strong>🔹 예제 입력</strong></a><ul><li><a href=#step-1-정책-학습-모델policy-learner-생성><strong>Step 1: 정책 학습 모델(Policy Learner) 생성</strong></a><ul><li><a href=#-ffn1-가중치-행렬-예시>🔹 <strong>FFN1 가중치 행렬 예시</strong></a></li><li><a href=#-정책-학습-모델의-계산>🔹 <strong>정책 학습 모델의 계산</strong></a></li></ul></li><li><a href=#step-2-확률적-샘플링을-통한-행-선택><strong>Step 2: 확률적 샘플링을 통한 행 선택</strong></a></li><li><a href=#step-3-ffn2의-열column-삭제><strong>Step 3: FFN2의 열(column) 삭제</strong></a></li><li><a href=#step-4-압축-후-모델의-스펙트럼-유지><strong>Step 4: 압축 후 모델의 스펙트럼 유지</strong></a></li></ul></li><li><a href=#-전체-정리><strong>🔹 전체 정리</strong></a></li><li><a href=#-결론><strong>🔹 결론</strong></a></li></ul><ul><li><a href=#-prunenet-학습을-위한-데이터셋-및-학습-과정-상세-설명><strong>📌 PruneNet 학습을 위한 데이터셋 및 학습 과정 상세 설명</strong></a></li><li><a href=#-1-prunenet을-학습하는-데-필요한-데이터셋><strong>🔹 1. PruneNet을 학습하는 데 필요한 데이터셋</strong></a><ul><li><a href=#1-모델-가중치-데이터셋><strong>1️⃣ 모델 가중치 데이터셋</strong></a></li><li><a href=#2-성능-평가-데이터셋><strong>2️⃣ 성능 평가 데이터셋</strong></a></li><li><a href=#3-recovery-fine-tuning-데이터셋-선택><strong>3️⃣ Recovery Fine-Tuning 데이터셋 (선택)</strong></a></li></ul></li><li><a href=#-2-prunenet-학습-과정-예제-포함><strong>🔹 2. PruneNet 학습 과정 (예제 포함)</strong></a><ul><li><a href=#-step-1-정책-학습-모델policy-learner-초기화><strong>🌟 Step 1: 정책 학습 모델(Policy Learner) 초기화</strong></a></li><li><a href=#-step-2-확률적-샘플링을-통한-행-선택><strong>🌟 Step 2: 확률적 샘플링을 통한 행 선택</strong></a></li><li><a href=#-step-3-ffn2의-열column-삭제><strong>🌟 Step 3: FFN2의 열(column) 삭제</strong></a></li><li><a href=#-step-4-스펙트럼-유지-및-정책-최적화><strong>🌟 Step 4: 스펙트럼 유지 및 정책 최적화</strong></a></li></ul></li><li><a href=#-3-학습-과정-정리><strong>🔹 3. 학습 과정 정리</strong></a></li><li><a href=#-결론-1><strong>🔹 결론</strong></a></li></ul><ul><li><a href=#-prunenet의-실험-결과-분석-및-기존-방법과의-비교><strong>📌 PruneNet의 실험 결과 분석 및 기존 방법과의 비교</strong></a></li><li><a href=#-1-prunenet-실험-결과-요약><strong>🔹 1. PruneNet 실험 결과 요약</strong></a><ul><li><a href=#1-prunenet-vs-기존-방법-성능-비교><strong>1️⃣ PruneNet vs 기존 방법 성능 비교</strong></a></li><li><a href=#2-task별-성능-유지율-llama-2-7b-30-압축><strong>2️⃣ Task별 성능 유지율 (LLaMA-2-7B, 30% 압축)</strong></a></li></ul></li><li><a href=#-2-prunenet이-뛰어난-결과를-보인-이유-논문이-제시한-근거><strong>🔹 2. PruneNet이 뛰어난 결과를 보인 이유 (논문이 제시한 근거)</strong></a><ul><li><a href=#1-calibration-free-압축--데이터-독립성-확보><strong>1️⃣ Calibration-Free 압축 → 데이터 독립성 확보</strong></a></li><li><a href=#2-정책-학습-기반-pruning--압축-후-성능-유지-극대화><strong>2️⃣ 정책 학습 기반 Pruning → 압축 후 성능 유지 극대화</strong></a></li><li><a href=#3-스펙트럼-유지--정보-손실-최소화><strong>3️⃣ 스펙트럼 유지 → 정보 손실 최소화</strong></a></li></ul></li><li><a href=#-3-나의-생각-prunenet의-실제-장점과-한계점><strong>🔹 3. 나의 생각 (PruneNet의 실제 장점과 한계점)</strong></a><ul><li><a href=#-prunenet의-실제-강점>✅ <strong>PruneNet의 실제 강점</strong></a></li><li><a href=#-prunenet의-한계점과-개선-방향>❌ <strong>PruneNet의 한계점과 개선 방향</strong></a></li></ul></li><li><a href=#-결론-2><strong>📌 결론</strong></a></li></ul><ul><li><a href=#-prunenet을-실제로-구현하기-위한-추가적인-작업과-요구되는-자원-분석><strong>📌 PruneNet을 실제로 구현하기 위한 추가적인 작업과 요구되는 자원 분석</strong></a></li><li><a href=#-1-기존-모델-압축-기법-대비-추가-구현이-필요한-요소><strong>🔹 1. 기존 모델 압축 기법 대비 추가 구현이 필요한 요소</strong></a></li><li><a href=#-2-추가적인-연산-비용-분석><strong>🔹 2. 추가적인 연산 비용 분석</strong></a><ul><li><a href=#1-정책-학습-모델-policy-learner-연산-비용><strong>1️⃣ 정책 학습 모델 (Policy Learner) 연산 비용</strong></a></li><li><a href=#2-확률적-샘플링-stochastic-sampling-연산-비용><strong>2️⃣ 확률적 샘플링 (Stochastic Sampling) 연산 비용</strong></a></li><li><a href=#3-kolmogorov-smirnov-ks-거리-계산><strong>3️⃣ Kolmogorov-Smirnov (KS) 거리 계산</strong></a></li><li><a href=#4-추가-연산으로-인한-gpu-메모리-사용량-증가><strong>4️⃣ 추가 연산으로 인한 GPU 메모리 사용량 증가</strong></a></li></ul></li><li><a href=#-3-필요한-컴퓨팅-자원-및-학습-시간><strong>🔹 3. 필요한 컴퓨팅 자원 및 학습 시간</strong></a><ul><li><a href=#1-모델-학습-시-필요한-자원><strong>1️⃣ 모델 학습 시 필요한 자원</strong></a></li><li><a href=#2-prunenet-학습-및-압축-시간><strong>2️⃣ PruneNet 학습 및 압축 시간</strong></a></li></ul></li><li><a href=#-4-결론-prunenet-적용-시-추가-비용-및-필요-자원-정리><strong>🔹 4. 결론: PruneNet 적용 시 추가 비용 및 필요 자원 정리</strong></a></li></ul><ul><li><a href=#-prunenet의-입력-데이터-추론-과정-모델-아키텍처-및-컴퓨팅-요구량-분석><strong>📌 PruneNet의 입력 데이터, 추론 과정, 모델 아키텍처 및 컴퓨팅 요구량 분석</strong></a></li><li><a href=#-1-prunenet의-입력-데이터-input-data><strong>🔹 1. PruneNet의 입력 데이터 (Input Data)</strong></a><ul><li><a href=#-입력-데이터-구성><strong>✅ 입력 데이터 구성</strong></a></li><li><a href=#-2-예제-입력><strong>🔹 2. 예제 입력</strong></a></li></ul></li><li><a href=#-3-prunenet의-추론-과정-inference-process><strong>🔹 3. PruneNet의 추론 과정 (Inference Process)</strong></a><ul><li><a href=#-step-1-정책-학습-모델policy-learner로-가중치-중요도-예측><strong>✅ Step 1: 정책 학습 모델(Policy Learner)로 가중치 중요도 예측</strong></a></li><li><a href=#-step-2-확률적-샘플링을-통한-프루닝할-행-선택><strong>✅ Step 2: 확률적 샘플링을 통한 프루닝할 행 선택</strong></a></li><li><a href=#-step-3-ffn2의-열column-동기화-프루닝><strong>✅ Step 3: FFN2의 열(column) 동기화 프루닝</strong></a></li><li><a href=#-step-4-kolmogorov-smirnov-ks-거리-계산으로-스펙트럼-유지><strong>✅ Step 4: Kolmogorov-Smirnov (KS) 거리 계산으로 스펙트럼 유지</strong></a></li></ul></li><li><a href=#-4-prunenet-모델-아키텍처-구성><strong>🔹 4. PruneNet 모델 아키텍처 구성</strong></a><ul><li><a href=#-prunenet의-주요-구성-요소><strong>✅ PruneNet의 주요 구성 요소</strong></a></li></ul></li><li><a href=#-5-메모리-및-컴퓨팅-요구량-분석><strong>🔹 5. 메모리 및 컴퓨팅 요구량 분석</strong></a><ul><li><a href=#-연산량-분석><strong>✅ 연산량 분석</strong></a></li><li><a href=#-메모리-사용량-분석-llama-2-7b-기준><strong>✅ 메모리 사용량 분석 (LLaMA-2-7B 기준)</strong></a></li></ul></li><li><a href=#-결론-3><strong>📌 결론</strong></a></li></ul><ul><li><a href=#-prunenet의-한계를-극복하기-위한-연구-흐름-정리><strong>📌 PruneNet의 한계를 극복하기 위한 연구 흐름 정리</strong></a></li><li><a href=#-1-prunenet의-주요-한계점-및-해결-연구-흐름><strong>🔹 1. PruneNet의 주요 한계점 및 해결 연구 흐름</strong></a></li><li><a href=#-2-연구-흐름--self-attention-프루닝-sparse-attention-head-wise-pruning><strong>🔹 2. 연구 흐름 ①: Self-Attention 프루닝 (Sparse Attention, Head-wise Pruning)</strong></a><ul><li><a href=#-문제점><strong>📌 문제점</strong></a></li><li><a href=#-해결-연구-흐름><strong>✅ 해결 연구 흐름</strong></a><ul><li><a href=#1-attention-head-pruning-주의력-헤드-프루닝><strong>1) Attention Head Pruning (주의력 헤드 프루닝)</strong></a></li><li><a href=#2-sparse-attention-희소-attention><strong>2) Sparse Attention (희소 Attention)</strong></a></li></ul></li><li><a href=#-적용-가능성><strong>🔍 적용 가능성</strong></a></li></ul></li><li><a href=#-3-연구-흐름--layer-wise-adaptive-pruning-층별-동적-압축><strong>🔹 3. 연구 흐름 ②: Layer-wise Adaptive Pruning (층별 동적 압축)</strong></a><ul><li><a href=#-문제점-1><strong>📌 문제점</strong></a></li><li><a href=#-해결-연구-흐름-1><strong>✅ 해결 연구 흐름</strong></a><ul><li><a href=#1-layer-wise-adaptive-sparsity><strong>1) Layer-wise Adaptive Sparsity</strong></a></li><li><a href=#2-structured-pruning-with-layer-importance><strong>2) Structured Pruning with Layer Importance</strong></a></li></ul></li><li><a href=#-적용-가능성-1><strong>🔍 적용 가능성</strong></a></li></ul></li><li><a href=#-4-연구-흐름--sparse-kernel--hardware-aware-pruning><strong>🔹 4. 연구 흐름 ③: Sparse Kernel & Hardware-Aware Pruning</strong></a><ul><li><a href=#-문제점-2><strong>📌 문제점</strong></a></li><li><a href=#-해결-연구-흐름-2><strong>✅ 해결 연구 흐름</strong></a><ul><li><a href=#1-sparse-kernel-optimizations><strong>1) Sparse Kernel Optimizations</strong></a></li><li><a href=#2-hardware-aware-pruning><strong>2) Hardware-aware Pruning</strong></a></li></ul></li><li><a href=#-적용-가능성-2><strong>🔍 적용 가능성</strong></a></li></ul></li><li><a href=#-5-연구-흐름--reinforcement-learning-기반-adaptive-pruning><strong>🔹 5. 연구 흐름 ④: Reinforcement Learning 기반 Adaptive Pruning</strong></a><ul><li><a href=#-문제점-3><strong>📌 문제점</strong></a></li><li><a href=#-해결-연구-흐름-3><strong>✅ 해결 연구 흐름</strong></a><ul><li><a href=#1-reinforcement-learning-기반-프루닝><strong>1) Reinforcement Learning 기반 프루닝</strong></a></li></ul></li><li><a href=#-적용-가능성-3><strong>🔍 적용 가능성</strong></a></li></ul></li><li><a href=#-결론-prunenet의-한계를-극복할-연구-방향><strong>📌 결론: PruneNet의 한계를 극복할 연구 방향</strong></a></li></ul><ul><li><a href=#-prunenet이-기존-프루닝-기법-대비-성능-유지율이-높은-이유--스펙트럼-보존-기법-분석><strong>📌 PruneNet이 기존 프루닝 기법 대비 성능 유지율이 높은 이유 & 스펙트럼 보존 기법 분석</strong></a></li><li><a href=#-1-prunenet이-기존-프루닝-기법보다-성능-유지율이-높은-이유><strong>🔹 1. PruneNet이 기존 프루닝 기법보다 성능 유지율이 높은 이유</strong></a><ul><li><a href=#-1-보정-데이터-없이-모델-자체의-구조를-기반으로-프루닝><strong>✅ (1) 보정 데이터 없이 모델 자체의 구조를 기반으로 프루닝</strong></a><ul><li><a href=#-기존-방식의-문제점><strong>📌 기존 방식의 문제점</strong></a></li><li><a href=#-prunenet의-차별점><strong>📌 PruneNet의 차별점</strong></a></li></ul></li><li><a href=#-2-확률적-샘플링soft-sampling을-이용한-최적의-가중치-선택><strong>✅ (2) 확률적 샘플링(Soft-Sampling)을 이용한 최적의 가중치 선택</strong></a><ul><li><a href=#-기존-방식의-문제점-1><strong>📌 기존 방식의 문제점</strong></a></li><li><a href=#-prunenet의-차별점-1><strong>📌 PruneNet의 차별점</strong></a></li></ul></li><li><a href=#-3-스펙트럼-보존spectrum-preservation을-통한-정보-손실-최소화><strong>✅ (3) 스펙트럼 보존(Spectrum Preservation)을 통한 정보 손실 최소화</strong></a><ul><li><a href=#-기존-방식의-문제점-2><strong>📌 기존 방식의 문제점</strong></a></li><li><a href=#-prunenet의-차별점-2><strong>📌 PruneNet의 차별점</strong></a></li></ul></li></ul></li><li><a href=#-2-스펙트럼-보존-기법이-실제-정보-손실을-줄이는-방식><strong>🔹 2. 스펙트럼 보존 기법이 실제 정보 손실을 줄이는 방식</strong></a><ul><li><a href=#-1-가중치-행렬의-특이값singular-values-개념><strong>✅ (1) 가중치 행렬의 특이값(Singular Values) 개념</strong></a></li><li><a href=#-2-기존-프루닝-방식의-문제점><strong>✅ (2) 기존 프루닝 방식의 문제점</strong></a></li><li><a href=#-3-prunenet의-스펙트럼-보존-기법><strong>✅ (3) PruneNet의 스펙트럼 보존 기법</strong></a><ul><li><a href=#-ks-거리kolmogorov-smirnov-distance란><strong>📌 KS 거리(Kolmogorov-Smirnov Distance)란?</strong></a></li></ul></li><li><a href=#-4-스펙트럼-보존-기법의-실제-적용-방식><strong>✅ (4) 스펙트럼 보존 기법의 실제 적용 방식</strong></a></li><li><a href=#-5-prunenet-vs-기존-기법의-스펙트럼-변화-비교><strong>✅ (5) PruneNet vs 기존 기법의 스펙트럼 변화 비교</strong></a></li></ul></li><li><a href=#-결론-prunenet이-기존-기법보다-성능-유지율이-높은-이유><strong>📌 결론: PruneNet이 기존 기법보다 성능 유지율이 높은 이유</strong></a><ul><li><a href=#-기존-프루닝-기법의-문제점><strong>✅ 기존 프루닝 기법의 문제점</strong></a></li><li><a href=#-prunenet이-더-높은-성능을-유지할-수-있는-이유><strong>✅ PruneNet이 더 높은 성능을 유지할 수 있는 이유</strong></a></li></ul></li></ul><ul><li><a href=#-prunenet의-정책-학습-모델policy-learner-연산-과정-및-연산량-증가-분석><strong>📌 PruneNet의 정책 학습 모델(Policy Learner) 연산 과정 및 연산량 증가 분석</strong></a></li><li><a href=#-1-정책-학습-모델policy-learner의-주요-연산-과정><strong>🔹 1. 정책 학습 모델(Policy Learner)의 주요 연산 과정</strong></a><ul><li><a href=#-1-가중치-행렬--w_><strong>✅ (1) 가중치 행렬 ( W_{up} ) 입력 및 중간 표현 계산</strong></a></li><li><a href=#-2-정책-모델을-이용한-중요도-점수-계산><strong>✅ (2) 정책 모델을 이용한 중요도 점수 계산</strong></a></li><li><a href=#-3-확률적-샘플링을-통한-프루닝할-행-선택><strong>✅ (3) 확률적 샘플링을 통한 프루닝할 행 선택</strong></a></li><li><a href=#-4-kolmogorov-smirnov-ks-거리-계산-및-보정><strong>✅ (4) Kolmogorov-Smirnov (KS) 거리 계산 및 보정</strong></a></li></ul></li><li><a href=#-2-정책-학습-모델로-인한-추가적인-연산량-증가-분석><strong>🔹 2. 정책 학습 모델로 인한 추가적인 연산량 증가 분석</strong></a><ul><li><a href=#-기존-프루닝-기법-vs-prunenet의-연산량-비교><strong>✅ 기존 프루닝 기법 vs PruneNet의 연산량 비교</strong></a></li><li><a href=#-gpu-메모리-사용량-분석-llama-2-7b-기준><strong>✅ GPU 메모리 사용량 분석 (LLaMA-2-7B 기준)</strong></a></li></ul></li><li><a href=#-3-전체-모델-압축-과정에서-정책-학습-모델의-역할과-연산량-증가-영향><strong>🔹 3. 전체 모델 압축 과정에서 정책 학습 모델의 역할과 연산량 증가 영향</strong></a><ul><li><a href=#-정책-학습-모델이-전체-압축-과정에서-추가하는-연산량><strong>✅ 정책 학습 모델이 전체 압축 과정에서 추가하는 연산량</strong></a></li><li><a href=#-추가-연산량이-전체-모델-압축-성능에-미치는-영향><strong>✅ 추가 연산량이 전체 모델 압축 성능에 미치는 영향</strong></a></li></ul></li></ul><ul><li><a href=#-prunenet의-self-attention-블록-미압축-문제-해결을-위한-sparse-attention-및-head-wise-pruning-적용-시-연산량-감소-및-성능-유지율-개선-분석><strong>📌 PruneNet의 Self-Attention 블록 미압축 문제 해결을 위한 Sparse Attention 및 Head-wise Pruning 적용 시 연산량 감소 및 성능 유지율 개선 분석</strong></a></li><li><a href=#-1-self-attention-연산의-연산량-분석><strong>🔹 1. Self-Attention 연산의 연산량 분석</strong></a><ul><li><a href=#-기존-self-attention-연산량><strong>✅ 기존 Self-Attention 연산량</strong></a></li></ul></li><li><a href=#-2-sparse-attention-적용-시-연산량-감소-효과><strong>🔹 2. Sparse Attention 적용 시 연산량 감소 효과</strong></a><ul><li><a href=#-sparse-attention의-종류><strong>✅ Sparse Attention의 종류</strong></a></li><li><a href=#-sparse-attention-적용-시-연산량-비교><strong>✅ Sparse Attention 적용 시 연산량 비교</strong></a></li></ul></li><li><a href=#-3-head-wise-pruning-적용-시-연산량-감소-효과><strong>🔹 3. Head-wise Pruning 적용 시 연산량 감소 효과</strong></a><ul><li><a href=#-multi-head-attention의-연산량><strong>✅ Multi-Head Attention의 연산량</strong></a></li><li><a href=#-head-wise-pruning-적용-시-연산량-비교><strong>✅ Head-wise Pruning 적용 시 연산량 비교</strong></a></li></ul></li><li><a href=#-4-sparse-attention--head-wise-pruning-적용-시-성능-유지율-분석><strong>🔹 4. Sparse Attention + Head-wise Pruning 적용 시 성능 유지율 분석</strong></a></li><li><a href=#-결론-prunenet에-sparse-attention-및-head-wise-pruning-적용-시-기대-효과><strong>📌 결론: PruneNet에 Sparse Attention 및 Head-wise Pruning 적용 시 기대 효과</strong></a><ul><li><a href=#-연산량-감소-효과><strong>✅ 연산량 감소 효과</strong></a></li><li><a href=#-성능-유지율-개선-효과><strong>✅ 성능 유지율 개선 효과</strong></a></li><li><a href=#-prunenet-적용-시-기대되는-개선점><strong>✅ PruneNet 적용 시 기대되는 개선점</strong></a></li></ul></li></ul></nav></div></nav></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground text-center text-xs"><span>제목을 클릭하면 해당 위치로 이동합니다</span></div></div></div></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>function initKaTeX(){if(typeof renderMathInElement=="undefined"){setTimeout(initKaTeX,100);return}var e=[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"}];renderMathInElement(document.body,{delimiters:e,throwOnError:!1,errorColor:"#cc0000",fleqn:!1,leqno:!1,trust:!1})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",initKaTeX):initKaTeX()</script><div id=search-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div><div id=search-modal class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300"><div class="border-border flex items-center gap-3 border-b p-4"><div class="text-muted-foreground h-5 w-5 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><div class="relative flex-1"><button id=search-clear class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200" title=지우기 aria-label=지우기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg>
</button>
<input id=search-input type=text placeholder="게시글 검색..." class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none" autocomplete=off spellcheck=false></div><button id=search-close class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div id=search-results class="max-h-96 overflow-y-auto"><div id=search-empty class="flex flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 시작</h3><p class="text-muted-foreground text-sm">검색어를 입력하세요</p></div><div id=search-loading class="flex hidden items-center justify-center py-8"><div class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div><span class=text-muted-foreground>검색 중...</span></div><div id=search-no-results class="flex hidden flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 결과 없음</h3><p class="text-muted-foreground text-sm">다른 검색어로 시도해보세요</p></div><div id=search-results-list class=hidden><div id=search-stats class="text-muted-foreground border-border border-b px-4 py-3 text-sm" data-template="%d개의 결과 찾음"></div><div id=search-items class="divide-border divide-y"></div></div></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground flex items-center justify-between text-xs"><div class="flex items-center gap-2 md:gap-4"><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↑↓</kbd>
<span class="hidden sm:inline">이동</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↵</kbd>
<span class="hidden sm:inline">선택</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">ESC</kbd>
<span class="hidden sm:inline">닫기</span></div></div><div class="search-hint-desktop flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">⌘K</kbd>
<span>단축키</span></div></div></div></div></body></html>