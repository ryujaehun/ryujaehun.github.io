<!doctype html><html lang=ko-kr data-theme=claude><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection | Jaehun's Blog</title><meta name=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문 **&ldquo;TokenSelect: Efficient..."><meta name=keywords content="개발,프로그래밍,에세이"><meta name=author content><meta name=language content="ko"><meta name=robots content="index, follow"><link rel=canonical href=https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.02886/><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.02886/"><meta property="og:site_name" content="Jaehun's Blog"><meta property="og:title" content="TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection"><meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문 **“TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection”**에 대해 분석한 내용을 아래와 같이 정리하겠습니다."><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="ko"><meta property="article:published_time" content="2024-11-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-18T00:00:00+00:00"><meta property="og:image" content="https://jaehun.me/images/og-default.avif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jaehun.me/images/og-default.avif"><meta name=twitter:title content="TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection"><meta name=twitter:description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문 **“TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection”**에 대해 분석한 내용을 아래와 같이 정리하겠습니다."><meta itemprop=name content="TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection"><meta itemprop=description content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문 **“TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection”**에 대해 분석한 내용을 아래와 같이 정리하겠습니다."><meta itemprop=datePublished content="2024-11-18T00:00:00+00:00"><meta itemprop=dateModified content="2024-11-18T00:00:00+00:00"><meta itemprop=wordCount content="16579"><meta itemprop=image content="https://jaehun.me/images/og-default.avif"><meta itemprop=keywords content="Paper-Review,With-Gpt"><script>window.HUGO_SEARCH_CONFIG={searchIndexURL:'"/ko/index.json"'}</script><script>window.HUGO_GALLERY_CONFIG={justified_gallery:!1,lightbox:!1,justified:"{}",lightbox_options:"{}"}</script><link rel=stylesheet href=/css/compiled.min.9165cace5b80a5be5daad6ea96eb2abc20a18a3f41d7fed7f0335b689fdf7883.css integrity="sha256-kWXKzluApb5dqtbqlusqvCChij9B1/7X8DNbaJ/feIM=" crossorigin=anonymous><link rel=stylesheet href=/css/chroma.min.491df36221f739b5948a747a0351f100ee7ccaaf466d9f46288bf06de1d59123.css integrity="sha256-SR3zYiH3ObWUinR6A1HxAO58yq9GbZ9GKIvwbeHVkSM=" crossorigin=anonymous><script src=/js/main.ba969e49a7a874e6938fe689a7dcaa00cb64cb4d429c2cf98d09bde512a16c6d.js integrity="sha256-upaeSaeodOaTj+aJp9yqAMtky01CnCz5jQm95RKhbG0=" crossorigin=anonymous></script><script src=/js/gumshoe.polyfills.min.js></script><script src=/js/toc.70654aebf0b738a78ae2df95297cc10ebc6e590f3c942dd1325fa852f50038b2.js integrity="sha256-cGVK6/C3OKeK4t+VKXzBDrxuWQ88lC3RMl+oUvUAOLI=" crossorigin=anonymous defer></script><script src=/js/search.de3eba3540834efb8328121784bff4774e06362412ee41eee665f5b4f987f393.js integrity="sha256-3j66NUCDTvuDKBIXhL/0d04GNiQS7kHu5mX1tPmH85M=" crossorigin=anonymous defer></script><script src=/js/dock.e02f422517e001d9fb6bea1e9e8b0e89e9d2e486ad4641f275bb3e36ce2ae79f.js integrity="sha256-4C9CJRfgAdn7a+oenosOienS5IatRkHydbs+Ns4q558=" crossorigin=anonymous defer></script><script>(function(){const e=localStorage.getItem("theme")||"system",n=localStorage.getItem("colorScheme")||"claude";document.documentElement.setAttribute("data-theme",n);function t(){e==="dark"||e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}t(),e==="system"&&window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",t)})()</script></head><body class="bg-background text-foreground min-h-screen antialiased"><div id=reading-progress-container class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out" data-height=3 data-smooth-scroll=true data-hide-on-complete=false><div class="reading-progress-bg w-full"></div><div id=reading-progress-bar class="from-primary to-primary/80 reading-progress-bar
transition-all duration-150 ease-out
absolute top-0 left-0 w-0 bg-gradient-to-r"></div></div><script>(function(){"use strict";const t=document.getElementById("reading-progress-container"),o=document.getElementById("reading-progress-bar");if(!t||!o)return;const a={smoothScroll:t.dataset.smoothScroll==="true",hideOnComplete:t.dataset.hideOnComplete==="true"};let n=!0,l=null;function r(){const t=window.pageYOffset||document.documentElement.scrollTop,n=document.documentElement.scrollHeight,s=window.innerHeight,e=n-s;return e<=0?0:Math.min(Math.max(t/e*100,0),100)}function c(){const e=r();o.style.width=e+"%",a.hideOnComplete&&e>=99.5?n&&(t.style.opacity="0",n=!1):n||(t.style.opacity="1",n=!0)}let s=!1;function e(){s||(requestAnimationFrame(()=>{c(),s=!1}),s=!0)}function i(){window.addEventListener("scroll",e,{passive:!0}),window.addEventListener("resize",e,{passive:!0}),document.addEventListener("visibilitychange",()=>{document.hidden||e()}),e(),window.addEventListener("beforeunload",()=>{window.removeEventListener("scroll",e),window.removeEventListener("resize",e)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script><header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6"><div class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm"><div class="hidden w-full items-center md:flex"><div class="flex items-center"><a href=/ko/ class="flex h-10 w-10 items-center justify-center overflow-hidden rounded-full transition-transform duration-200 hover:scale-105" aria-label="Jaehun's Blog"><img src=/images/logo.svg alt="Jaehun's Blog" class="h-full w-full object-cover"></a></div><nav class="mx-8 flex flex-1 items-center justify-center"><div class="flex items-center space-x-1"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></div></nav><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div><div class="flex w-full items-center justify-between md:hidden"><div class=relative><button id=mobile-menu-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200" data-dropdown-type=mobile-menu aria-label=메뉴 aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="메뉴"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button><div id=mobile-menu class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden" data-dropdown-type=mobile-menu role=menu aria-labelledby=mobile-menu-toggle><nav class="flex flex-col"><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.posts"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"/></svg>
<span>게시글
</span></a><a href=/ko/categories/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.categories"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"/></svg>
<span>카테고리
</span></a><a href=/ko/tags/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.tags"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512.0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828.0l-7-7A1.994 1.994.0 013 12V7a4 4 0 014-4z"/></svg>
<span>태그
</span></a><a href=/ko/ class="nav-link
text-muted-foreground hover:text-primary hover:bg-primary/10
focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none" role=menuitem><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="nav.archives"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4"/></svg>
<span>아카이브</span></a></nav></div></div><div class="flex items-center space-x-2"><div class=relative><button id=language-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=language aria-label="언어 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="언어 변경"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8 6 6m-7 0 6-6 2-3M2 5h12M7 2h1m14 20-5-10-5 10m2-4h6"/></svg></button><div id=language-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=language role=menu aria-labelledby=language-toggle><a href=/ko/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem aria-current=true><span class=font-medium>한국어</span></a><a href=/en/ class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role=menuitem><span class=font-medium>English</span></a></div></div><div class=relative><button id=color-scheme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=color-scheme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="테마 변경"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z"/></svg></button><div id=color-scheme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=color-scheme><button data-color-scheme=claude class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme=default class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme=bumblebee class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme=emerald class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme=nord class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme=sunset class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme=abyss class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme=dracula class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme=amethyst class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme=slate class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme=twitter class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div></div><div class=relative><button id=theme-toggle class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95" data-dropdown-type=theme aria-label="테마 변경" aria-expanded=false aria-haspopup=true>
<svg class="h-5 w-5 sun-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg>
<svg class="h-5 w-5 moon-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg>
<svg class="h-5 w-5 system-icon hidden relative z-10" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg></button><div id=theme-dropdown class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out" data-dropdown-type=theme><button data-theme=light class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="라이트"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364-.707-.707M6.343 6.343l-.707-.707m12.728.0-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"/></svg><span>라이트</span></button><button data-theme=dark class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="다크"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003.0 0012 21a9.003 9.003.0 008.354-5.646z"/></svg><span>다크</span></button><button data-theme=system class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
<svg class="h-5 w-5 mr-3 h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="시스템"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17 9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5A2 2 0 003 5v10a2 2 0 002 2z"/></svg><span>시스템</span></button></div></div></div></div></div></header><main class="mx-auto max-w-4xl px-4 py-6"><nav class="breadcrumb mb-4 md:mb-6 py-1" aria-label=경로><ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1"><li class=flex-shrink-0><a href=/ko/ class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="홈"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11 2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">홈</span></a></li><li class="flex items-center gap-1 md:gap-2 min-w-0"><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span><div class="flex items-center gap-1 md:gap-2 min-w-0"><a href=/ko class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5A2 2 0 003 7z"/></svg>
<span class="max-w-[4rem] md:max-w-none truncate">Ko</span>
</a><span class="text-muted-foreground/50 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></span>
<span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/></svg>
<span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</span></span></div></li></ol></nav><header class=mb-8><h1 class="text-foreground mb-4 text-3xl font-bold md:text-4xl">TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</h1></header><article class="prose prose-neutral dark:prose-invert max-w-none"><p><a href=https://arxiv.org/abs/2411.02886>논문 링크</a></p><h1 id=q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘>Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1><p>논문 **&ldquo;TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection&rdquo;**에 대해 분석한 내용을 아래와 같이 정리하겠습니다.</p><h3 id=1-논문의-강점-및-독창성>1. 논문의 강점 및 독창성</h3><p>이 논문은 <strong>대규모 언어 모델(LLMs)의 긴 문맥 처리 효율성을 향상</strong>시키기 위한 새로운 접근법인 <strong>TokenSelect</strong>를 제안합니다. 기존의 긴 문맥 추론에서 발생하는 성능 저하와 높은 계산 복잡도를 해결하기 위해 모델-불가지론적이고 훈련이 필요 없는 방법을 도입했다는 점에서 독창적입니다.</p><ul><li><strong>주요 강점</strong>:<ol><li><strong>모델-불가지론적</strong>으로, 사전 훈련된 LLM을 수정하지 않고도 긴 문맥 추론을 가속화할 수 있습니다.</li><li><strong>훈련이 필요 없는 접근법</strong>: 기존의 긴 문맥 길이를 확장하는 방법(예: 추가 훈련)과 달리, TokenSelect는 추가 훈련 없이도 긴 문맥 처리가 가능합니다.</li><li><strong>효율성</strong>: 기존의 방법 대비 최대 <strong>23.84배</strong> 빠른 주의(attention) 계산 속도와 <strong>2.28배</strong> 빠른 종단 간(latency) 추론 시간을 달성했습니다.</li><li><strong>토큰 레벨 선택</strong>: 기존의 블록 수준 또는 미리 정의된 희소 패턴이 아닌, <strong>동적으로 중요한 토큰을 선택</strong>하여 정확도를 유지하면서도 계산 효율을 크게 개선했습니다.</li></ol></li></ul><h3 id=2-핵심-알고리즘-및-과정-설명>2. 핵심 알고리즘 및 과정 설명</h3><p><strong>TokenSelect</strong> 알고리즘은 각 Query 토큰에 대해 <strong>동적으로 중요한 Key-Value(KV) 캐시 토큰을 선택</strong>하여 주의 계산에 활용합니다.</p><h4 id=1-기본-아이디어>(1) 기본 아이디어</h4><ul><li>기존 LLM의 **주의 메커니즘(Attention Mechanism)**은 입력 길이가 증가함에 따라 <strong>계산 복잡도</strong>가 **O(n²)**으로 증가하여 효율성이 떨어집니다.</li><li>TokenSelect는 <strong>동적 토큰 선택</strong>을 통해 중요한 토큰만 선택하여 주의 계산에 포함시킴으로써, 계산 복잡도를 줄이면서도 정확도를 유지합니다.</li></ul><h4 id=2-알고리즘-단계-설명>(2) 알고리즘 단계 설명</h4><ol><li><p><strong>Query-Key dot product 계산</strong>:</p><ul><li>각 Attention Head에서 Query와 KV Cache 사이의 dot product를 계산합니다.</li><li>이를 통해 각 헤드 별로 중요한 토큰의 중요도를 평가합니다.</li></ul></li><li><p><strong>Head Soft Vote 메커니즘</strong>:</p><ul><li>단순히 상위 k개의 토큰을 선택하는 것이 아니라, <strong>각 Attention Head가 독립적으로 투표</strong>하여 중요한 토큰을 선택합니다.</li><li>이를 위해 <strong>softmax를 통해 정규화된 중요도 점수</strong>를 사용하여 최종적으로 선택된 토큰을 결정합니다.</li></ul></li><li><p><strong>Paged KV Cache 관리</strong>:</p><ul><li>선택된 토큰만을 이용해 <strong>Paged Attention Kernel</strong>을 사용하여 효율적인 계산을 수행합니다.</li><li>연속적인 Query 유사성을 활용해 <strong>Selection Cache</strong>를 도입, 선택 빈도를 줄여 효율성을 높입니다.</li></ul></li></ol><h4 id=3-예시-입력과-과정>(3) 예시 입력과 과정</h4><ul><li>입력 문장이 1,000,000개의 토큰을 포함한다고 가정합니다.<ol><li>각 Query 토큰에 대해 KV Cache의 중요도를 평가합니다.</li><li>헤드별로 중요도가 높은 토큰을 soft vote 방식으로 선정합니다.</li><li>선정된 토큰만을 이용해 주의 계산을 수행하여 <strong>최적의 성능</strong>을 유지하면서도 <strong>계산 시간</strong>을 대폭 단축합니다.</li></ol></li></ul><h3 id=3-논문의-한계점>3. 논문의 한계점</h3><ol><li><p><strong>유사 Query에 대한 의존성</strong>:</p><ul><li>Selection Cache는 연속된 Query가 유사할 때 효과적이지만, 문맥이 급격히 변할 경우 성능 저하가 발생할 수 있습니다.</li></ul></li><li><p><strong>토큰 선택에 대한 추가적인 계산 오버헤드</strong>:</p><ul><li>TokenSelect는 토큰 선택 단계에서 추가적인 dot product 계산과 soft vote 과정을 포함하므로, 짧은 문맥의 경우 오히려 성능이 저하될 수 있습니다.</li></ul></li><li><p><strong>고정된 토큰 예산</strong>에 대한 한계:</p><ul><li>긴 문맥에서도 효율성을 유지하기 위해 고정된 토큰 예산(예: k=2K)을 사용하는데, 이로 인해 특정 상황에서 정보 손실이 발생할 수 있습니다.</li></ul></li></ol><h3 id=결론>결론</h3><p>이 논문은 LLM의 긴 문맥 추론을 효율적으로 개선할 수 있는 새로운 접근법을 제시했으며, 특히 <strong>웹 응용 프로그램에서의 실시간 대화형 시스템</strong>에서 매우 유용할 수 있습니다. 그러나 다양한 도메인에서 더 일반화된 성능을 보장하려면 추가적인 개선이 필요할 수 있습니다.</p><h1 id=q--head-soft-vote-메커니즘-방법에-대해서-예시를-추가해서-아주-자세하게-설명해줘>Q : Head Soft Vote 메커니즘 방법에 대해서 예시를 추가해서 아주 자세하게 설명해줘</h1><p><strong>Head Soft Vote 메커니즘</strong>은 논문의 핵심 알고리즘인 <strong>TokenSelect</strong>에서 중요한 토큰을 선택하는 방식으로 사용됩니다. 이 메커니즘은 다수의 Attention Head가 있는 상황에서 특정 토큰이 얼마나 중요한지 결정할 때, 각 Attention Head의 결과를 독립적으로 평가하고, 이를 종합적으로 반영해 최종적으로 중요한 토큰을 선택합니다.</p><p>아래에서는 이 메커니즘을 <strong>예시</strong>와 함께 매우 구체적으로 설명하겠습니다.</p><h3 id=1-head-soft-vote-메커니즘의-목적>1. Head Soft Vote 메커니즘의 목적</h3><p>일반적인 Attention 메커니즘에서 각 <strong>Attention Head</strong>는 입력된 Query와 Key 사이의 dot product를 계산하여 <strong>중요한 토큰에 더 높은 가중치</strong>를 할당합니다. 그러나, 단순히 dot product의 결과만을 기준으로 중요한 토큰을 선택할 경우, 일부 Attention Head가 지나치게 큰 값으로 인해 전체 선택 결과를 왜곡할 수 있습니다.</p><p>이를 방지하기 위해 <strong>Head Soft Vote 메커니즘</strong>은 각 Attention Head가 독립적으로 중요한 토큰을 평가하고, <strong>softmax 정규화</strong>를 통해 각 Head의 중요도를 조정한 다음, <strong>모든 Head의 결과를 종합</strong>하여 최종적으로 중요한 토큰을 선택합니다.</p><h3 id=2-head-soft-vote-메커니즘의-작동-원리>2. Head Soft Vote 메커니즘의 작동 원리</h3><p>먼저, 이 메커니즘을 수식으로 간단하게 정리한 후, 예시를 통해 설명하겠습니다.</p><h4 id=1-알고리즘-수식>(1) 알고리즘 수식</h4><ul><li><p>각 Attention Head ( h )에서 Query ( Q^h )와 KV Cache ( K^h_{\text{cache}} )의 dot product를 계산합니다.
[
\text{score}^h = Q^h \cdot (K^h_{\text{cache}})^T
]</p></li><li><p>각 Head에서 계산된 score를 softmax를 통해 정규화합니다.
[
\text{soft_score}^h = \text{softmax}(\text{score}^h)
]</p></li><li><p>Head마다 정규화된 점수를 더하여 종합 점수를 계산합니다.
[
\text{final_score} = \sum_{h=1}^{H} \text{soft_score}^h
]</p></li><li><p>종합 점수에 따라 상위 ( k )개의 중요한 토큰을 선택합니다.
[
I_{\text{select}} = \text{TopK}(\text{final_score})
]</p></li></ul><h3 id=3-head-soft-vote-메커니즘-예시>3. Head Soft Vote 메커니즘 예시</h3><h4 id=1-시나리오-설정>(1) 시나리오 설정</h4><ul><li><strong>Attention Heads</strong>: 4개 (Head 1, Head 2, Head 3, Head 4)</li><li><strong>KV Cache에 저장된 토큰 수</strong>: 5개 (Token A, Token B, Token C, Token D, Token E)</li><li><strong>선택할 토큰 수 ( k )</strong>: 2개</li></ul><h4 id=2-각-head의-query와-key-dot-product-결과>(2) 각 Head의 Query와 Key dot product 결과</h4><p>우선 각 Attention Head에서 Query와 KV Cache의 Key를 사용해 dot product를 계산했다고 가정합니다. 각 토큰에 대해 얻어진 점수는 다음과 같습니다:</p><table><thead><tr><th>토큰</th><th>Head 1</th><th>Head 2</th><th>Head 3</th><th>Head 4</th></tr></thead><tbody><tr><td>A</td><td>0.2</td><td>0.5</td><td>0.1</td><td>0.4</td></tr><tr><td>B</td><td>0.8</td><td>0.3</td><td>0.6</td><td>0.7</td></tr><tr><td>C</td><td>0.4</td><td>0.6</td><td>0.3</td><td>0.5</td></tr><tr><td>D</td><td>0.7</td><td>0.2</td><td>0.4</td><td>0.3</td></tr><tr><td>E</td><td>0.3</td><td>0.4</td><td>0.9</td><td>0.2</td></tr></tbody></table><h4 id=3-각-head의-softmax-정규화-결과>(3) 각 Head의 softmax 정규화 결과</h4><p>각 Head별로 <strong>softmax</strong>를 적용하여 점수를 정규화합니다. 정규화된 점수는 다음과 같습니다:</p><table><thead><tr><th>토큰</th><th>Head 1</th><th>Head 2</th><th>Head 3</th><th>Head 4</th></tr></thead><tbody><tr><td>A</td><td>0.13</td><td>0.27</td><td>0.10</td><td>0.26</td></tr><tr><td>B</td><td>0.36</td><td>0.19</td><td>0.22</td><td>0.36</td></tr><tr><td>C</td><td>0.18</td><td>0.30</td><td>0.13</td><td>0.24</td></tr><tr><td>D</td><td>0.27</td><td>0.16</td><td>0.16</td><td>0.19</td></tr><tr><td>E</td><td>0.16</td><td>0.22</td><td>0.38</td><td>0.15</td></tr></tbody></table><h4 id=4-각-head의-soft-vote-결과-종합>(4) 각 Head의 soft vote 결과 종합</h4><p>모든 Attention Head의 정규화된 점수를 합산하여 최종 종합 점수를 계산합니다:</p><table><thead><tr><th>토큰</th><th>종합 점수 (Head Soft Vote 결과)</th></tr></thead><tbody><tr><td>A</td><td>0.13 + 0.27 + 0.10 + 0.26 = 0.76</td></tr><tr><td>B</td><td>0.36 + 0.19 + 0.22 + 0.36 = 1.13</td></tr><tr><td>C</td><td>0.18 + 0.30 + 0.13 + 0.24 = 0.85</td></tr><tr><td>D</td><td>0.27 + 0.16 + 0.16 + 0.19 = 0.78</td></tr><tr><td>E</td><td>0.16 + 0.22 + 0.38 + 0.15 = 0.91</td></tr></tbody></table><h4 id=5-최종적으로-선택된-토큰>(5) 최종적으로 선택된 토큰</h4><ul><li>종합 점수가 가장 높은 상위 <strong>2개의 토큰</strong>을 선택합니다.<ul><li>Token B (1.13)</li><li>Token E (0.91)</li></ul></li></ul><p>따라서 <strong>Token B</strong>와 <strong>Token E</strong>가 최종적으로 Attention 계산에 사용됩니다.</p><h3 id=4-head-soft-vote-메커니즘의-장점>4. Head Soft Vote 메커니즘의 장점</h3><ul><li><strong>독립적인 Attention Head 평가</strong>: 각 Attention Head의 특성을 반영하여 중요한 토큰을 선택함으로써 특정 Head의 영향이 과도하게 반영되는 것을 방지합니다.</li><li><strong>효율적이고 정밀한 토큰 선택</strong>: 모든 Attention Head의 softmax 결과를 종합함으로써, 단순한 Top-k 방식보다 <strong>더 정밀한 선택</strong>이 가능합니다.</li><li><strong>계산 효율성 향상</strong>: 선택된 소수의 토큰만을 주의 계산에 사용함으로써, 긴 문맥을 처리할 때 <strong>계산 자원을 절약</strong>하고 속도를 높일 수 있습니다.</li></ul><h3 id=5-한계점>5. 한계점</h3><ul><li><strong>계산 오버헤드</strong>: softmax 정규화와 각 Head의 독립적인 평가로 인해, 짧은 문맥에서는 오히려 추가적인 계산 비용이 발생할 수 있습니다.</li><li><strong>모델에 따라 성능 편차</strong>: 모든 LLM이 동일한 Attention 구조를 갖고 있지 않기 때문에, 특정 모델에서는 이 메커니즘이 기대만큼의 성능 개선을 가져오지 않을 수 있습니다.</li></ul><p>이처럼 <strong>Head Soft Vote 메커니즘</strong>은 긴 문맥에서 효율성을 극대화하기 위한 중요한 구성 요소입니다. 이를 통해 기존의 긴 문맥 추론에서 발생하는 성능 저하를 효과적으로 해결할 수 있습니다.</p><h1 id=q--위-예시에서-32k-정도의-context-length를-가질때-연산과-메모리-입출력이-얼마나-줄어들-수-있을지-예시를-들어서-수치를-정리해줘>Q : 위 예시에서 32K 정도의 context length를 가질때 연산과 메모리 입출력이 얼마나 줄어들 수 있을지 예시를 들어서 수치를 정리해줘</h1><p><strong>TokenSelect의 Head Soft Vote 메커니즘</strong>을 사용할 경우, 긴 문맥을 처리할 때 <strong>계산 시간과 메모리 사용량</strong>을 어떻게 절약할 수 있는지를 <strong>32K (32,768 토큰)</strong> 길이의 문맥을 예시로 들어 수치적으로 분석해 보겠습니다.</p><h3 id=1-기존-attention-메커니즘의-계산-복잡도>1. 기존 Attention 메커니즘의 계산 복잡도</h3><p>Transformer 모델에서 <strong>기존 Scaled Dot-Product Attention</strong>의 계산 복잡도는 입력 길이 ( n )에 대해 **( O(n^2) )**입니다. 이는 다음과 같은 이유에서 발생합니다.</p><ul><li>Query와 Key의 dot product 계산은 각 Query 토큰이 모든 Key 토큰과 비교해야 하므로 **( n \times n = n^2 )**의 연산이 필요합니다.</li><li>예를 들어, ( n = 32,768 )일 때, **1,073,741,824회(약 10억 회)**의 연산이 필요합니다.</li></ul><h4 id=메모리-사용량-분석>메모리 사용량 분석</h4><ul><li>각 Attention Head마다 <strong>Query, Key, Value (Q, K, V)</strong> 행렬이 필요하며, 길이 ( n )과 차원 ( d )를 가질 때 메모리 복잡도는 **( O(n \times d) )**입니다.</li><li>일반적으로 ( d = 128 ) 또는 ( d = 256 )을 사용합니다.</li><li>( n = 32,768 )이고 ( d = 256 )일 경우, 각 행렬은 **8,388,608 (약 8.4MB)**의 메모리를 차지합니다.</li><li>Attention 계산에는 <strong>32개의 Attention Head</strong>를 사용하는 경우, 총 메모리 사용량은 약 <strong>268MB</strong>입니다.</li></ul><h3 id=2-tokenselect의-head-soft-vote-메커니즘-적용-시-개선>2. TokenSelect의 Head Soft Vote 메커니즘 적용 시 개선</h3><p>TokenSelect는 <strong>중요한 토큰만 선택</strong>하여 Attention 계산에 포함시킴으로써 연산과 메모리 사용량을 줄입니다.</p><h4 id=1-동작-방식-요약>(1) 동작 방식 요약</h4><ul><li><strong>Head Soft Vote</strong>를 사용하여 각 Attention Head에서 상위 ( k )개의 중요한 토큰만 선택합니다.</li><li>예를 들어, ( k = 512 )로 설정하면, 각 Query 토큰은 전체 32,768개의 Key 토큰을 비교하는 대신, 선택된 512개의 Key 토큰과만 비교합니다.</li></ul><h4 id=2-계산-복잡도-감소-분석>(2) 계산 복잡도 감소 분석</h4><ul><li>기존 방식: ( O(n^2) = 32,768^2 = 1,073,741,824 ) (약 10억 회)</li><li>TokenSelect 적용 후:<ul><li>선택된 토큰 수 ( k = 512 )일 때, <strong>( O(n \times k) = 32,768 \times 512 = 16,777,216 )</strong> (약 1,677만 회)의 연산만 필요합니다.</li><li>연산량이 <strong>64배</strong> (1,073,741,824 ÷ 16,777,216) 줄어듭니다.</li></ul></li></ul><h4 id=3-메모리-사용량-절감-분석>(3) 메모리 사용량 절감 분석</h4><ul><li>기존 방식에서는 모든 KV Cache를 메모리에 저장하여 사용했습니다.</li><li>TokenSelect에서는 선택된 <strong>( k = 512 )</strong> 개의 중요한 토큰만 캐시에 저장하여 사용하므로, 필요한 메모리 양이 크게 줄어듭니다.<ul><li>기존 방식: <strong>32,768개의 토큰 × 256 차원 × 32개 헤드 = 268MB</strong></li><li>TokenSelect 적용 시: <strong>512개의 토큰 × 256 차원 × 32개 헤드 = 4MB</strong></li></ul></li><li>결과적으로 **약 67배(268MB ÷ 4MB)**의 메모리 절약이 가능합니다.</li></ul><h3 id=3-메모리-및-연산-성능-비교-요약>3. 메모리 및 연산 성능 비교 요약</h3><table><thead><tr><th>항목</th><th>기존 방식</th><th>TokenSelect 적용 시</th></tr></thead><tbody><tr><td>연산량 (32K 길이 기준)</td><td>약 10억 회</td><td>약 1,677만 회</td></tr><tr><td>연산량 절감 비율</td><td>-</td><td><strong>약 64배 감소</strong></td></tr><tr><td>메모리 사용량</td><td>약 268MB</td><td>약 4MB</td></tr><tr><td>메모리 절감 비율</td><td>-</td><td><strong>약 67배 감소</strong></td></tr></tbody></table><h3 id=4-실제-활용-시-이점>4. 실제 활용 시 이점</h3><ul><li><strong>추론 속도 개선</strong>: 연산량이 크게 줄어들기 때문에, 추론 속도가 최대 <strong>2.28배</strong> 빨라집니다(논문 실험 결과 기준).</li><li><strong>메모리 사용 효율성 개선</strong>: GPU 메모리를 덜 사용하게 되어 <strong>더 긴 문맥</strong>을 처리하거나 <strong>더 많은 사용자 요청을 병렬 처리</strong>할 수 있습니다.</li><li><strong>웹 애플리케이션 및 실시간 서비스</strong>에서 LLM을 사용할 때, 대기 시간(latency)을 크게 줄일 수 있습니다.</li></ul><h3 id=5-추가-고려-사항>5. 추가 고려 사항</h3><ul><li><strong>TokenSelect의 효율성</strong>은 긴 문맥 처리 시 특히 두드러지므로, 짧은 문맥에서는 오히려 오버헤드가 발생할 수 있습니다.</li><li>TokenSelect는 <strong>유사한 Query를 반복적으로 처리하는 시나리오</strong>에서 특히 효율적입니다(예: 대화형 시스템, 실시간 검색 엔진).</li></ul><p>이와 같이, TokenSelect의 Head Soft Vote 메커니즘을 활용하면 <strong>긴 문맥 처리 시 연산 및 메모리 사용량을 획기적으로 줄여</strong> 대규모 언어 모델의 실시간 서비스 성능을 크게 개선할 수 있습니다.</p><h1 id=q--궁금한-지점이-있는데-qkv에-대해서-tokenselect-을-하려면-결국-토큰-f가-추가되면-top-k를-찾기-위해서-모든-head에서-q와-kv의-product를-해야하는-구조-같은데-이게-어떻게-decoding-step에서-연산량을-줄일-수-있는걸까>Q : 궁금한 지점이 있는데 QKV에 대해서 TokenSelect 을 하려면 결국 토큰 F가 추가되면 top k를 찾기 위해서 모든 head에서 Q와 KV의 product를 해야하는 구조 같은데 이게 어떻게 decoding step에서 연산량을 줄일 수 있는걸까?</h1><p>좋은 질문입니다! 이 부분이 TokenSelect 알고리즘의 핵심적인 <strong>효율성</strong>과 관련된 부분인데요. 질문해주신 대로, TokenSelect가 **모든 Attention Head에서 Query ( Q )와 Key ( K )**의 dot product를 계산하여 상위 ( k )개의 중요한 토큰을 선택한다면, <strong>모든 KV Cache를 처음부터 끝까지 계산하는 것과 큰 차이가 없을 것</strong>처럼 보일 수 있습니다. 그럼에도 불구하고, TokenSelect는 <strong>decoding 단계에서의 연산량을 줄이는</strong> 데 성공하고 있습니다. 어떻게 그런 일이 가능한지 자세히 설명드리겠습니다.</p><h3 id=1-문제-상황-decoding-단계에서의-효율성-문제>1. 문제 상황: Decoding 단계에서의 효율성 문제</h3><ul><li><strong>기존 Transformer 기반 모델</strong>에서는 <strong>decoding 단계</strong>에서 새로운 토큰 ( F )가 추가될 때마다, 이전에 저장된 모든 KV Cache와 새로운 Query ( Q )를 비교해야 합니다.</li><li>이 경우, <strong>새로운 Query 토큰이 추가</strong>될 때마다 <strong>전체 KV Cache</strong>(예: 수만~수십만 개의 토큰)를 모두 탐색해야 하므로, 연산량이 **O(n^2)**으로 증가합니다.</li><li>이는 <strong>긴 문맥을 처리</strong>할 때, 특히 자동 완성, 실시간 응답 생성 등에서 **시간 지연(latency)**을 초래하게 됩니다.</li></ul><h3 id=2-tokenselect의-핵심-아이디어>2. TokenSelect의 핵심 아이디어</h3><p>TokenSelect는 <strong>모든 Query 토큰에 대해 모든 Key 토큰을 비교하지 않고</strong>, <strong>가장 중요한 토큰만 선택하여 계산</strong>에 포함함으로써 연산량을 줄입니다. 하지만 이 과정이 어떻게 <strong>매번 Q와 KV를 비교하지 않고도</strong> 효율적으로 동작할 수 있는지에 대해 설명드리겠습니다.</p><h3 id=3-tokenselect의-작동-방식-선택적-sparse-attention>3. TokenSelect의 작동 방식: 선택적 Sparse Attention</h3><h4 id=1-prefill-단계와-decode-단계의-분리>(1) <strong>Prefill 단계와 Decode 단계의 분리</strong></h4><ul><li><strong>Prefill 단계</strong>: 사용자의 입력 문장이 처음 들어올 때, <strong>전체 입력 시퀀스</strong>에 대해 한 번만 <strong>KV Cache</strong>를 생성하고 저장합니다.</li><li><strong>Decode 단계</strong>: 모델이 <strong>새로운 토큰을 생성할 때마다</strong>(즉, 한 번에 한 토큰씩), 이전에 저장된 <strong>KV Cache와 새로운 Query</strong>를 사용하여 필요한 계산을 수행합니다.</li></ul><h4 id=2-decoding-단계에서-연산량-절감-방법>(2) <strong>Decoding 단계에서 연산량 절감 방법</strong></h4><ul><li><strong>TokenSelect</strong>는 <strong>모든 Key와 Value에 대해 dot product를 매번 수행하지 않고</strong>, 이전에 <strong>가장 중요한 ( k )개의 토큰만 선택</strong>하여 저장해 둡니다.</li><li>새로운 토큰이 추가되면, <strong>저장된 상위 ( k )개의 중요한 토큰</strong>만을 사용하여 attention 계산을 수행합니다.</li><li>즉, 새로운 Query 토큰이 추가되었을 때, <strong>모든 기존의 KV Cache와 비교하지 않고</strong>, 이전에 선택된 ( k )개의 토큰과만 비교합니다.</li></ul><h4 id=3-selection-cache-활용>(3) <strong>Selection Cache 활용</strong></h4><ul><li>TokenSelect는 <strong>Selection Cache</strong>라는 추가적인 최적화 기능을 사용합니다.</li><li><strong>연속된 Query가 유사한 경우</strong>, 이전에 계산한 <strong>토큰 선택 결과를 재사용</strong>합니다.</li><li>예를 들어, 두 개의 연속된 Query가 매우 유사한 경우, <strong>이전 단계에서 선택한 상위 ( k )개의 토큰</strong>을 다시 사용할 수 있습니다.</li><li>이를 통해 새로운 토큰이 추가되더라도, <strong>추가적인 dot product 계산을 피하고</strong> 캐시에 저장된 중요한 토큰만을 사용하여 빠르게 attention을 수행할 수 있습니다.</li></ul><h3 id=4-예시를-통한-이해>4. 예시를 통한 이해</h3><h4 id=시나리오>시나리오</h4><ul><li><strong>Context 길이</strong>: 32,768 토큰 (32K)</li><li><strong>Head 수</strong>: 8개</li><li><strong>TokenSelect의 상위 ( k ) 선택 토큰 수</strong>: 512개</li></ul><ol><li><strong>Prefill 단계</strong>에서 전체 시퀀스(32,768 토큰)에 대해 한 번만 <strong>전체 dot product를 계산</strong>하여 각 Attention Head에서 중요한 토큰을 선택하고, 그 결과를 <strong>Selection Cache</strong>에 저장합니다.</li><li><strong>Decode 단계</strong>에서 새로운 토큰 ( F )가 추가되면:<ul><li>일반적인 방식에서는 새로운 Query ( Q_F )와 **전체 KV Cache (32,768 토큰)**에 대해 dot product를 다시 계산해야 합니다.</li><li><strong>TokenSelect 방식</strong>에서는 <strong>이전에 선택된 상위 ( k = 512 )개의 토큰만</strong> 사용하여 attention 계산을 수행합니다.</li><li>이로 인해 **연산량이 ( O(32,768) )에서 ( O(512) )**로 줄어듭니다.</li></ul></li></ol><h3 id=5-연산량-절감의-구체적인-분석>5. 연산량 절감의 구체적인 분석</h3><ul><li><strong>기존 방식</strong>: 새로운 토큰이 추가될 때마다 ( O(n) ) = 32,768개의 Key와 dot product를 수행해야 함.</li><li><strong>TokenSelect 적용 시</strong>: ( O(k) = 512 )개의 선택된 토큰과만 dot product를 수행.<ul><li>기존 방식 대비 <strong>64배 연산량 감소 (32,768 ÷ 512)</strong>.</li><li>메모리 I/O 역시 <strong>선택된 512개의 토큰만 참조</strong>하므로, 메모리 대역폭 소모가 크게 줄어듦.</li></ul></li></ul><h3 id=6-요약>6. 요약</h3><p>TokenSelect는 <strong>모든 Q와 KV의 product를 매번 수행하는 것이 아니라</strong>, <strong>선택된 중요한 토큰만 사용</strong>하여 attention을 수행함으로써 <strong>긴 문맥에서도 연산량과 메모리 사용량을 크게 줄입니다</strong>. 이를 통해 <strong>decoding 단계에서의 효율성</strong>을 높여, 실시간 응답 생성이나 대화형 시스템에서 <strong>대기 시간을 단축</strong>할 수 있습니다.</p><ul><li><strong>Prefill 단계에서 전체 문맥 분석 후</strong>, 중요한 정보를 추출하여 저장하고, <strong>Decode 단계에서 반복적인 연산을 줄이는 방식</strong>으로 작동합니다.</li><li><strong>Selection Cache</strong>를 통해 연속된 Query 간의 유사성을 활용하여 추가적인 최적화를 수행합니다.</li></ul><p>이러한 방식은 특히 <strong>긴 문맥을 다루는 시나리오</strong>에서 큰 성능 향상을 가져옵니다.</p><h1 id=q--selection-cache를-자세하게-설명해주고--query의-유사성을-구하는-방법에-대해서도-아주-자세하게-설명해-부탁해>Q : Selection Cache를 자세하게 설명해주고 query의 유사성을 구하는 방법에 대해서도 아주 자세하게 설명해 부탁해!</h1><p><strong>Selection Cache</strong>와 <strong>Query 유사성 측정</strong>은 TokenSelect 알고리즘의 핵심적인 효율화 요소입니다. 이를 통해 **긴 문맥(Long Context)**을 처리할 때 연산 비용을 줄이고, <strong>실시간 추론 속도</strong>를 극대화합니다. 이번 답변에서는 <strong>Selection Cache의 작동 원리</strong>와 <strong>Query 유사성을 측정하는 방법</strong>을 매우 상세하게 설명드리겠습니다.</p><hr><h3 id=1-selection-cache의-개요>1. Selection Cache의 개요</h3><p><strong>Selection Cache</strong>는 TokenSelect 알고리즘에서 <strong>동일하거나 유사한 Query가 반복적으로 등장하는 상황</strong>에서 <strong>중복된 계산을 피하기 위해</strong> 도입된 최적화 기법입니다. 특히, 긴 문맥 추론에서는 새로운 Query가 추가될 때마다 이전의 모든 Key-Value(KV) Cache와의 dot product를 다시 계산하는 것은 연산량과 메모리 사용량을 크게 증가시킵니다.</p><h4 id=selection-cache의-목적><strong>Selection Cache의 목적</strong></h4><ul><li><strong>중복 계산 방지</strong>: 유사한 Query에 대해 이미 선택된 <strong>중요한 토큰 상위 ( k ) 목록</strong>을 재사용하여 연산량을 줄임.</li><li><strong>효율성 향상</strong>: 연속된 Query가 유사할 경우, 이전 단계에서 선택된 결과를 다시 사용해 <strong>추가적인 연산을 피하고</strong> 더 빠르게 응답 생성 가능.</li></ul><hr><h3 id=2-selection-cache의-작동-원리>2. Selection Cache의 작동 원리</h3><p>Selection Cache는 <strong>유사한 Query가 반복될 때마다 토큰 선택을 반복하지 않고, 이전에 선택된 결과를 재사용</strong>하여 효율성을 극대화합니다.</p><h4 id=selection-cache-알고리즘><strong>Selection Cache 알고리즘</strong></h4><p>아래의 알고리즘은 <strong>Selection Cache</strong>의 작동 방식을 단계별로 설명합니다.</p><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">PLAINTEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-0 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-0 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-0><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=ln> 1</span><span class=cl>Algorithm: Selection Cache for Decoding 단계
</span></span><span class=line><span class=ln> 2</span><span class=cl>Input: Q (현재 Query), k (선택할 토큰 수), 
</span></span><span class=line><span class=ln> 3</span><span class=cl>       CQ (이전 Query Cache), CI (선택된 인덱스 Cache),
</span></span><span class=line><span class=ln> 4</span><span class=cl>       S (토큰 선택 함수), θ (유사성 임계값)
</span></span><span class=line><span class=ln> 5</span><span class=cl>
</span></span><span class=line><span class=ln> 6</span><span class=cl>1. 초기 설정: 최초 실행 시에는 Cache가 비어 있으므로 모든 Query에 대해 선택 수행
</span></span><span class=line><span class=ln> 7</span><span class=cl>2. Query 유사성 측정: 새로운 Query \( Q \)와 이전 Query \( CQ \)의 유사도를 계산
</span></span><span class=line><span class=ln> 8</span><span class=cl>   - 유사도 &gt; θ (임계값)인 경우:
</span></span><span class=line><span class=ln> 9</span><span class=cl>     - 이전에 선택된 토큰 인덱스 \( CI \)를 사용
</span></span><span class=line><span class=ln>10</span><span class=cl>   - 유사도 ≤ θ인 경우:
</span></span><span class=line><span class=ln>11</span><span class=cl>     - 새로운 Query에 대해 토큰 선택 수행
</span></span><span class=line><span class=ln>12</span><span class=cl>     - 선택된 결과를 \( CI \)에 저장
</span></span><span class=line><span class=ln>13</span><span class=cl>     - 새로운 Query \( Q \)를 \( CQ \)에 저장
</span></span><span class=line><span class=ln>14</span><span class=cl>3. 선택된 인덱스 \( CI \)를 반환</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-0",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script><h4 id=selection-cache의-주요-포인트><strong>Selection Cache의 주요 포인트</strong></h4><ol><li><strong>유사한 Query가 연속적으로 나타나는 경우</strong>:<ul><li>이전 Query와 현재 Query가 유사할 경우, <strong>새로운 토큰 선택을 생략</strong>하고, 이전에 선택된 ( CI )를 재사용.</li></ul></li><li><strong>새로운 Query가 등장하는 경우</strong>:<ul><li>Query 간의 유사도가 낮다면, <strong>기존 Cache를 무시하고</strong> 새로운 선택 작업을 수행하여 ( CI )를 갱신.</li></ul></li><li><strong>효과</strong>:<ul><li>연속된 Query가 유사할수록 <strong>Token Selection 단계에서의 연산량을 크게 줄일 수 있음</strong>.</li></ul></li></ol><hr><h3 id=3-query-유사성-측정-방법>3. Query 유사성 측정 방법</h3><p>Selection Cache를 효과적으로 활용하기 위해서는 <strong>Query 간의 유사성</strong>을 정확하게 평가하는 것이 중요합니다. 이 과정에서 일반적으로 <strong>Cosine Similarity</strong>가 사용됩니다.</p><h4 id=query-유사성-측정-cosine-similarity><strong>Query 유사성 측정: Cosine Similarity</strong></h4><ul><li><strong>Cosine Similarity</strong>는 두 벡터 간의 방향 유사도를 측정하는 방식으로, -1에서 1 사이의 값을 가집니다.<ul><li>값이 <strong>1에 가까울수록 유사도</strong>가 높음을 의미합니다.</li><li>값이 <strong>0에 가까우면</strong> 두 벡터는 독립적임을 나타냅니다.</li></ul></li></ul><h5 id=cosine-similarity-공식><strong>Cosine Similarity 공식</strong></h5><p>[
\text{Cosine Similarity}(Q_1, Q_2) = \frac{Q_1 \cdot Q_2}{|Q_1| \times |Q_2|}
]</p><ul><li>( Q_1 )과 ( Q_2 )는 두 개의 Query 벡터입니다.</li><li>( Q_1 \cdot Q_2 )는 두 벡터의 내적(dot product)입니다.</li><li>( |Q_1| )과 ( |Q_2| )는 각 벡터의 **L2 노름(L2 norm)**입니다.</li></ul><h4 id=query-유사성-예시><strong>Query 유사성 예시</strong></h4><ol><li><p><strong>Query 벡터 예시</strong>:</p><ul><li>( Q_1 = [0.1, 0.3, 0.4, 0.6] )</li><li>( Q_2 = [0.1, 0.3, 0.5, 0.7] )</li></ul></li><li><p><strong>Cosine Similarity 계산</strong>:</p><ul><li>내적 계산: ( Q_1 \cdot Q_2 = (0.1 \times 0.1) + (0.3 \times 0.3) + (0.4 \times 0.5) + (0.6 \times 0.7) = 0.67 )</li><li>벡터 크기 계산:<ul><li>( |Q_1| = \sqrt{0.1^2 + 0.3^2 + 0.4^2 + 0.6^2} = 0.74 )</li><li>( |Q_2| = \sqrt{0.1^2 + 0.3^2 + 0.5^2 + 0.7^2} = 0.88 )</li></ul></li><li>Cosine Similarity:
[
\frac{0.67}{0.74 \times 0.88} \approx 0.87
]</li></ul></li><li><p><strong>유사도 평가</strong>:</p><ul><li>Cosine Similarity 값이 0.87로 **임계값 ( θ = 0.8 )**를 초과하므로, 이전 선택된 토큰을 재사용합니다.</li></ul></li></ol><hr><h3 id=4-selection-cache와-query-유사성-측정의-장점>4. Selection Cache와 Query 유사성 측정의 장점</h3><ul><li><strong>효율성 극대화</strong>: 유사한 Query가 연속해서 발생하는 경우, 이전에 계산된 Token Selection 결과를 재사용하여 <strong>추론 속도를 크게 향상</strong>시킵니다.</li><li><strong>메모리 사용 절약</strong>: 불필요한 dot product 계산을 줄이기 때문에 <strong>메모리 I/O</strong>도 줄어들어 <strong>GPU 메모리 대역폭</strong>을 효율적으로 활용할 수 있습니다.</li><li><strong>실시간 시스템에 적합</strong>: 대화형 시스템이나 실시간 검색 엔진에서 <strong>대기 시간을 줄여</strong> 사용자 경험을 향상시킬 수 있습니다.</li></ul><hr><h3 id=5-한계-및-개선-방안>5. 한계 및 개선 방안</h3><ul><li><strong>임계값 ( θ )</strong> 설정이 중요합니다. 너무 높으면 캐시를 활용하지 못해 효율성이 떨어지고, 너무 낮으면 성능 저하가 발생할 수 있습니다.</li><li><strong>다양한 Query 도메인</strong>에서 유사성 측정의 정확도를 높이기 위해, Cosine Similarity 외에도 <strong>Jaccard Similarity</strong> 또는 <strong>문장 임베딩 기반 유사도(Sentence Embedding Similarity)</strong> 등을 사용할 수 있습니다.</li><li><strong>Query 내용이 급변하는 상황</strong>에서는 캐시가 덜 효과적일 수 있습니다. 이 경우, <strong>Context-aware 캐시 전략</strong>이 필요할 수 있습니다.</li></ul><hr><h3 id=결론-1>결론</h3><p>Selection Cache와 Query 유사성 측정은 TokenSelect의 <strong>효율성을 극대화하는 주요 기법</strong>입니다. 이를 통해 긴 문맥을 처리할 때 <strong>연산 시간과 메모리 사용량을 대폭 줄일 수 있으며</strong>, 특히 <strong>실시간 추론 시스템에서 뛰어난 성능을 발휘</strong>할 수 있습니다.</p><h1 id=q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘>Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1><p>논문 **&ldquo;TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection&rdquo;**에서 제시한 결과와 이를 뒷받침하는 방법론에 대해 자세히 설명하고, 이 방법이 <strong>다른 기존 접근법에 비해 어떻게 더 우수한 성능을 달성했는지</strong>에 대해 논문에서 언급한 이유와 제 생각을 정리해 드리겠습니다.</p><h3 id=1-논문에서-제시한-실험-결과-분석>1. 논문에서 제시한 실험 결과 분석</h3><h4 id=1-성능-및-효율성-비교>(1) <strong>성능 및 효율성 비교</strong></h4><ul><li><strong>TokenSelect</strong>는 기존의 긴 문맥 추론(long-context inference) 기법들에 비해 <strong>23.84배 빠른 attention 계산</strong> 속도와 <strong>2.28배 낮은 end-to-end latency</strong>를 달성했습니다.</li><li>긴 문맥을 필요로 하는 다양한 벤치마크에서 TokenSelect는 <strong>더 높은 정확도</strong>를 기록했습니다.<ul><li>예를 들어, <strong>InfiniteBench</strong>와 <strong>RULER</strong>와 같은 다양한 데이터셋에서 기존 방법론 대비 더 높은 정확도와 효율성을 보였습니다.</li></ul></li><li>특히, <strong>최대 1백만 개의 토큰</strong>을 처리하는 시나리오에서도 기존 방법론에 비해 <strong>메모리 사용량을 크게 절감</strong>하면서도 <strong>성능 저하 없이</strong> 긴 문맥을 처리할 수 있었습니다.</li></ul><h4 id=2-다른-접근법과의-비교>(2) <strong>다른 접근법과의 비교</strong></h4><ul><li><strong>NTK-Aware Scaled RoPE</strong>나 <strong>SelfExtend</strong>와 같은 기존 방법론은 <strong>RoPE interpolation</strong>을 사용하여 긴 문맥 길이를 확장하지만, **추가적인 사후 훈련(post-training)**이 필요합니다.</li><li><strong>StreamingLLM</strong>과 <strong>InfLLM</strong> 같은 기법은 <strong>미리 정의된 희소 패턴</strong>을 활용하거나 <strong>블록 단위 선택</strong>을 통해 효율성을 높이려 했으나, 정보 손실로 인해 정확도 면에서 한계가 있었습니다.</li><li>반면, TokenSelect는 <strong>모델-불가지론적</strong>이면서도 <strong>훈련이 필요 없는 접근법</strong>을 통해 <strong>동적 토큰 선택</strong>을 실현했으며, 이를 통해 정확도를 유지하면서도 효율성을 극대화했습니다.</li></ul><hr><h3 id=2-tokenselect가-특출난-이유와-논문에서-제시한-방법론>2. TokenSelect가 특출난 이유와 논문에서 제시한 방법론</h3><h4 id=1-token-level-dynamic-kv-cache-selection>(1) <strong>Token-Level Dynamic KV Cache Selection</strong></h4><ul><li>기존의 대부분 접근법은 <strong>블록(block) 단위</strong> 또는 <strong>미리 정의된 희소 패턴</strong>을 사용하여 Attention을 가속화했습니다. 이러한 방법들은 <strong>단순히 연속된 블록</strong>을 선택하거나 <strong>히스토리 기반의 중요 토큰</strong>만을 고려하기 때문에 긴 문맥에서 정보 손실이 발생할 수 있었습니다.</li><li>TokenSelect는 <strong>토큰 단위</strong>로 중요한 KV Cache를 선택함으로써, <strong>보다 세밀한 제어</strong>가 가능해졌습니다.<ul><li>이로 인해, 긴 문맥에서도 중요한 정보 손실 없이 <strong>최적의 추론 성능</strong>을 유지할 수 있었습니다.</li></ul></li><li>특히, <strong>Head Soft Vote 메커니즘</strong>을 도입하여 각 Attention Head가 독립적으로 중요한 토큰을 투표하게 함으로써, 특정 Head가 독점적으로 선택 결과를 지배하지 않도록 하여 <strong>정보의 다양성</strong>을 유지했습니다.</li></ul><h4 id=2-selection-cache를-통한-유사-query의-처리-속도-개선>(2) <strong>Selection Cache를 통한 유사 Query의 처리 속도 개선</strong></h4><ul><li><strong>Selection Cache</strong>를 사용해 <strong>연속된 Query 간의 유사성</strong>을 활용했습니다. 이를 통해 <strong>반복적인 계산을 피하고</strong>, 유사한 문맥을 가진 연속된 요청에서는 이전에 선택된 토큰을 재사용하여 추론 속도를 대폭 향상시켰습니다.</li><li>논문에서는 실험적으로 연속된 Query 간의 <strong>유사도가 높을 경우, Selection Cache를 사용해 최대 75% 이상의 토큰 선택 재사용율</strong>을 기록했다고 제시했습니다.</li><li>이를 통해 <strong>실시간 시스템</strong>에서의 응답 속도와 효율성을 극대화할 수 있었습니다.</li></ul><hr><h3 id=3-논문에서-제시한-방법이-이러한-결과를-도출한-이유>3. 논문에서 제시한 방법이 이러한 결과를 도출한 이유</h3><h4 id=논문에서-제기하는-이유>논문에서 제기하는 이유</h4><ol><li><p><strong>비연속적 스파스성(non-contiguous sparsity) 탐지</strong>:</p><ul><li>기존의 블록 단위 선택 방식은 <strong>연속적인 블록이 중요할 것이라는 가정</strong>에 기반합니다. 하지만 실제로는 <strong>중요한 정보가 문맥 내에서 비연속적으로 분포</strong>할 수 있습니다.</li><li>TokenSelect는 토큰 단위의 선택을 통해 <strong>비연속적인 스파스성</strong>을 더 잘 포착할 수 있었습니다.</li></ul></li><li><p><strong>Attention Head 독립성 강화</strong>:</p><ul><li>Attention Head별로 중요도를 따로 평가하고, <strong>soft vote 방식</strong>을 통해 종합함으로써, 각 Head가 독립적으로 중요한 정보를 추출하도록 유도했습니다.</li><li>이 방식은 특정 Head에 의해 중요한 정보가 왜곡되는 것을 방지하여 <strong>정보의 다양성</strong>을 보장합니다.</li></ul></li><li><p><strong>유사한 Query의 재활용</strong>:</p><ul><li>연속된 Query의 <strong>유사성을 적극 활용</strong>하여 선택된 토큰을 재활용함으로써, 긴 문맥에서도 <strong>효율적인 추론 속도</strong>를 달성할 수 있었습니다.</li><li>실험적으로도, 연속된 Query에서 <strong>최대 2.28배 빠른 latency</strong>를 기록한 이유 중 하나입니다.</li></ul></li></ol><h4 id=제-생각>제 생각</h4><ul><li>이 논문은 긴 문맥을 다루는 데 있어 <strong>정확도와 효율성 간의 균형</strong>을 잘 맞춘 접근법이라고 생각합니다.</li><li>특히, <strong>토큰 레벨의 세밀한 선택</strong>과 <strong>유사 Query 재활용</strong>이라는 두 가지 혁신적인 아이디어를 결합함으로써, 긴 문맥 추론의 효율성을 극대화했습니다.</li><li>하지만 이 접근법이 <strong>연속된 Query 간의 유사성</strong>에 의존하기 때문에, <strong>문맥이 급격히 변화하는 시나리오</strong>에서는 성능이 다소 저하될 수 있을 것으로 보입니다. 이러한 상황에서는 <strong>Selection Cache가 오히려 오버헤드</strong>가 될 수 있기 때문에 추가적인 개선이 필요할 것으로 생각됩니다.</li></ul><hr><h3 id=4-결론-및-개선-가능성>4. 결론 및 개선 가능성</h3><p><strong>TokenSelect</strong>는 현재 긴 문맥을 효율적으로 처리할 수 있는 방법론 중 하나로, 특히 <strong>대화형 AI 시스템이나 실시간 추론</strong>에 적합합니다. 다만, 다양한 시나리오에서의 <strong>범용성</strong>을 높이기 위해서는 <strong>다이내믹한 Selection Cache 조정 전략</strong>이나, <strong>문맥 변화에 대한 민감도를 조절할 수 있는 기법</strong>이 추가적으로 연구될 필요가 있습니다.</p><p>이 논문이 제시한 접근법은 향후 <strong>LLM 기반의 응용 프로그램</strong>에서 중요한 역할을 할 것으로 기대됩니다.</p><h1 id=q--이-논문에서-제시된-방법을-실제로-사용하려면-기존-방법에서-어떠한-구현이-추가적으로-필요하고-이러한-구현에-소모되는-공수-및-연산에-필요한-컴퓨팅-자원의-크기에-대해서-계산해줄-수-있겠니>Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니?</h1><p><strong>논문에서 제시된 TokenSelect를 실제 시스템에 적용하기 위해 필요한 구현 요소</strong>, 그리고 이 구현에 소모되는 <strong>공수 및 컴퓨팅 자원 요구 사항</strong>에 대해 자세히 설명하겠습니다. 기존 시스템에서 TokenSelect를 도입하려면 여러 가지 변경 사항이 필요하며, 이 과정에서 발생하는 연산 오버헤드와 자원 소모를 분석해 보겠습니다.</p><hr><h3 id=1-기존-시스템에-tokenselect를-적용하기-위해-필요한-구현-요소>1. 기존 시스템에 TokenSelect를 적용하기 위해 필요한 구현 요소</h3><p>TokenSelect는 기존 Transformer 기반 LLM 시스템에 <strong>다음과 같은 주요 기능 추가</strong>를 요구합니다.</p><h4 id=1-동적-토큰-레벨-선택-알고리즘-추가>(1) <strong>동적 토큰 레벨 선택 알고리즘 추가</strong></h4><ul><li>기존의 Attention 메커니즘에서는 **전체 Query-Key-Value(QKV)**를 사용해 Attention을 계산했습니다.</li><li>TokenSelect는 <strong>토큰 단위로 중요한 KV 쌍을 선택</strong>하는 기능이 필요하므로, 각 Query에 대해 KV Cache의 중요도를 평가하고 <strong>상위 ( k )개의 토큰만 선택</strong>하는 로직을 추가해야 합니다.<ul><li>이를 위해 <strong>Query-Key dot product 계산 및 softmax 정규화</strong>를 포함한 <strong>Head Soft Vote 메커니즘</strong>이 구현되어야 합니다.</li></ul></li></ul><h4 id=2-selection-cache-기능-추가>(2) <strong>Selection Cache 기능 추가</strong></h4><ul><li>연속된 Query가 유사할 경우, 이전에 선택된 <strong>토큰 인덱스를 재사용</strong>할 수 있도록 Selection Cache를 구현해야 합니다.</li><li>이를 위해 **Query 유사도 측정 알고리즘(Cosine Similarity)**과 <strong>Cache 관리 로직</strong>이 추가적으로 필요합니다.</li></ul><h4 id=3-효율적인-메모리-관리-및-커널-최적화>(3) <strong>효율적인 메모리 관리 및 커널 최적화</strong></h4><ul><li>TokenSelect는 <strong>Paged Attention</strong>과 <strong>Paged Dot Product Kernel</strong>을 사용하여 메모리 I/O 병목을 줄입니다.</li><li>GPU에서 <strong>효율적인 메모리 접근</strong>을 위해 <strong>Triton</strong>과 같은 라이브러리를 사용해 <strong>최적화된 커널</strong>을 구현할 필요가 있습니다.</li></ul><hr><h3 id=2-구현에-필요한-공수-및-소요-시간-추정>2. 구현에 필요한 공수 및 소요 시간 추정</h3><h4 id=개발-공수-추정><strong>개발 공수 추정</strong></h4><ul><li><strong>동적 토큰 선택 알고리즘 구현</strong> (Head Soft Vote 메커니즘 포함): 약 <strong>2~3주</strong><ul><li>Query-Key dot product 계산 및 토큰 선택 알고리즘 구현.</li><li>Head Soft Vote 메커니즘을 추가하여 Head별 독립적인 선택 수행.</li></ul></li><li><strong>Selection Cache 구현</strong> (Query 유사도 계산 및 캐시 관리): 약 <strong>1~2주</strong><ul><li>Query 유사도를 계산하는 함수(예: Cosine Similarity)와 Cache를 관리하는 로직 구현.</li><li>Selection Cache와 기존 Attention 메커니즘의 통합 테스트.</li></ul></li><li><strong>커널 최적화 및 메모리 관리 개선</strong>: 약 <strong>2~4주</strong><ul><li>Triton 또는 CUDA를 사용해 Paged Dot Product Kernel을 최적화.</li><li>GPU 메모리 사용량 최적화 및 I/O 병목 문제 해결.</li></ul></li></ul><h4 id=총-개발-공수-추정-약-6><strong>총 개발 공수 추정</strong>: 약 <strong>6~9주</strong></h4><ul><li>기존 LLM 시스템에 대한 <strong>심층적인 이해</strong>가 필요하므로, 프로젝트 팀에 <strong>경험이 있는 엔지니어 2~3명</strong>이 투입된다고 가정.</li></ul><hr><h3 id=3-컴퓨팅-자원의-요구-사항-분석>3. 컴퓨팅 자원의 요구 사항 분석</h3><p>TokenSelect를 기존 시스템에 적용하면 추가적인 연산이 발생합니다. 이를 위해 필요한 <strong>컴퓨팅 자원</strong>을 분석해 보겠습니다.</p><h4 id=1-추가-연산-및-메모리-요구-사항>(1) <strong>추가 연산 및 메모리 요구 사항</strong></h4><ul><li>기존 Attention 메커니즘 대비 TokenSelect는 <strong>토큰 선택 단계</strong>에서 추가적인 연산이 발생합니다.<ul><li>각 Query에 대해 <strong>Q-K dot product 계산</strong> 및 <strong>softmax 정규화</strong> 수행.</li><li>선택된 토큰만을 사용한 <strong>선택적 Sparse Attention 계산</strong>.</li></ul></li><li><strong>Selection Cache를 위한 추가 메모리</strong>:<ul><li>Cache는 이전 Query와 선택된 토큰 인덱스를 저장해야 하므로, 각 Query에 대해 추가 메모리 공간이 필요합니다.</li></ul></li></ul><h4 id=2-연산량-증가-분석>(2) <strong>연산량 증가 분석</strong></h4><ul><li>기존 방식에서 <strong>Attention 연산량</strong>: ( O(n^2) ) (예: ( n = 32,768 )일 때 약 10억 회 연산)</li><li>TokenSelect 적용 시:<ul><li>토큰 선택 단계: ( O(n \times k) ) (예: ( n = 32,768 ), ( k = 512 )일 때 약 1,677만 회 연산)</li><li>선택된 토큰에 대해서만 Attention 수행: ( O(k^2) )</li></ul></li><li>추가적인 dot product와 softmax 계산으로 인해 <strong>전체 연산량은 약 10~15% 정도 증가</strong>할 수 있습니다.</li></ul><h4 id=3-추가-메모리-요구-사항>(3) <strong>추가 메모리 요구 사항</strong></h4><ul><li><strong>Selection Cache 메모리</strong>:<ul><li>예를 들어, 1M(1,000,000) 토큰의 KV Cache를 다룰 때, Selection Cache에 <strong>512개의 토큰 인덱스와 Query 벡터</strong>를 저장하는 데 약 <strong>10~20MB</strong>의 추가 메모리 필요.</li></ul></li><li><strong>GPU 메모리 요구 사항</strong>:<ul><li>기존 메모리 사용량: 약 <strong>8GB (32K 토큰 기준)</strong></li><li>TokenSelect 적용 시: <strong>약 10~15% 추가 메모리</strong> 사용 예상.</li></ul></li></ul><hr><h3 id=4-예상되는-성능-향상과-자원-소모-분석>4. 예상되는 성능 향상과 자원 소모 분석</h3><p><strong>TokenSelect 적용 후 기대되는 성능 향상</strong>:</p><ul><li><strong>추론 속도</strong>: 긴 문맥에서 기존 대비 <strong>최대 2.28배의 latency 감소</strong>.</li><li><strong>메모리 절약</strong>: Paged Attention을 활용하여 <strong>메모리 I/O 대역폭 소모 감소</strong>.</li><li><strong>유사한 Query에 대한 Cache 재사용</strong>으로 <strong>최대 75%의 선택된 토큰 재사용율</strong>을 통해 추가적인 연산 감소.</li></ul><h4 id=컴퓨팅-자원-비용-분석><strong>컴퓨팅 자원 비용 분석</strong></h4><ul><li><strong>GPU 비용 추정</strong>:<ul><li>NVIDIA A100 GPU (40GB VRAM)를 기준으로, <strong>1~2개의 GPU가 필요한 수준</strong>.</li><li>연산량 증가를 감안할 때, <strong>추가 GPU 사용 비용은 약 10~20% 증가</strong>.</li></ul></li><li><strong>운영 비용 증가</strong>:<ul><li>클라우드 환경에서 A100 GPU 사용 시 시간당 약 <strong>$2~$3</strong>의 추가 비용 발생 가능.</li><li>시스템 통합 및 테스트에 따른 초기 비용 외에는 운영 중 추가적인 자원 소모는 미미할 것으로 예상.</li></ul></li></ul><hr><h3 id=5-결론>5. 결론</h3><p><strong>TokenSelect</strong>를 기존 시스템에 도입하면 <strong>추가적인 구현과 연산 오버헤드가 발생</strong>하지만, 긴 문맥을 다루는 상황에서 <strong>추론 속도 및 메모리 효율성</strong>을 크게 향상시킬 수 있습니다.</p><ul><li><strong>추가적인 개발 공수</strong>는 약 6<del>9주가 소요될 수 있으며, 2</del>3명의 엔지니어가 필요합니다.</li><li><strong>GPU 자원</strong>이 기존 대비 10~20% 더 필요할 수 있지만, <strong>긴 문맥에서의 성능 향상</strong>으로 인해 <strong>실제 운영 비용은 절감</strong>될 수 있습니다.</li></ul><p>결론적으로, <strong>장기적인 관점에서 TokenSelect는 높은 ROI(Return on Investment)를 제공할 수 있는 최적화 기술</strong>로 판단됩니다. 특히 <strong>실시간 대화 시스템</strong>이나 <strong>긴 문맥 분석이 중요한 웹 서비스</strong>에서는 도입을 고려할 가치가 높습니다.</p><h1 id=q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘>Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1><p>논문 **&ldquo;TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection&rdquo;**의 <strong>입력 데이터, 추론 과정, 모델 아키텍처</strong> 및 <strong>컴퓨팅 요구 사항</strong>에 대해 아주 자세하게 설명하겠습니다. 이를 위해 <strong>구체적인 예시</strong>를 통해 이해를 돕고, 모델을 구성하는 연산 및 메모리 요구 사항까지 분석하겠습니다.</p><hr><h3 id=1-입력-데이터와-추론-과정-예시>1. 입력 데이터와 추론 과정 예시</h3><h4 id=1-입력-데이터-예시>(1) <strong>입력 데이터 예시</strong></h4><p>우리가 다루는 시나리오를 예를 들어 설명하겠습니다:</p><ul><li><p><strong>예시 상황</strong>: 긴 텍스트 분석을 수행하는 대화형 AI 시스템</p></li><li><p><strong>입력 데이터</strong>:</p><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">PLAINTEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-1 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-1 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-1><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=ln>1</span><span class=cl>사용자가 입력한 긴 대화 문맥:
</span></span><span class=line><span class=ln>2</span><span class=cl>&#34;안녕하세요. 오늘 날씨는 어떤가요? 그리고 다음 주에 여행을 갈 계획인데, 추천할만한 여행지가 있을까요? 특히 자연 경관이 아름다운 곳을 찾고 있어요.&#34;</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-1",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script></li><li><p><strong>모델의 최대 처리 길이</strong>: 32,768 토큰 (32K)</p><ul><li>긴 문맥을 다룰 수 있는 모델로, 사용자 입력이 길어질수록 이전 대화 내역을 반영할 수 있습니다.</li></ul></li></ul><h4 id=2-tokenselect를-이용한-추론-과정>(2) <strong>TokenSelect를 이용한 추론 과정</strong></h4><ol><li><p><strong>Prefill 단계</strong> (초기 입력 처리):</p><ul><li>사용자의 입력을 모델에 전달하여 <strong>처음부터 끝까지 한 번에 처리</strong>합니다.</li><li>이 단계에서는 <strong>전체 입력 시퀀스에 대해 Query-Key-Value (QKV) Cache</strong>를 생성합니다.</li><li>각 레이어에서 <strong>Query ( Q_{\text{prefill}} )</strong>, <strong>Key ( K_{\text{prefill}} )</strong>, <strong>Value ( V_{\text{prefill}} )</strong> 행렬을 계산하여 KV Cache에 저장합니다.</li></ul><p>예를 들어, 입력된 문장이 총 <strong>5,000개의 토큰</strong>으로 구성되어 있다면, 모델은 이 문장 전체를 한 번에 처리하여 KV Cache를 생성합니다.</p></li><li><p><strong>Decode 단계</strong> (추론 및 답변 생성):</p><ul><li>이제 모델은 <strong>자동 완성 방식</strong>으로 한 번에 하나씩 새로운 토큰을 생성합니다.</li><li>각 토큰을 생성할 때마다, 현재 Query와 저장된 KV Cache를 사용하여 새로운 토큰을 예측합니다.</li><li>TokenSelect를 통해 <strong>모든 KV Cache를 다시 계산하지 않고</strong>, <strong>가장 중요한 토큰만 선택</strong>하여 효율적으로 Attention을 수행합니다.</li></ul><p>예시: 사용자가 추가 질문을 입력한 경우</p><div class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md"><div class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3"><div class="flex items-center gap-2"><div class="text-muted-foreground flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4 4 4-4 4M6 16l-4-4 4-4"/></svg></div><span class="text-muted-foreground text-sm font-medium">PLAINTEXT</span></div><div class="flex items-center gap-2"><button class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-2 data-default-state=expanded data-collapsed=false data-auto-collapse-lines=30 data-auto-collapse-height=400 data-collapsed-height=120 title=접기 aria-label=접기>
<span class=collapse-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41 12 10.83l4.59 4.58L18 14l-6-6-6 6z"/></svg>
</span><span class="collapse-text hidden sm:inline">접기</span>
</button>
<button class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none" data-code-id=code-2 title=복사 aria-label=복사>
<span class=copy-icon><svg class="h-3 w-3" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</span><span class="copy-text hidden sm:inline">복사</span></button></div></div><div class="code-block-content relative" id=code-2><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=ln>1</span><span class=cl>&#34;그리고 요즘 인기 있는 여행지는 어디인가요?&#34;</span></span></code></pre></div><div class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300"><div class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">클릭하여 더 보기</div></div></div></div><script>(function(){const s="code-2",n=document.querySelector('.copy-code-btn[data-code-id="'+s+'"]'),t=document.querySelector('.collapse-code-btn[data-code-id="'+s+'"]'),e=document.getElementById(s);if(!e)return;if(n){const s=n.querySelector(".copy-icon"),t=n.querySelector(".copy-text");n.addEventListener("click",async function(){try{let o="";const i=e.querySelector(".lntd:last-child code");if(i)o=i.textContent||i.innerText;else{const t=e.querySelector("code");if(t){const e=t.querySelector(".ln");if(e){const e=t.querySelectorAll(".cl");if(e.length>0)o=Array.from(e).map(e=>{const t=e.textContent||e.innerText;return t.replace(/\n+$/,"")}).join(`
`).replace(/\n+$/,"");else{const e=t.textContent||t.innerText;o=e.replace(/^\d+/gm,"").replace(/^\s+/gm,"")}}else o=t.textContent||t.innerText}else o=e.textContent||e.innerText}o=o.trim(),await navigator.clipboard.writeText(o),s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
</svg>`,t&&(t.textContent="복사됨"),n.classList.add("text-green-600"),setTimeout(()=>{s.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
</svg>`,t&&(t.textContent="복사"),n.classList.remove("text-green-600")},2e3)}catch(o){console.error("复制失败:",o);const s=document.createRange(),i=e.querySelector("code")||e;s.selectNodeContents(i);const n=window.getSelection();n.removeAllRanges(),n.addRange(s),t&&(t.textContent="선택됨"),setTimeout(()=>{t&&(t.textContent="복사"),n.removeAllRanges()},2e3)}})}if(t){const d=t.querySelector(".collapse-icon"),c=t.querySelector(".collapse-text"),n=e.querySelector(".collapse-overlay");let s=e.querySelector("pre.chroma");s||(s=e.querySelector("pre"));const m=t.dataset.defaultState||"expanded",f=t.dataset.collapsed==="true",u=parseInt(t.dataset.autoCollapseLines)||30,h=parseInt(t.dataset.autoCollapseHeight)||400,p=parseInt(t.dataset.collapsedHeight)||120;let l=!1;function i(){const e=f||m==="collapsed"||a();e&&o(!0,!1)}function a(){if(s){const e=s.querySelectorAll(".line, .cl"),t=s.offsetHeight;return e.length>u||t>h}const t=e.offsetHeight;if(t>h)return!0;const n=e.textContent||e.innerText||"",o=n.split(`
`).length;return o>u}function o(s,o=!0){if(!n)return;l=s,s?(e.style.maxHeight=p+"px",e.style.overflow="hidden",n.style.opacity="1",n.style.pointerEvents="auto",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7" />
</svg>`,c&&(c.textContent="펼치기"),t.title="펼치기"):(e.style.maxHeight="",e.style.overflow="",n.style.opacity="0",n.style.pointerEvents="none",d.innerHTML=`
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/></svg>`,c&&(c.textContent="접기"),t.title="접기"),o&&(e.style.transition="max-height 0.3s ease-out",setTimeout(()=>{e.style.transition=""},300))}function r(){o(!l,!0)}t.addEventListener("click",r),n&&n.addEventListener("click",()=>{l&&o(!1,!0)}),i()}})()</script><ul><li>이 추가 입력이 들어오면, <strong>이전 대화 내역을 참고</strong>하여 새로운 답변을 생성하기 위해 TokenSelect를 활용합니다.</li><li>이전의 <strong>5,000개의 토큰과 새로 추가된 입력</strong>을 고려하여 필요한 정보만 선택해 추론을 수행합니다.</li></ul></li></ol><h4 id=3-tokenselect를-사용한-효율적인-attention>(3) <strong>TokenSelect를 사용한 효율적인 Attention</strong></h4><ul><li>TokenSelect는 각 Query가 추가될 때마다 <strong>전체 문맥을 다시 계산하지 않고</strong>, 이전에 저장된 <strong>중요한 ( k )개의 KV 토큰만</strong> 선택하여 Attention을 수행합니다.</li><li>예를 들어, 새로운 토큰을 생성할 때마다 이전 32,768개의 토큰 전체를 다시 비교하는 대신, <strong>상위 512개의 중요한 토큰만</strong> 선택하여 연산을 줄입니다.</li></ul><hr><h3 id=2-모델-아키텍처-구성-및-주요-연산>2. 모델 아키텍처 구성 및 주요 연산</h3><h4 id=1-모델-아키텍처-구성>(1) <strong>모델 아키텍처 구성</strong></h4><ul><li><strong>Transformer 기반 아키텍처</strong>를 사용합니다.<ul><li><strong>Decoder-only 구조</strong>로, 주로 대화형 AI 모델에 사용되는 형태입니다.</li></ul></li><li><strong>아키텍처 구성 요소</strong>:<ul><li><strong>Word Embedding Layer</strong>: 입력된 텍스트를 임베딩 벡터로 변환합니다.</li><li><strong>Multi-Head Attention (MHA) 레이어</strong>: Query-Key-Value 연산을 통해 문맥 정보를 추출합니다.</li><li><strong>Feed-Forward Networks (FFN)</strong>: 각 Attention 레이어의 출력을 후처리하여 다음 레이어로 전달합니다.</li><li><strong>Layer Normalization</strong>: 안정적인 학습과 추론을 위해 각 레이어 출력의 분포를 정규화합니다.</li></ul></li><li><strong>토큰 선택을 위한 TokenSelect 모듈</strong>:<ul><li><strong>Head Soft Vote 메커니즘</strong>과 <strong>Selection Cache</strong>가 포함되어 있습니다.</li></ul></li></ul><h4 id=2-모델-구성-연산-분석>(2) <strong>모델 구성 연산 분석</strong></h4><ul><li><p><strong>Attention 연산</strong>:</p><ul><li>기존 Transformer에서 <strong>Query-Key dot product</strong>를 통해 모든 Key 토큰과 비교하여 Attention Score를 계산합니다.</li><li>TokenSelect에서는 <strong>상위 ( k )개의 중요한 토큰만 선택</strong>하여 Attention 계산을 수행합니다.</li><li>연산 복잡도:<ul><li>기존 방식: ( O(n^2) ) (예: 32K 토큰 → 약 10억 회 연산)</li><li>TokenSelect 적용 시: ( O(n \times k) ) (예: ( k = 512 ), ( n = 32,768 ) → 약 1,677만 회 연산)</li></ul></li></ul></li><li><p><strong>Selection Cache</strong>:</p><ul><li>연속된 Query의 유사성을 활용해 <strong>토큰 선택 결과를 재사용</strong>함으로써 추가적인 연산을 줄입니다.</li></ul></li></ul><hr><h3 id=3-메모리-및-컴퓨팅-요구량-분석>3. 메모리 및 컴퓨팅 요구량 분석</h3><h4 id=1-메모리-요구-사항>(1) <strong>메모리 요구 사항</strong></h4><ul><li><p><strong>기존 Attention 메커니즘</strong>:</p><ul><li><strong>Q, K, V 행렬</strong> 각각의 크기: ( n \times d )</li><li>예를 들어, ( n = 32,768 ), ( d = 256 )일 때, 각 행렬의 크기는 약 <strong>8MB</strong>.</li><li><strong>Multi-Head Attention</strong>에서 12개 헤드를 사용하면, 각 레이어에서 약 <strong>96MB</strong>의 메모리 필요.</li><li>Transformer가 <strong>24개의 레이어</strong>로 구성된 경우, 총 메모리 사용량은 약 <strong>2.3GB</strong>.</li></ul></li><li><p><strong>TokenSelect 적용 시</strong>:</p><ul><li>선택된 토큰만을 사용하므로, 메모리 사용량이 크게 감소합니다.</li><li>예를 들어, ( k = 512 )만 사용하면, 각 레이어에서 약 <strong>1.5MB</strong>로 줄어듦.</li><li>전체 모델에서 메모리 사용량이 <strong>최대 10~15% 절약</strong> 가능.</li></ul></li></ul><h4 id=2-컴퓨팅-자원-요구-사항>(2) <strong>컴퓨팅 자원 요구 사항</strong></h4><ul><li><p><strong>추론 시 필요한 연산량</strong>:</p><ul><li>기존 Transformer: ( O(n^2) )의 연산 복잡도.</li><li>TokenSelect 적용 시: ( O(n \times k) + O(k^2) ).</li><li>기존 방식 대비 약 <strong>64배 연산량 절감</strong>.</li></ul></li><li><p><strong>GPU 메모리 및 연산 성능 요구 사항</strong>:</p><ul><li>NVIDIA A100 GPU(40GB VRAM)를 기준으로, 기존 방식에서는 <strong>약 80~90%의 메모리 사용률</strong>을 기록할 수 있습니다.</li><li>TokenSelect를 활용하면 메모리 사용량을 줄여, <strong>동일한 GPU에서 더 많은 입력 데이터를 병렬 처리</strong>할 수 있습니다.</li><li><strong>추론 속도</strong>가 개선되어, 클라우드 환경에서 <strong>비용 절감</strong> 효과가 있습니다.</li></ul></li></ul><hr><h3 id=4-요약-및-결론>4. 요약 및 결론</h3><ul><li><strong>TokenSelect</strong>는 긴 문맥을 다루는 추론 시 <strong>연산 효율성</strong>을 크게 높이는 접근법입니다.</li><li><strong>토큰 레벨의 동적 선택</strong>과 <strong>유사 Query 재활용</strong>을 통해 <strong>연산량과 메모리 사용량을 최적화</strong>할 수 있습니다.</li><li>이를 실제 시스템에 도입하면 <strong>추론 속도와 자원 효율성</strong>이 크게 향상되어, 특히 <strong>실시간 대화형 시스템이나 대규모 텍스트 분석 시스템</strong>에서 큰 이점을 얻을 수 있습니다.</li></ul><p>결론적으로, TokenSelect를 기존 Transformer 시스템에 통합하면 <strong>긴 문맥 처리 시 성능을 극대화하면서도 연산 자원 소모를 줄일 수 있는 효과적인 방법</strong>입니다. 이를 통해 <strong>실시간 응답 속도가 중요한 서비스에서 더 나은 사용자 경험을 제공할 수 있습니다</strong>.</p><h1 id=q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘>Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1><p>**논문 &ldquo;TokenSelect&rdquo;**에서 제시된 접근법은 긴 문맥을 다루는 대규모 언어 모델(LLMs)의 <strong>효율성을 크게 개선</strong>했지만, 여전히 몇 가지 한계가 존재합니다. 이러한 한계를 극복하기 위해 현재의 연구 흐름과 잠재적인 개선 방안에 대해 설명하겠습니다. 특히, <strong>기존 문제를 해결하기 위한 최신 연구 동향과 방향</strong>을 바탕으로 논의하겠습니다.</p><hr><h3 id=1-tokenselect의-한계-요약>1. TokenSelect의 한계 요약</h3><h4 id=1-cache-miss-문제>(1) <strong>Cache Miss 문제</strong></h4><ul><li><strong>Prefill 단계에서 생성된 KV Cache</strong>가 <strong>Decoding 단계에서 유효하지 않게 되는 경우</strong>(즉, Cache Miss) 성능 저하가 발생할 수 있습니다.</li><li>특히, <strong>문맥이 급변하거나 새로운 주제가 도입될 때</strong> Cache Miss가 빈번하게 발생하여, <strong>TokenSelect의 효율성이 저하</strong>될 수 있습니다.</li></ul><h4 id=2-동적-토큰-선택의-추가-연산-오버헤드>(2) <strong>동적 토큰 선택의 추가 연산 오버헤드</strong></h4><ul><li><strong>동적으로 중요한 토큰을 선택하는 과정에서 추가적인 dot product 및 softmax 연산</strong>이 필요합니다.</li><li>특히 짧은 문맥에서는 이러한 추가 연산이 오히려 성능을 저하시킬 수 있습니다.</li></ul><h4 id=3-selection-cache의-유사도-측정-한계>(3) <strong>Selection Cache의 유사도 측정 한계</strong></h4><ul><li>현재 Selection Cache는 주로 <strong>Cosine Similarity</strong>를 사용하여 Query 간의 유사성을 측정합니다.</li><li>그러나 단순한 유사도 측정 방식은 <strong>문장의 의미 변화</strong>를 감지하는 데 한계가 있을 수 있습니다.</li></ul><hr><h3 id=2-한계를-극복하기-위한-연구-흐름>2. 한계를 극복하기 위한 연구 흐름</h3><h4 id=1-문맥-변화-감지를-위한-context-aware-attention-기법>(1) <strong>문맥 변화 감지를 위한 Context-aware Attention 기법</strong></h4><ul><li>기존의 TokenSelect는 <strong>토큰 선택</strong>에 중점을 두고 있지만, <strong>문맥 변화 감지</strong>에는 한계가 있습니다.</li><li>이를 해결하기 위해, <strong>문맥 변화 감지</strong>를 위한 <strong>Context-aware Attention</strong> 기법이 주목받고 있습니다.<ul><li>예를 들어, <strong>Adaptive Context Attention (ACA)</strong> 기법을 활용해 <strong>문맥이 급변하는 경우 기존 Cache를 무효화</strong>하거나 <strong>새로운 KV Cache를 동적으로 생성</strong>하는 방식이 연구되고 있습니다.</li><li>이를 통해, <strong>문맥 변화에 유연하게 대응</strong>하면서도 효율성을 유지할 수 있습니다.</li></ul></li></ul><h4 id=2-하이브리드-토큰-및-블록-선택-방식>(2) <strong>하이브리드 토큰 및 블록 선택 방식</strong></h4><ul><li>TokenSelect는 <strong>토큰 단위로 중요한 정보를 선택</strong>하지만, <strong>블록(block) 단위</strong>의 선택 방식이 더 효율적일 수 있는 경우가 있습니다.</li><li>최근 연구에서는 <strong>하이브리드 접근법</strong>이 주목받고 있습니다.<ul><li>문맥이 <strong>연속적이고 유사할 경우</strong>에는 토큰 단위 선택을 사용하고, <strong>문맥이 급변하거나 새로운 주제가 도입될 경우</strong>에는 블록 단위 선택으로 전환하는 방식입니다.</li><li>이러한 접근법은 <strong>Sparse Transformer</strong>와 <strong>LongFormer</strong>와 같은 모델에서 활용되는 <strong>로컬 및 글로벌 Attention</strong> 방식을 혼합하는 형태로 발전하고 있습니다.</li></ul></li></ul><h4 id=3-문장-의미-기반의-유사도-측정-semantic-similarity>(3) <strong>문장 의미 기반의 유사도 측정 (Semantic Similarity)</strong></h4><ul><li>현재의 Selection Cache는 단순한 <strong>Cosine Similarity</strong>를 기반으로 Query 유사성을 판단합니다. 하지만, 이 방식은 <strong>문장 간의 의미적 변화</strong>를 포착하는 데 한계가 있습니다.</li><li>이를 극복하기 위해, <strong>Sentence Transformers</strong>나 <strong>BERT 기반의 Sentence Embedding</strong>을 활용하여 <strong>문장 의미를 더 깊이 이해</strong>할 수 있는 방식이 연구되고 있습니다.<ul><li>예를 들어, <strong>SBERT (Sentence-BERT)</strong> 모델을 사용하여 <strong>문장 간의 의미적 유사도</strong>를 평가함으로써, 보다 정확한 Cache 재사용이 가능합니다.</li><li>이를 통해 <strong>Cache Miss를 줄이고</strong>, 새로운 주제가 도입되더라도 <strong>보다 빠르게 적응</strong>할 수 있습니다.</li></ul></li></ul><h4 id=4-meta-learning-기반의-캐시-갱신-전략>(4) <strong>Meta-Learning 기반의 캐시 갱신 전략</strong></h4><ul><li><strong>Meta-Learning</strong>을 통해 모델이 <strong>Cache를 동적으로 조정</strong>하도록 학습시키는 접근법이 떠오르고 있습니다.<ul><li>예를 들어, Meta-Learning 알고리즘을 사용하여 모델이 <strong>문맥 변화에 따라 Cache를 자동으로 무효화</strong>하거나 <strong>새롭게 갱신</strong>할 수 있도록 합니다.</li><li>이 방식은 <strong>Few-shot Learning</strong>과 유사하게 <strong>이전 문맥에서 학습된 정보를 활용하여 새로운 상황에 빠르게 적응</strong>할 수 있게 합니다.</li><li>Meta-Learning 기반 접근은 특히 **적응형 추론(adaptive inference)**이 중요한 실시간 시스템에서 유용할 수 있습니다.</li></ul></li></ul><h4 id=5-sparse-attention의-개선>(5) <strong>Sparse Attention의 개선</strong></h4><ul><li>TokenSelect는 <strong>Paged Attention Kernel</strong>을 사용하여 <strong>Sparse Attention</strong>을 구현하고 있습니다.</li><li>그러나, 더 효율적인 Sparse Attention을 위해, <strong>Low-Rank Approximation</strong> 기법이 추가적으로 연구되고 있습니다.<ul><li>예를 들어, <strong>Linformer</strong>나 <strong>Performer</strong>와 같은 접근법을 통해 <strong>Attention 연산의 복잡도를 선형적으로 줄이는 연구</strong>가 진행 중입니다.</li><li>이를 TokenSelect에 적용하면, 긴 문맥에서도 <strong>더 낮은 연산량으로 효율적인 추론</strong>이 가능할 수 있습니다.</li></ul></li></ul><hr><h3 id=3-향후-연구-방향과-적용-가능성>3. 향후 연구 방향과 적용 가능성</h3><h4 id=1-context-aware-cache-management>(1) <strong>Context-aware Cache Management</strong></h4><ul><li><strong>문맥 변화에 적응하는 캐시 관리 시스템</strong>을 구축하여, TokenSelect의 효율성을 더욱 높일 수 있습니다.</li><li>예를 들어, <strong>Query 유사도 분석 시 Threshold를 동적으로 조정</strong>하여 Cache Miss를 줄이는 방안이 연구될 수 있습니다.</li></ul><h4 id=2-cross-attention-기반의-선택적-kv-갱신>(2) <strong>Cross-attention 기반의 선택적 KV 갱신</strong></h4><ul><li><strong>Cross-attention 메커니즘</strong>을 활용하여 <strong>다른 시퀀스 간의 상관 관계</strong>를 분석하고, 이를 바탕으로 <strong>중요한 토큰을 선택</strong>할 수 있습니다.</li><li>이를 통해, 긴 문맥에서도 <strong>중복된 정보 대신 새로운 정보를 선택</strong>하도록 개선할 수 있습니다.</li></ul><h4 id=3-추론-단계의-reinforcement-learning-적용>(3) <strong>추론 단계의 Reinforcement Learning 적용</strong></h4><ul><li>**강화 학습(Reinforcement Learning, RL)**을 활용하여, 모델이 <strong>Cache Miss를 최소화하는 방향으로 학습</strong>할 수 있습니다.</li><li>RL 기반의 접근법은 모델이 <strong>실시간으로 Cache Hit/Miss를 평가</strong>하고, 이에 따라 <strong>최적의 행동을 선택하도록 학습</strong>시킬 수 있습니다.</li></ul><hr><h3 id=4-결론>4. 결론</h3><ul><li><strong>TokenSelect</strong>는 긴 문맥을 다루는 대규모 언어 모델에서 효율성을 개선하는 혁신적인 접근법이지만, 여전히 해결해야 할 한계가 존재합니다.</li><li>이를 극복하기 위해, <strong>Context-aware Attention, Hybrid Token Selection, Meta-Learning</strong> 등 다양한 연구 흐름이 발전하고 있습니다.</li><li>특히, <strong>문맥 변화에 적응하고 Cache Miss를 줄이기 위한 새로운 전략들</strong>은 TokenSelect의 실질적인 성능을 한층 더 향상시킬 수 있을 것입니다.</li><li>앞으로 이러한 연구들이 결합되어, <strong>긴 문맥을 다루는 AI 모델이 더 높은 정확도와 효율성</strong>을 달성할 수 있을 것으로 기대됩니다.</li></ul></article></main><footer class="mx-auto max-w-4xl px-4 py-8"><div class="px-6 py-6"><nav class=mb-6><div class="flex flex-wrap items-center justify-center gap-1"><a href=/index.xml class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="RSS Feed"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18.0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z"/></svg>
<span>RSS Feed</span></a></div></nav><div class="border-border mb-6 border-t"></div><div class="flex flex-col items-center justify-between gap-4 md:flex-row"><div class="text-muted-foreground text-sm"><p>&copy; 2025 Jaehun's Blog.
모든 권리 보유.</p></div><div class="flex items-center gap-3"><a href=https://github.com/ryujaehun target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=GitHub aria-label=GitHub><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8.0 00-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35.0-3.5.0.0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35.0 3.5A5.4 5.4.0 004 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
</a><a href=https://www.linkedin.com/in/jaehunryu/ target=_blank rel="noopener noreferrer" class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Linkedin aria-label=Linkedin><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
</a><a href=mailto:jaehunryu@icloud.com class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none" title=Email aria-label=Email><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" aria-label="Email"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22.0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5A2 2 0 003 7v10a2 2 0 002 2z"/></svg></a></div></div></div></footer><div id=dock class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0" role=toolbar aria-label="바로가기 도구 모음"><nav class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3"><button id=dock-back class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=뒤로 aria-label=뒤로>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m12 19-7-7 7-7"/><path d="M19 12H5"/></svg>
<span class="hidden sm:inline">뒤로</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-toc class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title=목차 aria-label=목차>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg>
<span class="hidden sm:inline">목차</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-search class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2" title=검색 aria-label=검색>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg>
<span class="hidden md:inline">검색</span></button><div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div><button id=dock-top class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2" title="맨 위로" aria-label="맨 위로">
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path d="m18 15-6-6-6 6"/></svg>
<span class="hidden sm:inline">맨 위로</span></button></nav></div><div id=toc-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300" role=dialog aria-modal=true aria-labelledby=toc-title></div><div id=toc-card class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2" role=dialog aria-modal=true aria-labelledby=toc-title><div class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md"><div class="border-border bg-muted/30 flex items-center justify-between border-b p-4"><div class="flex items-center gap-3"><svg class="h-5 w-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><line x1="8" x2="21" y1="6" y2="6"/><line x1="8" x2="21" y1="12" y2="12"/><line x1="8" x2="21" y1="18" y2="18"/><line x1="3" x2="3.01" y1="6" y2="6"/><line x1="3" x2="3.01" y1="12" y2="12"/><line x1="3" x2="3.01" y1="18" y2="18"/></svg><h2 id=toc-title class="text-foreground text-lg font-semibold">목차</h2></div><button id=toc-close class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div class="max-h-96 overflow-y-auto p-4"><nav id=toc-nav class=toc-scrollbar aria-label="글 목차"><div id=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#1-논문의-강점-및-독창성>1. 논문의 강점 및 독창성</a></li><li><a href=#2-핵심-알고리즘-및-과정-설명>2. 핵심 알고리즘 및 과정 설명</a><ul><li><a href=#1-기본-아이디어>(1) 기본 아이디어</a></li><li><a href=#2-알고리즘-단계-설명>(2) 알고리즘 단계 설명</a></li><li><a href=#3-예시-입력과-과정>(3) 예시 입력과 과정</a></li></ul></li><li><a href=#3-논문의-한계점>3. 논문의 한계점</a></li><li><a href=#결론>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#1-head-soft-vote-메커니즘의-목적>1. Head Soft Vote 메커니즘의 목적</a></li><li><a href=#2-head-soft-vote-메커니즘의-작동-원리>2. Head Soft Vote 메커니즘의 작동 원리</a><ul><li><a href=#1-알고리즘-수식>(1) 알고리즘 수식</a></li></ul></li><li><a href=#3-head-soft-vote-메커니즘-예시>3. Head Soft Vote 메커니즘 예시</a><ul><li><a href=#1-시나리오-설정>(1) 시나리오 설정</a></li><li><a href=#2-각-head의-query와-key-dot-product-결과>(2) 각 Head의 Query와 Key dot product 결과</a></li><li><a href=#3-각-head의-softmax-정규화-결과>(3) 각 Head의 softmax 정규화 결과</a></li><li><a href=#4-각-head의-soft-vote-결과-종합>(4) 각 Head의 soft vote 결과 종합</a></li><li><a href=#5-최종적으로-선택된-토큰>(5) 최종적으로 선택된 토큰</a></li></ul></li><li><a href=#4-head-soft-vote-메커니즘의-장점>4. Head Soft Vote 메커니즘의 장점</a></li><li><a href=#5-한계점>5. 한계점</a></li></ul></li></ul><ul><li><ul><li><a href=#1-기존-attention-메커니즘의-계산-복잡도>1. 기존 Attention 메커니즘의 계산 복잡도</a><ul><li><a href=#메모리-사용량-분석>메모리 사용량 분석</a></li></ul></li><li><a href=#2-tokenselect의-head-soft-vote-메커니즘-적용-시-개선>2. TokenSelect의 Head Soft Vote 메커니즘 적용 시 개선</a><ul><li><a href=#1-동작-방식-요약>(1) 동작 방식 요약</a></li><li><a href=#2-계산-복잡도-감소-분석>(2) 계산 복잡도 감소 분석</a></li><li><a href=#3-메모리-사용량-절감-분석>(3) 메모리 사용량 절감 분석</a></li></ul></li><li><a href=#3-메모리-및-연산-성능-비교-요약>3. 메모리 및 연산 성능 비교 요약</a></li><li><a href=#4-실제-활용-시-이점>4. 실제 활용 시 이점</a></li><li><a href=#5-추가-고려-사항>5. 추가 고려 사항</a></li></ul></li></ul><ul><li><ul><li><a href=#1-문제-상황-decoding-단계에서의-효율성-문제>1. 문제 상황: Decoding 단계에서의 효율성 문제</a></li><li><a href=#2-tokenselect의-핵심-아이디어>2. TokenSelect의 핵심 아이디어</a></li><li><a href=#3-tokenselect의-작동-방식-선택적-sparse-attention>3. TokenSelect의 작동 방식: 선택적 Sparse Attention</a><ul><li><a href=#1-prefill-단계와-decode-단계의-분리>(1) <strong>Prefill 단계와 Decode 단계의 분리</strong></a></li><li><a href=#2-decoding-단계에서-연산량-절감-방법>(2) <strong>Decoding 단계에서 연산량 절감 방법</strong></a></li><li><a href=#3-selection-cache-활용>(3) <strong>Selection Cache 활용</strong></a></li></ul></li><li><a href=#4-예시를-통한-이해>4. 예시를 통한 이해</a><ul><li><a href=#시나리오>시나리오</a></li></ul></li><li><a href=#5-연산량-절감의-구체적인-분석>5. 연산량 절감의 구체적인 분석</a></li><li><a href=#6-요약>6. 요약</a></li></ul></li></ul><ul><li><ul><li><a href=#1-selection-cache의-개요>1. Selection Cache의 개요</a><ul><li><a href=#selection-cache의-목적><strong>Selection Cache의 목적</strong></a></li></ul></li><li><a href=#2-selection-cache의-작동-원리>2. Selection Cache의 작동 원리</a><ul><li><a href=#selection-cache-알고리즘><strong>Selection Cache 알고리즘</strong></a></li><li><a href=#selection-cache의-주요-포인트><strong>Selection Cache의 주요 포인트</strong></a></li></ul></li><li><a href=#3-query-유사성-측정-방법>3. Query 유사성 측정 방법</a><ul><li><a href=#query-유사성-측정-cosine-similarity><strong>Query 유사성 측정: Cosine Similarity</strong></a></li><li><a href=#query-유사성-예시><strong>Query 유사성 예시</strong></a></li></ul></li><li><a href=#4-selection-cache와-query-유사성-측정의-장점>4. Selection Cache와 Query 유사성 측정의 장점</a></li><li><a href=#5-한계-및-개선-방안>5. 한계 및 개선 방안</a></li><li><a href=#결론-1>결론</a></li></ul></li></ul><ul><li><ul><li><a href=#1-논문에서-제시한-실험-결과-분석>1. 논문에서 제시한 실험 결과 분석</a><ul><li><a href=#1-성능-및-효율성-비교>(1) <strong>성능 및 효율성 비교</strong></a></li><li><a href=#2-다른-접근법과의-비교>(2) <strong>다른 접근법과의 비교</strong></a></li></ul></li><li><a href=#2-tokenselect가-특출난-이유와-논문에서-제시한-방법론>2. TokenSelect가 특출난 이유와 논문에서 제시한 방법론</a><ul><li><a href=#1-token-level-dynamic-kv-cache-selection>(1) <strong>Token-Level Dynamic KV Cache Selection</strong></a></li><li><a href=#2-selection-cache를-통한-유사-query의-처리-속도-개선>(2) <strong>Selection Cache를 통한 유사 Query의 처리 속도 개선</strong></a></li></ul></li><li><a href=#3-논문에서-제시한-방법이-이러한-결과를-도출한-이유>3. 논문에서 제시한 방법이 이러한 결과를 도출한 이유</a><ul><li><a href=#논문에서-제기하는-이유>논문에서 제기하는 이유</a></li><li><a href=#제-생각>제 생각</a></li></ul></li><li><a href=#4-결론-및-개선-가능성>4. 결론 및 개선 가능성</a></li></ul></li></ul><ul><li><ul><li><a href=#1-기존-시스템에-tokenselect를-적용하기-위해-필요한-구현-요소>1. 기존 시스템에 TokenSelect를 적용하기 위해 필요한 구현 요소</a><ul><li><a href=#1-동적-토큰-레벨-선택-알고리즘-추가>(1) <strong>동적 토큰 레벨 선택 알고리즘 추가</strong></a></li><li><a href=#2-selection-cache-기능-추가>(2) <strong>Selection Cache 기능 추가</strong></a></li><li><a href=#3-효율적인-메모리-관리-및-커널-최적화>(3) <strong>효율적인 메모리 관리 및 커널 최적화</strong></a></li></ul></li><li><a href=#2-구현에-필요한-공수-및-소요-시간-추정>2. 구현에 필요한 공수 및 소요 시간 추정</a><ul><li><a href=#개발-공수-추정><strong>개발 공수 추정</strong></a></li><li><a href=#총-개발-공수-추정-약-6><strong>총 개발 공수 추정</strong>: 약 <strong>6~9주</strong></a></li></ul></li><li><a href=#3-컴퓨팅-자원의-요구-사항-분석>3. 컴퓨팅 자원의 요구 사항 분석</a><ul><li><a href=#1-추가-연산-및-메모리-요구-사항>(1) <strong>추가 연산 및 메모리 요구 사항</strong></a></li><li><a href=#2-연산량-증가-분석>(2) <strong>연산량 증가 분석</strong></a></li><li><a href=#3-추가-메모리-요구-사항>(3) <strong>추가 메모리 요구 사항</strong></a></li></ul></li><li><a href=#4-예상되는-성능-향상과-자원-소모-분석>4. 예상되는 성능 향상과 자원 소모 분석</a><ul><li><a href=#컴퓨팅-자원-비용-분석><strong>컴퓨팅 자원 비용 분석</strong></a></li></ul></li><li><a href=#5-결론>5. 결론</a></li></ul></li></ul><ul><li><ul><li><a href=#1-입력-데이터와-추론-과정-예시>1. 입력 데이터와 추론 과정 예시</a><ul><li><a href=#1-입력-데이터-예시>(1) <strong>입력 데이터 예시</strong></a></li><li><a href=#2-tokenselect를-이용한-추론-과정>(2) <strong>TokenSelect를 이용한 추론 과정</strong></a></li><li><a href=#3-tokenselect를-사용한-효율적인-attention>(3) <strong>TokenSelect를 사용한 효율적인 Attention</strong></a></li></ul></li><li><a href=#2-모델-아키텍처-구성-및-주요-연산>2. 모델 아키텍처 구성 및 주요 연산</a><ul><li><a href=#1-모델-아키텍처-구성>(1) <strong>모델 아키텍처 구성</strong></a></li><li><a href=#2-모델-구성-연산-분석>(2) <strong>모델 구성 연산 분석</strong></a></li></ul></li><li><a href=#3-메모리-및-컴퓨팅-요구량-분석>3. 메모리 및 컴퓨팅 요구량 분석</a><ul><li><a href=#1-메모리-요구-사항>(1) <strong>메모리 요구 사항</strong></a></li><li><a href=#2-컴퓨팅-자원-요구-사항>(2) <strong>컴퓨팅 자원 요구 사항</strong></a></li></ul></li><li><a href=#4-요약-및-결론>4. 요약 및 결론</a></li></ul></li></ul><ul><li><ul><li><a href=#1-tokenselect의-한계-요약>1. TokenSelect의 한계 요약</a><ul><li><a href=#1-cache-miss-문제>(1) <strong>Cache Miss 문제</strong></a></li><li><a href=#2-동적-토큰-선택의-추가-연산-오버헤드>(2) <strong>동적 토큰 선택의 추가 연산 오버헤드</strong></a></li><li><a href=#3-selection-cache의-유사도-측정-한계>(3) <strong>Selection Cache의 유사도 측정 한계</strong></a></li></ul></li><li><a href=#2-한계를-극복하기-위한-연구-흐름>2. 한계를 극복하기 위한 연구 흐름</a><ul><li><a href=#1-문맥-변화-감지를-위한-context-aware-attention-기법>(1) <strong>문맥 변화 감지를 위한 Context-aware Attention 기법</strong></a></li><li><a href=#2-하이브리드-토큰-및-블록-선택-방식>(2) <strong>하이브리드 토큰 및 블록 선택 방식</strong></a></li><li><a href=#3-문장-의미-기반의-유사도-측정-semantic-similarity>(3) <strong>문장 의미 기반의 유사도 측정 (Semantic Similarity)</strong></a></li><li><a href=#4-meta-learning-기반의-캐시-갱신-전략>(4) <strong>Meta-Learning 기반의 캐시 갱신 전략</strong></a></li><li><a href=#5-sparse-attention의-개선>(5) <strong>Sparse Attention의 개선</strong></a></li></ul></li><li><a href=#3-향후-연구-방향과-적용-가능성>3. 향후 연구 방향과 적용 가능성</a><ul><li><a href=#1-context-aware-cache-management>(1) <strong>Context-aware Cache Management</strong></a></li><li><a href=#2-cross-attention-기반의-선택적-kv-갱신>(2) <strong>Cross-attention 기반의 선택적 KV 갱신</strong></a></li><li><a href=#3-추론-단계의-reinforcement-learning-적용>(3) <strong>추론 단계의 Reinforcement Learning 적용</strong></a></li></ul></li><li><a href=#4-결론>4. 결론</a></li></ul></li></ul></nav></div></nav></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground text-center text-xs"><span>제목을 클릭하면 해당 위치로 이동합니다</span></div></div></div></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>function initKaTeX(){if(typeof renderMathInElement=="undefined"){setTimeout(initKaTeX,100);return}var e=[{display:!0,left:"$$",right:"$$"},{display:!1,left:"$",right:"$"}];renderMathInElement(document.body,{delimiters:e,throwOnError:!1,errorColor:"#cc0000",fleqn:!1,leqno:!1,trust:!1})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",initKaTeX):initKaTeX()</script><div id=search-overlay class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div><div id=search-modal class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300"><div class="border-border flex items-center gap-3 border-b p-4"><div class="text-muted-foreground h-5 w-5 flex-shrink-0"><svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><div class="relative flex-1"><button id=search-clear class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200" title=지우기 aria-label=지우기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg>
</button>
<input id=search-input type=text placeholder="게시글 검색..." class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none" autocomplete=off spellcheck=false></div><button id=search-close class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200" title=닫기 aria-label=닫기>
<svg class="h-4 w-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18 18 6M6 6l12 12"/></svg></button></div><div id=search-results class="max-h-96 overflow-y-auto"><div id=search-empty class="flex flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5A7 7 0 113 10a7 7 0 0114 0z"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 시작</h3><p class="text-muted-foreground text-sm">검색어를 입력하세요</p></div><div id=search-loading class="flex hidden items-center justify-center py-8"><div class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div><span class=text-muted-foreground>검색 중...</span></div><div id=search-no-results class="flex hidden flex-col items-center justify-center py-12 text-center"><div class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full"><svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7"/></svg></div><h3 class="text-foreground mb-2 text-lg font-semibold">검색 결과 없음</h3><p class="text-muted-foreground text-sm">다른 검색어로 시도해보세요</p></div><div id=search-results-list class=hidden><div id=search-stats class="text-muted-foreground border-border border-b px-4 py-3 text-sm" data-template="%d개의 결과 찾음"></div><div id=search-items class="divide-border divide-y"></div></div></div><div class="border-border bg-muted/20 border-t px-4 py-3"><div class="text-muted-foreground flex items-center justify-between text-xs"><div class="flex items-center gap-2 md:gap-4"><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↑↓</kbd>
<span class="hidden sm:inline">이동</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">↵</kbd>
<span class="hidden sm:inline">선택</span></div><div class="flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">ESC</kbd>
<span class="hidden sm:inline">닫기</span></div></div><div class="search-hint-desktop flex items-center gap-1"><kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs">⌘K</kbd>
<span>단축키</span></div></div></div></div></body></html>