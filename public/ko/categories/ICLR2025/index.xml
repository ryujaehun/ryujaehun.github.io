<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ICLR2025 on Jaehun's Blog</title><link>https://jaehun.me/ko/categories/ICLR2025/</link><description>Recent content in ICLR2025 on Jaehun's Blog</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Tue, 25 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehun.me/ko/categories/ICLR2025/index.xml" rel="self" type="application/rss+xml"/><item><title>Dynamic Diffusion Transformer</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2410.03456/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2410.03456/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.03456">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h2>
&lt;p>이 논문은 **Dynamic Diffusion Transformer (DyDiT)**라는 새로운 모델을 제안하며, 기존 **Diffusion Transformer (DiT)**의 &lt;strong>과도한 연산량 문제&lt;/strong>를 해결하는 것을 목표로 한다. DyDiT는 &lt;strong>시간축(Timestep)과 공간축(Spatial)에서 동적으로 연산을 조정하는 방식&lt;/strong>을 도입하여 효율성을 크게 향상시켰다.&lt;/p></description></item><item><title>HEXGEN-2: DISAGGREGATED GENERATIVE INFERENCE OF LLMS IN HETEROGENEOUS ENVIRONMENT</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.07903v1/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.07903v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.07903v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-기여점">&lt;strong>논문의 핵심 요약 및 기여점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>HEXGEN-2&lt;/strong>라는 새로운 분산 LLM(대형 언어 모델) 추론 시스템을 제안합니다. 기존 동질적인 고성능 GPU 클러스터를 이용하는 방식과 달리, &lt;strong>이기종 GPU 환경&lt;/strong>에서 &lt;strong>Prefill(입력 처리)과 Decoding(출력 생성) 단계를 분리(disaggregated inference)&lt;/strong> 하여 비용 효율성을 극대화하는 것이 핵심 아이디어입니다.&lt;/p></description></item><item><title>Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.08020v1/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.08020v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.08020v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>이 논문의 핵심 기여는 &lt;strong>Collaborative Speculative Decoding (CoSD)&lt;/strong> 알고리즘을 제안하여, 훈련 없이도 여러 LLM의 지식을 효과적으로 융합하는 방법을 제시한 점이다. CoSD의 강점은 다음과 같다.&lt;/p></description></item><item><title>You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.15296">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>PruneNet&lt;/strong>이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:&lt;/p></description></item><item><title>FlashMask: Efficient and Rich Mask Extension of FlashAttention</title><link>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01359/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01359/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01359">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h3>
&lt;p>논문 **&amp;ldquo;FLASHMASK: Efficient and Rich Mask Extension of FlashAttention&amp;rdquo;**는 기존 FlashAttention이 지원하는 마스크 타입을 확장하고, 메모리 복잡도를 **O(N²)에서 O(N)**으로 줄이면서 성능을 향상시킨 새로운 &lt;strong>column-wise sparse mask&lt;/strong> 표현을 제안한다. 이로 인해 복잡한 마스킹을 처리할 때도 계산량을 줄일 수 있으며, &lt;strong>최신 FlexAttention 대비 12.1%&lt;del>60.7% 향상된 성능(TFLOPs/s)과 1.65x&lt;/del>3.22x 속도 증가&lt;/strong>를 달성했다.&lt;/p></description></item><item><title>TypedThinker: Typed Thinking Improves Large Language Model Reasoning</title><link>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01952/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01952/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01952">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h2>
&lt;h3 id="1-강점">1. &lt;strong>강점&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>다양한 추론 방식의 활용&lt;/strong>&lt;br>
기존 LLM(대형 언어 모델)의 한계를 극복하기 위해 &lt;strong>연역(deductive), 귀납(inductive), 가설(abductive), 유추(analogical)&lt;/strong> 네 가지의 논리적 추론 방식을 적용함.&lt;br>
각각의 추론 방식이 특정 유형의 문제에서 효과적인지를 실험적으로 분석함.&lt;/p></description></item></channel></rss>