<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Jaehun's Blog</title><link>https://jaehun.me/ko/categories/LLM/</link><description>Recent content in LLM on Jaehun's Blog</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehun.me/ko/categories/LLM/index.xml" rel="self" type="application/rss+xml"/><item><title>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.05100v4/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.05100v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.05100v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 BLOOM(176B 파라미터 오픈소스 다국어 언어 모델)에 대한 연구를 담고 있습니다. 아래에서 논문의 강점, 독창적인 점, 알고리즘의 핵심 흐름, 그리고 한계점을 자세히 정리하겠습니다.&lt;/p></description></item><item><title>Galactica: A Large Language Model for Science</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.09085v1/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.09085v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.09085v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문에서 다루는 내용인 **&amp;ldquo;Galactica: A Large Language Model for Science&amp;rdquo;**를 바탕으로 요청하신 논문 분석을 다음과 같이 수행했습니다:&lt;/p></description></item><item><title>Transformers are Multi-State RNNs</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2401.06104/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2401.06104/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.06104">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>트랜스포머와 RNN의 연결 제시&lt;/strong>:
이 논문은 트랜스포머를 &amp;ldquo;무한 상태 RNN(MSRNN)&amp;ldquo;으로 재정의함으로써, 전통적으로 개념적으로 구별되어 온 두 아키텍처 간의 연결성을 제시합니다. 이는 두 모델의 근본적인 공통점을 부각시키는 독창적인 접근입니다.&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-radford2018improving/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-radford2018improving/</guid><description>&lt;p>&lt;a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Improving Language Understanding by Generative Pre-Training&amp;rdquo;**의 주요 내용을 자세히 정리하고, 강점과 독창성, 알고리즘의 예제 과정을 설명한 뒤, 한계점도 다루겠습니다.&lt;/p></description></item><item><title>PaLM 2 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2305.10403v3/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2305.10403v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.10403v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 Google의 PaLM 2에 대한 기술 보고서로, 언어 모델의 새로운 상태를 정립한 모델로서, 다중 언어 처리 능력과 추론 능력을 강화하고, 이전 모델에 비해 컴퓨팅 효율성을 크게 개선한 것이 핵심입니다. 아래에 이 논문의 강점, 독창적인 지점, 핵심 알고리즘 설명, 그리고 한계점을 정리했습니다.&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-1810.04805v2/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-1810.04805v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1810.04805v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 Google AI Language 팀에서 작성한 **&amp;ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&amp;rdquo;**로, 자연어 처리(NLP) 분야에서 혁신적인 성과를 이룬 대표적인 논문입니다. BERT(Bidirectional Encoder Representations from Transformers)는 언어 모델 사전 학습 및 미세 조정을 활용하여 NLP 작업에서 큰 성능 향상을 이루었습니다. 아래에 논문의 주요 내용을 정리하고, 강점, 독창성, 핵심 알고리즘, 그리고 한계점을 설명하겠습니다.&lt;/p></description></item></channel></rss>