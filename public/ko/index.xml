<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jaehun's Blog</title><link>https://jaehun.me/ko/</link><description>Recent content on Jaehun's Blog</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Wed, 09 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehun.me/ko/index.xml" rel="self" type="application/rss+xml"/><item><title>Massive Activations in Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2402.17762v2/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2402.17762v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.17762v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="massive-activations-hidden-biases-재해석된-self-attention의-비밀">Massive Activations, Hidden Biases: 재해석된 Self-Attention의 비밀&lt;/h1>
&lt;hr>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>수천만 개 중 &lt;strong>4–10 개의 초거대 스칼라(×10 000)&lt;/strong> 가 LLM · ViT의 성능을 사실상 ‘혼자서’ 지탱한다.
이 스칼라는 &lt;strong>입력과 무관한 상수형 자기-어텐션 bias&lt;/strong> 로 작동하며, -0-으로 만들면 모델이 즉시 붕괴(PPL → ∞)한다.
저자들은 이 암묵적 현상을 &lt;strong>Explicit Attention Bias(EAB)&lt;/strong> 라는 모듈로 치환해 &lt;strong>성능을 완전히 복구&lt;/strong>하면서 분석·압축·안전성까지 한 번에 열어젖혔다. &lt;/p></description></item><item><title>Peri-LN: Revisiting Normalization Layer in the Transformer Architecture</title><link>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2502.02732v3/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2502.02732v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.02732v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="peri-layernorm-post-ln와-pre-ln을-넘어서는-세-번째-선택지">Peri-LayerNorm: Post-LN와 Pre-LN을 넘어서는 세 번째 선택지&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Residual 직후에 LayerNorm을 한 번 더 넣는 “Peri-LayerNorm(Peri-LN)” 구조만으로 — 400 M→3.2 B 파라미터 LLM에서 FP16 오버플로를 0 건으로 만들고, 다섯 가지 벤치마크 평균 정확도를 최대 +2.8 pp 끌어올렸다.&lt;/strong>&lt;/p></description></item><item><title>SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training</title><link>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2505.11594v1/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-09-paper-2505.11594v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2505.11594v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="sageattention-3--sagebwd--fp4로-달리고-8-bit로-학습한다">SageAttention 3 &amp;amp; SageBwd — FP4로 달리고 8-bit로 학습한다&lt;/h1>
&lt;h2 id="-한-줄-요약-tldr">📝 한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>Blackwell 세대 GPU의 &lt;strong>FP4 Tensor Core&lt;/strong>를 100 % 활용하도록 설계된 &lt;strong>SageAttention 3&lt;/strong>(추론)과 &lt;strong>SageBwd&lt;/strong>(훈련)는 -&lt;/p></description></item><item><title>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition</title><link>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2504.21801v1/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2504.21801v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.21801v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="subgoal-curriculum--cot-consistency-deepseek-prover-v2가-자동-정리-증명의-판을-갈아엎다">Subgoal Curriculum + CoT Consistency: &lt;strong>DeepSeek-Prover-V2&lt;/strong>가 자동 정리 증명의 판을 갈아엎다&lt;/h1>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>&lt;em>DeepSeek-Prover-V2&lt;/em>는 **“문제를 잘게 쪼개고, 쪼갠 대로 끝까지 맞춘다”**는 원칙으로 소형 7 B 모델에서도 MiniF2F Pass@32 &lt;strong>82.4 %&lt;/strong>(671 B)·&lt;strong>75.6 %&lt;/strong>(7 B)의 새 SOTA를 달성했다. 핵심은 &lt;strong>Subgoal-guided Curriculum&lt;/strong>과 **Chain-of-Thought-Lean 일치 보상(GRPO)**의 결합이다.&lt;/p></description></item><item><title>Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</title><link>https://jaehun.me/ko/ko/posts/2025-07-08-paper-helix/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-08-paper-helix/</guid><description>&lt;p>&lt;a href="https://research.nvidia.com/publication/2025-07_helix-parallelism-rethinking-sharding-strategies-interactive-multi-million">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="helix-parallelism-초장기-llm-디코딩의-지연-처리량-벽을-깨다">Helix Parallelism: 초장기 LLM 디코딩의 지연-처리량 벽을 깨다&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Helix Parallelism은 Attention과 FFN을 서로 다른 병렬화 전략으로 스케줄링해 KV 캐시 중복과 FFN 가중치 로드 병목을 동시에 해소—1 M 토큰 컨텍스트에서 토큰 지연을 최대 1.5 × 줄이고, 동일 지연 예산으로 동시 사용자 수를 32 × 까지 늘린다.&lt;/strong>&lt;/p></description></item><item><title>Inference-Time Scaling for Generalist Reward Modeling</title><link>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2504.02495v2/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2504.02495v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.02495v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="inference-time-scaling-deepseek-grm이-초대형-모델을-넘어선-비결">Inference-Time Scaling: DeepSeek-GRM이 초대형 모델을 넘어선 비결&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>“27 B 모델 × 32배 샘플”&lt;/strong>—Generative Reward Model(GRM)과 k-Vote 합산만으로 GPT-4o·Nemotron-340B보다 높은 **종합 정확도 72.8 %**를 기록, &lt;em>모델 크기 대신 추론 컴퓨트&lt;/em>라는 새로운 스케일링 축을 제시했다.&lt;/p></description></item><item><title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title><link>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2505.09343v1/</link><pubDate>Tue, 08 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-08-paper-2505.09343v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2505.09343v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseek-v3-2-048-대-h800으로-405-b-급-llm을-돌린다는-것의-의미">DeepSeek-V3: 2 048 대 H800으로 405 B-급 LLM을 돌린다는 것의 의미&lt;/h1>
&lt;h2 id="tldr--한-줄-요약">TL;DR ― 한 줄 요약&lt;/h2>
&lt;p>&lt;strong>Multi-Head Latent Attention (MLA) + FP8 MoE + Dual-Pipe + 2-계층 MPFT&lt;/strong> 덕분에 DeepSeek-V3는
&lt;em>KV 70 KB/tok · 250 GFLOPs/tok · 14.8 ms TPOT&lt;/em>을 달성, &lt;strong>dense 405 B LLM이 요구하던 자원 대비 비용·전력을 40 % 이상 줄였다.&lt;/strong>&lt;/p></description></item><item><title>Code I/O: Condensing Reasoning Patterns via Code Input-Output Prediction</title><link>https://jaehun.me/ko/ko/posts/2025-07-07-paper-2502.07316v4/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-07-paper-2502.07316v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.07316v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="code-io-코드-입출력--자연어-cot로-범용-추론까지데이터-설계만으로-7b-30b-llm을-평균-2-점-끌어올리다">CODE I/O: 코드 입·출력 + 자연어 CoT로 범용 추론까지 — 데이터 설계만으로 7B-30B LLM을 평균 +2 점 끌어올리다&lt;/h1>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>&lt;strong>“코드 함수 → 입력·출력 예측 + 체계적 Chain-of-Thought(CoT)”라는 단일 데이터 파이프라인만으로, 3.5 M 샘플이 14 M 규모 SOTA 데이터보다 더 크고 균형 잡힌 이득(+2.9 점)을 만든다.&lt;/strong>
검증 가능·저비용·다양성 세 마리 토끼를 잡은 &lt;strong>CODE I/O&lt;/strong>는 “데이터 품질 &amp;gt; 데이터 양”이라는 사실을 실험적으로 증명한다.&lt;/p></description></item><item><title>Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</title><link>https://jaehun.me/ko/ko/posts/2025-07-07-paper-2502.11089v2/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-07-paper-2502.11089v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.11089v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="native-sparse-attention-nsa--64-k-토큰도-11-빠르게-정확도는-그대로">Native Sparse Attention (NSA) — 64 k 토큰도 11× 빠르게, 정확도는 그대로&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>NSA는 ‘압축 → 선택 → 슬라이딩’ 3 분기 희소 어텐션과 GQA/MQA-친화 커널을 결합해 64 k 컨텍스트에서 디코딩 속도를 11.6 배, 학습 속도를 최대 9 배 높이면서도 Full-Attention보다 평균 성능을 향상시킨다.&lt;/strong>&lt;/p></description></item><item><title>디지털 드래곤의 심장: AI 시대, 중국 데이터센터 배터리 시장에 투자해야 하는 이유</title><link>https://jaehun.me/ko/ko/posts/2025-07-07-paper-finance1/</link><pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-07-paper-finance1/</guid><description>&lt;p>아래 글은 gemini의 deep research를 이용하여 생성한 리포트입니다.
앞으로의 시대에서 2차전지는 전기차와 휴머노이드와 같은 산업에서 많은 수요가 있을것으로 예상이 됩니다. 그중에서도 데이터센터에 필요한 2차전지에 주목한다면(UPS/AI와 같은 워크로드가 순간적으로 많은 전력을 요구할시에 이에대한 전력요구가 필요/SMR,재생에너지와 같은 에너지원이 그리드가 갖추어지기전 저장하기 위해서 필요) 어떤 투자 인사이트를 얻을 수 있을지 궁금해서 리포트를 만들어 보았습니다.&lt;/p></description></item><item><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title><link>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.12948v1/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.12948v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.12948v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseek-r1-공개-rl-only-파이프라인으로-o1-급-추론을-재현하다">DeepSeek-R1: 공개 RL-Only 파이프라인으로 &lt;em>o1&lt;/em> 급 추론을 재현하다&lt;/h1>
&lt;h2 id="tldr">TL;DR**&lt;/h2>
&lt;p>DeepSeek-R1은 &lt;em>critic-less&lt;/em> GRPO RL + 소량 Cold-Start SFT + 다단계 RL/SFT + 지식 증류 파이프라인으로, &lt;strong>AIME-24 pass@1 79.8 %·MATH-500 97.3 %·Codeforces Elo 2029&lt;/strong>를 달성해 공개 모델 최초로 OpenAI _o1-1217_과 동급 성능을 실증했다. 학습 비용은 기존 PPO RL의 절반 수준이며, 7 B–32 B 학생 모델로도 추론 능력을 이식해 “재현 가능한 SoTA”라는 연구 공백을 메웠다.&lt;/p></description></item><item><title>Janus-Pro: UnifiedMultimodalUnderstanding and Generation with Data and Model Scaling</title><link>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.17811v1/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.17811v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.17811v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="janus-pro-7b-dual-encoder-multimodal-llm-that-outsmarts-bigger-models">&lt;strong>Janus-Pro 7B: Dual-Encoder Multimodal LLM That Outsmarts Bigger Models&lt;/strong>&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;em>SigLIP 이해 인코더 + VQ 생성 인코더&lt;/em>를 완전히 분리한 뒤 &lt;strong>7 B 파라미터 LLM&lt;/strong>에 붙이는 ‘Dual-Encoder + Adapter’ 설계로, 13 B 통합 모델(TokenFlow-XL)을 &lt;strong>이해·생성 양쪽에서 동시에 제치는&lt;/strong> 첫 사례. – &lt;strong>MMBench 79.2 (+10.3 pt) / GenEval 0.80 (+45 %)&lt;/strong>&lt;/p></description></item><item><title>셀프 로드 자전거 정비 가이드</title><link>https://jaehun.me/ko/ko/posts/2025-07-06-paper-daliy1/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-06-paper-daliy1/</guid><description>&lt;p>안녕하세요! 오랜만에 일상글을 작성해보네요 오늘은 출퇴근 및 라이딩에 사용하고 있는 제 자전거(첼로 스칼라티 D7)를 집에서 셀프 정비하기 위한 가이드를 만들었는데 공유해봅니다.&lt;/p></description></item><item><title>DeepSeek-V3 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.19437v2/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.19437v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.19437v2">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>DeepSeek-V3는 671 B-parameter MoE LLM에 &lt;code>Aux-loss-free Load-Balancing Bias&lt;/code> + FP8 혼정밀 훈련 + Multi-Token Prediction을 결합해, dense 405 B 모델과 동급 (또는 우위) 성능을 절반 이하의 GPU 시간·비용으로 달성한 오픈소스 SOTA이다.&lt;/strong> &lt;/p></description></item><item><title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title><link>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.10302v1/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.10302v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.10302v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseek-vl2--작고-빠르면서-고해상도까지-정확한-멀티모달-llm">DeepSeek-VL2 — “작고 빠르면서 고해상도까지 정확한” 멀티모달 LLM&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>Dynamic Tiling × MLA-MoE × 800 B VL 데이터라는 세 축의 설계로, &lt;strong>4.5 B 활성 파라미터&lt;/strong> 모델이 &lt;strong>DocVQA 92.3 / MMStar 61.3&lt;/strong> 등 동급 공개 VLM 대비 ▲2 – ▲10 pt 성능을 내면서도 &lt;strong>단일 10 GB GPU&lt;/strong>에서 고해상도(최대 3.5 K²) 이미지를 실시간 처리한다.&lt;/p></description></item><item><title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2410.13848v1/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2410.13848v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.13848v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="-janus-두-얼굴의-인코더로-멀티모달-이해--생성의-딜레마를-깬-13-b-모델">📝 Janus: &lt;strong>두 얼굴의 인코더&lt;/strong>로 멀티모달 이해 + 생성의 딜레마를 깬 1.3 B 모델&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Janus&lt;/strong>는 멀티모달 이해와 이미지 생성에 요구되는 서로 다른 표현 ― 고차원 semantic vs. 저차원 texture ― 의 “표현 충돌”을 &lt;strong>듀얼 비전 인코더&lt;/strong>(SigLIP + VQ Tokenizer)로 분리함으로써, 단 1.3 B 파라미터 모델이 기존 단일-인코더 통합 모델보다 **이해 +41 %, FID −8 %**를 동시에 달성한 첫 사례다.&lt;/p></description></item><item><title>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2411.07975v2/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2411.07975v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.07975v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h1>
&lt;p>&lt;strong>JanusFlow&lt;/strong>는 &lt;em>Rectified Flow&lt;/em> + &lt;em>분리형 비전 인코더&lt;/em> + &lt;em>표현 정렬&lt;/em>로 1.3 B 파라미터 단일 모델만으로 &lt;strong>이미지 이해·생성 양쪽에서 동급 최고&lt;/strong> 성능(FID 9.51 / GenEval 0.63 / MMBench 74.9)을 달성한다.&lt;/p></description></item><item><title>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</title><link>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.15664v1/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.15664v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.15664v1">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>Loss-Free Balancing(LFB)은 &lt;em>auxiliary loss&lt;/em>를 완전히 제거한 채, &lt;strong>전문가별 bias 한 줄 업데이트&lt;/strong>만으로 Mixture-of-Experts(MoE) 모델의 ‘로드 밸런스 ↔ 성능’ 딜레마를 깨고 **Perplexity ≈ 0.5↓, MaxVio ≥ 90 %↓**를 동시에 달성했다.&lt;/p></description></item><item><title>DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search</title><link>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.08152v1/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.08152v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.08152v1">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;strong>“끊어 쓰고(Truncate-and-Resume) 탐험하며(내재 보상 MCTS) Lean 4 증명을 찾자”&lt;/strong> – DeepSeek-Prover-V1.5는 7 B 모델로 &lt;em>miniF2F 63.5 %&lt;/em> / *ProofNet 25.3 %*를 달성해 이전 SOTA를 &lt;strong>+9.0 pp / +7.2 pp&lt;/strong> 앞지르면서도 탐색 샘플을 &lt;strong>최대 64 × 절감&lt;/strong>했다.&lt;/p>
 &lt;/div>
 &lt;/blockquote></description></item><item><title>Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning</title><link>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.14158v2/</link><pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-01-paper-2408.14158v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.14158v2">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Fire-Flyer 2&lt;/strong>는 1만 대의 &lt;strong>PCIe A100 GPU&lt;/strong>를 &lt;strong>HFReduce&lt;/strong>·&lt;strong>HaiScale&lt;/strong> 스택으로 묶어 &lt;strong>DGX-A100&lt;/strong>의 80 %대 성능을 &lt;strong>장비·전력 40 % 절감&lt;/strong>으로 달성한 ‘가성비형 초대규모 LLM 훈련 클러스터’다. &lt;/p></description></item><item><title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title><link>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2406.11931v1/</link><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2406.11931v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.11931v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseek-coder-v2-오픈소스-코드-llm이-gpt-4-급으로-뛰어오른-순간">DeepSeek-Coder-V2: 오픈소스 코드-LLM이 GPT-4 급으로 뛰어오른 순간&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>236 B 파라미터 MoE + 10.2 T 토큰 + YaRN 128 K 컨텍스트&lt;/strong> 만으로, **HumanEval 90.2 %·MATH 75.7 %**를 기록해 오픈모델 최초로 GPT-4-Turbo와 동일 선상에 올라섰다.&lt;/p></description></item><item><title>DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</title><link>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2401.14196v2/</link><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2401.14196v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.14196v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseek-coder-gpt-35-turbo를-추월한-최초의-오픈소스-코드-llm">DeepSeek-Coder: GPT-3.5 Turbo를 추월한 최초의 오픈소스 코드 LLM&lt;/h1>
&lt;p>AI 실무자가 꿈꿔 온 **“상용 수준을 넘어서는 공개 코드 모델”**이 마침내 등장했다. DeepSeek-Coder는 &lt;em>레포지터리 전역 의존성 학습, 16 K 토큰 장문맥, FIM 중심 사전학습&lt;/em>이라는 세 가지 설계 축으로 GPT-3.5 Turbo를 넘어서는 성능을 달성한다.&lt;/p>
&lt;hr>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;strong>레포지터리-단위 데이터 + 16 K RoPE + FIM 50 % 학습&lt;/strong> 만으로, 33 B 오픈모델이 HumanEval 79 %로 GPT-3.5 Turbo를 추월했다.&lt;/p>
 &lt;/div>
 &lt;/blockquote></description></item><item><title>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2407.01906v2/</link><pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-30-paper-2407.01906v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.01906v2">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>ESFT (Expert-Specialized Fine-Tuning)은 &lt;strong>MoE LLM의 상위 5 ~ 15 % 전문가만 선택적으로 학습&lt;/strong>해 &lt;strong>파라미터·스토리지 90 % 절감&lt;/strong>과 &lt;strong>FFT급 성능&lt;/strong>을 동시에 달성한, _Sparse LLM 시대의 새로운 PEFT 표준_이다.&lt;/p></description></item><item><title>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title><link>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.02954v1/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.02954v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.02954v1">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>FLOPs / token로 재정의한 &lt;em>DeepSeek Scaling Law&lt;/em> 하나로 모델·데이터·하이퍼파라미터를 자동 결정하여, 2 T token만으로 67 B 파라미터 모델이 LLaMA-2 70 B를 코드·수학·대화·다국어에서 추월했다.&lt;/strong>&lt;/p>
&lt;hr></description></item><item><title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.06066v1/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.06066v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.06066v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseekmoe-정리--dense-성능을-40--flops로-끌어낸-세분화-전문가-트릭">DeepSeekMoE 정리 – Dense 성능을 &lt;strong>40 % FLOPs&lt;/strong>로 끌어낸 ‘세분화-전문가’ 트릭&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Fine-Grained Expert Segmentation (FGES) + Shared Experts (SEI)&lt;/strong> 로 FFN-MoE를 재설계한 &lt;strong>DeepSeekMoE&lt;/strong>는
동일 FLOPs에서 &lt;strong>Dense 상한선의 95 %↑&lt;/strong> 성능을 달성하고, 16 B 모델 기준 &lt;strong>LLaMA-2 7 B와 동급 품질을 연산량 0.4×&lt;/strong> 로 구현한다.&lt;/p></description></item><item><title>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</title><link>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2310.16818v2/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2310.16818v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.16818v2">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="-한-줄-요약-tldr">📝 한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>DreamCraft3D&lt;/strong>는 &lt;em>2-스테이지 3D 생성 파이프라인&lt;/em>과 *Bootstrapped Score Distillation(BSD)*라는 비밀 병기를 결합해, 단 한 장의 2D 이미지(또는 텍스트 프롬프트)만으로도 &lt;strong>360° 기하 일관성과 사진급 고해상도 텍스처&lt;/strong>를 동시에 달성한다.&lt;/p></description></item><item><title>Accelerated Test-Time Scaling with Model-Free Speculative Sampling</title><link>https://jaehun.me/ko/ko/posts/2025-06-26-paper-2506.04708v1/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-26-paper-2506.04708v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2506.04708v1">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;code>STAND (STochastic Adaptive N-gram Drafting)&lt;/code>은 &lt;strong>추가 학습이나 별도 드래프트 모델 없이&lt;/strong>도 대규모 LLM의 테스트-타임 스케일링(Test-Time Scaling)을 가속한다. 표준 오토리그레시브 디코딩 대비 &lt;strong>추론 지연을 60 ~ 65 % 줄이고 처리량을 14 ~ 28 % 높이며 정확도는 유지&lt;/strong>한다.&lt;/p></description></item><item><title>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</title><link>https://jaehun.me/ko/ko/posts/2025-06-26-paper-2505.23416v1/</link><pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-26-paper-2505.23416v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2505.23416v1">논문 링크&lt;/a>&lt;/p>
&lt;h2 id="tldr">⚡️TL;DR&lt;/h2>
&lt;p>&lt;strong>KVzip&lt;/strong>는 LLM의 KV 캐시를 &lt;em>한 번&lt;/em> 압축해도 이후 모든 질의 · 세션에서 재사용할 수 있도록 설계된 &lt;strong>query-agnostic 캐시 컴프레서&lt;/strong>다.
그 결과 &lt;strong>정확도 손실 ≤ 3 pp&lt;/strong>를 유지하면서 **레이턴시 2 × ↓, 메모리 60–394 × ↓**를 동시에 달성한다.&lt;/p></description></item><item><title>Compress, Gather, and Recompute: REFORMingLong-Context Processing in Transformers</title><link>https://jaehun.me/ko/ko/posts/2025-06-24-paper-2506.01215v1/</link><pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-24-paper-2506.01215v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2506.01215v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="-reform-100만-토큰도-잊지-않는-llm-추론-파이프라인">⚡️ REFORM: 100만 토큰도 잊지 않는 LLM 추론 파이프라인&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>REFORM&lt;/strong>은 &lt;em>압축(Compress) → 선별(Gather) → 선택적 재계산(Recompute)&lt;/em> 세 단계를 하나로 묶어,&lt;br>
단일 H100 GPU에서도 &lt;strong>100 만 토큰&lt;/strong> 문맥을 &lt;em>망각 없이&lt;/em> 다루고 &lt;strong>추론 지연·메모리&lt;/strong>까지 줄여 준다.&lt;/p></description></item><item><title>Mamba Drafters for Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2025-06-24-paper-2506.01206v1/</link><pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-24-paper-2506.01206v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2506.01206v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="-mamba-drafters-선형-ssm으로-외부-speculative-decoding-sota-다시-쓰기">📝 Mamba Drafters: 선형 SSM으로 외부 Speculative Decoding SOTA 다시 쓰기&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>Transformer 대신 &lt;strong>Mamba SSM&lt;/strong>을 외부 드래프터로 쓰고, &lt;strong>트리 초안 + UCB 탐색&lt;/strong>을 결합해 &lt;em>재학습 없이&lt;/em>도 Self-spec(EAGLE) 수준 속도·품질을 달성하며 메모리까지 절감했다.&lt;/p></description></item><item><title>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</title><link>https://jaehun.me/ko/ko/posts/2025-06-23-paper-gemini2.5/</link><pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-23-paper-gemini2.5/</guid><description>&lt;p>&lt;a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="gemini-25-1-m-토큰멀티모달초저지연-llm의-비밀을-벗기다">Gemini 2.5: 1 M 토큰·멀티모달·초저지연 LLM의 비밀을 벗기다&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;strong>Sparse-MoE + Slice-Elastic TPU 학습 + Flash Distillation&lt;/strong> 덕분에, Gemini 2.5는 1 M 토큰 컨텍스트와 비디오·오디오까지 통합하면서도 &lt;strong>코딩·수학·롱컨텍스트·비디오&lt;/strong> 벤치마크에서 GPT-4/Claude 4를 큰 폭으로 제치고, Flash 변형은 &lt;strong>≈ 320 tokens/s&lt;/strong>·비용 **35 %↓**로 “성능-지연-비용” Pareto 프런티어를 다시 그렸다.&lt;/p>
 &lt;/div>
 &lt;/blockquote></description></item><item><title>Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</title><link>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.06319v1/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.06319v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.06319v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="llm-추론-가속화를-위한-비동기식-kv-캐시-프리페칭-논문-상세-분석">LLM 추론 가속화를 위한 비동기식 KV 캐시 프리페칭 논문 상세 분석&lt;/h2>
&lt;p>제공된 논문 &amp;ldquo;Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching&amp;quot;은 대규모 언어 모델(LLM) 추론 시 발생하는 메모리 병목 현상을 해결하기 위한 독창적인 방법을 제시합니다. 이 논문은 GPU의 L2 캐시를 활용한 비동기식 프리페칭(prefetching) 기법을 통해 추론 처리량을 획기적으로 개선하는 데 중점을 둡니다.&lt;/p></description></item><item><title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title><link>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.06261v3/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.06261v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.06261v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="hogwild-inference-논문-심층-분석">Hogwild! Inference 논문 심층 분석&lt;/h2>
&lt;p>제공해주신 &amp;ldquo;Hogwild! Inference: Parallel LLM Generation via Concurrent Attention&amp;rdquo; 논문을 자세히 분석하여 강점과 독창성, 핵심 알고리즘, 그리고 한계점을 설명해 드리겠습니다.&lt;/p></description></item><item><title>MMInference: Accelerating Pre-filling for Long-Context Visual Language Models via Modality-Aware Permutation Sparse Attention</title><link>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.16083v2/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.16083v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.16083v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="mminference-논문-리뷰-vlm의-긴-컨텍스트-추론-순열로-속도의-벽을-넘다">MMInference 논문 리뷰: VLM의 긴 컨텍스트 추론, &amp;lsquo;순열&amp;rsquo;로 속도의 벽을 넘다&lt;/h2>
&lt;p>최근 Vision Language Model(VLM)은 이미지와 텍스트를 넘어 긴 비디오까지 이해하는 능력으로 무한한 가능성을 보여주고 있습니다. [cite_start]하지만 수백만 개의 토큰으로 이루어진 긴 비디오를 입력받을 때, 모델이 본격적인 답변 생성을 시작하기 전 입력 전체를 처리하는 &amp;lsquo;Pre-filling&amp;rsquo; 단계에서 엄청난 지연이 발생합니다. [cite: 2] [cite_start]이는 어텐션 메커니즘의 연산량이 입력 길이의 제곱에 비례하여 증가하기 때문인데, 현실적인 서비스 적용에 큰 걸림돌이 되어 왔습니다. [cite: 2, 19]&lt;/p></description></item><item><title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title><link>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.08791v1/</link><pubDate>Thu, 19 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-19-paper-2504.08791v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.08791v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="가정용-기기로-700억-매개변수-llm을-구동하다-primacpp-논문-심층-리뷰">가정용 기기로 700억 매개변수 LLM을 구동하다: prima.cpp 논문 심층 리뷰&lt;/h2>
&lt;p>최근 대규모 언어 모델(LLM)의 발전은 놀랍지만, 대부분의 강력한 모델은 막대한 자원을 갖춘 클라우드 데이터센터에서만 접근 가능했습니다. GPT-4, Claude 3.5와 같은 모델을 개인 기기에서 사용하는 것은 먼 꿈처럼 여겨졌죠. 하지만 만약 여러분의 가정에 있는 노트북, 데스크톱, 스마트폰, 태블릿을 하나로 묶어 거대한 LLM을 구동할 수 있다면 어떨까요?&lt;/p></description></item><item><title>Slim attention: cut your context memory in half without loss– K-cache is all you need for MHA</title><link>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2503.05840v2/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2503.05840v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.05840v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="slim-attention-논문-심층-분석">Slim Attention 논문 심층 분석&lt;/h2>
&lt;p>제공해주신 &amp;ldquo;Slim attention: cut your context memory in half without loss&amp;rdquo; 논문을 정독하고 요청하신 내용에 따라 상세하게 분석해 드리겠습니다.&lt;/p></description></item><item><title>Towards Economical Inference: Enabling DeepSeek’s Multi-Head Latent Attention in Any Transformer-based LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2502.14837v1/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2502.14837v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.14837v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="mha2mla-논문-분석-기존-llm을-경제적으로-만드는-혁신">MHA2MLA 논문 분석: 기존 LLM을 경제적으로 만드는 혁신&lt;/h2>
&lt;p>제출된 논문 &amp;ldquo;Towards Economical Inference: Enabling DeepSeek&amp;rsquo;s Multi-Head Latent Attention in Any Transformer-based LLMS&amp;quot;는 기존의 대규모 언어 모델(LLM)이 가진 고질적인 문제인 막대한 추론 비용을 해결하기 위한 혁신적이고 실용적인 방법을 제시합니다. 논문의 핵심 내용, 강점, 독창성, 그리고 한계점을 상세히 분석해 드립니다.&lt;/p></description></item><item><title>TransMLA: Multi-Head Latent Attention Is All You Need</title><link>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2502.07864v5/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2502.07864v5/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.07864v5">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>네, TransMLA 논문(arXiv:2502.07864v5)을 심도 있게 분석하여 강점과 독창성, 핵심 알고리즘, 그리고 한계점을 상세히 설명해 드리겠습니다.&lt;/p></description></item><item><title>X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression</title><link>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2503.11132v2/</link><pubDate>Mon, 16 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-16-paper-2503.11132v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.11132v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="x-ecomla-논문-분석-강점-핵심-알고리즘-한계점">X-EcoMLA 논문 분석: 강점, 핵심 알고리즘, 한계점&lt;/h2>
&lt;p>&amp;ldquo;X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression&amp;rdquo; 논문은 기존의 대규모 언어 모델(LLM)이 가진 메모리 문제를 해결하기 위한 실용적이고 효율적인 접근법을 제시합니다. 본 분석에서는 논문의 독창적인 강점, 예시를 통한 핵심 알고리즘 설명, 그리고 연구의 잠재적 한계점을 심층적으로 다룹니다.&lt;/p></description></item><item><title>A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers</title><link>https://jaehun.me/ko/ko/posts/2025-06-10-paper-1/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-10-paper-1/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=JuGWOSB68T">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="bring-your-own-model-논문-상세-분석">&amp;ldquo;Bring-Your-Own-Model&amp;rdquo; 논문 상세 분석&lt;/h2>
&lt;p>요청하신 &amp;ldquo;A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers&amp;rdquo; 논문을 심층적으로 분석하여 강점과 독창성, 핵심 알고리즘, 그리고 한계점에 대해 설명해 드리겠습니다.&lt;/p></description></item><item><title>Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework</title><link>https://jaehun.me/ko/ko/posts/2025-06-10-paper-5/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-10-paper-5/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=HPHrIBlJYw">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문(&amp;ldquo;KNOW WHERE YOU&amp;rsquo;RE UNCERTAIN WHEN PLANNING WITH MULTIMODAL FOUNDATION MODELS: A FORMAL FRAMEWORK&amp;rdquo;)은 로봇이 멀티모달 파운데이션 모델(시각과 언어를 함께 이해하는 모델)을 사용하여 계획을 수립할 때 발생하는 불확실성을 체계적으로 분석하고 해결하는 새로운 프레임워크를 제시합니다. 논문을 정밀하게 분석하여 강점과 독창성, 핵심 알고리즘의 작동 방식, 그리고 잠재적인 한계점을 설명해 드리겠습니다.&lt;/p></description></item><item><title>ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation</title><link>https://jaehun.me/ko/ko/posts/2025-06-10-paper-4/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-10-paper-4/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=yLU1zRf95d">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문-분석-real-대규모-언어-모델의-효율적인-rlhf-훈련을-위한-매개변수-재할당">논문 분석: REAL: 대규모 언어 모델의 효율적인 RLHF 훈련을 위한 매개변수 재할당&lt;/h2>
&lt;p>이 문서는 &amp;ldquo;REAL: Efficient RLHF Training of Large Language Models with Parameter Reallocation&amp;rdquo; 논문을 심층적으로 분석하여 논문의 핵심 강점과 독창성, 주요 알고리즘의 작동 방식, 그리고 잠재적인 한계점을 설명합니다.&lt;/p></description></item><item><title>Rubick: Exploiting Job Reconfigurability for Deep Learning Cluster Scheduling</title><link>https://jaehun.me/ko/ko/posts/2025-06-10-paper-3/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-10-paper-3/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=tuziIJjRj5">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="rubick-논문-상세-분석-강점-핵심-알고리즘-그리고-한계점">Rubick 논문 상세 분석: 강점, 핵심 알고리즘, 그리고 한계점&lt;/h2>
&lt;p>제공된 논문 &amp;ldquo;RUBICK: EXPLOITING JOB RECONFIGURABILITY FOR DEEP LEARNING CLUSTER SCHEDULING&amp;quot;에 대한 심층 분석 결과는 다음과 같습니다.&lt;/p></description></item><item><title>Supply-Chain Attacks in Machine Learning Frameworks</title><link>https://jaehun.me/ko/ko/posts/2025-06-10-paper-2/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-10-paper-2/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=EH5PZW6aCr">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="ai-모델-공급망-공격에-대한-연구-논문-심층-분석">AI 모델 공급망 공격에 대한 연구 논문 심층 분석&lt;/h2>
&lt;p>사용자께서 제공하신 &amp;ldquo;Supply-Chain Attacks in Machine Learning Frameworks&amp;rdquo; 논문은 머신러닝(ML) 시스템이 직면한 새로운 유형의 공급망 공격을 심도 있게 분석하고, ML 커뮤니티의 보안 인식 수준을 평가한 중요한 연구입니다. 아래에 논문의 핵심 내용, 강점, 독창성, 핵심 공격 기법, 그리고 한계점을 상세히 설명해 드리겠습니다.&lt;/p></description></item><item><title>Accelerating MoE Model Inference with Expert Sharding</title><link>https://jaehun.me/ko/ko/posts/2025-06-05-paper-2503.08467v1/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-05-paper-2503.08467v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.08467v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>제공해주신 논문 &amp;ldquo;Accelerating MoE Model Inference with Expert Sharding&amp;rdquo; (MOESHARD)에 대한 자세한 분석은 다음과 같습니다.&lt;/p></description></item><item><title>FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2025-06-05-paper-2503.03777v1/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-05-paper-2503.03777v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.03777v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="flexinfer-논문-상세-분석-온디바이스-llm-추론의-메모리-제약-극복">FlexInfer 논문 상세 분석: 온디바이스 LLM 추론의 메모리 제약 극복&lt;/h2>
&lt;p>FlexInfer 논문 &amp;ldquo;FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference&amp;rdquo; [cite: 1]은 대규모 언어 모델(LLM)을 메모리가 제한된 온디바이스 환경에서 효율적으로 실행하기 위한 혁신적인 오프로딩 프레임워크를 제시합니다. 기존 방식들이 성능 저하나 유연성 부족의 한계를 가졌던 반면[cite: 2], FlexInfer는 비동기 프리페칭, 균형 잡힌 메모리 고정, 유연한 텐서 보존이라는 세 가지 핵심 기술을 통해 이러한 문제 해결을 시도합니다. [cite: 3]&lt;/p></description></item><item><title>ScaleFusion: Scalable Inference of Spatial-Temporal Diffusion Transformers for High-Resolution Long Video Generation</title><link>https://jaehun.me/ko/ko/posts/2025-06-05-paper-SCALEFUSION/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-05-paper-SCALEFUSION/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=anZWBeWnWh">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="scalefusion-논문-상세-분석-고해상도-장편-비디오-생성을-위한-공간-시간-확산-트랜스포머의-확장-가능한-추론">ScaleFusion 논문 상세 분석: 고해상도 장편 비디오 생성을 위한 공간-시간 확산 트랜스포머의 확장 가능한 추론&lt;/h2>
&lt;p>ScaleFusion 논문 &amp;ldquo;SCALEFUSION: SCALABLE INFERENCE OF SPATIAL-TEMPORAL DIFFUSION TRANSFORMERS FOR HIGH-RESOLUTION LONG VIDEO GENERATION&amp;quot;은 고해상도(예: 1080p) 및 장시간(예: 20초 이상) 비디오 생성 시 공간-시간 확산 트랜스포머(ST-DiT) 모델의 높은 추론 지연 시간을 해결하기 위한 혁신적인 방법을 제시합니다. [cite: 1, 2] 주요 목표는 여러 GPU 시스템에 걸쳐 ST-DiT 추론을 최적으로 확장하여 시간 제약적인 시나리오에서의 적용 가능성을 높이는 것입니다. [cite: 3, 4, 5]&lt;/p></description></item><item><title>SOLA: Optimizing SLO Attainment for Large Language Model Serving with State-Aware Scheduling</title><link>https://jaehun.me/ko/ko/posts/2025-06-05-paper-sola/</link><pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-05-paper-sola/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=ubIvpetAd6">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 SOLA 논문은 대규모 언어 모델(LLM) 서비스에서 서비스 수준 목표(SLO), 특히 첫 토큰까지의 시간(TTFT)과 출력 토큰당 시간(TPOT) 달성률을 최적화하기 위한 새로운 스케줄링 프레임워크를 제안합니다. 기존 시스템의 고정된 원칙에 따른 스케줄링 방식이 야기하는 TTFT와 TPOT 간의 지연 시간 분포 편향 및 요청 간 분포 분산 문제를 해결하고자 합니다. [cite: 3]&lt;/p></description></item><item><title>XGRAMMAR: FLEXIBLE AND EFFICIENT STRUCTURED GENERATION ENGINE FOR LARGE LANGUAGE MODELS</title><link>https://jaehun.me/ko/ko/posts/2025-06-02-paper-2411.15100v3/</link><pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-02-paper-2411.15100v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.15100v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="xgrammar-논문-상세-분석-유연하고-효율적인-llm-구조화-생성-엔진">XGrammar 논문 상세 분석: 유연하고 효율적인 LLM 구조화 생성 엔진&lt;/h2>
&lt;p>제공해주신 논문 &amp;ldquo;XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models&amp;rdquo; [cite: 1]은 대규모 언어 모델(LLM)이 JSON, SQL, 코드 등 특정 구조를 가진 텍스트를 생성해야 하는 요구에 부응하기 위한 새로운 엔진 XGrammar를 제안합니다. 기존의 문맥 자유 문법(Context-Free Grammar, CFG) 기반 제약 디코딩 방식이 가진 실행 시간 오버헤드 문제를 해결하는 데 초점을 맞추고 있습니다. [cite: 2, 3, 4]&lt;/p></description></item><item><title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title><link>https://jaehun.me/ko/ko/posts/2025-05-17-paper-2505.09343v1/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-17-paper-2505.09343v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2505.09343v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="deepseek-v3-논문-심층-분석-확장성-문제-해결-및-ai-하드웨어-아키텍처에-대한-성찰">DeepSeek-V3 논문 심층 분석: 확장성 문제 해결 및 AI 하드웨어 아키텍처에 대한 성찰&lt;/h2>
&lt;p>DeepSeek-AI에서 발표한 &amp;ldquo;DeepSeek-V3: 확장성 문제 해결 및 AI 하드웨어 아키텍처에 대한 성찰&amp;rdquo; 논문은 대규모 언어 모델(LLM)의 급격한 확장에 따른 현재 하드웨어 아키텍처의 한계를 분석하고, DeepSeek-V3 모델을 통해 이러한 문제점을 효과적으로 해결하는 방안을 제시합니다. 본 논문은 하드웨어 인식 모델 공동 설계를 통해 비용 효율적인 대규모 학습 및 추론을 가능하게 하는 혁신적인 접근 방식을 상세히 설명합니다. [cite: 1, 2, 3, 4]&lt;/p></description></item><item><title>RODIMUS*: BREAKING THE ACCURACY-EFFICIENCY TRADE-OFF WITH EFFICIENT ATTENTIONS</title><link>https://jaehun.me/ko/ko/posts/2025-05-17-paper-2410.06577v1/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-17-paper-2410.06577v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.06577v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="rodimus-논문-분석-정확도-효율성-균형을-깨는-새로운-언어-모델">Rodimus* 논문 분석: 정확도-효율성 균형을 깨는 새로운 언어 모델&lt;/h2>
&lt;p>대규모 언어 모델(LLM)의 발전은 자연어 처리 분야에 혁신을 가져왔지만, 기존 소프트맥스 어텐션 메커니즘의 높은 연산 비용($\mathcal{O}(T)$ 복잡도)은 효율성 측면에서 한계로 지적되어 왔습니다. [cite: 2] 최근 발표된 &amp;ldquo;RODIMUS*: BREAKING THE ACCURACY-EFFICIENCY TRADE-OFF WITH EFFICIENT ATTENTIONS&amp;rdquo; 논문은 이러한 문제를 해결하기 위해 Rodimus와 Rodimus+라는 새로운 모델을 제시하며, LLM의 정확도를 유지하면서도 연산 복잡도를 획기적으로 낮추는 방법을 제안합니다.&lt;/p></description></item><item><title>An Empirical Study of Qwen3 Quantization</title><link>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2505.02214v1/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2505.02214v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2505.02214v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">✅ 결론 요약&lt;/h3>
&lt;p>Qwen3의 정밀한 사전학습 덕분에 고성능을 보이나, 3bit 이하의 ultra-low-bit quantization에 매우 민감하여 기존 모델보다 성능 저하가 크다. 4bit 이상에서는 GPTQ와 AWQ가 최상의 trade-off를 제공하며, BiLLM은 1bit에서도 유의미한 결과를 보인다. 이는 Qwen3이 redundancy가 적은 모델 구조라는 점에서 비롯된 것이며, 향후 LLM quantization 연구가 이를 고려해 더욱 정교한 전략을 개발해야 함을 시사한다.&lt;/p></description></item><item><title>Gemini Embedding: Generalizable Embeddings from Gemini</title><link>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.07891v1/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.07891v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.07891v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">✅ 결론 요약&lt;/h3>
&lt;p>&lt;strong>Gemini Embedding&lt;/strong>은 Google Gemini LLM에서 초기화된 &lt;strong>범용 임베딩 모델로&lt;/strong>, MTEB(Multilingual) 기준 평균 +5.09 점의 성능 향상과 SOTA 달성을 기록하며 &lt;strong>분류, 검색, 클러스터링&lt;/strong> 등 다양한 태스크에서 강력한 일반화 능력을 보여줍니다.&lt;/p></description></item><item><title>Gemma 3 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.19786v1/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.19786v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.19786v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="-결론-요약">✅ 결론 요약&lt;/h2>
&lt;p>&lt;strong>Gemma 3는 장문 컨텍스트(최대 128K), 멀티모달 이미지 입력, 다국어 지원을 갖춘 1B~27B 오픈 모델 계열로, 효율적인 KV 캐시 설계와 향상된 distillation 및 RLHF 기반 후처리로 성능·메모리·활용성에서 매우 뛰어납니다. 특히 27B IT 모델은 Chatbot Arena Elo 1338로 LLaMA 3 70B보다 우위에 있으며, 시각 벤치마크에서도 최고 수준 성능을 달성합니다.&lt;/strong>&lt;/p></description></item><item><title>MELODI: Exploring Memory Compression for Long Contexts</title><link>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2410.03156v1/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2410.03156v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.03156v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">📌 결론 요약&lt;/h3>
&lt;p>논문 &amp;ldquo;MELODI: Exploring Memory Compression for Long Contexts&amp;quot;는 Transformer의 긴 문맥 처리 문제를 해결하기 위해 &lt;strong>계층적 메모리 압축(hierarchical memory compression)&lt;/strong> 구조를 제안합니다.
핵심은 **다층 반복 압축 기반의 단기 메모리(SM)**와 **단일층 추가 압축 기반의 장기 메모리(LM)**를 조합한 **“샌드위치 구조”**를 사용하여 긴 문서를 짧은 윈도우(예: 512 tokens)로 효율적으로 처리하는 것입니다.
메모리 사용량은 기존 Dense Memory 방식인 Memorizing Transformer보다 &lt;strong>최대 8배 절감&lt;/strong>하면서도 성능(PPL 기준)은 오히려 &lt;strong>향상&lt;/strong>됩니다.&lt;/p></description></item><item><title>Seesaw: High-throughput LLM Inference via Model Re-sharding</title><link>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.06433v1/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-05-12-paper-2503.06433v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.06433v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">📌 결론 요약&lt;/h3>
&lt;p>&lt;strong>논문 ‘Seesaw: High-throughput LLM Inference via Model Re-sharding’은 LLM의 두 주요 단계인 prefill과 decode에서 병렬화 전략을 동적으로 변경하는 ‘model re-sharding’ 기법을 제안하여, 평균 1.36배, 최대 1.78배의 추론 처리량 개선을 달성&lt;/strong>합니다. 이 방식은 기존 vLLM처럼 고정된 병렬화 전략에 비해 throughput 최적화를 달성하며, tiered KV cache buffering과 transition-minimizing scheduling을 통해 재샤딩 비용까지 효과적으로 줄입니다.&lt;/p></description></item><item><title>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</title><link>https://jaehun.me/ko/ko/posts/2025-04-16-paper-2503.07703v1/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-16-paper-2503.07703v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.13265v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약-핵심-강점-및-독창성">✅ 결론 요약 (핵심 강점 및 독창성)&lt;/h3>
&lt;p>Seedream 2.0은 &lt;strong>중국어와 영어 모두를 native하게 이해하고 시각화하는 최초 수준의 이중언어 텍스트-이미지 생성 모델&lt;/strong>이다. 주요 강점은 다음과 같다:&lt;/p></description></item><item><title>Toward Efficient Inference for Mixture of Experts</title><link>https://jaehun.me/ko/ko/posts/2025-04-16-paper-1/</link><pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-16-paper-1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2503.13265v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약-핵심-기여">✅ 결론 요약 (핵심 기여)&lt;/h3>
&lt;p>이 논문 **&amp;ldquo;Toward Efficient Inference for Mixture of Experts&amp;rdquo;**는 Mixture-of-Experts (MoE) 기반 Transformer 모델의 &lt;strong>추론 효율성을 극적으로 향상&lt;/strong>시키는 3가지 핵심 기법을 제안합니다:&lt;/p></description></item><item><title>Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</title><link>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2502.19811v3/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2502.19811v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.19811v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts&amp;rdquo;**는 MoE 모델의 핵심 병목인 GPU 간 통신 지연을 &lt;strong>fine-grained 수준에서 컴퓨팅과 통신을 정교하게 겹치도록 설계&lt;/strong>함으로써 실행 성능을 크게 개선한 ByteDance의 시스템 최적화 논문입니다.&lt;/p></description></item><item><title>MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</title><link>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2504.02263v2/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2504.02263v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.02263v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="-결론-요약-핵심-기여-및-성능">📌 결론 요약 (핵심 기여 및 성능)&lt;/h2>
&lt;p>&lt;strong>MegaScale-Infer&lt;/strong>는 대규모 Mixture-of-Experts (MoE) 모델 서빙을 위한 효율적 시스템으로, &lt;strong>Attention과 FFN 모듈을 분리(disaggregate)&lt;/strong> 하여 GPU 활용률을 극대화하고 &lt;strong>최대 1.9×의 GPU throughput 개선&lt;/strong> 및 &lt;strong>1.86× 비용 대비 성능 향상&lt;/strong>을 달성합니다.&lt;/p></description></item><item><title>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</title><link>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2312.07987v3/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-14-paper-2312.07987v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.07987v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 「SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention」을 매우 자세하게 읽고 분석한 내용을 바탕으로, 논문의 강점과 독창적인 지점, 핵심 알고리즘의 상세한 설명과 함께 예시 입력을 이용한 동작 과정을 소개하고, 마지막으로 한계점을 명확하게 설명하겠습니다.&lt;/p></description></item><item><title>Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching</title><link>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2409.01141v1/</link><pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2409.01141v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2409.01141v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">📌 결론 요약&lt;/h3>
&lt;p>논문 *&amp;ldquo;Duplex: A Device for Large Language Models with Mixture of Experts, Grouped Query Attention, and Continuous Batching&amp;rdquo;*은 저연산량(Op/B)이 지배적인 MoE 및 GQA 기반 LLM 추론을 위한 하드웨어 아키텍처 &lt;strong>Duplex&lt;/strong>를 제안하며, GPU 단독 대비 &lt;strong>최대 2.67×의 추론 속도&lt;/strong>와 &lt;strong>42.03%의 에너지 절감&lt;/strong> 효과를 보여줍니다. 핵심은 **xPU (GPU 수준 고성능 연산기)**와 **Logic-PIM (로직 다이에 탑재된 저 Op/B 특화 연산기)**를 &lt;strong>동시에 활용&lt;/strong>하여 MoE와 Attention Layer를 공동 처리(co-processing)하는 방식입니다.&lt;/p></description></item><item><title>Mirage: A Multi-Level Superoptimizer for Tensor Programs</title><link>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2405.05751v2/</link><pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2405.05751v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.05751v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『Mirage: A Multi-Level Superoptimizer for Tensor Programs』를 자세히 분석한 내용을 정리하여 설명합니다.&lt;/p></description></item><item><title>MoEUT: Mixture-of-Experts Universal Transformers</title><link>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2405.16039v2/</link><pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-13-paper-2405.16039v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.16039v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약-핵심-강점-독창성-핵심-메커니즘">✅ 결론 요약 (핵심 강점, 독창성, 핵심 메커니즘)&lt;/h3>
&lt;p>&lt;strong>MoEUT&lt;/strong>는 Universal Transformer(UT)의 구조적 강점(레이어 파라미터 공유 기반의 recurrence)을 유지하면서도 기존 한계였던 &lt;strong>parameter-compute ratio 문제&lt;/strong>를 해결한 구조다. 핵심은 다음 세 가지다:&lt;/p></description></item><item><title>FLEX ATTENTION: A PROGRAMMING MODEL FOR GENERATING OPTIMIZED ATTENTION KERNELS</title><link>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2412.05496v1/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2412.05496v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.05496v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-논문-요약-및-분석-flexattention-a-programming-model-for-generating-optimized-attention-kernels">📌 &lt;strong>논문 요약 및 분석 (FlexAttention: A Programming Model for Generating Optimized Attention Kernels)&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h2 id="-논문의-강점-및-독창적인-지점">✅ &lt;strong>논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>유연성 (Flexibility)&lt;/strong>&lt;/p></description></item><item><title>Inference-Time Scaling for Generalist Reward Modeling</title><link>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2504.02495v1/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2504.02495v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.02495v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문을 자세히 읽고 분석하여 강점, 독창적인 지점, 핵심 알고리즘의 예시와 한계점을 아래와 같이 정리하겠습니다.&lt;/p></description></item><item><title>LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers</title><link>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2405.10480v2/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2405.10480v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.10480v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『LeanAttention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers』에 대한 상세한 분석을 다음과 같이 제시합니다.&lt;/p></description></item><item><title>MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</title><link>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2504.02263v1/</link><pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-07-paper-2504.02263v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2504.02263v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-논문의-강점-및-독창적인-지점-megascale-infer">📌 &lt;strong>논문의 강점 및 독창적인 지점 (MegaScale-Infer)&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 모델 분리 (Disaggregated Expert Parallelism)&lt;/strong>&lt;/p></description></item><item><title>AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds</title><link>https://jaehun.me/ko/ko/posts/2025-04-02-paper-2501.06706/</link><pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-02-paper-2501.06706/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.06706">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds』를 매우 상세히 분석한 결과, 다음과 같은 핵심 사항과 독창적 특징을 확인할 수 있었습니다.&lt;/p></description></item><item><title>SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations</title><link>https://jaehun.me/ko/ko/posts/2025-04-02-paper-2502.16949/</link><pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-04-02-paper-2502.16949/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.16949">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations』의 강점, 독창적인 지점, 핵심 알고리즘의 예시와 전체적인 과정, 그리고 한계점을 정리하여 전달드립니다.&lt;/p></description></item><item><title>Context Parallelism for Scalable Million-Token Inference</title><link>https://jaehun.me/ko/ko/posts/2025-03-31-paper-2411.01783/</link><pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-31-paper-2411.01783/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.01783">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『Context Parallelism for Scalable Million-Token Inference』는 대규모 언어 모델(LLM)의 긴 문맥 처리를 위한 **context parallelism (CP)**을 제안하며, 특히 긴 문맥에서 기존의 텐서 병렬화(Tensor Parallelism, TP)에 비해 효율적인 성능을 달성하는 데 초점을 맞춘 연구입니다.&lt;/p></description></item><item><title>NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2025-03-31-paper-2411.01142/</link><pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-31-paper-2411.01142/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.01142">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-논문-제목-neo-saving-gpu-memory-crisis-with-cpu-offloading-for-online-llm-inference">📌 &lt;strong>논문 제목:&lt;/strong> NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference&lt;/h3>
&lt;h3 id="-저자-xuanlin-jiang-yang-zhou-shiyi-cao-ion-stoica-minlan-yu">📌 &lt;strong>저자:&lt;/strong> Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, Minlan Yu&lt;/h3>
&lt;hr>
&lt;h2 id="1-결론-요약-강점--독창적인-지점">&lt;strong>1. 결론 요약 (강점 &amp;amp; 독창적인 지점)&lt;/strong>&lt;/h2>
&lt;p>NEO는 GPU 메모리의 제한으로 인해 발생하는 LLM 추론의 병목을 해결하기 위해 &lt;strong>비대칭 GPU-CPU 파이프라이닝과 부하 인식 스케줄링을 적용한 새로운 시스템&lt;/strong>입니다. 주요 강점은 다음과 같습니다.&lt;/p></description></item><item><title>PipeFill: Using GPUs During Bubbles in Pipeline-parallel LLM Training</title><link>https://jaehun.me/ko/ko/posts/2025-03-25-paper-2410.07192/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-25-paper-2410.07192/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.07192">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문『PipeFill: Using GPUs During Bubbles in Pipeline-parallel LLM Training』의 핵심 내용을 상세히 분석하여, 논문의 강점, 독창적인 지점, 핵심 알고리즘의 전체적인 과정 및 한계점을 요약하였습니다.&lt;/p></description></item><item><title>SELF-DATA DISTILLATION FOR RECOVERING QUALITY IN PRUNED LARGE LANGUAGE MODELS</title><link>https://jaehun.me/ko/ko/posts/2025-03-25-paper-2410.09982/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-25-paper-2410.09982/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.09982">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 핵심을 정리하여 결론부터 간략히 제시한 후, 구체적인 수치를 통해 강점 및 독창적인 지점을 설명하고, 논문에서 제안한 핵심 알고리즘을 예시와 함께 설명하며, 논문의 한계점을 논의하겠습니다.&lt;/p></description></item><item><title>On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions</title><link>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2402.16442/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2402.16442/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.16442">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">✅ 결론 요약&lt;/h3>
&lt;p>이 논문은 &amp;ldquo;메모리 용량을 초과하는 대규모 데이터셋에서 대표적인 subset을 효율적으로 선택&amp;quot;하는 문제를 다룬다. 기존 방법들은 중앙 서버가 전체 subset을 메모리에 올릴 수 있어야 한다는 제약이 있었지만, 이 논문은 &lt;strong>중앙 서버 없이도 분산 환경에서 고품질 subset을 선택&lt;/strong>할 수 있는 새로운 알고리즘 2가지를 제안한다:&lt;/p></description></item><item><title>SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</title><link>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2406.15486/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2406.15486/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.15486">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">✅ 결론 요약&lt;/h3>
&lt;p>&lt;strong>SampleAttention은 기존 LLM의 attention을 거의 정확도 손실 없이 대체하면서, 최대 2.42배 TTFT(Time-to-First-Token) 지연을 줄이는 구조화된 adaptive sparse attention 기법이다.&lt;/strong>&lt;br>
핵심은 두 가지 sparse 패턴인 &lt;code>local window&lt;/code>와 &lt;code>column stripe&lt;/code>를 활용하여 각 attention head에 대해 동적으로 희소 attention mask를 구성하고, FlashAttention 대비 더 높은 하드웨어 효율성과 가속 성능을 달성한다.&lt;/p></description></item><item><title>TRAINING ULTRA LONG CONTEXT LANGUAGE MODEL WITH FULLY PIPELINED DISTRIBUTED TRANSFORMER</title><link>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2408.16978/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-24-paper-2408.16978/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.16978">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="-결론-요약">✅ 결론 요약&lt;/h3>
&lt;p>이 논문은 &lt;strong>초장문(long-context)&lt;/strong> LLM을 &lt;strong>저렴한 하드웨어(예: 4 GPU)&lt;/strong> 상에서 효율적으로 훈련할 수 있게 하는 &lt;strong>FPDT (Fully Pipelined Distributed Transformer)&lt;/strong> 구조를 제안함.&lt;br>
기존 대비 &lt;strong>최대 16배 더 긴 시퀀스&lt;/strong>(예: 2M tokens)를 &lt;strong>55% 이상의 MFU(Model FLOPs Utilization)&lt;/strong> 효율로 훈련 가능함.&lt;/p></description></item><item><title>QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</title><link>https://jaehun.me/ko/ko/posts/2025-03-18-paper-2405.04532/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-18-paper-2405.04532/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.04532">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;h4 id="1-강점">&lt;strong>1. 강점&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>저비트 정량화(Quantization)의 실용적 개선:&lt;/strong> 기존 INT4 정량화 기법들이 클라우드 기반 LLM 서빙에서 성능 개선을 보이지 못하는 문제를 해결.&lt;/li>
&lt;li>&lt;strong>QoQ(W4A8KV4) 알고리즘 제안:&lt;/strong>
&lt;ul>
&lt;li>**4비트 가중치(W4), 8비트 활성화(A8), 4비트 KV 캐시(KV4)**를 적용하여 정량화에 따른 정확도 손실을 최소화.&lt;/li>
&lt;li>&lt;strong>진행형 그룹 정량화(Progressive Group Quantization):&lt;/strong> 8비트 중간 표현을 활용하여 INT8 텐서 코어에서 연산을 수행, 기존 INT4 방식보다 높은 성능 제공.&lt;/li>
&lt;li>&lt;strong>SmoothAttention 기법:&lt;/strong> 4비트 KV 정량화에 따른 정확도 저하를 완화하는 메커니즘.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>QServe 시스템과 알고리즘 공동 설계(System-Algorithm Co-design):&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>GPU 서빙 성능 극대화:&lt;/strong> CUDA 코어에서 수행되는 비효율적인 연산을 줄이고, 텐서 코어 활용도를 극대화.&lt;/li>
&lt;li>&lt;strong>레지스터 수준 병렬성(Register-Level Parallelism) 활용:&lt;/strong> INT4→INT8 변환 시 감산 후 곱셈(Subtraction after Multiplication) 방식을 적용하여 연산량 감소.&lt;/li>
&lt;li>&lt;strong>연산 중심 가중치 재배열(Compute-aware Weight Reordering):&lt;/strong> CUDA 코어에서의 포인터 연산량을 줄여 L1 캐시 활용 최적화.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="2-독창적인-지점">&lt;strong>2. 독창적인 지점&lt;/strong>&lt;/h4>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>기존 기법&lt;/th>
 &lt;th>QoQ (논문 기법)&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>W4A4의 낮은 정확도 문제&lt;/td>
 &lt;td>W4A8로 INT8 텐서 코어 활용 가능&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>W4A16의 높은 메모리 사용량&lt;/td>
 &lt;td>KV4 도입으로 메모리 효율 개선&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>INT4 GEMM에서 발생하는 CUDA Core 연산 병목&lt;/td>
 &lt;td>Register-Level Parallelism으로 해결&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>기존 KV 캐시 정량화의 정확도 저하&lt;/td>
 &lt;td>SmoothAttention으로 키(Key) 값 정규화&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h3 id="핵심-알고리즘-과정-설명">&lt;strong>핵심 알고리즘 과정 설명&lt;/strong>&lt;/h3>
&lt;h4 id="1-qoq-정량화-알고리즘">&lt;strong>1. QoQ 정량화 알고리즘&lt;/strong>&lt;/h4>
&lt;p>QoQ는 두 단계의 정량화로 이루어짐:&lt;/p></description></item><item><title>Venn: Resource Management Across Federated Learning Jobs</title><link>https://jaehun.me/ko/ko/posts/2025-03-18-paper-2312.08298/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-18-paper-2312.08298/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.08298">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h2>
&lt;p>이 논문은 &lt;strong>연합 학습(Federated Learning, FL)의 리소스 관리 문제&lt;/strong>를 다루며, 특히 다수의 FL 작업이 동일한 디바이스 풀에서 실행될 때 발생하는 &lt;strong>자원 경쟁(Resource Contention)&lt;/strong> 을 해결하는 &lt;strong>Venn&lt;/strong>이라는 새로운 리소스 관리 시스템을 제안합니다.&lt;/p></description></item><item><title>AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution</title><link>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.03519/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.03519/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.03519">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문『AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution』의 주요 강점과 독창적인 지점, 핵심 알고리즘 및 한계점을 압축하여 설명하면 다음과 같습니다.&lt;/p></description></item><item><title>Balancing Pipeline Parallelism with Vocabulary Parallelism</title><link>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.05288/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.05288/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.05288">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 핵심 내용을 요약하면 다음과 같습니다.&lt;/p></description></item><item><title>DIFFSERVE: EFFICIENTLY SERVING TEXT-TO-IMAGE DIFFUSION MODELS WITH QUERY-AWARE MODEL SCALING</title><link>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.15381/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-17-paper-2411.15381/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.15381">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-기여">&lt;strong>논문의 핵심 요약 및 기여&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>DIFFSERVE&lt;/strong>는 &lt;strong>query-aware model scaling&lt;/strong> 개념을 도입하여 &lt;strong>Text-to-Image Diffusion Model&lt;/strong>의 효율적인 서빙을 가능하게 하는 시스템이다. 기존 서빙 시스템이 모든 요청에 대해 동일한 크기의 모델을 사용하는 반면, DIFFSERVE는 입력 쿼리의 난이도에 따라 &lt;strong>경량(lightweight) 모델과 고성능(heavyweight) 모델을 선택적으로 사용&lt;/strong>하는 &lt;strong>모델 캐스케이드(model cascade)&lt;/strong> 기법을 적용한다. 이를 통해 &lt;strong>최대 24% 품질 향상&lt;/strong>, &lt;strong>19-70% SLO(서비스 레벨 목표) 위반 감소&lt;/strong>를 달성한다.&lt;/p></description></item><item><title>EFFICIENT LLM INFERENCE USING DYNAMIC INPUT PRUNING AND CACHE-AWARE MASKING</title><link>https://jaehun.me/ko/ko/posts/2025-03-12-paper-2412.01380/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-12-paper-2412.01380/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.01380">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-내용과-독창적인-기여">&lt;strong>논문의 핵심 내용과 독창적인 기여&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>대형 언어 모델(LLM)의 추론 속도를 향상&lt;/strong>시키기 위해 &lt;strong>Dynamic Input Pruning(DIP)&lt;/strong> 및 &lt;strong>Cache-Aware Masking&lt;/strong> 기법을 제안한다. 기존 LLM들은 &lt;strong>메모리 대역폭의 병목 현상&lt;/strong>으로 인해 모바일 디바이스에서 효율적으로 동작하기 어려웠다. 특히 최신 LLM들이 ReLU 대신 SwiGLU를 사용하는데, 이는 자연적인 활성화 희소성이 낮아 기존 동적 희소화 기법이 비효율적이었다.&lt;/p></description></item><item><title>Marconi: Prefix Caching for the Era of Hybrid LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-03-12-paper-2411.19379/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-12-paper-2411.19379/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.19379">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-평가">논문의 핵심 요약 및 평가&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong>&lt;br>
&lt;strong>Marconi: Prefix Caching for the Era of Hybrid LLMs&lt;/strong>&lt;/p></description></item><item><title>LAVA: LIFETIME-AWARE VM ALLOCATION WITH LEARNED DISTRIBUTIONS AND ADAPTATION TO MISPREDICTIONS</title><link>https://jaehun.me/ko/ko/posts/2025-03-11-paper-2412.09840/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-11-paper-2412.09840/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.09840">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-강점과-독창성">&lt;strong>논문의 강점과 독창성&lt;/strong>&lt;/h2>
&lt;h3 id="강점">&lt;strong>강점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>기존 VM 스케줄링 방식보다 높은 효율성&lt;/strong>&lt;/p></description></item><item><title>TurboAttention: Efficient Attention Approximation for High Throughputs LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-03-11-paper-2412.08585/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-11-paper-2412.08585/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.08585">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『TurboAttention: Efficient Attention Approximation for High Throughputs LLMs』는 기존의 Attention 연산의 속도와 메모리 효율성을 동시에 개선한 통합적인 접근 방법을 제안하고 있으며, 이는 두 가지 핵심 알고리즘인 FlashQ와 Sparsity-based Softmax Approximation (SAS)을 통해 구현되었습니다.&lt;/p></description></item><item><title>A PRACTICAL CROSS-LAYER APPROACH FOR ML-DRIVEN STORAGE PLACEMENT IN WAREHOUSE-SCALE COMPUTERS</title><link>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2501.05651/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2501.05651/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.05651">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">&lt;strong>논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>대규모 데이터 센터에서 기계 학습(ML)을 활용한 저장소 배치(Storage Placement) 문제&lt;/strong>를 다루며, 기존 접근법의 한계를 극복하기 위해 &lt;strong>크로스-레이어(Cross-Layer) 접근 방식&lt;/strong>을 제안했다. 주요 강점과 독창적인 점은 다음과 같다.&lt;/p></description></item><item><title>Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</title><link>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2502.16949/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2502.16949/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.16949">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-기여점">&lt;strong>논문의 핵심 요약 및 기여점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>Comet&lt;/strong>이라는 새로운 MoE(Mixture-of-Experts) 시스템을 제안하여 &lt;strong>계산-통신 오버래핑&lt;/strong>을 더욱 세밀하게 수행함으로써 MoE 모델의 실행 속도를 크게 향상시켰다. 기존 MoE 모델에서 통신 비용이 전체 실행 시간의 47%를 차지하는 문제를 해결하기 위해, &lt;strong>세밀한 수준의(overlapping fine-grained) 계산-통신 오버래핑 기법&lt;/strong>을 도입했다.&lt;/p></description></item><item><title>Scaling Deep Learning Training with MPMD Pipeline Parallelism</title><link>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2412.14374/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-10-paper-2412.14374/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.14374">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>본 논문은 JaxPP라는 시스템을 제안하여, 기존의 Single-Program Multiple-Data (SPMD) 방식의 한계를 극복하고 Multiple-Program Multiple-Data (MPMD) 파이프라인 병렬화를 통해 대규모 딥러닝 모델 학습의 확장성과 성능을 향상한 연구이다. 특히, JaxPP는 사용자가 pipeline 스케줄링을 유연하게 정의할 수 있도록 지원하며, 자동화된 작업 분배와 통신 패턴 추론을 통해 하드웨어 자원을 효율적으로 사용하여 기존 SPMD 대비 최대 1.11배 향상된 성능을 보였다.&lt;/p></description></item><item><title>LSERVE: EFFICIENT LONG-SEQUENCE LLM SERVING WITH UNIFIED SPARSE ATTENTION</title><link>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.14866/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.14866/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.14866">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 핵심 내용을 먼저 간략히 요약한 후, 강점과 독창적인 지점을 자세히 설명하고, 핵심 알고리즘의 동작 원리를 예시와 함께 제시한 뒤, 논문의 한계점을 마지막으로 정리하겠습니다.&lt;/p></description></item><item><title>ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments</title><link>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.09334/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.09334/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.09334">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments』를 상세히 분석하여 다음과 같은 내용을 압축적으로 정리하였다.&lt;/p></description></item><item><title>VOLUT: EFFICIENT VOLUMETRIC STREAMING ENHANCED BY LUT-BASED SUPER-RESOLUTION</title><link>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.12151/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-06-paper-2502.12151/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.12151">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 『VoLUT: Efficient Volumetric Streaming Enhanced by LUT-based Super-resolution』을 분석하여 핵심 사항을 아래와 같이 압축하여 전달하고, 강점과 독창성, 알고리즘의 동작 과정, 한계점을 차례로 제시합니다.&lt;/p></description></item><item><title>Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences</title><link>https://jaehun.me/ko/ko/posts/2025-03-04-paper-2502.08142v1/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-04-paper-2502.08142v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.08142v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-기여점">논문의 핵심 요약 및 기여점&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> &lt;em>Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences&lt;/em>&lt;br>
&lt;strong>핵심 내용:&lt;/strong>&lt;br>
이 논문은 &lt;em>Wildflare GuardRail&lt;/em>이라는 프레임워크를 제안하여 대형 언어 모델(LLM)의 안전성과 신뢰성을 향상시키는 시스템을 개발했다. LLM의 주요 문제점인 &lt;strong>비허용 콘텐츠 탐지, 환각(hallucination), 편향성(bias), 악성 URL 삽입 등의 보안 위험을 감지하고 수정하는 파이프라인&lt;/strong>을 구축하는 것이 핵심 목표다.&lt;/p></description></item><item><title>Forget the Data and Fine-Tuning! Just Fold the Network to Compress</title><link>https://jaehun.me/ko/ko/posts/2025-03-04-paper-2502.10216/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-03-04-paper-2502.10216/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.10216">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-내용-요약-및-강점">&lt;strong>논문의 핵심 내용 요약 및 강점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>Model Folding&lt;/strong>이라는 새로운 데이터 프리 모델 압축 기법을 제안한다. 기존의 압축 방법(가지치기, 양자화)과 달리, &lt;strong>Model Folding&lt;/strong>은 층(layer) 간 유사한 뉴런을 병합하여 모델 크기를 줄인다. 특히, &lt;strong>데이터 없이(data-free), 파인튜닝 없이(fine-tuning-free)&lt;/strong> 모델을 압축하는 것이 가장 큰 특징이다. 주요 강점은 다음과 같다.&lt;/p></description></item><item><title>Dynamic Diffusion Transformer</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2410.03456/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2410.03456/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.03456">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h2>
&lt;p>이 논문은 **Dynamic Diffusion Transformer (DyDiT)**라는 새로운 모델을 제안하며, 기존 **Diffusion Transformer (DiT)**의 &lt;strong>과도한 연산량 문제&lt;/strong>를 해결하는 것을 목표로 한다. DyDiT는 &lt;strong>시간축(Timestep)과 공간축(Spatial)에서 동적으로 연산을 조정하는 방식&lt;/strong>을 도입하여 효율성을 크게 향상시켰다.&lt;/p></description></item><item><title>HEXGEN-2: DISAGGREGATED GENERATIVE INFERENCE OF LLMS IN HETEROGENEOUS ENVIRONMENT</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.07903v1/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.07903v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.07903v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-기여점">&lt;strong>논문의 핵심 요약 및 기여점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>HEXGEN-2&lt;/strong>라는 새로운 분산 LLM(대형 언어 모델) 추론 시스템을 제안합니다. 기존 동질적인 고성능 GPU 클러스터를 이용하는 방식과 달리, &lt;strong>이기종 GPU 환경&lt;/strong>에서 &lt;strong>Prefill(입력 처리)과 Decoding(출력 생성) 단계를 분리(disaggregated inference)&lt;/strong> 하여 비용 효율성을 극대화하는 것이 핵심 아이디어입니다.&lt;/p></description></item><item><title>Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.08020v1/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2502.08020v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.08020v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>이 논문의 핵심 기여는 &lt;strong>Collaborative Speculative Decoding (CoSD)&lt;/strong> 알고리즘을 제안하여, 훈련 없이도 여러 LLM의 지식을 효과적으로 융합하는 방법을 제시한 점이다. CoSD의 강점은 다음과 같다.&lt;/p></description></item><item><title>You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING</title><link>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/</link><pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-25-paper-2501.15296/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.15296">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>PruneNet&lt;/strong>이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:&lt;/p></description></item><item><title>FlashMask: Efficient and Rich Mask Extension of FlashAttention</title><link>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01359/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01359/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01359">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h3>
&lt;p>논문 **&amp;ldquo;FLASHMASK: Efficient and Rich Mask Extension of FlashAttention&amp;rdquo;**는 기존 FlashAttention이 지원하는 마스크 타입을 확장하고, 메모리 복잡도를 **O(N²)에서 O(N)**으로 줄이면서 성능을 향상시킨 새로운 &lt;strong>column-wise sparse mask&lt;/strong> 표현을 제안한다. 이로 인해 복잡한 마스킹을 처리할 때도 계산량을 줄일 수 있으며, &lt;strong>최신 FlexAttention 대비 12.1%&lt;del>60.7% 향상된 성능(TFLOPs/s)과 1.65x&lt;/del>3.22x 속도 증가&lt;/strong>를 달성했다.&lt;/p></description></item><item><title>TypedThinker: Typed Thinking Improves Large Language Model Reasoning</title><link>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01952/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-24-paper-2410.01952/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01952">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h2>
&lt;h3 id="1-강점">1. &lt;strong>강점&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>다양한 추론 방식의 활용&lt;/strong>&lt;br>
기존 LLM(대형 언어 모델)의 한계를 극복하기 위해 &lt;strong>연역(deductive), 귀납(inductive), 가설(abductive), 유추(analogical)&lt;/strong> 네 가지의 논리적 추론 방식을 적용함.&lt;br>
각각의 추론 방식이 특정 유형의 문제에서 효과적인지를 실험적으로 분석함.&lt;/p></description></item><item><title>LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid</title><link>https://jaehun.me/ko/ko/posts/2025-02-17-paper-2502.07563v1/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-17-paper-2502.07563v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.07563v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">&lt;strong>논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>1. 기존 Sequence Parallelism (SP)의 한계 개선&lt;/strong>&lt;br>
기존 SP 기법(LASP-1, Ring Attention 등)은 &lt;code>Right-product-first&lt;/code> 특징을 제대로 활용하지 못하고, 링 스타일(Ring-style) 통신 방식을 사용해 통신-연산 병렬성이 낮았음. LASP-2는 &lt;code>AllGather&lt;/code> 연산을 활용하여 &lt;strong>단일 통신 스텝&lt;/strong> 만으로 전체 메모리 상태를 공유하도록 최적화하여 통신량을 줄이고 계산 병렬성을 극대화함.&lt;/p></description></item><item><title>BitsAI-CR: Automated Code Review via LLM in Practice</title><link>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2501.15134v1/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2501.15134v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.15134v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 주요 강점과 독창적인 지점&lt;/p></description></item><item><title>Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign</title><link>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2502.02068v2/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2502.02068v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.02068v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 핵심 요약&lt;/p></description></item><item><title>SmolLM2: When Smol Goes Big Data-Centric Training of a Small Language Mode</title><link>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2502.02737v1/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-13-paper-2502.02737v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2502.02737v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 요약 및 분석: SmolLM2 - When Smol Goes Big&lt;/p></description></item><item><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title><link>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.12948v1/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.12948v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.12948v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>1. 순수 강화학습 기반 모델 개발:&lt;/strong>&lt;/p></description></item><item><title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title><link>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2412.10302v1/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2412.10302v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.10302v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-개요-및-강점">논문 개요 및 강점&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> &lt;em>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding&lt;/em>&lt;/p></description></item><item><title>Humanity's Last Exam</title><link>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.14249v2/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.14249v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.14249v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>문제 인식 및 대응&lt;/strong>: 이 논문은 기존 벤치마크(MMLU 등)가 LLM(대형 언어 모델)에 의해 거의 90% 이상의 정확도를 기록하며 포화 상태에 도달한 문제를 지적하고 있습니다. 이를 해결하기 위해 **Humanity&amp;rsquo;s Last Exam (HLE)**이라는 새로운 벤치마크를 제안합니다.&lt;/p></description></item><item><title>Qwen2.5-1M Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.15383v1/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-12-paper-2501.15383v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.15383v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-개요-및-강점">논문의 개요 및 강점&lt;/h3>
&lt;p>&lt;strong>Qwen2.5-1M&lt;/strong>은 Alibaba Group의 Qwen 팀이 개발한 초장문 컨텍스트 지원 대규모 언어 모델 시리즈입니다. 이 모델은 기존 128K 토큰에서 &lt;strong>1백만 토큰&lt;/strong>으로 컨텍스트 길이를 확장했으며, 이를 통해 코드 작성, 문서 요약, 복잡한 질의 응답 등의 작업에서 탁월한 성능을 보여줍니다.&lt;/p></description></item><item><title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title><link>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2406.11931v1/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2406.11931v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.11931v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-개요-및-강점">&lt;strong>논문의 개요 및 강점&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>논문 제목&lt;/strong>: &lt;em>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence&lt;/em>&lt;/p></description></item><item><title>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2411.07975v1/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2411.07975v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.07975v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>JanusFlow 논문의 분석&lt;/p></description></item><item><title>Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at AnyResolution</title><link>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2409.12191v2/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-11-paper-2409.12191v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2409.12191v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점과 독창적인 지점&lt;/p></description></item><item><title>DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search</title><link>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2408.08152v1/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2408.08152v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.08152v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">&lt;strong>논문의 강점 및 독창성&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강화 학습과 증명 보조기 피드백의 통합&lt;/strong>&lt;br>
DeepSeek-Prover-V1.5는 증명 보조기(Lean 4)의 피드백을 활용한 강화 학습(RLPAF)을 통해 모델의 성능을 개선했습니다. 기존 모델들은 단순한 감독 학습에 의존했으나, 이 모델은 증명 검증 결과를 직접적으로 학습에 반영함으로써 정확성을 높였습니다.&lt;/p></description></item><item><title>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2407.01906v2/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2407.01906v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.01906v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> &lt;em>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models&lt;/em>&lt;/p></description></item><item><title>Qwen2 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2407.10671v4/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-10-paper-2407.10671v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.10671v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-강점-및-독창성">논문의 주요 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>모델 성능 향상&lt;/strong>: Qwen2 시리즈는 이전 버전인 Qwen1.5와 비교해 대부분의 벤치마크에서 우수한 성능을 보입니다. 특히 Qwen2-72B 모델은 MMLU(84.2), GPQA(37.9), HumanEval(64.6), GSM8K(89.5), BBH(82.4) 등에서 두각을 나타냈습니다.&lt;/p></description></item><item><title>DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data</title><link>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2405.14333v1/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2405.14333v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.14333v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점 및 독창적인 지점&lt;/p></description></item><item><title>DeepSeek-VL: Towards Real-World Vision-Language Understanding</title><link>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2403.05525v2/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2403.05525v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.05525v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점과 독창적인 지점&lt;/p></description></item><item><title>How to Train Data-Efficient LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2402.09668v1/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-09-paper-2402.09668v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.09668v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 개요: “How to Train Data-Efficient LLMs”&lt;/p></description></item><item><title>DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence</title><link>https://jaehun.me/ko/ko/posts/2025-02-07-paper-2401.14196v2/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-07-paper-2401.14196v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.14196v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 요약: “DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence”&lt;/p></description></item><item><title>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-02-07-paper-2402.03300v3/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-07-paper-2402.03300v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.03300v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 개요 및 강점&lt;/p></description></item><item><title>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title><link>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2401.02954v1/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2401.02954v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.02954v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>논문 &lt;strong>&amp;ldquo;DeepSeek LLM: Scaling Open-Source Language Models with Longtermism&amp;rdquo;&lt;/strong> 은 대규모 오픈소스 언어 모델 개발과 스케일링 법칙(scaling laws)에 대한 연구를 다루고 있습니다. 주요 강점과 독창적인 지점을 정리하면 다음과 같습니다.&lt;/p></description></item><item><title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2401.06066v1/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2401.06066v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.06066v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>핵심 기여:&lt;/strong>&lt;/p></description></item><item><title>Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2311.07919v2/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-05-paper-2311.07919v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2311.07919v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-핵심-내용">논문 요약 및 핵심 내용&lt;/h3>
&lt;p>이 논문 *&amp;ldquo;Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models&amp;rdquo;*에서는 다양한 오디오 유형과 태스크를 다룰 수 있는 대규모 오디오-언어 모델(Qwen-Audio)을 소개합니다. 기존 모델들이 특정 오디오 유형(예: 음성 인식)에 초점을 맞춘 반면, Qwen-Audio는 30개 이상의 태스크와 8개 언어를 지원하며, 사람의 음성뿐만 아니라 자연 소리, 음악, 노래 등도 포함하여 학습했습니다.&lt;/p></description></item><item><title>Qwen Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-02-04-paper-2309.16609v1/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-04-paper-2309.16609v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2309.16609v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>강점 요약:&lt;/strong>&lt;/p></description></item><item><title>Qwen-VL: AVersatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</title><link>https://jaehun.me/ko/ko/posts/2025-02-04-paper-2308.12966v3/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-04-paper-2308.12966v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2308.12966v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약-및-평가">&lt;strong>논문의 핵심 요약 및 평가&lt;/strong>&lt;/h3>
&lt;h4 id="1-논문의-강점과-독창적인-지점">&lt;strong>1. 논문의 강점과 독창적인 지점&lt;/strong>&lt;/h4>
&lt;p>이 논문은 &lt;strong>Qwen-VL 시리즈&lt;/strong>라는 대규모 &lt;strong>Vision-Language Model (LVLM)&lt;/strong> 을 소개하며, 기존 모델 대비 &lt;strong>다양한 시각적 이해 및 언어적 응용 능력&lt;/strong>을 향상시킨 것이 핵심이다.&lt;br>
다음과 같은 주요 강점과 차별점이 있다:&lt;/p></description></item><item><title>Janus:DecouplingVisualEncoding for Unified Multimodal Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2025-02-03-paper-2410.13848v1/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-02-03-paper-2410.13848v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.13848v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석-janus---decoupling-visual-encoding-for-unified-multimodal-understanding-and-generation">&lt;strong>논문 요약 및 분석: Janus - Decoupling Visual Encoding for Unified Multimodal Understanding and Generation&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h2 id="1-연구의-주요-기여">&lt;strong>1. 연구의 주요 기여&lt;/strong>&lt;/h2>
&lt;p>Janus는 멀티모달 이해(Understanding)와 생성(Generation)을 통합한 자율회귀(Autoregressive) 모델로, 기존 접근 방식의 한계를 해결하기 위해 &lt;strong>시각적 인코딩(Visual Encoding)을 분리하는 전략을 제안&lt;/strong>한다.&lt;/p></description></item><item><title>A Hardware Evaluation Framework for Large Language Model Inference</title><link>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2312.03134/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2312.03134/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.03134">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>빠르고 정확한 하드웨어 평가 프레임워크 (LLMCompass) 제공&lt;/strong>&lt;/p></description></item><item><title>Compressed Context Memory For Online Language Model Interaction</title><link>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2312.03414/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2312.03414/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.03414">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>압축된 컨텍스트 메모리(CCM) 도입:&lt;/strong>&lt;/p></description></item><item><title>DeepSeek-V3 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2412.19437v1/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2412.19437v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.19437v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점과 독창적인 지점, 핵심 알고리즘 및 한계점을 다음과 같이 요약할 수 있습니다.&lt;/p></description></item><item><title>Fast State Restoration in LLM Serving with HCache</title><link>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2410.05004v1/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2410.05004v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.05004v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h2>
&lt;h3 id="강점">강점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 상태 복원 기술 (HCache) 제안&lt;/strong>&lt;/p></description></item><item><title>Qwen2.5 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2412.15115v2/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-21-paper-2412.15115v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.15115v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title>AIOS: LLM Agent Operating System</title><link>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2403.16971v3/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2403.16971v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.16971v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>AIOS 시스템의 혁신성&lt;/strong>: 이 논문은 대형 언어 모델(LLM) 기반의 지능형 에이전트들을 위한 새로운 운영 체제, **AIOS (LLM Agent Operating System)**를 제안합니다. 기존의 에이전트 시스템들은 자원 관리의 비효율성 및 잠재적인 보안 문제에 직면했으나, AIOS는 LLM과 관련된 자원들을 분리하고 효율적으로 관리할 수 있는 &lt;strong>AIOS 커널&lt;/strong>을 통해 이러한 문제를 해결합니다. 이를 통해 에이전트가 자원을 효율적으로 사용할 수 있게 하며, 성능을 크게 향상시킬 수 있습니다.&lt;/p></description></item><item><title>DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2412.14838v1/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2412.14838v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.14838v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>적응형 KV 캐시 압축 기법 (DynamicKV)&lt;/strong>&lt;/p></description></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2410.13276v2/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2410.13276v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.13276v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>학습 기반의 Attention 희소성 활용:&lt;/strong>&lt;/p></description></item><item><title>TAIPAN: EFFICIENT AND EXPRESSIVE STATE SPACE LANGUAGE MODELS WITH SELECTIVE ATTENTION</title><link>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2410.18572v1/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2410.18572v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.18572v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약">논문 요약&lt;/h3>
&lt;p>이 논문은 &lt;strong>Taipan&lt;/strong>이라는 새로운 하이브리드 모델을 제시합니다. Taipan은 Mamba-2와 **Selective Attention Layers(SALs)**를 결합하여 긴 문맥을 효과적으로 처리하면서도 계산 효율성을 유지하는 모델입니다. 주요 아이디어는 모든 토큰에 대해 동일한 계산 자원을 할당하는 것이 아니라, 중요도가 높은 토큰만을 선택적으로 attention을 적용하여 메모리와 계산 비용을 줄이는 것입니다. 이 모델은 1M 토큰 이상의 긴 시퀀스를 처리하면서도, Transformer와 Mamba-2의 한계를 극복하려는 목표를 가지고 있습니다.&lt;/p></description></item><item><title>TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication</title><link>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2412.20501v1/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-20-paper-2412.20501v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.20501v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title> SANA: EFFICIENT HIGH-RESOLUTION IMAGE SYN THESIS WITH LINEAR DIFFUSION TRANSFORMERS</title><link>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2410.10629v3/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2410.10629v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.10629v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-독창적-지점-그리고-핵심-내용">논문의 강점, 독창적 지점, 그리고 핵심 내용&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title>Block Transformer: Global-to-Local Language Modeling for Fast Inference</title><link>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2406.02657v2/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2406.02657v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.02657v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적-지점">논문의 강점과 독창적 지점&lt;/h3>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title>FLAME: Factuality-Aware Alignment for Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2405.01525v1/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2405.01525v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.01525v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-독창성-핵심-알고리즘-설명-그리고-한계점-분석">논문의 강점, 독창성, 핵심 알고리즘 설명, 그리고 한계점 분석&lt;/h3>
&lt;h4 id="강점-및-독창성">강점 및 독창성&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>문제 인식과 해결&lt;/strong>:&lt;/p></description></item><item><title>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</title><link>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2402.16840v1/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2402.16840v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.16840v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;p>&lt;strong>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT&lt;/strong>&lt;/p></description></item><item><title>Rethinking Optimization and Architecture for Tiny Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2402.02791v2/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-15-paper-2402.02791v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.02791v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="요약-및-강점">요약 및 강점&lt;/h3>
&lt;p>논문 &amp;ldquo;Rethinking Optimization and Architecture for Tiny Language Models&amp;quot;은 경량 언어 모델(LM)을 모바일 디바이스에서도 실행 가능하도록 최적화하는 데 초점을 맞춥니다. 이를 위해 &lt;strong>신경망 아키텍처 설계, 파라미터 초기화, 최적화 전략&lt;/strong>에 대해 면밀히 분석하며, 특히 PanGu-π Pro 모델 시리즈(1B 및 1.5B 파라미터)에 적용하여 기존 모델보다 뛰어난 성능을 달성했습니다.&lt;/p></description></item><item><title>Cascade Speculative Drafting for Even Faster LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.11462/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.11462/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.11462">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="결론-요약">결론 요약&lt;/h3>
&lt;p>논문 **&amp;ldquo;Cascade Speculative Drafting for Even Faster LLM Inference&amp;rdquo;**는 대형 언어 모델(LLM)의 추론 속도를 향상시키기 위해 제안된 &lt;strong>CS Drafting&lt;/strong> 알고리즘을 다룹니다. 이는 기존의 &lt;strong>Speculative Decoding&lt;/strong>의 성능을 개선하여 최대 &lt;strong>81% 추가 속도 향상&lt;/strong>을 달성하며, 모델 출력의 품질을 유지합니다. &lt;strong>수직적&lt;/strong> 및 &lt;strong>수평적 계단식 구조&lt;/strong>를 결합해 효율성을 극대화한 점이 독창적입니다.&lt;/p></description></item><item><title>Distributed Inference and Fine-tuning of Large Language Models Over The Internet</title><link>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.08361/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.08361/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.08361">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>독창적인 분산 시스템 설계&lt;/strong>:&lt;/p></description></item><item><title>EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism</title><link>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.04916/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.04916/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.04916">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-강점과-독창성">논문의 주요 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 대규모 LLM 훈련 및 추론&lt;/strong>:&lt;/p></description></item><item><title>Gated Linear Attention Transformers with Hardware-Efficient Training</title><link>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.06635/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.06635/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.06635">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="결론-요약">결론 요약&lt;/h3>
&lt;p>이 논문은 **Gated Linear Attention Transformer (GLA Transformer)**라는 새로운 변형을 제안하며, 이를 통해 기존의 Linear Attention의 한계를 극복하고 하드웨어 효율성을 크게 개선했습니다. 특히, &lt;strong>FLASHLINEARATTENTION&lt;/strong> 알고리즘을 통해 기존 FlashAttention보다 짧은 시퀀스에서도 더 빠른 속도를 보이며, GLA는 데이터 종속 게이팅 메커니즘을 활용해 소프트맥스 기반 Transformer와 경쟁할 수 있는 성능을 보여줍니다. 실험 결과, 모델은 언어 모델링 및 긴 시퀀스 일반화에서 강점을 보였으며, 연산량 대비 높은 효율성을 유지했습니다.&lt;/p></description></item><item><title>LLM in a flash : Efficient Large Language Model Inference with Limited Memory</title><link>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.11514/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-01-02-paper-2312.11514/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.11514">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-및-독창성">논문의 주요 내용 및 독창성&lt;/h3>
&lt;p>이 논문은 **대용량 언어 모델(LLM)**을 &lt;strong>제한된 DRAM 환경&lt;/strong>에서도 효과적으로 실행할 수 있는 방법을 제안합니다. 이 방법은 &lt;strong>플래시 메모리&lt;/strong>를 사용해 모델 파라미터를 저장하고, 필요할 때 DRAM으로 불러오는 구조를 제안하며, 다음과 같은 주요 기법들을 포함합니다:&lt;/p></description></item><item><title>Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge</title><link>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2312.05693/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2312.05693/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.05693">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-독창성-핵심-알고리즘-그리고-한계점">논문의 강점, 독창성, 핵심 알고리즘, 그리고 한계점&lt;/h3>
&lt;h4 id="결론-요약">&lt;strong>결론 요약&lt;/strong>&lt;/h4>
&lt;p>이 논문은 대규모 언어 모델(LLM)을 엣지 디바이스에서 효과적으로 실행하기 위한 새로운 양자화 프레임워크인 &lt;strong>Agile-Quant&lt;/strong>을 제안합니다. 기존의 가중치 중심 양자화와 달리, 활성화 값까지 4비트 정밀도로 양자화하여 성능 손실 없이 최대 &lt;strong>2.55배 속도 향상&lt;/strong>을 달성합니다. 또한, &lt;strong>SIMD 기반 4비트 곱셈기&lt;/strong>와 &lt;strong>TRIP 행렬 곱셈 알고리즘&lt;/strong>을 설계하여 실제 하드웨어에서 최적화된 동작을 보장합니다.&lt;/p></description></item><item><title>Improving alignment of dialogue agents via targeted human judgements</title><link>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2209.14375/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2209.14375/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2209.14375">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>Language Models are General-Purpose Interfaces</title><link>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2206.06336/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2206.06336/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2206.06336">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>일반 목적 인터페이스 모델(METALM)&lt;/strong>:&lt;/p></description></item><item><title>OPT: Open Pre-trained Transformer Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2205.01068/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2205.01068/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2205.01068">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>논문 &amp;ldquo;OPT: Open Pre-trained Transformer Language Models&amp;quot;는 다음과 같은 강점과 독창적인 지점을 지니고 있습니다.&lt;/p></description></item><item><title>SparQ Attention: Bandwidth-Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2312.04985/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-31-paper-2312.04985/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.04985">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-강점-독창성-한계점-분석">논문 요약 및 강점, 독창성, 한계점 분석&lt;/h3>
&lt;h4 id="요약">요약&lt;/h4>
&lt;p>&amp;ldquo;SparQ Attention: Bandwidth-Efficient LLM Inference&amp;rdquo; 논문은 대형 언어 모델(LLM) 추론 시 발생하는 메모리 대역폭 병목 현상을 해결하기 위한 &lt;strong>SparQ Attention&lt;/strong> 기법을 제안합니다. 이 기법은 Transformer 모델의 Attention 레이어에서 Key-Value(KV) 캐시를 효율적으로 활용해 데이터 전송량을 줄이고, 최대 8배의 데이터 전송 감소를 달성하면서도 정확도 손실을 최소화합니다. 이는 모델 재학습 없이 기존 사전 학습된 모델에 직접 적용 가능하며, 다양한 작업에서 성능을 검증했습니다.&lt;/p></description></item><item><title>CBQ: Cross-Block Quantization for Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-30-paper-2312.07950/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-30-paper-2312.07950/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.07950">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h2 id="결론-및-주요-결과">결론 및 주요 결과&lt;/h2>
&lt;p>논문 *&amp;ldquo;CBQ: Cross-Block Quantization for Large Language Models&amp;rdquo;*의 주요 기여와 성과를 요약하면 다음과 같습니다:&lt;/p></description></item><item><title>SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion</title><link>https://jaehun.me/ko/ko/posts/2024-12-30-paper-2312.07305/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-30-paper-2312.07305/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.07305">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창적인-지점">강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 **Shifted Cross Chunk Attention(SCCA)**와 **Shifted Dilated Attention(SDA)**를 도입하여 긴 문맥을 효율적으로 처리하는 데 초점을 둡니다. 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</title><link>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.03507v2/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.03507v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.03507v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>메모리 효율성&lt;/strong>: GaLore는 &lt;strong>Gradient Low-Rank Projection&lt;/strong>이라는 독창적인 방식을 통해 기존의 Low-Rank Adaptation (LoRA)보다 메모리 효율성을 크게 개선하였습니다. 특히,&lt;/p></description></item><item><title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title><link>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.05530v5/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.05530v5/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.05530v5">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-강점-및-독창성">논문의 주요 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>초대형 컨텍스트 처리&lt;/strong>:&lt;/p></description></item><item><title>Gemma: Open Models Based on Gemini Research and Technology</title><link>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.08295v4/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2403.08295v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.08295v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약">논문의 요약&lt;/h3>
&lt;p>논문은 Google DeepMind에서 개발한 &lt;strong>Gemma&lt;/strong> 모델에 대해 다루고 있으며, 이는 기존 Gemini 모델의 연구와 기술에 기반한 &lt;strong>경량 언어 모델&lt;/strong>입니다. 이 모델은 2B와 7B 파라미터로 제공되며, 다음과 같은 주요 기여를 포함합니다:&lt;/p></description></item><item><title>LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2404.16710v4/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2404.16710v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.16710v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;p>&lt;strong>1. 주요 기여점&lt;/strong>:&lt;/p></description></item><item><title>The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction</title><link>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2312.13558/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-26-paper-2312.13558/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.13558">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>핵심 요약&lt;/strong>: 이 논문은 Transformer 기반 대형 언어 모델(LLM)의 특정 층에서 &lt;strong>랭크 감소(LASER)&lt;/strong> 기법을 통해 성능을 개선할 수 있음을 발견했습니다. 특히, 훈련이 끝난 모델에 적용 가능한 후처리 방법으로, 특정 층의 고차 요소(작은 특이값에 해당하는 성분)를 제거하여 성능을 향상시키는 점이 독창적입니다.&lt;/p></description></item><item><title>Abseil Tip 231 여기와 저기 사이 – 간과되기 쉬운 몇 가지 알고리즘</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-tow231/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-tow231/</guid><description>&lt;p>아래는 **&amp;ldquo;이번 주의 팁 #231: 여기와 저기 사이 – 간과되기 쉬운 몇 가지 알고리즘&amp;rdquo;**에 대한 한글 번역입니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #231: 여기와 저기 사이 – 간과되기 쉬운 몇 가지 알고리즘&amp;rdquo;&lt;/p></description></item><item><title>Abseil Tip 232 변수 선언 시 auto를 언제 사용할 것인가</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-tow232/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-tow232/</guid><description>&lt;p>아래는 **&amp;ldquo;이번 주의 팁 #232: 변수 선언 시 &lt;code>auto&lt;/code>를 언제 사용할 것인가&amp;rdquo;**에 대한 한글 번역입니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #232: 변수 선언 시 &lt;code>auto&lt;/code>를 언제 사용할 것인가&amp;rdquo;&lt;/p></description></item><item><title>Abseil Tip 234 값, 포인터, 참조로 전달하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-tow234/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-tow234/</guid><description>&lt;p>아래는 **&amp;ldquo;이번 주의 팁 #234: 값, 포인터, 참조로 전달하기&amp;rdquo;**에 대한 한글 번역입니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #234: 값, 포인터, 참조로 전달하기&amp;rdquo;&lt;/p></description></item><item><title>Gemma 2: Improving Open Language Models at a Practical Size</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2408.00118v3/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2408.00118v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.00118v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="요약">요약&lt;/h3>
&lt;p>논문 *&amp;ldquo;Gemma 2: Improving Open Language Models at a Practical Size&amp;rdquo;*은 Google DeepMind의 Gemma 2 모델을 소개합니다. 이 모델은 경량 모델로서, 2억~27억 파라미터 크기의 다양한 버전으로 제공되며, 다음과 같은 주요 기술적 기여와 혁신을 포함합니다:&lt;/p></description></item><item><title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2407.00079v3/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2407.00079v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.00079">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>&lt;strong>Mooncake: A KVCache-Centric Disaggregated Architecture for LLM Serving&lt;/strong>&lt;/p></description></item><item><title>SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2410.02367v2/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2410.02367v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.02367v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>효율적이고 정확한 INT8 양자화 기반 주의 메커니즘(SageAttention)&lt;/strong>: SageAttention은 INT8 양자화를 적용하여 기존 FlashAttention2 및 xformers보다 약 2.1배, 2.7배 빠른 성능을 제공합니다. 이는 효율성과 정확성을 모두 확보한 혁신적인 접근 방식입니다.&lt;/li>
&lt;li>&lt;strong>플러그앤플레이 방식&lt;/strong>: 사전 훈련이 필요 없이 기존 모델의 주의 메커니즘을 대체할 수 있어 대규모 모델에서도 쉽게 사용할 수 있습니다.&lt;/li>
&lt;li>&lt;strong>Smooth Matrix K&lt;/strong>: K 행렬에서 발생하는 채널 방향 이상값 문제를 해결하여 정확도를 크게 향상시켰으며, 연산 속도에 거의 영향을 미치지 않는(0.2% 이하) 혁신적인 접근입니다.&lt;/li>
&lt;li>&lt;strong>FP16 Accumulator 활용&lt;/strong>: INT8로 P, V를 양자화하지 않고 FP16 누산기를 사용하여 정확도 손실 없이 속도를 약 2배 향상시켰습니다.&lt;/li>
&lt;li>&lt;strong>적응형 양자화&lt;/strong>: 모델의 각 레이어에서 최적의 양자화 설정을 자동으로 선택해 성능과 정확도 간의 균형을 최적화했습니다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="핵심-알고리즘-sageattention의-예시-설명">핵심 알고리즘 (SageAttention)의 예시 설명&lt;/h3>
&lt;p>SageAttention은 주의 메커니즘을 다음 단계로 최적화합니다:&lt;/p></description></item><item><title>The Llama 3 Herd of Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2407.21783v3/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2407.21783v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.21783v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-독창성-핵심-알고리즘-그리고-한계점-요약">논문의 강점, 독창성, 핵심 알고리즘, 그리고 한계점 요약&lt;/h3>
&lt;hr>
&lt;h4 id="결론-요약">&lt;strong>결론 요약&lt;/strong>&lt;/h4>
&lt;p>이 논문은 Meta의 최신 언어 모델 &lt;strong>Llama 3&lt;/strong>의 개발, 구조, 학습, 성능 평가, 그리고 안전성을 다룹니다. &lt;strong>405B 매개변수&lt;/strong>를 가지며 &lt;strong>128K 토큰의 긴 문맥 처리&lt;/strong>, 멀티모달 확장, 다국어 지원, 툴 사용을 기본적으로 지원하는 것이 특징입니다. 이 모델은 GPT-4에 필적하거나 특정 분야에서 더 우수한 성능을 보여줍니다. 주요 독창성은 &lt;strong>확장된 학습 스케일과 고품질 데이터 사용&lt;/strong>, &lt;strong>긴 문맥 처리 최적화&lt;/strong>, 그리고 &lt;strong>안전성 보장&lt;/strong>에 있습니다. 하지만, &lt;strong>모델의 높은 자원 요구&lt;/strong>, 일부 언어의 성능 한계, 그리고 안전성 조치의 복잡성이 한계로 지적됩니다.&lt;/p></description></item><item><title>Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</title><link>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2405.21060v1/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-24-paper-2405.21060v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.21060">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점과-독창성">강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>강력한 이론적 연결&lt;/strong>: 논문은 Transformer와 Structured State Space Models(SSMs)을 연결하는 &amp;ldquo;State Space Duality&amp;rdquo; (SSD)를 제안했습니다. 이는 SSM을 Attention 기반 모델의 한 변형으로 새롭게 해석할 수 있게 하며, 이를 통해 SSM과 Transformer 간의 이론적 격차를 줄였습니다.&lt;/li>
&lt;li>&lt;strong>효율성 향상&lt;/strong>: 제안된 Mamba-2 아키텍처는 SSM의 계산 효율성을 크게 개선했습니다. 이는 기존 Mamba 대비 2~8배 빠른 처리 속도를 제공하며, Transformer와 비교해도 경쟁력 있는 성능을 제공합니다.&lt;/li>
&lt;li>&lt;strong>광범위한 적용 가능성&lt;/strong>: SSD 프레임워크는 다양한 응용 분야에 적합한 새로운 모델과 알고리즘을 설계할 수 있는 가능성을 열어줍니다.&lt;/li>
&lt;li>&lt;strong>강화된 연속 길이 모델링&lt;/strong>: 이론적 프레임워크를 바탕으로, 긴 시퀀스를 다루는 문제에서 Transformer를 능가하는 성능을 보이는 Mamba-2 아키텍처를 설계했습니다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="핵심-알고리즘-예시-입력">핵심 알고리즘 (예시 입력)&lt;/h3>
&lt;p>Mamba-2의 핵심 알고리즘은 Selective SSM을 기반으로 설계되었으며, 다음의 과정을 포함합니다:&lt;/p></description></item><item><title>Communication Compression for Tensor Parallel LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.09510v2/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.09510v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.09510v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;p>이 논문은 &lt;strong>대규모 언어 모델(LLM)의 Tensor Parallel 전략에서 발생하는 통신 병목 현상을 해결&lt;/strong>하기 위해 &lt;strong>정교한 양자화 기술&lt;/strong>을 사용하여 &lt;strong>통신 압축&lt;/strong>을 제안한 점에서 독창적입니다. 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>Context Parallelism for Scalable Million-Token Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.01783v2/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.01783v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.01783v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창적-기여">강점 및 독창적 기여&lt;/h3>
&lt;p>이 논문은 **Context Parallelism (CP)**라는 기법을 통해 대규모 언어 모델(LLM)에서의 긴 문맥 길이(최대 1M 토큰) 추론 효율성을 대폭 향상시키는 방법을 제안합니다. 주요 강점과 독창적인 기여는 다음과 같습니다:&lt;/p></description></item><item><title>FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs</title><link>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2410.16663v1/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2410.16663v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.16663v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="결론-요약">결론 요약&lt;/h3>
&lt;p>논문 *&amp;ldquo;FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs for Efficient Inference&amp;rdquo;*은 FlashAttention 시리즈의 성능을 NPU와 저자원 GPU로 확장하며, 초저자원 환경에서 Transformer 기반 모델의 추론 성능을 극적으로 향상시키는 새로운 기법을 제안합니다. FastAttention은 기존 FlashAttention 대비 &lt;strong>최대 15배 속도 향상&lt;/strong>을 달성하며, 초장문 입력(최대 256K 토큰)까지도 효율적으로 처리할 수 있도록 설계되었습니다. 핵심 기여로는 &lt;strong>NPU와 GPU에 최적화된 새로운 타일링 전략&lt;/strong>, &lt;strong>CPU-GPU 협업 전략&lt;/strong>, 그리고 &lt;strong>메모리 효율성을 극대화하는 mask 생성 기법&lt;/strong>이 포함됩니다.&lt;/p></description></item><item><title>SimpleFSDP: Simpler Fully Sharded Data Parallel with torch.compile</title><link>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.00284v2/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-23-paper-2411.00284v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.00284v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>\&lt;/p></description></item><item><title>ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.03213v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.03213v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.03213v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 요약: ClusterKV: Semantic Space에서 KV Cache를 관리하는 효율적인 압축 기법&lt;/p></description></item><item><title>Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.17089v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.17089v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.17089v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 핵심 요약&lt;/p></description></item><item><title>Large Concept Models: Language Modeling in a Sentence Representation Space</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.08821v2/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.08821v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.08821v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>요약 및 주요 내용&lt;/p></description></item><item><title>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.10958v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.10958v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.10958v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점 및 독창성
1.	효율성 증가:
•	SageAttention2는 4비트 정밀도(INT4)와 FP8(8비트 부동소수점)을 활용하여 기존 FlashAttention2 대비 약 3.1배 빠른 연산 속도를 제공함.
•	RTX4090에서 최대 485 TOPS 성능을 달성하며, FlashAttention2와 xformers 대비 각각 3.1배, 5.4배 속도 향상을 입증.
2.	정확도 유지:
•	INT4 및 FP8로 매트릭스를 양자화하면서도 End-to-End 정확도 손실이 미미함.
•	텍스트, 이미지, 비디오 생성 모델에서 정확도 손실 없이 기존 모델의 성능을 유지.
3.	적응형 양자화 기법:
•	특정 레이어와 타임스텝에서 높은 정확도를 유지하기 위해 INT8 양자화를 혼합하는 방법을 제안.
•	이를 통해 다양한 입력과 모델 구조에서도 높은 범용성을 보장.
4.	전처리 개선:
•	Smooth Q와 Smooth V 방법론을 통해 양자화 오류를 줄여 INT4의 제한된 수치 범위 내에서도 정확도를 향상.&lt;/p></description></item><item><title>Sharing and Throughput-oriented Token Batching</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.03594v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2412.03594v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.03594v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점 및 독창적인 지점
1.	Global Prefix Sharing의 명시적 적용
기존의 LLM 추론 엔진은 LRU 기반 캐시를 사용해 접두사(prefix)를 공유했지만, 이는 대규모 배치에서 최적화를 달성하지 못했습니다. BatchLLM은 전역적으로 접두사를 명시적으로 식별하고 관리하여 공유 접두사를 최대로 재사용하도록 설계되었습니다.
•	결과: Token 저장 비율에서 vLLM 대비 약 10% 이상의 개선.
2.	Throughput 중심의 Token Batching
Token 배치를 GPU 메모리 사용량 기반으로 조정하여 더 큰 배치를 구성하고 “Valley” 현상(이전 배치에서 GPU가 비효율적으로 작동하는 상태)을 줄였습니다.
•	GPU 활용률 향상: 최대 15%의 처리 시간 단축.
3.	Attention 커널 최적화
기존의 다중 커널 접근 대신, 수평적 융합(Horizontal Fusion)을 적용해 커널 호출 오버헤드와 대기 시간을 줄였습니다. 이는 특히 접두사 공유의 효과를 극대화합니다.
•	성능 향상: vLLM 대비 최대 1.5~2배 향상된 처리량.
4.	적응형 스케줄링
요청의 프롬프트 길이 및 디코딩 비율에 따라 작업을 재정렬하여, 긴 디코딩 요청을 우선 처리하고 효율적으로 배치를 구성합니다. 이는 처리량과 GPU 사용률을 모두 높입니다.&lt;/p></description></item><item><title>SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.12692v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.12692v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.12692v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 강점과 독창성&lt;/p></description></item><item><title>Star Attention: Efficient LLM Inference over Long Sequences</title><link>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.17116v1/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-20-paper-2411.17116v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.17116v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>결론 요약&lt;/p></description></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2412.09871v1/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2412.09871v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.09871v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>요약&lt;/p></description></item><item><title>Efficient Memory Management for Large Language Model Serving with PagedAttention</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2309.06180v1/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2309.06180v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2309.06180v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>결론 및 핵심 요약&lt;/p></description></item><item><title>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2006.16668v1/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2006.16668v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2006.16668v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>스케일링 접근 방식의 혁신성&lt;/strong>: GShard는 6000억 개 이상의 파라미터를 가진 모델을 분산 TPU 클러스터에서 효과적으로 학습할 수 있도록 설계되었으며, 이를 통해 비용 효율적인 방식으로 초대형 신경망 모델을 학습할 수 있음을 입증했습니다.&lt;/p></description></item><item><title>GSPMD: General and Scalable Parallelization for ML Computation Graphs</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2105.04663v2/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2105.04663v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2105.04663v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h4 id="1-강점">&lt;strong>1. 강점&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>일반화된 병렬화 시스템&lt;/strong>:&lt;/p></description></item><item><title>Memory Layers at Scale</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2412.09764v1/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2412.09764v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.09764v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>결론 요약&lt;/p></description></item><item><title>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</title><link>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2306.02707v1/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-19-paper-2306.02707v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2306.02707v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 주요 내용을 요약하자면, Orca 모델은 GPT-4와 같은 대형 언어 모델(LFM)의 복잡한 설명 과정(Reasoning Process)을 학습하여, 기존의 Instruction-tuning 모델들에 비해 뛰어난 성능을 보이는 13B 파라미터 모델입니다. Orca의 주요 혁신은 설명 튜닝(Explanation Tuning) 기법을 사용하여, GPT-4로부터 생성된 단계별 사고 과정 데이터를 활용해 학습하는 데 있습니다. 이를 통해, 단순한 스타일 모방에 그치지 않고 대형 모델의 논리적 사고를 학습하려고 시도합니다.&lt;/p></description></item><item><title>Abseil Tip 197 Reader Lock은 드물게 사용해야 합니다</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-tow197/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-tow197/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #197: Reader Lock은 드물게 사용해야 합니다&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2021년 7월 29일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2024년 4월 1일&lt;/p>
&lt;p>&lt;strong>작성자:&lt;/strong> &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/p></description></item><item><title>Abseil Tip 224 vector.at() 사용 피하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-tow224/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-tow224/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #224: &lt;code>vector.at()&lt;/code> 사용 피하기&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2023년 8월 24일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2024년 1월 24일&lt;/p>
&lt;p>&lt;strong>작성자:&lt;/strong> &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/224">abseil.io/tips/224&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="at">&lt;strong>&lt;code>at()&lt;/code>는 무엇을 하나요?&lt;/strong>&lt;/h2>
&lt;p>&lt;code>at(size_type pos)&lt;/code> 메서드의 동작은 다음과 같이 정의됩니다:&lt;/p></description></item><item><title>Abseil Tip 227 빈 컨테이너와 부호 없는 정수 연산 주의하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-tow227/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-tow227/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #227: 빈 컨테이너와 부호 없는 정수 연산 주의하기&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2023년 11월 16일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2024년 3월 11일&lt;/p></description></item><item><title>Abseil Tip 229 템플릿 메타프로그래밍을 위한 순위 기반 오버로드</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-tow229/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-tow229/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #229: 템플릿 메타프로그래밍을 위한 순위 기반 오버로드&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2024년 2월 5일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2024년 2월 20일&lt;/p></description></item><item><title>Fast Inference of Mixture-of-Experts Language Models with Offloading</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2312.17238/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2312.17238/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.17238">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">논문의 핵심 요약&lt;/h3>
&lt;p>이 논문은 &lt;strong>Mixture-of-Experts (MoE)&lt;/strong> 기반 대형 언어 모델(Mixtral-8x7B)을 제한된 GPU 메모리 환경에서 효율적으로 추론할 수 있는 방법을 제안합니다. 주요 기여는 다음과 같습니다:&lt;/p></description></item><item><title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2307.08691v1/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2307.08691v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2307.08691v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-flashattention-2">논문 분석: FlashAttention-2&lt;/h3>
&lt;p>FlashAttention-2는 기존 FlashAttention의 성능 병목을 개선하여 &lt;strong>Transformer 모델의 Attention 연산에서 효율성을 극대화&lt;/strong>한 알고리즘입니다. 특히, GPU의 병렬성 활용을 최적화하여 긴 시퀀스를 다루는 데 있어 탁월한 성능을 보여줍니다.&lt;/p></description></item><item><title>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2407.08608v2/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2407.08608v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.08608v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="flashattention-3-논문-분석">FlashAttention-3 논문 분석&lt;/h3>
&lt;p>FlashAttention-3는 Transformer의 Attention 연산을 최적화하여 성능과 메모리 효율성을 크게 향상시킨 알고리즘입니다. 이 논문은 기존 FlashAttention-2의 한계를 극복하며, NVIDIA Hopper GPU의 특화된 하드웨어 기능을 활용하여 &lt;strong>비동기 처리(asynchrony)&lt;/strong> 와 &lt;strong>저정밀도 연산(low-precision)&lt;/strong> 을 도입했습니다.&lt;/p></description></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2205.14135v2/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2205.14135v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2205.14135v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="요약-및-결론">요약 및 결론&lt;/h3>
&lt;p>이 논문, &lt;strong>&amp;ldquo;FlashAttention&amp;rdquo;&lt;/strong>, 은 Transformer 모델의 self-attention 모듈에서 메모리 및 계산 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 이를 통해 긴 시퀀스를 다루는 Transformer의 병목을 해소하며 기존 방법 대비 속도와 메모리 사용에서 큰 개선을 이룹니다.&lt;/p></description></item><item><title>Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2401.02669/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2401.02669/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.02669">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약">논문 요약&lt;/h3>
&lt;p>&lt;strong>Infinite-LLM&lt;/strong>은 초대형 언어 모델(LLM)의 동적 컨텍스트 길이 문제를 해결하기 위해 제안된 효율적인 서비스 시스템입니다. 논문은 특히 &lt;strong>DistAttention&lt;/strong> 메커니즘과 &lt;strong>Distributed KVCache&lt;/strong>를 도입해 LLM 요청 서비스의 확장성을 획기적으로 개선합니다. 이를 통해 긴 컨텍스트 길이를 효과적으로 처리하며, 시스템 전체의 계산 및 메모리 자원 활용도를 높입니다.&lt;/p></description></item><item><title>SuperServe: Fine-Grained Inference Serving for Unpredictable Workloads</title><link>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2312.16733v1/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-18-paper-2312.16733v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.16733v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;hr>
&lt;h4 id="논문의-강점">&lt;strong>논문의 강점&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>문제 정의의 명확성&lt;/strong>: 예측 불가능하고 폭발적인 워크로드 하에서 ML 추론 시스템이 지연(SLO)을 만족하면서 높은 정확도를 유지해야 하는 문제를 효과적으로 다룸.&lt;/li>
&lt;li>&lt;strong>독창적인 접근 방식 - SubNetAct&lt;/strong>:
&lt;ul>
&lt;li>&lt;strong>SubNetAct&lt;/strong>는 SuperNet을 활용하여 메모리 효율성과 지연 최소화를 동시에 달성.&lt;/li>
&lt;li>SuperNet 내에서 &lt;strong>LayerSelect&lt;/strong>, &lt;strong>SubnetNorm&lt;/strong>, &lt;strong>WeightSlice&lt;/strong> 연산자를 사용해 모델 스위칭 대신 즉각적인 서브넷 활성화 구현.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>유연성과 실용성&lt;/strong>:
&lt;ul>
&lt;li>SlackFit이라는 스케줄링 정책을 제안하여 서비스 로드에 맞춰 실시간으로 정확도와 지연 간의 균형을 조정.&lt;/li>
&lt;li>다양한 실제 데이터셋(Microsoft Azure Functions) 및 합성 데이터로 실험을 통해 성능 검증.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>실질적인 성과&lt;/strong>:
&lt;ul>
&lt;li>경쟁 시스템 대비 4.67% 높은 정확도를 유지하며 동일한 SLO 달성률을 제공.&lt;/li>
&lt;li>같은 정확도에서 2.85배 더 높은 SLO 달성률 달성.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h4 id="핵심-알고리즘-subnetact와-slackfit">&lt;strong>핵심 알고리즘: SubNetAct와 SlackFit&lt;/strong>&lt;/h4>
&lt;p>&lt;strong>SubNetAct의 주요 작동 과정&lt;/strong>:&lt;/p></description></item><item><title>Abseil Tip 124 absl::StrFormat()</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow124/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow124/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #124: &lt;code>absl::StrFormat()&lt;/code>&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2016년 10월 11일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2022년 11월 16일&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/124">abseil.io/tips/124&lt;/a>&lt;/p>
&lt;hr>
&lt;h2>&lt;strong>&lt;code>str_format&lt;/code> 라이브러리와 &lt;code>absl::StrFormat()&lt;/code>&lt;/strong>&lt;/h2>
&lt;p>오랜 테스트와 개발 과정을 거친 끝에, &lt;code>str_format&lt;/code> 라이브러리를 정식으로 공개하게 되었습니다. &lt;code>str_format&lt;/code> 라이브러리는 매우 효율적이고 타입 안전하며 확장 가능한 라이브러리로, 모든 &lt;code>printf&lt;/code> 형식의 문법을 구현합니다. 거의 모든 &lt;code>printf&lt;/code> 스타일 변환은 손쉽게 &lt;code>absl::StrFormat()&lt;/code>으로 업그레이드할 수 있습니다. 자세한 문서는 &lt;a href="https://abseil.io/docs/cpp/guides/format">여기&lt;/a>에서 확인하세요. 이 라이브러리는 printf 스타일 포맷팅에 가장 적합하지만, 이 글에서는 printf 스타일의 사용 여부에 대한 입장은 따로 표명하지 않습니다.&lt;/p></description></item><item><title>Abseil Tip 18 Substitute를 활용한 문자열 포맷팅</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow18/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow18/</guid><description>&lt;p>물론입니다! 아래는 번역된 내용입니다:&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #18: Substitute를 활용한 문자열 포맷팅&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2012년 10월 4일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2022년 11월 16일&lt;/p></description></item><item><title>Abseil Tip 198 태그 타입(Tag Types)</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow198/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow198/</guid><description>&lt;p>아래는 **&amp;ldquo;이번 주의 팁 #198: 태그 타입(Tag Types)&amp;rdquo;**에 대한 한글 번역입니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #198: 태그 타입(Tag Types)&amp;rdquo;&lt;/p></description></item><item><title>Abseil Tip 215 AbslStringify()를 사용한 사용자 정의 타입 문자열화"</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow215/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow215/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #215: &lt;code>AbslStringify()&lt;/code>를 사용한 사용자 정의 타입 문자열화&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2022년 11월 2일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2022년 11월 16일&lt;/p></description></item><item><title>Abseil Tip 218 FTADLE로 확장 지점 설계하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow218/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow218/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #218: FTADLE로 확장 지점 설계하기&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2023년 1월 19일&lt;/p>
&lt;p>&lt;strong>작성자:&lt;/strong> &lt;a href="mailto:asoffer@google.com">Andy Soffer&lt;/a>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/218">abseil.io/tips/218&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="확장-지점-설계하기">&lt;strong>확장 지점 설계하기&lt;/strong>&lt;/h2>
&lt;p>가정해 봅시다. 여러분은 &lt;code>sketchy&lt;/code>라는 라이브러리를 개발 중이고, 이 라이브러리는 캔버스에 그림을 그리는 기능을 제공합니다. 이미 점, 선, 텍스트 같은 기본 도형을 그리는 기능을 제공하지만, 사용자가 자신만의 타입을 그리는 방식을 정의할 수 있도록 **확장 지점(extension point)**을 설계하려고 합니다.&lt;/p></description></item><item><title>Abseil Tip 3 문자열 연결과 operator+ vs. StrCat()</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-tow3/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-tow3/</guid><description>&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #3: 문자열 연결과 &lt;code>operator+&lt;/code> vs. &lt;code>StrCat()&lt;/code>&amp;rdquo;&lt;/p>
&lt;p>&lt;strong>원문 게시일:&lt;/strong> 2012년 5월 11일&lt;br>
&lt;strong>업데이트:&lt;/strong> 2022년 11월 16일&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/3">abseil.io/tips/3&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="개요">&lt;strong>개요&lt;/strong>&lt;/h2>
&lt;p>리뷰어가 &amp;ldquo;문자열 연결 연산자 &lt;code>+&lt;/code>를 사용하지 마세요. 비효율적입니다.&amp;ldquo;라고 말하면 많은 개발자가 의아해합니다. &lt;code>string::operator+&lt;/code>가 왜 비효율적일까요? 이런 간단한 작업에서 실수를 하기 어려울 것 같은데 말이죠.&lt;/p></description></item><item><title>Fast and Effective Weight Update for Pruned Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.02938/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.02938/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.02938">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">&lt;strong>논문의 강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적이고 정확한 Weight Update&lt;/strong>&lt;br>
논문은 &lt;strong>Alternating Direction Method of Multipliers (ADMM)&lt;/strong> 기반의 효율적인 Layer-wise Weight Update 알고리즘을 제안합니다. 기존 Gradient Descent 기반 방법들(Adaprune)과 달리, ADMM은 매우 빠른 수렴 속도를 보이며 SparseGPT보다도 높은 정확도를 제공합니다.&lt;/p></description></item><item><title>FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04044/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04044/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.04044">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">&lt;strong>논문의 강점과 독창성&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>핵심 아이디어 - Heavy Hitter 기반 FFN 분할 (FFSplit)&lt;/strong>:&lt;/p></description></item><item><title>FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.03868/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.03868/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.03868">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;p>&lt;strong>제목:&lt;/strong> &lt;em>FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs&lt;/em>&lt;br>
&lt;strong>주제:&lt;/strong> FPGA 기반 대형 언어 모델(LLM) 추론을 위한 효율적 아키텍처&lt;/p></description></item><item><title>Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04658/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04658/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.04658">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">&lt;strong>논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>이론적 한계를 실질적 이점으로 전환&lt;/strong>: 기존 Linear Attention은 계산 복잡도를 (O(n^2))에서 (O(n))으로 낮추었으나, &lt;em>cumsum&lt;/em> 연산의 병목으로 인해 &lt;em>causal&lt;/em> 설정에서는 성능 개선이 한정적이었습니다. Lightning Attention-2는 &amp;ldquo;타일링(Tiling)&amp;rdquo; 기법과 &lt;strong>블록 기반 계산 전략&lt;/strong>을 도입하여 이 한계를 해결했습니다.&lt;/p></description></item><item><title>MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts</title><link>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04081/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-17-paper-2401.04081/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.04081">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적-지점">&lt;strong>논문의 강점과 독창적 지점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>혁신적 구조&lt;/strong>:&lt;br>
이 논문은 &lt;em>State Space Models (SSMs)&lt;/em> 기반인 &lt;strong>Mamba&lt;/strong>와 &lt;em>Mixture of Experts (MoE)&lt;/em> 방식을 결합한 &lt;strong>MoE-Mamba&lt;/strong> 모델을 제안합니다. 이를 통해 SSM의 선형 시간 복잡도와 MoE의 효율적 확장성을 동시에 확보했습니다.&lt;/p></description></item><item><title>DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.08671/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.08671/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.08671">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="요약">요약&lt;/h3>
&lt;p>&lt;strong>논문의 제목:&lt;/strong> DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference&lt;br>
&lt;strong>핵심 기여:&lt;/strong>&lt;/p></description></item><item><title>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</title><link>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.09670v3/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.09670v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.09670v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-내용-및-독창성">논문의 핵심 내용 및 독창성&lt;/h3>
&lt;p>&lt;strong>논문 제목&lt;/strong>: &lt;strong>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving&lt;/strong>&lt;/p></description></item><item><title>Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads</title><link>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.11181v1/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.11181v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.11181v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="결론-tetriinfer의-주요-기여-및-핵심-아이디어">결론: TetriInfer의 주요 기여 및 핵심 아이디어&lt;/h3>
&lt;p>TetriInfer는 대규모 언어 모델(LLM)의 추론 과정에서 발생하는 **간섭 문제(interference)**를 해결하기 위해 설계된 클라우드 기반 추론 시스템입니다. 주요 강점은 **사전 단계(prefill)**와 **디코드 단계(decode)**의 분리(disaggregation)와, 이를 통해 효율적인 자원 활용을 달성했다는 점입니다. 이를 기반으로 **TTFT(Time-To-First-Token)**를 97%, **작업 완료 시간(JCT)**을 47% 단축하고, **자원 효율성(performance per dollar)**을 최대 2.4배 개선했습니다.&lt;/p></description></item><item><title>INFERFLOW: AN EFFICIENT AND HIGHLY CONFIG URABLE INFERENCE ENGINE FOR LARGE LANGUAGE MODELS</title><link>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.08294/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.08294/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.08294">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;h4 id="결론-요약">결론 요약&lt;/h4>
&lt;p>논문 &lt;strong>Inferflow&lt;/strong>는 대규모 언어 모델(LLM)을 위한 효율적이고 확장 가능한 추론 엔진으로, 3.5비트 양자화 및 하이브리드 모델 분할과 같은 독창적인 기술을 제안하여 기존 엔진 대비 성능(추론 속도, 메모리 사용량)과 유연성을 개선했습니다. 주요 강점은 &lt;strong>구성 파일 기반의 모듈화된 프레임워크&lt;/strong>, &lt;strong>혼합 GPU 분할&lt;/strong>, &lt;strong>동적 배치 처리&lt;/strong>로 새로운 모델 도입과 실시간 서비스 환경에서의 높은 효율성을 제공합니다.&lt;/p></description></item><item><title>MEDUSA: Simple LLMInference Acceleration Framework with Multiple Decoding Heads</title><link>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.10774/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-16-paper-2401.10774/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.10774">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="핵심-요약">핵심 요약&lt;/h3>
&lt;p>&lt;strong>논문의 제목: &amp;ldquo;MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads&amp;rdquo;&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 186 함수는 무명 네임스페이스에 두는 것을 선호하세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-tow186/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-tow186/</guid><description>&lt;p>원문 게시: 2020년 11월 5일, 주간 팁 #186&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:jdennett@google.com">James Dennett&lt;/a>, &lt;a href="mailto:jrennie@google.com">Jason Rennie&lt;/a>&lt;/em>&lt;/p>
&lt;p>최종 수정: 2020-11-05&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/186">abseil.io/tips/186&lt;/a>&lt;/p>
&lt;p>&lt;em>&amp;ldquo;모든 것은 가능한 한 단순해야 하지만, 지나치게 단순해서는 안 된다.&amp;rdquo; - 아인슈타인의 생각을 해석한 로저 세션스&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 187 std::unique_ptr Must Be Moved"</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-tow187/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-tow187/</guid><description>&lt;p>원문 게시: 2020년 11월 5일, 주간 팁 #187&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:asoffer@google.com">Andy Soffer&lt;/a>&lt;/em>&lt;/p>
&lt;p>최종 수정: 2020-11-05&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/187">abseil.io/tips/187&lt;/a>&lt;/p>
&lt;p>&lt;em>“첫 번째 장에 &lt;code>std::unique_ptr&lt;/code>가 벽에 걸려 있다면, 두 번째 또는 세 번째 장에서는 반드시 이동되어야 합니다. 이동하지 않을 거라면 처음부터 거기 있으면 안 됩니다.” ~ 안톤 체호프의 말에 약간의 변형을 가하여&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 188 스마트 포인터를 함수 매개변수로 사용할 때 주의하세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-tow188/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-tow188/</guid><description>&lt;p>원문 게시: 2020년 12월 10일, 주간 팁 #188&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:krzysio@google.com">Krzysztof Kosiński&lt;/a>&lt;/em>&lt;/p>
&lt;p>최종 수정: 2020-12-10&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/188">abseil.io/tips/188&lt;/a>&lt;/p>
&lt;h2 id="이-코드의-문제는-무엇일까요">이 코드의 문제는 무엇일까요?&lt;/h2>
&lt;pre class="prettyprint lang-cpp bad-code">
bool CanYouPetTheDog(const std::shared_ptr&amp;lt;Dog&amp;gt;&amp; dog,
 absl::Duration min_delay) {
 return dog-&amp;gt;GetLastPetTime() + min_delay &amp;lt; absl::Now();
}
&lt;/pre>
&lt;p>&lt;code>CanYouPetTheDog&lt;/code> 함수는 &lt;code>dog&lt;/code> 매개변수의 소유권을 변경하지 않습니다. 그럼에도 불구하고, 함수의 시그니처는 &lt;code>dog&lt;/code>가 반드시 &lt;code>std::shared_ptr&lt;/code>에 저장되어야 함을 요구합니다. 이는 함수의 동작에 필요하지 않은 특정 소유권 모델에 대한 불필요한 의존성을 초래합니다. 이러한 의존성은 호출자가 &lt;code>std::unique_ptr&lt;/code>과 같은 다른 소유권 모델을 사용하거나 객체를 스택에 생성하는 것을 방해합니다.&lt;/p></description></item><item><title>Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.02057/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.02057/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.02057">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 병렬 디코딩&lt;/strong>: LOOKAHEAD DECODING은 기존의 자동회귀 디코딩의 한계를 극복하기 위해 설계되었으며, 여러 토큰을 병렬로 생성하면서도 정확성을 유지하는 점이 강점입니다. 이는 계산 자원을 효율적으로 활용하여 디코딩 속도를 최대 1.8배(단일 GPU 기준), 다중 GPU에서 최대 4배까지 증가시킵니다.&lt;/p></description></item><item><title>Decoding Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.01528/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.01528/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.01528">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점과-독창성">강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 추론 가속화&lt;/strong>&lt;br>
이 논문은 대규모 언어 모델(LLM)의 추론 속도를 높이기 위해 &lt;strong>Speculative Decoding&lt;/strong> 방식을 체계적으로 분석하고 최적화하는 데 초점을 맞추고 있습니다. 기존의 초안 모델 설계는 정확성 위주였으나, 이 논문은 &lt;strong>초안 모델의 설계 방향을 폭넓고 얕은 구조로 전환&lt;/strong>하여 추론 효율성을 획기적으로 높였습니다.&lt;/p></description></item><item><title>Efficient Prompt Caching via Embedding Similarity</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.01173/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.01173/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.01173">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;h4 id="강점">강점:&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 프롬프트 캐싱 문제 해결&lt;/strong>:&lt;/p></description></item><item><title>FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2401.14112/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2401.14112/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.14112">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>FP6 기반의 효율적인 양자화 지원&lt;/strong>:&lt;/p></description></item><item><title>TP-Aware Dequantization</title><link>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.04925/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-15-paper-2402.04925/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.04925">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약과-분석">논문 요약과 분석&lt;/h3>
&lt;h4 id="1-핵심-요약">1. &lt;strong>핵심 요약&lt;/strong>&lt;/h4>
&lt;p>이 논문에서는 &lt;strong>TP-Aware Dequantization&lt;/strong>이라는 새로운 방법을 제안하여 대형 언어 모델(LLM)의 분산 추론 과정에서 지연(latency)을 줄이는 최적화를 소개합니다. 이 방법은 텐서 병렬(Tensor Parallel, TP) 환경에서 **데이터 국지성(data locality)**을 보장하고 **전역 통신(global communication)**을 줄이는 방식으로 작동합니다. 이를 통해 NVIDIA A100, H100 DGX 시스템에서 최대 1.81배의 속도 향상을 달성하였습니다.&lt;/p></description></item><item><title>Abseil Tip 116 함수 인자에서 참조 사용 시 주의사항</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-tow116/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-tow116/</guid><description>&lt;h3 id="tip-of-the-week-116-함수-인자에서-참조-사용-시-주의사항">Tip of the Week #116: 함수 인자에서 참조 사용 시 주의사항&lt;/h3>
&lt;p>작성자: &lt;a href="mailto:pilki@google.com">Alex Pilkiewicz&lt;/a>&lt;br>
최초 작성일: 2016년 5월 26일&lt;br>
업데이트: 2020년 6월 1일&lt;br>
원문 링크: &lt;a href="https://abseil.io/tips/116">abseil.io/tips/116&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>그림에서 이미지로, 이미지에서 텍스트로, 텍스트에서 음성으로, 상상의 포인터가 하나의 고유한 공간을 안정화하려고 시도하며 참조 체계를 가리키고 고정합니다. — 미셸 푸코, 《이것은 파이프가 아니다》&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 165 초기화 구문을 포함한 if와 switch 문 사용하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-tow165/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-tow165/</guid><description>&lt;h3 id="tip-of-the-week-165-초기화-구문을-포함한-if와-switch-문-사용하기">Tip of the Week #165: 초기화 구문을 포함한 &lt;code>if&lt;/code>와 &lt;code>switch&lt;/code> 문 사용하기&lt;/h3>
&lt;p>작성자: &lt;a href="mailto:tkoeppe@google.com">Thomas Köppe&lt;/a>&lt;br>
최초 작성일: 2019년 8월 17일&lt;br>
업데이트: 2020년 1월 17일&lt;br>
원문 링크: &lt;a href="https://abseil.io/tips/165">abseil.io/tips/165&lt;/a>&lt;/p></description></item><item><title>Abseil Tip 181 StatusOr&lt;T> 값 접근하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-tow181/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-tow181/</guid><description>&lt;h3 id="tip-of-the-week-181-statusort-값-접근하기">Tip of the Week #181: &lt;code>StatusOr&amp;lt;T&amp;gt;&lt;/code> 값 접근하기&lt;/h3>
&lt;p>작성자: &lt;a href="mailto:msheely@google.com">Michael Sheely&lt;/a>&lt;br>
최초 작성일: 2020년 7월 9일&lt;br>
업데이트: 2020년 9월 2일&lt;br>
원문 링크: &lt;a href="https://abseil.io/tips/181">abseil.io/tips/181&lt;/a>&lt;/p>
&lt;hr>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;code>StatusOr&amp;lt;Readability&amp;gt;&lt;/code>: 선택하지 않아도 됩니다!&lt;/p></description></item><item><title>Abseil Tip 76 absl::Status 사용하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-tow76/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-tow76/</guid><description>&lt;h3 id="tip-of-the-week-76-abslstatus-사용하기">Tip of the Week #76: &lt;code>absl::Status&lt;/code> 사용하기&lt;/h3>
&lt;p>작성자: &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;br>
최초 작성일: 2014년 5월 4일&lt;br>
업데이트: 2020년 2월 6일&lt;br>
원문 링크: &lt;a href="https://abseil.io/tips/76">abseil.io/tips/76&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="abslstatus를-사용해야-하는-이유와-사용-시-유의점">&lt;code>absl::Status&lt;/code>를 사용해야 하는 이유와 사용 시 유의점&lt;/h3>
&lt;p>&lt;code>absl::Status&lt;/code>를 언제, 어떻게 사용해야 하는지 궁금한 분들을 위해 몇 가지 이유와 고려 사항을 소개합니다.&lt;/p></description></item><item><title>Benchmarking and Dissecting the Nvidia Hopper GPU Architecture</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.13499v1/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.13499v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.13499v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-요약">논문의 주요 내용 요약&lt;/h3>
&lt;p>이 논문은 Nvidia Hopper GPU 아키텍처에 대한 심층적인 벤치마킹과 분석을 수행했습니다. 특히 Hopper 아키텍처의 다음 세 가지 주요 특징을 다루었습니다:&lt;/p></description></item><item><title>Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inferenc</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.09398/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.09398/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.09398">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약">논문의 요약&lt;/h3>
&lt;p>논문 &amp;ldquo;Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference&amp;quot;는 대규모 언어 모델(LLM)의 추론에서 발생하는 키-값(KV) 캐시 메모리 병목 문제를 해결하기 위해 **LESS(Low-rank Embedding Sidekick with Sparse policy)**를 제안합니다. LESS는 기존 희소 정책 기반 KV 캐시 알고리즘에 저차원 임베딩을 추가해 정보 손실을 최소화하면서 메모리 효율을 크게 향상시킵니다. 주요 기여는 다음과 같습니다:&lt;/p></description></item><item><title>Hydragen: High-Throughput LLM Inference with Shared Prefixes</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.05099/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.05099/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.05099">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-요약">&lt;strong>논문의 핵심 요약&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>Hydragen&lt;/strong>이라는 새로운 방법을 소개하여, 공통 프리픽스를 공유하는 텍스트 배치에 대한 대규모 언어 모델(LLM) 추론을 최적화합니다. 이 방법은 특히 &lt;strong>프리픽스와 서픽스에 대한 어텐션 계산을 분리&lt;/strong>하고, &lt;strong>인터-시퀀스 배칭&lt;/strong>을 통해 하드웨어 효율성을 극대화하여 GPU 메모리 대역폭 병목현상을 줄입니다.&lt;/p></description></item><item><title>QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.10076/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.10076/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.10076">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-강점-독창성">논문 요약 및 강점, 독창성&lt;/h3>
&lt;p>&lt;strong>강점과 독창성:&lt;/strong>&lt;/p></description></item><item><title>RelayAttention for Efficient Large Language Model Serving with Long System Prompts</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.14808/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2402.14808/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.14808">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약과-분석">논문 요약과 분석&lt;/h3>
&lt;h4 id="핵심-내용-요약">&lt;strong>핵심 내용 요약&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>문제 정의&lt;/strong>: 긴 시스템 프롬프트를 포함하는 대규모 언어 모델(LLM)의 추론 효율성 문제. 기존 접근법은 메모리 접근의 비효율성으로 인해 처리 속도와 레이턴시가 저하됨.&lt;/li>
&lt;li>&lt;strong>기술적 기여&lt;/strong>: RelayAttention이라는 새로운 알고리즘을 제안하여 공유 시스템 프롬프트의 캐시된 키-값(KV) 상태를 한 번만 메모리에서 읽도록 하여 불필요한 메모리 접근 제거.&lt;/li>
&lt;li>&lt;strong>결과&lt;/strong>: RelayAttention은 모델 재훈련 없이도 처리 효율성을 크게 개선했으며, 최대 2.2배 요청 처리율 증가 및 최대 76%의 처리 시간 감소를 달성함.&lt;/li>
&lt;/ul>
&lt;h4 id="논문의-강점">&lt;strong>논문의 강점&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>독창적 접근&lt;/strong>: 기존 FlashAttention 및 PagedAttention과는 달리, RelayAttention은 시스템 프롬프트의 불필요한 메모리 접근을 제거하여 근본적인 I/O 병목 문제를 해결.&lt;/li>
&lt;li>&lt;strong>폭넓은 호환성&lt;/strong>: 모델 재훈련 없이 기존 추론 시스템(vLLM)과 쉽게 통합 가능.&lt;/li>
&lt;li>&lt;strong>효율성 입증&lt;/strong>: 여러 GPU 환경(A40, A100) 및 다양한 모델(Llama2-7B, Llama-30B)에서 성능 개선을 실험적으로 검증.&lt;/li>
&lt;li>&lt;strong>이론적 뒷받침&lt;/strong>: 메모리 접근 감소와 관련된 이론적 속도 향상을 수식으로 분석하여 신뢰성을 높임.&lt;/li>
&lt;/ol>
&lt;h4 id="핵심-알고리즘-설명-예시-입력-기반">&lt;strong>핵심 알고리즘 설명 (예시 입력 기반)&lt;/strong>&lt;/h4>
&lt;h5 id="기존-문제">&lt;strong>기존 문제:&lt;/strong>&lt;/h5>
&lt;ul>
&lt;li>긴 시스템 프롬프트를 포함한 배치 요청 처리 시, 각 요청마다 시스템 프롬프트의 KV 상태를 메모리에서 반복적으로 읽어 I/O 병목 발생.&lt;/li>
&lt;/ul>
&lt;h5 id="relayattention-프로세스">&lt;strong>RelayAttention 프로세스&lt;/strong>:&lt;/h5>
&lt;ol>
&lt;li>&lt;strong>입력&lt;/strong>:
&lt;ul>
&lt;li>시스템 프롬프트 (&lt;code>Shared KV&lt;/code>) 길이: 512&lt;/li>
&lt;li>사용자 프롬프트 (&lt;code>User KV&lt;/code>) 길이: 128&lt;/li>
&lt;li>생성 토큰 길이: 64&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>단계&lt;/strong>:
&lt;ol>
&lt;li>&lt;strong>시스템 주의(System Attention)&lt;/strong>:
&lt;ul>
&lt;li>시스템 프롬프트와 Query를 매핑하여 중간 출력을 계산.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>문맥 주의(Context Attention)&lt;/strong>:
&lt;ul>
&lt;li>사용자 프롬프트와 현재까지 생성된 토큰에 대해 주의 계산.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>릴레이 융합(Relay Fusion)&lt;/strong>:
&lt;ul>
&lt;li>시스템 주의 결과와 문맥 주의 결과를 가중 평균(Convex Combination)하여 최종 결과를 생성.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>산출물&lt;/strong>:
&lt;ul>
&lt;li>현재 배치의 모든 요청에서 공유 시스템 프롬프트를 단 한 번 메모리에서 읽고, 최적화된 토큰 생성 속도를 달성.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h4 id="결과-분석">&lt;strong>결과 분석&lt;/strong>&lt;/h4>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;strong>GPU&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>vLLM&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>vLLM-PS&lt;/strong>&lt;/th>
 &lt;th>&lt;strong>vLLM-RA&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>A40 (5-shot)&lt;/strong>&lt;/td>
 &lt;td>2660초&lt;/td>
 &lt;td>850초 (↓68%)&lt;/td>
 &lt;td>641초 (↓76%)&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>A100 (5-shot)&lt;/strong>&lt;/td>
 &lt;td>1308초&lt;/td>
 &lt;td>432초 (↓67%)&lt;/td>
 &lt;td>316초 (↓76%)&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>RelayAttention은 긴 프롬프트에서도 처리 효율을 극대화하며, 특히 배치 크기가 클수록(예: 32) 더 큰 성능 향상을 보임.&lt;/p></description></item><item><title>TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2102.07988v2/</link><pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-14-paper-2102.07988v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2102.07988v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창성-요약">강점 및 독창성 요약&lt;/h3>
&lt;p>이 논문 &lt;strong>TeraPipe&lt;/strong>는 Transformer 기반 언어 모델(Large Language Models, LLMs)의 훈련을 가속화하기 위한 &lt;strong>Token-Level Pipeline Parallelism&lt;/strong>을 제안합니다. 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.15805/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.15805/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.15805">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창성">강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>동적 맥락 정리 (Dynamic Context Pruning):&lt;/strong>&lt;/p></description></item><item><title>Fast Transformer Decoding: One Write-Head is All You Need</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-1911.02150/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-1911.02150/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1911.02150">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>독창적인 접근법: Multi-query Attention&lt;/strong>&lt;br>
이 논문은 Transformer 모델의 효율성을 높이기 위해 기존 Multi-head Attention을 Multi-query Attention으로 대체하는 혁신적인 방식을 제안합니다. Multi-query Attention에서는 모든 attention head가 동일한 key-value 세트를 공유합니다. 이를 통해 메모리 대역폭 요구사항이 크게 감소하며, 특히 **증분 디코딩(incremental decoding)**에서 성능을 극적으로 향상시킵니다.&lt;/p></description></item><item><title>GQA:Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.13245/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.13245/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.13245">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&amp;rdquo;**를 분석한 내용을 아래에 정리하였습니다.&lt;/p></description></item><item><title>H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2306.14048/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2306.14048/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2306.14048">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 KV 캐시 관리&lt;/strong>:&lt;/p></description></item><item><title>Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.17118/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2305.17118/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.17118">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점">논문의 강점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 메모리 관리&lt;/strong>:&lt;/p></description></item><item><title>Token Merging: Your ViT But Faster</title><link>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2210.09461/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-13-paper-2210.09461/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2210.09461">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문의 주요 내용과 분석을 다음과 같이 요약하겠습니다:&lt;/p></description></item><item><title>Abseil Tip 140 상수(Constant) 처리 안전한 관용구</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow140/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow140/</guid><description>&lt;h1 id="주간-팁-140-상수constant-처리-안전한-관용구">주간 팁 #140: 상수(Constant) 처리: 안전한 관용구&lt;/h1>
&lt;p>&lt;strong>원래 TotW #140으로 2017년 12월 8일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:gmatta@gmail.com">Matt Armstrong&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020년 5월 6일 업데이트됨&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 163 std::optional 매개변수 전달하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow163/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow163/</guid><description>&lt;h1 id="주간-팁-163-codestdoptionalcode-매개변수-전달하기">주간 팁 #163: &lt;code>&amp;lt;code&amp;gt;std::optional&amp;lt;/code&amp;gt;&lt;/code> 매개변수 전달하기&lt;/h1>
&lt;p>&lt;strong>원래 TotW #163으로 2019년 7월 11일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:puddles@google.com">Ian Eldred Pudney&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/163">abseil.io/tips/163&lt;/a>&lt;/p>
&lt;p>&lt;em>Null 참조는 정말로 10억 달러의 실수일까요?&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 171 Sentinel 값 피하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow171/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow171/</guid><description>&lt;h1 id="주간-팁-171-sentinel-값-피하기">주간 팁 #171: Sentinel 값 피하기&lt;/h1>
&lt;p>&lt;strong>원래 TotW #171로 2019년 11월 8일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:hwright@google.com">Hyrum Wright&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/171">abseil.io/tips/171&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Sentinel_value">Sentinel 값&lt;/a>은 특정 컨텍스트에서 특별한 의미를 가지는 값입니다. 예를 들어, 다음과 같은 API를 생각해봅시다:&lt;/p></description></item><item><title>Abseil Tip 172 지정 초기화자(Designated Initializers)</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow172/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow172/</guid><description>&lt;h1 id="주간-팁-172-지정-초기화자designated-initializers">주간 팁 #172: 지정 초기화자(Designated Initializers)&lt;/h1>
&lt;p>&lt;strong>원래 TotW #172로 2019년 12월 11일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:jacobsa@google.com">Aaron Jacobs&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/172">abseil.io/tips/172&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://en.cppreference.com/w/cpp/language/aggregate_initialization#Designated_initializers">지정 초기화자&lt;/a>는 구조체의 내용을 간결하면서도 읽기 쉽고 유지보수 가능한 방식으로 지정하기 위한 C++20 표준의 문법입니다. 다음과 같은 반복적인 코드를 작성하는 대신:&lt;/p></description></item><item><title>Abseil Tip 173 옵션 구조체로 인수 래핑하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow173/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow173/</guid><description>&lt;h1 id="주간-팁-173-옵션-구조체로-인수-래핑하기">주간 팁 #173: 옵션 구조체로 인수 래핑하기&lt;/h1>
&lt;p>&lt;strong>원래 TotW #173으로 2019년 12월 19일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:jbandela@google.com">John Bandela&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/173">abseil.io/tips/173&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>상자는 없었고, 가방도 없었어요. 그는 머리가 아플 때까지 고민했어요.&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 175 C++14와 C++17의 리터럴 상수 변경 사항</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow175/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow175/</guid><description>&lt;h1 id="주간-팁-175-c14와-c17의-리터럴-상수-변경-사항">주간 팁 #175: C++14와 C++17의 리터럴 상수 변경 사항&lt;/h1>
&lt;p>&lt;strong>원래 TotW #175으로 2020년 1월 30일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:jdennett@google.com">James Dennett&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/175">abseil.io/tips/175&lt;/a>&lt;/p></description></item><item><title>Abseil Tip 176 출력 매개변수 대신 반환 값을 선호하세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow176/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow176/</guid><description>&lt;h1 id="주간-팁-176-출력-매개변수-대신-반환-값을-선호하세요">주간 팁 #176: 출력 매개변수 대신 반환 값을 선호하세요&lt;/h1>
&lt;p>&lt;strong>원래 TotW #176으로 2020년 3월 12일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:edechamps@google.com">Etienne Dechamps&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/176">abseil.io/tips/176&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="문제">문제&lt;/h2>
&lt;p>다음과 같은 함수를 고려해봅시다:&lt;/p></description></item><item><title>Abseil Tip 177 할당 가능성과 데이터 멤버 타입</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow177/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow177/</guid><description>&lt;h1 id="주간-팁-177-할당-가능성과-데이터-멤버-타입">주간 팁 #177: 할당 가능성과 데이터 멤버 타입&lt;/h1>
&lt;p>&lt;strong>원래 TotW #177로 2020년 4월 6일 게시됨&lt;/strong>&lt;br>
&lt;strong>작성자: &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/strong>&lt;br>
&lt;strong>2020-04-06 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/177">abseil.io/tips/177&lt;/a>&lt;/p>
&lt;hr>
&lt;p>타입을 구현할 때는 먼저 타입 설계에 대해 결정하세요. 구현 세부사항보다 API를 우선시하는 것이 중요합니다. 이를 잘 보여주는 일반적인 예는 타입의 할당 가능성과 데이터 멤버의 한정자 간의 트레이드오프입니다.&lt;/p></description></item><item><title>Abseil Tip 5 사라지는 객체의 함정</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-tow5/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-tow5/</guid><description>&lt;h1 id="주간-팁-5-사라지는-객체의-함정">주간 팁 #5: 사라지는 객체의 함정&lt;/h1>
&lt;p>&lt;strong>원래 TotW #5로 2012년 6월 26일 게시됨&lt;/strong>&lt;br>
&lt;strong>2020년 6월 1일 업데이트됨&lt;/strong>&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/5">abseil.io/tips/5&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;무언가를 잃고 나서야 그것의 소중함을 알게 된다.&amp;rdquo;&lt;/em>&lt;br>
&lt;em>- 신데렐라(Cinderella)&lt;/em>&lt;/p></description></item><item><title>FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.18789v1/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.18789v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.18789v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>이 논문 **H2O (Heavy-Hitter Oracle)**는 대형 언어 모델(LLM)의 추론 과정에서 발생하는 &lt;strong>KV 캐시 메모리 문제&lt;/strong>를 효율적으로 해결하기 위해 새로운 접근법을 제안합니다. 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>GLM-130B: An Open Bilingual Pre-trained Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2210.02414v2/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2210.02414v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2210.02414v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;GLM-130B: An Open Bilingual Pre-Trained Model&amp;rdquo;**에 대한 분석은 아래와 같습니다:&lt;/p></description></item><item><title>LLM Inference Unveiled: Survey and Roofline Model Insights</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.16363/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.16363/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.16363">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점">논문의 강점&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>독창적인 Roofline 모델 분석&lt;/strong>: 이 논문은 Roofline 모델을 사용하여 LLM(대형 언어 모델)의 추론 효율성을 평가하고 병목 현상을 분석하는 데 초점을 맞추고 있습니다. 이는 기존의 일반적인 리뷰 논문과 차별화되며, 모델의 메모리 및 계산 병목을 체계적으로 진단합니다.&lt;/li>
&lt;li>&lt;strong>다양한 최적화 전략 통합&lt;/strong>: 모델 압축, 빠른 디코딩 알고리즘, 시스템 수준 및 하드웨어 수준 최적화를 모두 포괄적으로 다룹니다. 이를 통해 연구자와 개발자가 실질적인 개선 방법을 쉽게 참고할 수 있습니다.&lt;/li>
&lt;li>&lt;strong>실제 도구 제공&lt;/strong>: LLM-Viewer라는 오픈소스 분석 도구를 통해 LLM 성능 및 효율성 분석을 지원합니다. 이 도구는 배치 크기, 시퀀스 길이 등의 다양한 설정에서 최적화를 테스트할 수 있도록 설계되었습니다.&lt;/li>
&lt;li>&lt;strong>포괄적인 문헌 리뷰&lt;/strong>: 이 논문은 최근 LLM 추론 효율성을 개선하기 위한 기술 동향을 체계적으로 정리하였으며, 다양한 사례와 방법론을 구체적으로 다룹니다.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="핵심-알고리즘의-예시-과정">핵심 알고리즘의 예시 과정&lt;/h3>
&lt;p>핵심 알고리즘인 Roofline 모델을 기반으로 한 분석 과정을 간단한 예시를 통해 설명하겠습니다.&lt;/p></description></item><item><title>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2310.05736/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2310.05736/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.05736">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2310.01801/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2310.01801/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.01801">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">&lt;strong>논문 요약 및 분석&lt;/strong>&lt;/h3>
&lt;p>논문 **&amp;ldquo;Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs&amp;rdquo;**는 대규모 언어 모델(LLMs)의 효율적인 추론을 위한 &lt;strong>KV 캐시 압축(FastGen)&lt;/strong> 방법을 제안합니다. 이 방법은 메모리 사용량을 줄이고 추론 속도를 향상시키며, 기존의 캐시 압축 방식과 비교하여 &lt;strong>질적 손실 없이 메모리 효율성을 높이는 점&lt;/strong>에서 독창적입니다.&lt;/p></description></item><item><title>No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.18096/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2402.18096/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.18096">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-상세-분석">논문의 상세 분석&lt;/h3>
&lt;p>&lt;strong>1. 요약과 강점&lt;/strong>&lt;/p></description></item><item><title>Scaling Instruction-Finetuned Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2210.11416v5/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-12-paper-2210.11416v5/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2210.11416v5">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약과-분석">논문 요약과 분석&lt;/h3>
&lt;p>이 논문은 구글 연구팀이 작성한 **&amp;ldquo;Scaling Instruction-Finetuned Language Models&amp;rdquo;**로, 대규모 언어 모델(예: PaLM, T5)을 &lt;strong>Instruction Fine-Tuning&lt;/strong> 기법으로 미세조정하여 &lt;strong>새로운 작업&lt;/strong>에 대한 일반화 능력을 크게 향상시키는 방법을 연구한 결과를 다룹니다.&lt;/p></description></item><item><title> Benchmarks as Limits to Arbitrage: Understanding the Low-Volatility Anomaly</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-Volatility2/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-Volatility2/</guid><description>&lt;p>&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1585031">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-및-독창성">논문의 주요 내용 및 독창성&lt;/h3>
&lt;p>&lt;strong>논문 제목&lt;/strong>: Benchmarks as Limits to Arbitrage: Understanding the Low-Volatility Anomaly&lt;br>
&lt;strong>저자&lt;/strong>: Malcolm Baker, Brendan Bradley, Jeffrey Wurgler&lt;br>
&lt;strong>출판년도&lt;/strong>: 2011&lt;/p></description></item><item><title>Abseil Tip 108 std::bind를 피하세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow108/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow108/</guid><description>&lt;p>&lt;strong>Roman Perepelitsa&lt;/strong> &lt;a href="mailto:roman.perepelitsa@gmail.com">(roman.perepelitsa@gmail.com)&lt;/a> 작성&lt;br>
최초 게시일: 2016년 1월 7일&lt;br>
최종 업데이트: 2020년 8월 19일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/108">abseil.io/tips/108&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="stdbind를-피하세요">&lt;code>std::bind&lt;/code>를 피하세요&lt;/h2>
&lt;p>이 팁에서는 코드 작성 시 &lt;code>std::bind()&lt;/code>를 피해야 하는 이유를 요약합니다.&lt;/p></description></item><item><title>Abseil Tip 132 Avoid Redundant Map Lookups</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow132/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow132/</guid><description>&lt;p>&lt;strong>Matt Kulukundis&lt;/strong> &lt;a href="mailto:kfm@google.com">(kfm@google.com)&lt;/a> 작성&lt;br>
최초 게시일: 2017년 3월 30일&lt;br>
최종 업데이트: 2019년 11월 25일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/132">abseil.io/tips/132&lt;/a>&lt;/p>
&lt;hr>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;strong>&amp;ldquo;이곳이 바로 스나크가 있을 곳이야!&amp;rdquo;&lt;/strong> 선장이 외쳤다.&lt;br>
그는 조심스럽게 그의 승무원들을 상륙시키며,&lt;br>
조수의 꼭대기에 머물며 머리카락을 손가락으로 얽어 쥐고 있었다.&lt;br>&lt;/p></description></item><item><title>Abseil Tip 146 기본 초기화와 값 초기화</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow146/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow146/</guid><description>&lt;p>&lt;strong>Dominic Hamon&lt;/strong> &lt;a href="mailto:dominic@google.com">(dominic@google.com)&lt;/a> 작성&lt;br>
최초 게시일: 2018년 4월 19일&lt;br>
최종 업데이트: 2020년 4월 6일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/146">abseil.io/tips/146&lt;/a>&lt;/p>
&lt;hr>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;em>&amp;ldquo;성공으로 가는 길은 항상 공사 중이다.&amp;rdquo;&lt;/em>&lt;br>
– 릴리 톰린&lt;/p></description></item><item><title>Abseil Tip 161 좋은 지역 변수와 나쁜 지역 변수</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow161/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow161/</guid><description>&lt;p>&lt;strong>James Dennett&lt;/strong> &lt;a href="mailto:jdennett@google.com">(jdennett@google.com)&lt;/a> 작성&lt;br>
최초 게시일: 2019년 4월 16일&lt;br>
최종 업데이트: 2020년 4월 6일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/161">abseil.io/tips/161&lt;/a>&lt;/p>
&lt;hr>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;em>&amp;ldquo;우리는 전역적으로 당황할 수 있지만, 지역적으로 고통받는다.&amp;rdquo;&lt;/em>&lt;br>
– 조너선 프랜즌&lt;/p></description></item><item><title>Abseil Tip 166 복사가 복사가 아닐 때</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow166/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow166/</guid><description>&lt;p>&lt;strong>Richard Smith&lt;/strong> &lt;a href="mailto:richardsmith@google.com">(richardsmith@google.com)&lt;/a> 작성&lt;br>
최초 게시일: 2019년 8월 28일&lt;br>
최종 업데이트: 2020년 4월 6일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/166">abseil.io/tips/166&lt;/a>&lt;/p>
&lt;hr>

 &lt;blockquote
 class="border-primary bg-muted/30 hover:bg-muted/40 my-6 rounded-r-lg border-l-4 py-4 pr-4 pl-6 italic transition-all duration-200 ease-out hover:translate-x-1">
 &lt;div class="prose prose-sm max-w-none">
 &lt;p>&lt;em>&amp;ldquo;필요하지 않은 한 존재를 늘리지 말라.&amp;rdquo;&lt;/em> – 윌리엄 오컴&lt;br>
&lt;em>&amp;ldquo;어디로 가야 할지 모르면 잘못된 길로 갈 가능성이 높다.&amp;rdquo;&lt;/em> – 테리 프래쳇&lt;/p></description></item><item><title>Abseil Tip 168 inline 변수</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-tow168/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-tow168/</guid><description>&lt;p>&lt;strong>James Dennett&lt;/strong> &lt;a href="mailto:jdennett@google.com">(jdennett@google.com)&lt;/a> 작성&lt;br>
최초 게시일: 2019년 9월 12일&lt;br>
최종 업데이트: 2020년 4월 6일&lt;/p>
&lt;p>&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/168">abseil.io/tips/168&lt;/a>&lt;/p>
&lt;hr>
&lt;p>C++17의 &lt;code>inline&lt;/code> 변수를 사용하여 헤더 파일에서 문자열 상수를 정의하는 안전한 방법은 다음과 같습니다:&lt;/p></description></item><item><title>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.05100v4/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.05100v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.05100v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 BLOOM(176B 파라미터 오픈소스 다국어 언어 모델)에 대한 연구를 담고 있습니다. 아래에서 논문의 강점, 독창적인 점, 알고리즘의 핵심 흐름, 그리고 한계점을 자세히 정리하겠습니다.&lt;/p></description></item><item><title>CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2310.07240/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2310.07240/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.07240">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving&amp;rdquo;**에 대한 분석을 아래와 같이 제공합니다.&lt;/p></description></item><item><title>CHAI: Clustered Head Attention for Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2403.08058/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2403.08058/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.08058">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-제목-chai-clustered-head-attention-for-efficient-llm-inference">논문의 제목: &lt;strong>CHAI: Clustered Head Attention for Efficient LLM Inference&lt;/strong>&lt;/h3>
&lt;h4 id="논문의-주요-내용-요약">&lt;strong>논문의 주요 내용 요약&lt;/strong>&lt;/h4>
&lt;p>이 논문은 대규모 언어 모델(LLM)의 추론 시 발생하는 메모리 및 계산 병목현상을 줄이기 위해 **Clustered Head Attention (CHAI)**를 제안합니다. CHAI는 다중 헤드 어텐션(MHA)에서 주의할 토큰에 대해 유사한 출력을 생성하는 헤드를 클러스터링하여 메모리 및 계산 요구를 줄입니다.&lt;/p></description></item><item><title>Compressed Context Memory For Online Language Model Interaction</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2312.03414/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2312.03414/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.03414">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점">논문의 강점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적 메모리 관리&lt;/strong>:&lt;/p></description></item><item><title>Galactica: A Large Language Model for Science</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.09085v1/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2211.09085v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.09085v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문에서 다루는 내용인 **&amp;ldquo;Galactica: A Large Language Model for Science&amp;rdquo;**를 바탕으로 요청하신 논문 분석을 다음과 같이 수행했습니다:&lt;/p></description></item><item><title>High Idiosyncratic Volatility and Low Returns: International and Further U.S. Evidence</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-Volatility1/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-Volatility1/</guid><description>&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0304405X08001542">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>이 논문은 고유 변동성이 높은 주식의 낮은 미래 평균 수익률이라는 현상을 국제 시장과 미국 시장에서 종합적으로 분석합니다. 이 연구의 강점과 독창적인 지점은 다음과 같습니다:&lt;/p></description></item><item><title>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2310.06839/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2310.06839/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.06839">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-강점-독창성-알고리즘-예시-한계점">논문의 주요 내용, 강점, 독창성, 알고리즘 예시, 한계점&lt;/h3>
&lt;h4 id="논문의-개요">&lt;strong>논문의 개요&lt;/strong>&lt;/h4>
&lt;p>논문 &amp;ldquo;LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression&amp;quot;은 대규모 언어 모델(LLM)이 긴 문맥을 처리할 때 발생하는 주요 문제(계산 비용, 성능 저하, 위치 편향)를 해결하기 위해 설계된 LongLLMLingua라는 프롬프트 압축 기법을 제안합니다. 이 방법은 압축을 통해 비용을 절감하고, 성능을 향상시키며, 문맥 위치에 따른 성능 편차를 최소화합니다.&lt;/p></description></item><item><title>QAQ: Quality Adaptive Quantization for LLM KV Cache</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2403.04643/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2403.04643/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.04643">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약-강점-독창성-그리고-한계점">논문의 요약, 강점, 독창성, 그리고 한계점&lt;/h3>
&lt;h4 id="논문-요약">&lt;strong>논문 요약&lt;/strong>&lt;/h4>
&lt;p>이 논문은 **QAQ (Quality Adaptive Quantization)**라는 방법을 제안하여 대규모 언어 모델(LLM)의 Key-Value (KV) 캐시 메모리 사용량을 효율적으로 줄이는 방법을 다룹니다. KV 캐시는 LLM의 긴 문맥 처리에서 중요한 메모리 자원이지만, 캐시 크기가 문맥 길이에 비례하여 증가하므로 GPU 메모리에 부담을 줍니다. QAQ는 KV 캐시의 비균일(non-uniform) 양자화 방식을 사용하여 키와 값 벡터의 서로 다른 민감도를 고려하고, 이를 통해 모델 성능을 크게 저하시키지 않으면서 최대 10배의 캐시 압축을 달성합니다.&lt;/p></description></item><item><title>Transformers are Multi-State RNNs</title><link>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2401.06104/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-10-paper-2401.06104/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.06104">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>트랜스포머와 RNN의 연결 제시&lt;/strong>:
이 논문은 트랜스포머를 &amp;ldquo;무한 상태 RNN(MSRNN)&amp;ldquo;으로 재정의함으로써, 전통적으로 개념적으로 구별되어 온 두 아키텍처 간의 연결성을 제시합니다. 이는 두 모델의 근본적인 공통점을 부각시키는 독창적인 접근입니다.&lt;/p></description></item><item><title>Abseil Tip 147 Exhaustive switch 문을 책임감 있게 사용하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-tow147/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-tow147/</guid><description>&lt;h2 id="주간-팁-147-exhaustive-switch-문을-책임감-있게-사용하기">주간 팁 #147: Exhaustive &lt;code>switch&lt;/code> 문을 책임감 있게 사용하기&lt;/h2>
&lt;p>원래 게시일: 2018년 4월 25일&lt;br>
&lt;em>작성자: &lt;a href="mailto:jnewsome@google.com">Jim Newsome&lt;/a>&lt;/em>&lt;br>
최종 업데이트: 2020년 4월 6일&lt;/p>
&lt;p>&lt;strong>바로가기:&lt;/strong> &lt;a href="https://abseil.io/tips/147">abseil.io/tips/147&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="소개">소개&lt;/h3>
&lt;p>&lt;code>-Werror&lt;/code> 컴파일러 플래그를 사용하면 &lt;code>enum&lt;/code> 타입의 값을 대상으로 하는 &lt;code>switch&lt;/code> 문에 &lt;code>default&lt;/code> 레이블이 없을 경우, 모든 열거자가 &lt;code>case&lt;/code> 문으로 처리되지 않으면 컴파일 오류가 발생합니다. 이를 &lt;em>exhaustive&lt;/em> 또는 &lt;em>defaultless&lt;/em> &lt;code>switch&lt;/code> 문이라고 부르기도 합니다.&lt;/p></description></item><item><title>Abseil Tip 158 Abseil 연관 컨테이너와 contains()</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-tow158/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-tow158/</guid><description>&lt;h2 id="주간-팁-158-abseil-연관-컨테이너와-contains">주간 팁 #158: Abseil 연관 컨테이너와 &lt;code>contains()&lt;/code>&lt;/h2>
&lt;p>원래 게시일: 2019년 1월 3일&lt;br>
&lt;em>작성자: &lt;a href="mailto:jdennett@google.com">James Dennett&lt;/a>&lt;/em>&lt;br>
최종 업데이트: 2020년 4월 20일&lt;/p>
&lt;p>&lt;strong>바로가기:&lt;/strong> &lt;a href="https://abseil.io/tips/158">abseil.io/tips/158&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="이-컨테이너에-이-값이-포함되어-있나요">이 컨테이너에 이 값이 포함되어 있나요?&lt;/h3>
&lt;p>C++에서 집합(set)에 특정 값이 포함되어 있는지, 맵(map)에 특정 키가 포함되어 있는지 확인하려면 전통적으로 다음과 같은 다소 장황한 코드를 작성해야 했습니다.&lt;/p></description></item><item><title>Abseil Tip 180 Dangling References(유효하지 않은 참조) 피하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-tow180/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-tow180/</guid><description>&lt;h2 id="주간-팁-180-dangling-references유효하지-않은-참조-피하기">주간 팁 #180: Dangling References(유효하지 않은 참조) 피하기&lt;/h2>
&lt;p>원래 게시일: 2020년 6월 11일&lt;br>
&lt;em>작성자: &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/em>&lt;br>
최종 업데이트: 2020년 6월 11일&lt;/p>
&lt;p>&lt;strong>바로가기:&lt;/strong> &lt;a href="https://abseil.io/tips/180">abseil.io/tips/180&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="소개">소개&lt;/h3>
&lt;p>C++는 다른 많은 언어와 달리, 유효하지 않은 메모리를 참조하는 것을 방지하는 안전 장치가 부족합니다. 이미 &lt;code>delete&lt;/code>된 객체의 포인터를 역참조하거나 스코프에서 벗어난 객체를 참조할 수 있습니다. 클래스 타입조차도 이러한 위험을 내포하고 있습니다. 특히, &lt;code>view&lt;/code>와 &lt;code>span&lt;/code>이라는 이름을 사용하는 객체는 &amp;ldquo;참조 의미론(reference semantics)을 가지며 dangling(유효하지 않음)이 발생할 수 있음&amp;quot;을 나타냅니다. 이러한 타입은 참조 의미론을 가지며, 항상 포인터가 가리키는 데이터를 소유하지 않는다는 점에서 주의가 필요합니다.&lt;/p></description></item><item><title>Abseil Tip 182 정수형 변수를 초기화하세요!</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-tow182/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-tow182/</guid><description>&lt;h2 id="주간-팁-182-정수형-변수를-초기화하세요">주간 팁 #182: 정수형 변수를 초기화하세요!&lt;/h2>
&lt;p>원래 게시일: 2020년 7월 23일&lt;br>
최종 업데이트: 2020년 7월 23일&lt;/p>
&lt;p>&lt;strong>바로가기:&lt;/strong> &lt;a href="https://abseil.io/tips/182">abseil.io/tips/182&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;결정의 순간, 할 수 있는 가장 좋은 일은 옳은 일을 하는 것이고, 그다음으로 좋은 것은 틀린 일을 하는 것이며, 가장 나쁜 것은 아무것도 하지 않는 것이다.&amp;rdquo;&lt;br>
&amp;ndash; 시어도어 루즈벨트&lt;/em>&lt;/p></description></item><item><title>DeepCache: Accelerating Diffusion Models for Free</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2312.00858v2/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2312.00858v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.00858v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.09398/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.09398/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.09398">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 제목은 **&amp;ldquo;Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference&amp;rdquo;**입니다. 논문에서 제안된 LESS(Low-rank Embedding Sidekick with Sparse policy)는 대형 언어 모델(LLM)의 키-값(KV) 캐시 메모리 병목 문제를 해결하기 위해 Sparse(희소) 정책과 Low-rank(저차원) 캐시를 결합한 새로운 방법론입니다. 이제 논문의 주요 강점과 독창성, 알고리즘, 한계점을 자세히 설명하겠습니다.&lt;/p></description></item><item><title>KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.02750/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.02750/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.02750">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점과-독창성">강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적이고 튜닝이 필요 없는 2비트 양자화 알고리즘&lt;/strong>:&lt;/p></description></item><item><title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2401.18079/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2401.18079/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.18079">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization&amp;rdquo;**은 초대형 문맥 창 길이를 가진 대규모 언어 모델(LLM)에서 효율적인 추론을 가능하게 하는 방법론을 제안합니다. 아래는 논문의 강점, 독창적인 지점, 핵심 알고리즘, 한계점 등을 분석한 내용입니다.&lt;/p></description></item><item><title>Mixed Precision Quantization</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.18096/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.18096/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.18096">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약">논문 요약:&lt;/h3>
&lt;p>**&amp;ldquo;No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization&amp;rdquo;**는 대규모 언어 모델(LLM)의 추론 시 발생하는 메모리 병목 현상을 해결하기 위한 Key-Value(KV) 캐싱 압축 기법을 제안합니다.&lt;/p></description></item><item><title>Momentum Strategies</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-Momentum-Strategies/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-Momentum-Strategies/</guid><description>&lt;p>&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=225438">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Momentum Strategies&amp;rdquo; (1996)**를 분석하고 요청하신 내용을 자세히 설명드리겠습니다.&lt;/p></description></item><item><title>WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</title><link>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.12065/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-09-paper-2402.12065/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2402.12065">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점과-독창적인-지점">&lt;strong>강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>WKVQuant&lt;/strong>은 대규모 언어 모델(LLM)의 메모리 사용 문제를 해결하기 위해 설계된 &lt;strong>Post-Training Quantization (PTQ)&lt;/strong> 프레임워크입니다. 주요 강점과 독창적 특징은 다음과 같습니다:&lt;/p></description></item><item><title>Abseil Tip 103 플래그는 전역 변수입니다</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-tow103/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-tow103/</guid><description>&lt;h3 id="주간-팁-103-플래그는-전역-변수입니다">주간 팁 #103: 플래그는 전역 변수입니다&lt;/h3>
&lt;p>&lt;strong>작성자: &lt;a href="mailto:marmstrong@google.com">Matt Armstrong&lt;/a>&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>&lt;code>.cc&lt;/code> 파일에서 전역 범위로 플래그를 정의하세요. 해당 &lt;code>.h&lt;/code> 파일에서는 최대 한 번만 선언하세요.&lt;/p>
&lt;hr>
&lt;h2 id="왜-헤더-파일에-선언할까요">왜 헤더 파일에 선언할까요?&lt;/h2>
&lt;p>헤더 파일 사용은 대부분 우리에게 자연스러운 습관이지만, 그 이유를 잊었을 수도 있습니다. 헤더 파일을 사용하는 이유는 다음과 같습니다:&lt;/p></description></item><item><title>Abseil Tip 45 플래그를 피하라, 특히 라이브러리 코드에서</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-tow45/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-tow45/</guid><description>&lt;h3 id="주간-팁-45-플래그를-피하라-특히-라이브러리-코드에서">주간 팁 #45: 플래그를 피하라, 특히 라이브러리 코드에서&lt;/h3>
&lt;p>&lt;strong>최초 게시일:&lt;/strong> 2013년 6월 3일&lt;br>
&lt;strong>작성자:&lt;/strong> &lt;a href="mailto:titus@google.com">Titus Winters&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;내 코드의 동작이 전역 변수로 제어되기를 바란다. 이 변수는 코드만으로는 값이 예측되지 않으며, 사용 기록이 불완전하고, 코드를 통해 제거하는 데 큰 어려움이 따른다.&amp;rdquo; — 아무도 그런 말을 하지 않는다.&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 90 Retired Flags(사용 중단된 플래그)</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-tow90/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-tow90/</guid><description>&lt;h2 id="주간-팁-90-retired-flags사용-중단된-플래그">주간 팁 #90: Retired Flags(사용 중단된 플래그)&lt;/h2>
&lt;p>원래 게시일: 2015년 3월 19일&lt;br>
작성자: &lt;a href="mailto:titus@google.com">Titus Winters&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="소개">소개&lt;/h3>
&lt;p>명령줄 플래그(command-line flags)를 잘못 사용하면 플래그를 바이너리 및 프로덕션 서버에서 안전하게 제거하는 것이 어려워집니다. (&lt;a href="https://abseil.io/tips/45">주간 팁 #45&lt;/a>에서 다룬 플래그의 잘못된 사용 사례를 참고하세요.)&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-radford2018improving/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-radford2018improving/</guid><description>&lt;p>&lt;a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Improving Language Understanding by Generative Pre-Training&amp;rdquo;**의 주요 내용을 자세히 정리하고, 강점과 독창성, 알고리즘의 예제 과정을 설명한 뒤, 한계점도 다루겠습니다.&lt;/p></description></item><item><title>PaLM 2 Technical Report</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2305.10403v3/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2305.10403v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.10403v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 Google의 PaLM 2에 대한 기술 보고서로, 언어 모델의 새로운 상태를 정립한 모델로서, 다중 언어 처리 능력과 추론 능력을 강화하고, 이전 모델에 비해 컴퓨팅 효율성을 크게 개선한 것이 핵심입니다. 아래에 이 논문의 강점, 독창적인 지점, 핵심 알고리즘 설명, 그리고 한계점을 정리했습니다.&lt;/p></description></item><item><title>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2312.12456v1/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2312.12456v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2312.12456v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약">논문의 요약&lt;/h3>
&lt;p>&lt;strong>PowerInfer&lt;/strong>는 소비자용 GPU로 대형 언어 모델(LLM)을 빠르게 추론할 수 있도록 설계된 시스템입니다. LLM 추론에서 고도 국소성을 활용하여 GPU와 CPU의 협력을 통해 효율성을 극대화합니다.&lt;/p></description></item><item><title>QAQ: Quality Adaptive Quantization for LLM KV Cache</title><link>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2403.04643/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-08-paper-2403.04643/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.04643">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-qaq-quality-adaptive-quantization-for-llm-kv-cache">논문 요약: &lt;strong>QAQ: Quality Adaptive Quantization for LLM KV Cache&lt;/strong>&lt;/h3>
&lt;h4 id="주요-기여">주요 기여&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>품질 적응형 양자화(Quality Adaptive Quantization)&lt;/strong>: LLM의 Key-Value(KV) 캐시를 위한 새로운 양자화 방식 제안. 이는 GPU 메모리 사용량을 10배까지 줄이면서 성능 손실을 최소화함.&lt;/li>
&lt;li>&lt;strong>이론적 분석 및 실험적 검증&lt;/strong>:
&lt;ul>
&lt;li>Key와 Value 캐시는 양자화에 대해 서로 다른 민감도를 보임. 이에 따라 각기 다른 양자화 전략을 설계.&lt;/li>
&lt;li>기존 중요성 지속성 가설(Persistence of Importance)을 보완해, 예외적인 토큰 중요도 변화를 처리하는 방법 제안.&lt;/li>
&lt;li>Outliers(이상치)의 중요성을 강조하고 이를 개별적으로 처리하여 성능 손실 최소화.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>우수한 성능&lt;/strong>: 기존 방법 대비 1.6~1.8배 높은 압축률을 달성하면서 성능 손실을 거의 없는 수준으로 유지.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h4 id="강점과-독창성">강점과 독창성&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>독창적 설계&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 136 Unordered Containers</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-tow136/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-tow136/</guid><description>&lt;h3 id="주간-팁-136-unordered-containers">주간 팁 #136: Unordered Containers&lt;/h3>
&lt;p>&lt;em>2017년 6월 23일 처음 게시된 TotW #136을 업데이트한 내용입니다.&lt;/em>&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:kfm@google.com">Matt Kulukundis&lt;/a>&lt;/em>&lt;br>
업데이트: 2020-04-06&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/136">abseil.io/tips/136&lt;/a>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;때로는 정말 좋은 자료를 접했을 때, 그걸 최고의 쇼로 만들고자 스스로에게 기대를 걸게 됩니다. 그냥 평범한 일만 하고 집에 가는 것과는 다르죠.&amp;rdquo;&lt;/em>&lt;br>
– 피터 딩클리지&lt;/p></description></item><item><title>Abseil Tip 144 연관 컨테이너에서의 이종 조회(Heterogeneous Lookup)</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-tow144/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-tow144/</guid><description>&lt;h3 id="주간-팁-144-연관-컨테이너에서의-이종-조회heterogeneous-lookup">주간 팁 #144: 연관 컨테이너에서의 이종 조회(Heterogeneous Lookup)&lt;/h3>
&lt;p>&lt;em>2018년 3월 23일 처음 게시된 TotW #144을 업데이트한 내용입니다.&lt;/em>&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:sbenza@google.com">Samuel Benzaquen&lt;/a>&lt;/em>&lt;br>
업데이트: 2020-04-06&lt;/p></description></item><item><title>Abseil Tip 152 AbslHashValue과 함께</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-tow152/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-tow152/</guid><description>&lt;hr>
&lt;h2 id="order-152">title: &amp;ldquo;주간 팁 #152: &lt;code>AbslHashValue&lt;/code>과 함께&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/152
type: markdown
order: &amp;ldquo;152&amp;rdquo;&lt;/h2>
&lt;p>원래 2018년 6월 21일에 TotW #152로 게시됨&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:kfm@google.com">Matt Kulukundis&lt;/a>&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 153 using-directives를 사용하지 마세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-tow153/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-tow153/</guid><description>&lt;hr>
&lt;h2 id="order-153">title: &amp;ldquo;주간 팁 #153: &lt;code>using-directives&lt;/code>를 사용하지 마세요&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/153
type: markdown
order: &amp;ldquo;153&amp;rdquo;&lt;/h2>
&lt;p>원래 2018년 7월 17일에 TotW #153으로 게시됨&lt;/p></description></item><item><title>ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.17312/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.17312/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.17312">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-alisa-대규모-언어-모델-추론-가속화를-위한-희소성-기반-kv-캐싱">논문 요약: &lt;strong>ALISA: 대규모 언어 모델 추론 가속화를 위한 희소성 기반 KV 캐싱&lt;/strong>&lt;/h3>
&lt;p>이 논문은 대규모 언어 모델(LLM)의 추론 속도를 개선하기 위해 &lt;strong>ALISA&lt;/strong>라는 새로운 알고리즘-시스템 공동 설계 방안을 제시합니다. 특히, GPU 메모리가 제한된 환경에서 LLM 추론 효율을 극대화하기 위한 다양한 방법론을 다룹니다.&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-1810.04805v2/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-1810.04805v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1810.04805v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문은 Google AI Language 팀에서 작성한 **&amp;ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&amp;rdquo;**로, 자연어 처리(NLP) 분야에서 혁신적인 성과를 이룬 대표적인 논문입니다. BERT(Bidirectional Encoder Representations from Transformers)는 언어 모델 사전 학습 및 미세 조정을 활용하여 NLP 작업에서 큰 성능 향상을 이루었습니다. 아래에 논문의 주요 내용을 정리하고, 강점, 독창성, 핵심 알고리즘, 그리고 한계점을 설명하겠습니다.&lt;/p></description></item><item><title>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.09636/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.09636/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.09636">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-강점-독창성-핵심-알고리즘-설명">논문 요약 및 강점, 독창성, 핵심 알고리즘 설명&lt;/h3>
&lt;h4 id="논문-요약">논문 요약&lt;/h4>
&lt;p>논문은 대형 언어 모델(LLM)의 비효율적인 메모리 사용 문제를 해결하기 위해 &lt;strong>Dynamic Memory Compression (DMC)&lt;/strong> 방법을 제안합니다. 이 방법은 모델의 키-값(KV) 캐시를 실시간으로 동적으로 압축하여, 더 적은 메모리로 유사한 성능을 유지하면서도 추론 속도를 증가시킵니다. DMC는 학습 중 각 레이어와 헤드마다 서로 다른 압축 비율을 자동으로 학습하며, 기존 LLM에 새로운 파라미터를 추가하지 않고 간단한 후속 학습으로 적용 가능합니다.&lt;/p></description></item><item><title>Fast Inference from Transformers via Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2211.17192v2/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2211.17192v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.17192v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-fast-inference-from-transformers-via-speculative-decoding">논문 분석: &lt;strong>Fast Inference from Transformers via Speculative Decoding&lt;/strong>&lt;/h3>
&lt;h4 id="1-논문의-강점">1. &lt;strong>논문의 강점&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>속도 향상&lt;/strong>: Speculative Decoding 기법을 통해 기존의 T5-XXL과 같은 대규모 모델보다 2배에서 3배 빠른 추론 속도를 달성.&lt;/li>
&lt;li>&lt;strong>모델 수정 불필요&lt;/strong>: 모델 아키텍처나 재학습 없이도 기존 모델에서 바로 적용 가능.&lt;/li>
&lt;li>&lt;strong>출력 보존&lt;/strong>: 출력의 분포가 변경되지 않음을 보장, 기존의 정확도를 유지.&lt;/li>
&lt;li>&lt;strong>범용성&lt;/strong>: 다양한 어플리케이션(번역, 요약 등)에 대해 동일한 방법론 적용 가능.&lt;/li>
&lt;li>&lt;strong>효율성&lt;/strong>: 메모리 대역폭이 병목현상이 되는 환경에서 추가적인 계산 자원을 활용하여 병렬화를 효과적으로 구현.&lt;/li>
&lt;/ul>
&lt;h4 id="2-독창적인-지점">2. &lt;strong>독창적인 지점&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Speculative Sampling&lt;/strong>: 큰 모델(T5-XXL, LaMDA 등)의 출력 분포를 보존하면서 작은 근사 모델을 사용해 미리 예측을 생성하고, 이를 검증 후 필요하면 보정.&lt;/li>
&lt;li>&lt;strong>병렬 처리&lt;/strong>: 각 단계에서 여러 토큰을 동시에 처리할 수 있는 방식으로 병렬성을 높임.&lt;/li>
&lt;li>&lt;strong>새로운 확률 추출 방식&lt;/strong>: Speculative Sampling을 통해 불필요한 계산 낭비를 줄이면서도 정확성을 유지.&lt;/li>
&lt;li>&lt;strong>학습 필요 없음&lt;/strong>: 근사 모델(Mq)을 이미 존재하는 작은 모델로 설정하여 추가적인 학습 없이 구현 가능.&lt;/li>
&lt;/ul>
&lt;h4 id="3-핵심-알고리즘-설명">3. &lt;strong>핵심 알고리즘 설명&lt;/strong>&lt;/h4>
&lt;p>논문에 제시된 Speculative Decoding 알고리즘의 주요 과정은 다음과 같습니다:&lt;/p></description></item><item><title>FASTDECODE: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.11421/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.11421/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.11421">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>이 논문은 대규모 언어 모델(LLM) 서빙을 위해 &lt;strong>FASTDECODE&lt;/strong>라는 새로운 시스템을 제안합니다. 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title><link>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.12968/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-06-paper-2403.12968/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.12968">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 117 복사 생략과 값으로 전달하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-tow117/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-tow117/</guid><description>&lt;p>원래 TotW #117로 2016년 6월 8일 게시됨&lt;/p>
&lt;p>&lt;em>작성: Geoff Romer &lt;a href="mailto:gromer@gmail.com">(gromer@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>&amp;ldquo;모든 것이 멀리 있다. 복사의 복사의 복사. 불면증 거리 속의 모든 것, 당신은 어떤 것도 만질 수 없고, 아무것도 당신을 만질 수 없다.&amp;rdquo; — 척 팔라니uk&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 148 Overload Sets</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-tow148/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-tow148/</guid><description>&lt;p>원래 TotW #148로 2018년 5월 3일 게시됨&lt;/p>
&lt;p>&lt;em>작성: &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/em>&lt;/p>
&lt;p>업데이트: 2020-04-06&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;전기 정보를 가지고 사는 것의 한 가지 효과는 정보 과부하 상태에서 항상 살아가는 것입니다. 항상 감당할 수 있는 것보다 많은 정보가 존재합니다.&amp;rdquo; — 마셜 맥루언&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 149 Object Lifetimes vs = delete</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-tow149/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-tow149/</guid><description>&lt;h2 id="order-149">title: &amp;ldquo;이번 주의 팁 #149: 객체 수명 vs. &lt;code>= delete&lt;/code>&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/149
type: markdown
order: &amp;ldquo;149&amp;rdquo;&lt;/h2>
&lt;p>원래 TotW #149로 2018년 5월 3일 게시됨&lt;/p>
&lt;p>&lt;em>작성: &lt;a href="mailto:titus@cs.ucr.edu">Titus Winters&lt;/a>&lt;/em>&lt;/p>
&lt;p>업데이트: 2020-04-06&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;ldquo;다시 파란 속으로, 돈이 사라진 후에&lt;br/> 인생에 한 번, 지하로 흐르는 물처럼&amp;rdquo; — 데이비드 번&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 24 복사, 축약</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-tow24/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-tow24/</guid><description>&lt;h2 id="order-024">title: &amp;ldquo;이번 주의 팁 #24: 복사, 축약&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/24
type: markdown
order: &amp;ldquo;024&amp;rdquo;&lt;/h2>
&lt;p>원래 TotW #24로 2012년 11월 26일 게시됨&lt;/p>
&lt;p>&lt;em>작성: Titus Winters &lt;a href="mailto:titus@gmail.com">(titus@google.com)&lt;/a> 및 Chandler Carruth &lt;a href="mailto:chandlerc@google.com">(chandlerc@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>&amp;ldquo;다른 사람을 따라 하는 것은 필요하지만, 자신을 따라 하는 것은 비참하다.&amp;rdquo; - 파블로 피카소&lt;/em>&lt;/p></description></item><item><title>ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.17312/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.17312/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.17312">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>독창적인 알고리즘 및 시스템 설계&lt;/strong>:&lt;/p></description></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2305.18290v3/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2305.18290v3/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.18290v3">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문은 **Direct Preference Optimization (DPO)**라는 새로운 방법을 제안하며, 인간 선호 데이터 기반으로 대규모 언어 모델(LLM)을 효율적으로 미세 조정(fine-tuning)하는 과정을 크게 간소화합니다. 아래는 논문의 주요 기여점, 강점, 핵심 알고리즘 설명, 한계점에 대한 분석입니다.&lt;/p></description></item><item><title>HIERARCHICAL CONTEXT MERGING: BETTER LONG CONTEXT UNDERSTANDING FOR PRE-TRAINED LLMS</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2404.10308/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2404.10308/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.10308">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;p>논문 제목: &lt;em>Hierarchical Context Merging: Better Long Context Understanding for Pre-Trained LLMs&lt;/em>&lt;/p></description></item><item><title>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.12968/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.12968/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.12968">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;p>논문 제목: &lt;strong>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression&lt;/strong>&lt;/p></description></item><item><title>MELTing point: Mobile Evaluation of Language Transformers</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.12844v4/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.12844v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.12844v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-평가">논문 요약 및 평가&lt;/h3>
&lt;h4 id="논문의-주요-내용-및-독창성">논문의 주요 내용 및 독창성&lt;/h4>
&lt;p>이 논문은 **&amp;ldquo;MELTing Point: Mobile Evaluation of Language Transformers&amp;rdquo;**로, 모바일 및 엣지 디바이스에서 대형 언어 모델(LLM)의 실행 가능성을 평가하는 것을 목표로 합니다. 이를 위해 MELT라는 자동화된 벤치마킹 도구를 개발하여 다양한 디바이스와 프레임워크에서 LLM의 성능, 에너지 효율성, 정확성을 체계적으로 분석했습니다. 주요 기여는 다음과 같습니다:&lt;/p></description></item><item><title>MuxServe:FlexibleSpatial-TemporalMultiplexingforMultipleLLMServing</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2404.02015v2/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2404.02015v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.02015v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving&lt;/strong>는 다중 대형 언어 모델(LLM)을 효율적으로 서빙하기 위한 공간-시간 다중화(spatial-temporal multiplexing) 시스템을 제안합니다. 이 논문의 주요 강점과 독창적인 점은 다음과 같습니다:&lt;/p></description></item><item><title>The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2301.13688v2/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2301.13688v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2301.13688v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-강점-및-독창성">논문의 주요 강점 및 독창성&lt;/h3>
&lt;p>논문은 Google Research의 Flan-T5 및 Flan 2022 데이터를 중심으로, 지시 조정(instruction tuning)의 방법론적 개선을 제시하며 이를 통해 상당한 성능 향상을 입증합니다. 주요 강점과 독창성을 다음과 같이 요약할 수 있습니다.&lt;/p></description></item><item><title>Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.20041/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2403.20041/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2403.20041">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약">논문의 요약&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs&lt;br>
&lt;strong>저자:&lt;/strong> Luchang Li 외 다수&lt;br>
&lt;strong>목적:&lt;/strong>&lt;br>
Transformer-Lite는 모바일 GPU에서 대형 언어 모델(LLM)을 효과적으로 배포하기 위한 최적화 기술을 제안하며, 이는 클라우드 기반 배포의 높은 비용을 줄이고 모바일 기기에서의 응용 가능성을 확대하려는 목표를 가지고 있습니다.&lt;/p></description></item><item><title>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2305.10601v2/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-05-paper-2305.10601v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.10601v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&amp;rdquo;**는 대형 언어 모델(LLM)을 활용한 문제 해결 능력을 향상시키기 위해 &amp;ldquo;사고의 나무(Tree of Thoughts, ToT)&amp;ldquo;라는 새로운 프레임워크를 제안합니다. 아래는 논문의 주요 내용과 강점, 독창성, 한계점에 대한 상세한 설명입니다.&lt;/p></description></item><item><title>Abseil Tip 11 반환 정책</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-tow11/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-tow11/</guid><description>&lt;h2 id="주간-팁-11-반환-정책">주간 팁 #11: 반환 정책&lt;/h2>
&lt;p>원래 게시일: 2012년 8월 16일&lt;/p>
&lt;p>&lt;em>작성자: Paul S. R. Chisholm &lt;a href="mailto:p.s.r.chisholm@gmail.com">(p.s.r.chisholm@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Frodo: 돌아오는 여정에는 남은 것이 없을 거야.&lt;/em>&lt;br>
&lt;em>Sam: 돌아오는 여정은 없을 거예요, 미스터 프로도.&lt;/em>&lt;br>
– &lt;em>반지의 제왕: 왕의 귀환&lt;/em> (J.R.R. 톨킨 원작, 프란 월시, 필리파 보이엔스, 피터 잭슨 각본)&lt;/p></description></item><item><title>Abseil Tip 120 반환 값은 건드리지 마세요</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-tow120/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-tow120/</guid><description>&lt;h2 id="주간-팁-120-반환-값은-건드리지-마세요">주간 팁 #120: 반환 값은 건드리지 마세요&lt;/h2>
&lt;p>원래 게시일: 2012년 8월 16일&lt;/p>
&lt;p>&lt;em>작성자: Samuel Benzaquen, &lt;a href="mailto:sbenza@gmail.com">(sbenza@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>다음과 같은 코드 조각이 있다고 가정해 봅시다. 이 코드는 RAII 클린업 함수에 의존하고 있으며, 예상대로 작동하는 것처럼 보입니다:&lt;/p></description></item><item><title>Abseil Tip 143 C++11 삭제된 함수 (= delete)</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-tow143/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-tow143/</guid><description>&lt;h2 id="주간-팁-143-c11-삭제된-함수--delete">주간 팁 #143: C++11 삭제된 함수 (&lt;code>= delete&lt;/code>)&lt;/h2>
&lt;p>원래 게시일: 2018년 3월 2일&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:mosescu@google.com">레오나르드 모세스쿠&lt;/a>&lt;/em>&lt;/p>
&lt;p>업데이트: 2020-04-06&lt;/p>
&lt;p>퀵 링크: &lt;a href="https://abseil.io/tips/143">abseil.io/tips/143&lt;/a>&lt;/p>
&lt;h3 id="소개">소개&lt;/h3>
&lt;p>인터페이스는 일반적으로 호출할 수 있는 연산의 집합을 정의합니다. 그러나 때때로 우리는 반대로 &lt;em>사용하지 말아야 할&lt;/em> 연산 집합을 명시적으로 정의하고 싶을 때가 있습니다. 예를 들어, 복사 생성자와 복사 대입 연산자를 비활성화하는 것은 특정 타입에 대한 복사 동작을 제한하는 일반적인 방법입니다.&lt;/p></description></item><item><title>CORM: Cache Optimization with Recent Message for Large Language Model Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.15949/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.15949/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.15949">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-독창성-핵심-알고리즘-그리고-한계점에-대한-분석">논문의 강점, 독창성, 핵심 알고리즘, 그리고 한계점에 대한 분석&lt;/h3>
&lt;hr>
&lt;h4 id="논문의-강점">&lt;strong>논문의 강점&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 메모리 사용&lt;/strong>:&lt;/p></description></item><item><title>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.07143/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.07143/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.07143">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-분석">&lt;strong>논문의 주요 내용 분석&lt;/strong>&lt;/h3>
&lt;h4 id="논문의-강점">&lt;strong>논문의 강점&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>무한 컨텍스트 처리 능력&lt;/strong>:&lt;/p></description></item><item><title>Llama 2: Open Foundation and Fine-Tuned Chat Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2307.09288v2/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2307.09288v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2307.09288v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>다음은 논문 *&amp;ldquo;Llama 2: Open Foundation and Fine-Tuned Chat Models&amp;rdquo;*에 대한 한글 요약 및 분석입니다. 논문의 강점, 독창적인 기여, 핵심 알고리즘 설명(예시 포함), 그리고 한계점에 대해 설명드리겠습니다.&lt;/p></description></item><item><title>Mistral 7B</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2310.06825v1/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2310.06825v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2310.06825v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-mistral-7b">논문 요약: &lt;strong>Mistral 7B&lt;/strong>&lt;/h3>
&lt;p>Mistral 7B는 70억 개의 매개변수를 가진 언어 모델로, 효율성과 성능을 모두 갖춘 모델을 목표로 설계되었습니다. 이 모델은 OpenAI의 Llama 2와 같은 기존 모델을 다양한 벤치마크에서 능가하며, 특히 수학, 코드 생성, 논리 추론에서 탁월한 성능을 보입니다. 주요 기술 혁신은 다음과 같습니다:&lt;/p></description></item><item><title>Retrieval Head Mechanistically Explains Long-Context Factuality</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.15574/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.15574/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.15574">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 &amp;ldquo;Retrieval Head Mechanistically Explains Long-Context Factuality&amp;quot;는 대규모 언어 모델(LLMs)이 긴 맥락에서 정보를 검색하는 메커니즘을 설명하며, 이를 통해 모델의 정확성과 합리성을 이해하고 개선하려는 연구입니다. 아래는 논문의 핵심 내용, 강점, 알고리즘, 그리고 한계점에 대한 설명입니다.&lt;/p></description></item><item><title>SnapKV: LLM Knows What You are Looking for Before Generation</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.14469/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.14469/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.14469">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-snapkv">논문 분석: SnapKV&lt;/h3>
&lt;p>이 논문은 **대형 언어 모델(LLM)**의 &lt;strong>Key-Value(KV) 캐시 최적화&lt;/strong>를 목표로 한 새로운 알고리즘 &lt;strong>SnapKV&lt;/strong>를 제안합니다. 아래는 논문의 핵심 기여, 알고리즘 동작 방식, 강점, 한계점 등을 한국어로 정리한 내용입니다.&lt;/p></description></item><item><title>SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.04793/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.04793/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.04793">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 &lt;strong>SqueezeAttention&lt;/strong>이라는 2D KV-캐시 압축 알고리즘을 제안하여 대형 언어 모델(LLM)의 추론 효율성을 개선하는 방법을 소개합니다. 주요 강점과 독창적인 지점은 다음과 같습니다:&lt;/p></description></item><item><title>Toward Inference-optimal Mixture-of-Expert Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.02852/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-04-paper-2404.02852/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.02852">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적-기여">논문의 강점 및 독창적 기여&lt;/h3>
&lt;p>이 논문, &lt;strong>&amp;ldquo;Toward Inference-optimal Mixture-of-Expert Large Language Models&amp;rdquo;&lt;/strong>, 은 대규모 언어 모델에서 Mixture-of-Experts (MoE) 접근법을 연구하며, 주요 기여는 다음과 같습니다:&lt;/p></description></item><item><title>Abseil Tip 134 make_unique와 private 생성자</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-tow134/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-tow134/</guid><description>&lt;h3 id="주간-팁-134-make_unique와-private-생성자">주간 팁 #134: &lt;code>make_unique&lt;/code>와 &lt;code>private&lt;/code> 생성자&lt;/h3>
&lt;hr>
&lt;p>&lt;strong>원래 게시일&lt;/strong>: 2017년 5월 10일 (TotW #134)&lt;br>
&lt;strong>작성자&lt;/strong>: Yitzhak Mandelbaum, Google 엔지니어&lt;br>
&lt;strong>최종 업데이트&lt;/strong>: 2020년 4월 6일&lt;br>
&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/134">abseil.io/tips/134&lt;/a>&lt;/p></description></item><item><title>Abseil Tip 141 bool로의 암시적 변환에 주의하라</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-tow141/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-tow141/</guid><description>&lt;h3 id="주간-팁-141-bool로의-암시적-변환에-주의하라">주간 팁 #141: &lt;code>bool&lt;/code>로의 암시적 변환에 주의하라&lt;/h3>
&lt;hr>
&lt;p>&lt;strong>원래 게시일&lt;/strong>: 2018년 1월 19일 (TotW #141)&lt;br>
&lt;strong>작성자&lt;/strong>: &lt;a href="mailto:sfreilich@google.com">Samuel Freilich&lt;/a>&lt;br>
&lt;strong>최종 업데이트&lt;/strong>: 2020년 4월 6일&lt;br>
&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/141">abseil.io/tips/141&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="두-가지-널-포인터-검사-방법">두 가지 널 포인터 검사 방법&lt;/h2>
&lt;p>널 포인터를 역참조하기 전에 검사는 충돌과 버그를 방지하기 위해 중요합니다. 이 검사는 두 가지 방법으로 수행할 수 있습니다:&lt;/p></description></item><item><title>Abseil Tip 61 기본 멤버 초기화 (Default Member Initializers)</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-tow61/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-tow61/</guid><description>&lt;hr>
&lt;h3 id="주간-팁-61-기본-멤버-초기화-default-member-initializers">주간 팁 #61: 기본 멤버 초기화 (Default Member Initializers)&lt;/h3>
&lt;hr>
&lt;p>&lt;strong>원래 게시일&lt;/strong>: 2013년 11월 12일 (TotW #61)&lt;br>
&lt;strong>작성자&lt;/strong>: Michael Chastain &lt;a href="mailto:mec.desktop@gmail.com">(mec.desktop@gmail.com)&lt;/a>&lt;br>
&lt;strong>최종 업데이트&lt;/strong>: 2016년 10월&lt;/p>
&lt;hr>
&lt;h2 id="기본-멤버-초기화-선언하기">기본 멤버 초기화 선언하기&lt;/h2>
&lt;p>기본 멤버 초기화는 생성 시 멤버의 기본 값을 선언하며, 다음과 같은 형태로 작성됩니다:&lt;/p></description></item><item><title>Abseil Tip 93 absl::Span 사용하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-tow93/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-tow93/</guid><description>&lt;hr>
&lt;h3 id="주간-팁-93-abslspan-사용하기">주간 팁 #93: &lt;code>absl::Span&lt;/code> 사용하기&lt;/h3>
&lt;hr>
&lt;p>&lt;strong>원래 게시일&lt;/strong>: 2015년 4월 23일 (TotW #93)&lt;br>
&lt;strong>작성자&lt;/strong>: &lt;a href="mailto:sbenza@google.com">Samuel Benzaquen&lt;/a>&lt;br>
&lt;strong>최종 업데이트&lt;/strong>: 2023년 5월 8일&lt;br>
&lt;strong>빠른 링크&lt;/strong>: &lt;a href="https://abseil.io/tips/93">abseil.io/tips/93&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Google에서는 소유하지 않은 문자열을 처리할 때 &lt;code>string_view&lt;/code>를 함수 매개변수와 반환 타입으로 사용하는 것이 일반적입니다.&lt;br>
이렇게 하면 API가 더 유연해지고, &lt;code>string&lt;/code>으로 불필요한 변환을 피함으로써 성능이 개선될 수 있습니다. (&lt;a href="https://jaehun.me/tips/1">팁 #1&lt;/a> 참조)&lt;/p></description></item><item><title>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2405.04434/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2405.04434/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.04434">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;p>논문 **&amp;ldquo;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&amp;rdquo;**는 다음과 같은 강점과 독창성을 보여줍니다:&lt;/p></description></item><item><title>Fast Inference from Transformers via Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2211.17192v2/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2211.17192v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2211.17192v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>이 논문은 &lt;strong>Speculative Decoding&lt;/strong>이라는 새로운 방법론을 소개하며, 대규모 자기회귀 모델의 추론 속도를 개선합니다. 이 알고리즘의 독창성과 강점은 다음과 같습니다:&lt;/p></description></item><item><title>MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2406.10290v1/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2406.10290v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.10290v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>이 논문 **&amp;ldquo;MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases&amp;rdquo;**의 주요 내용을 자세히 분석하고 요약하겠습니다. 또한 논문의 강점, 독창적인 점, 알고리즘 설명, 한계점 등을 정리하겠습니다.&lt;/p></description></item><item><title>Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2404.12022/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2404.12022/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.12022">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 병렬 디코딩:&lt;/strong>&lt;/p></description></item><item><title>PowerInfer-2: Fast Large Language Model Inference on a Smartphone</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2406.06282v2/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2406.06282v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.06282v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;PowerInfer-2: Fast Large Language Model Inference on a Smartphone&amp;rdquo;**은 스마트폰에서 대형 언어 모델(LLM)을 효과적으로 실행하기 위한 혁신적인 프레임워크인 &lt;strong>PowerInfer-2&lt;/strong>를 소개합니다. 아래는 논문에 대한 상세 분석입니다:&lt;/p></description></item><item><title>RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2404.12457/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2404.12457/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2404.12457">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation&amp;rdquo;**는 Retrieval-Augmented Generation (RAG) 시스템의 효율성을 높이기 위해 제안된 다단계 캐싱 시스템을 소개합니다. 이 시스템은 외부 지식을 효율적으로 캐시하고 이를 다수의 요청에 재활용함으로써 RAG의 주요 성능 병목 현상을 극복합니다. 아래에 논문의 주요 내용, 강점, 독창성, 알고리즘 과정, 한계점을 분석합니다.&lt;/p></description></item><item><title>Tree-based Speculative Inference and Verification</title><link>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2305.09781v4/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-03-paper-2305.09781v4/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2305.09781v4">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-및-독창적인-점">논문의 주요 내용 및 독창적인 점&lt;/h3>
&lt;h4 id="논문의-강점과-독창적인-점">논문의 강점과 독창적인 점&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>트리 기반 추측적 추론 (Tree-based Speculative Inference):&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 142 다중 매개변수 생성자와 explicit</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-tow142/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-tow142/</guid><description>&lt;hr>
&lt;h2 id="order-142">title: &amp;ldquo;Tip of the Week #142: 다중 매개변수 생성자와 &lt;code>explicit&lt;/code>&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/142
type: markdown
order: &amp;ldquo;142&amp;rdquo;&lt;/h2>
&lt;p>처음 게시: 2018-01-29 (TotW #142)&lt;/p>
&lt;p>&lt;em>작성자: &lt;a href="mailto:jdennett@google.com">James Dennett&lt;/a>&lt;/em>&lt;/p>
&lt;p>업데이트: 2020-04-06&lt;/p>
&lt;p>바로가기: &lt;a href="https://abseil.io/tips/142">abseil.io/tips/142&lt;/a>&lt;/p>
&lt;p>“명시적(explicit)이 암시적(implicit)보다 낫다.” &amp;ndash;&lt;br>
&lt;a href="https://www.python.org/dev/peps/pep-0020/">PEP 20&lt;/a>&lt;/p></description></item><item><title>Abseil Tip 59 튜플 연결하기</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-tow59/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-tow59/</guid><description>&lt;hr>
&lt;h2 id="order-059">title: &amp;ldquo;Tip of the Week #59: 튜플 연결하기&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/59
type: markdown
order: &amp;ldquo;059&amp;rdquo;&lt;/h2>
&lt;p>처음 게시: 2013-10-21 (totw/59)&lt;/p>
&lt;p>&lt;em>작성자: Greg Miller &lt;a href="mailto:jgm@google.com">(jgm@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>업데이트: 2018-01-24&lt;/p>
&lt;p>&lt;em>&amp;ldquo;이제 손을 맞잡고, 마음도 함께 나누십시오.&amp;rdquo; &amp;ndash;헨리 6세, 윌리엄 셰익스피어&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 88 초기화 방법 =, (), 그리고 {}</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-tow88/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-tow88/</guid><description>&lt;h2 id="order-088">title: &amp;ldquo;Tip of the Week #88: 초기화 방법: =, (), 그리고 {}&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/88
type: markdown
order: &amp;ldquo;088&amp;rdquo;&lt;/h2>
&lt;p>처음 게시: 2015-01-27 (TotW #88)&lt;/p>
&lt;p>&lt;em>작성자: Titus Winters &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>, Google C++ 스타일 심사 위원회를 대표하여&lt;/em>&lt;/p>
&lt;p>C++11은 &amp;ldquo;통일된 초기화 문법&amp;quot;이라고 불리는 새로운 문법을 도입했습니다. 이는 다양한 초기화 스타일을 통합하고, &lt;a href="http://en.wikipedia.org/wiki/Most_vexing_parse">가장 성가신 구문 분석 문제(Most Vexing Parse)&lt;/a>를 피하며, 협소 변환(narrowing conversion)을 방지하기 위해 설계되었습니다. 하지만 이 새로운 메커니즘은 또 다른 초기화 문법을 추가했을 뿐이며, 이 역시 고유한 장단점을 가지고 있습니다.&lt;/p></description></item><item><title>KV Cache Compression</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.12591/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.12591/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.12591">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약-및-강점">논문의 요약 및 강점&lt;/h3>
&lt;p>이 논문은 대형 언어 모델(LLM)의 &lt;strong>키-값(KV) 캐시를 압축하기 위한 데이터 없이 저비트 양자화(Data-free Low-bit Quantization)&lt;/strong> 기술, &lt;strong>DecoQuant&lt;/strong>를 제안합니다. 이 기술은 텐서 분해(Matrix Decomposition)를 사용하여 KV 캐시 메모리를 효율적으로 줄이고 모델 추론을 가속화합니다. 주요 기여는 다음과 같습니다:&lt;/p></description></item><item><title>Layer-Condensed KV Cache for Efficient Inference of Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.10637/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.10637/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.10637">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-제목-layer-condensed-kv-cache-for-efficient-inference-of-large-language-models">논문 제목: &lt;strong>Layer-Condensed KV Cache for Efficient Inference of Large Language Models&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="1-논문의-강점-및-독창성">1. &lt;strong>논문의 강점 및 독창성&lt;/strong>&lt;/h3>
&lt;p>이 논문은 대형 언어 모델(LLM)의 효율적인 추론을 위해 &lt;strong>Layer-Condensed KV Cache (LCKV)&lt;/strong> 라는 새로운 방법론을 제안했습니다. 주요 강점 및 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.12532/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.12532/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.12532">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>&lt;strong>PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference&lt;/strong>라는 논문은 대형 언어 모델(LLM)에서 Transformer 기반 KV 캐시(key-value cache)를 효율적으로 압축하는 새로운 방법을 제안합니다. 아래는 논문의 주요 내용과 기여점, 강점, 한계점, 핵심 알고리즘(PyramidInfer)의 작동 원리를 예제를 통해 설명한 내용입니다.&lt;/p></description></item><item><title>SKVQ:Sliding-window Key and Value Cache Quantization for Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.06219/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.06219/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.06219">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models&amp;rdquo;**는 대규모 언어 모델(LLMs)이 긴 문맥을 처리할 때 발생하는 메모리와 계산 부담을 줄이기 위해 새로운 양자화(Quantization) 전략을 제안하고 있습니다. 주요 내용과 강점, 알고리즘의 작동 방식, 그리고 한계점을 아래와 같이 설명하겠습니다.&lt;/p></description></item><item><title>You Only Cache Once: Decoder-Decoder Architectures for Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.05254/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-12-02-paper-2405.05254/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.05254">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-개요-you-only-cache-once-decoder-decoder-architectures-for-language-models-yoco">논문의 개요: &amp;ldquo;You Only Cache Once: Decoder-Decoder Architectures for Language Models (YOCO)&amp;rdquo;&lt;/h3>
&lt;p>이 논문은 대규모 언어 모델에서 메모리 및 속도 최적화를 위해 YOCO(You Only Cache Once)라는 새로운 디코더-디코더 아키텍처를 제안합니다. 이 구조는 GPU 메모리 소비를 줄이고 추론 속도를 개선하며, 긴 문맥을 처리할 수 있는 효율성을 보여줍니다. YOCO는 전통적인 Transformer와 달리 한 번만 키-값(KV) 캐시를 저장하여 메모리를 효율적으로 관리합니다.&lt;/p></description></item><item><title>Abseil Tip 10 문자열 분리, 골치 아프지 않게</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-tow10/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-tow10/</guid><description>&lt;hr>
&lt;h2 id="order-010">title: &amp;ldquo;이번 주의 팁 #10: 문자열 분리, 골치 아프지 않게!&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/10
type: markdown
order: &amp;ldquo;010&amp;rdquo;&lt;/h2>
&lt;p>원래 2012-08-16에 totw/10으로 게시됨&lt;/p></description></item><item><title>Abseil Tip 3 문자열 연결과 operator+ vs. StrCat()</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-tow3/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-tow3/</guid><description>&lt;hr>
&lt;h2 id="order-003">title: &amp;ldquo;이번 주의 팁 #3: 문자열 연결과 operator+ vs. StrCat()&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/3
type: markdown
order: &amp;ldquo;003&amp;rdquo;&lt;/h2>
&lt;p>원래 2012-05-11에 totw/3으로 게시됨&lt;/p>
&lt;p>2017-09-18 업데이트; 2018-01-22 수정&lt;/p>
&lt;p>리뷰어가 &amp;ldquo;문자열 연결 연산자를 사용하지 마세요, 비효율적이에요&amp;quot;라고 말했을 때 종종 사용자들은 놀라곤 합니다. &lt;code>std::string::operator+&lt;/code>가 비효율적이라니요? 그렇게 구현하기 어렵기라도 한 걸까요?&lt;/p></description></item><item><title>Abseil Tip 36 새로운 Join API</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-tow36/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-tow36/</guid><description>&lt;hr>
&lt;h2 id="order-131">title: &amp;ldquo;주간 팁 #131: 특별 멤버 함수와 &lt;code>= default&lt;/code>&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/131
type: markdown
order: &amp;ldquo;131&amp;rdquo;&lt;/h2>
&lt;p>원래 2017-03-24에 totw/131로 게시됨&lt;/p>
&lt;p>작성자: James Dennett &lt;a href="mailto:jdennett@google.com">(jdennett@google.com)&lt;/a>&lt;/p>
&lt;p>C++는 초기부터 &lt;em>특별 멤버 함수&lt;/em>라 불리는 몇 가지 기능을 지원했습니다: 기본 생성자, 소멸자, 복사 생성자 및 복사 할당 연산자입니다. C++11은 여기에 이동 생성자와 이동 할당 연산자를 추가하고, &lt;code>=default&lt;/code>와 &lt;code>=delete&lt;/code>라는 문법을 통해 이러한 기본값을 선언 및 정의하는 시점을 제어할 수 있게 했습니다.&lt;/p></description></item><item><title>LoCoCo: Dropping In Convolutions for Long Context Compression</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.05317/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.05317/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.05317">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>Loki: Low-rank Keys for Efficient Sparse Attention</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.02542/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.02542/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.02542">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Loki: Low-rank Keys for Efficient Sparse Attention&amp;rdquo;**에 대한 상세한 분석을 제공하겠습니다. 이 논문은 자가회귀 기반의 대규모 언어 모델(LLM)에서 사용되는 &lt;strong>자가주의 메커니즘의 계산 및 메모리 병목 현상을 개선&lt;/strong>하기 위한 새로운 방법론을 제안합니다.&lt;/p></description></item><item><title>MiniCache: KV Cache Compression in Depth Dimension for Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2405.14366/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2405.14366/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.14366">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창성">강점 및 독창성&lt;/h3>
&lt;p>논문 &lt;em>MiniCache: KV Cache Compression in Depth Dimension for Large Language Models&lt;/em>는 대형 언어 모델(LLM)에서의 메모리 효율성을 향상시키기 위한 혁신적인 접근 방식을 제시합니다. 이 논문의 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.02069/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2406.02069/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.02069">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>피라미드 정보 집중(Pyramidal Information Funneling)의 발견&lt;/strong>:&lt;/p></description></item><item><title>Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title><link>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2405.12981/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-29-paper-2405.12981/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2405.12981">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>1. 독창적인 기여:&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 131 Special 멤버 함수와 = default</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-tow131/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-tow131/</guid><description>&lt;hr>
&lt;h2 id="order-131">title: &amp;ldquo;주간 팁 #131: 특별 멤버 함수와 &lt;code>= default&lt;/code>&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/131
type: markdown
order: &amp;ldquo;131&amp;rdquo;&lt;/h2>
&lt;p>원래 2017-03-24에 totw/131로 게시됨&lt;/p>
&lt;p>작성자: James Dennett &lt;a href="mailto:jdennett@google.com">(jdennett@google.com)&lt;/a>&lt;/p>
&lt;p>C++는 초기부터 &lt;em>특별 멤버 함수&lt;/em>라 불리는 몇 가지 기능을 지원했습니다: 기본 생성자, 소멸자, 복사 생성자 및 복사 할당 연산자입니다. C++11은 여기에 이동 생성자와 이동 할당 연산자를 추가하고, &lt;code>=default&lt;/code>와 &lt;code>=delete&lt;/code>라는 문법을 통해 이러한 기본값을 선언 및 정의하는 시점을 제어할 수 있게 했습니다.&lt;/p></description></item><item><title>Abseil Tip 42 초기화 메서드보다 팩토리 함수를 선호하세요</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-tow42/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-tow42/</guid><description>&lt;hr>
&lt;h2 id="order-042">title: &amp;ldquo;주간 팁 #42: 초기화 메서드보다 팩토리 함수를 선호하세요&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/42
type: markdown
order: &amp;ldquo;042&amp;rdquo;&lt;/h2>
&lt;p>원래 2013-05-10에 totw/42로 게시됨&lt;br>
작성자: Geoffrey Romer &lt;a href="mailto:gromer@google.com">(gromer@google.com)&lt;/a>&lt;br>
2017-12-21 개정&lt;/p></description></item><item><title>Abseil Tip 74 위임 생성자와 상속 생성자</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-tow74/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-tow74/</guid><description>&lt;p>원래 2014-04-21에 totw/74로 게시됨&lt;br>
작성자: Bradley White &lt;a href="mailto:bww@google.com">(bww@google.com)&lt;/a>&lt;/p>
&lt;p>&lt;em>&amp;ldquo;위임 작업이 효과를 발휘하려면, 위임하는 사람도 함께 일해야 합니다.&amp;rdquo; &amp;ndash; 로버트 하프&lt;/em>&lt;/p></description></item><item><title>ASimple and Effective L2 Norm-Based Strategy for KV Cache Compression</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.11430/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.11430/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.11430">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>The uploaded document appears to detail a method for compressing the Key-Value (KV) Cache in large language models (LLMs) using an L2 norm-based strategy. Here is a breakdown of the requested analysis:&lt;/p></description></item><item><title>Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.12335/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.12335/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.12335">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-강점-및-독창성">논문의 주요 강점 및 독창성&lt;/h3>
&lt;p>&lt;strong>강점:&lt;/strong>&lt;/p></description></item><item><title>CItruS : ChunkedInstruction-aware State Eviction for Long Sequence Modeling</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.12018/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.12018/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.12018">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>The uploaded document introduces CItruS, a novel method for improving long-sequence modeling in transformer-based large language models (LLMs). Here’s a detailed breakdown based on your request:&lt;/p></description></item><item><title>Effectively Compress KV Heads for LLM</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.07056/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.07056/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.07056">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>문제 정의의 중요성&lt;/strong>:&lt;/p></description></item><item><title>MLKV:Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.09297/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-28-paper-2406.09297/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.09297">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약">논문의 요약&lt;/h3>
&lt;p>이 논문은 **MLKV (Multi-Layer Key-Value Sharing)**라는 새로운 방법을 제안하여 Transformer 모델에서 KV(Key-Value) 캐시 메모리 사용량을 효율적으로 줄이는 방안을 탐구합니다. 기존 MQA(Multi-Query Attention) 및 GQA(Grouped-Query Attention)보다 더 많은 메모리 절약을 가능하게 하면서도 성능 저하를 최소화하려는 것이 목표입니다. MLKV는 Key-Value 헤드를 레이어 간에 공유하여 KV 캐시 크기를 모델 레이어 수의 작은 비율까지 줄이는 혁신적인 접근 방식을 제시합니다.&lt;/p></description></item><item><title>Benchmark of Long Context Capable Approaches</title><link>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.01527/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.01527/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.01527">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>The uploaded document appears to be a comprehensive benchmarking paper evaluating various methods to enhance long-context capabilities in Large Language Models (LLMs), such as optimizing the Key-Value (KV) cache. Below is a detailed breakdown based on the paper:&lt;/p></description></item><item><title>Dynamic Discriminative Operations (D2O) for Efficient Generative Inference of Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2406.13035/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2406.13035/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.13035">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약-강점-및-독창성-알고리즘-설명-한계점">논문의 요약, 강점 및 독창성, 알고리즘 설명, 한계점&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> &lt;em>Dynamic Discriminative Operations (D2O) for Efficient Generative Inference of Large Language Models&lt;/em>&lt;/p></description></item><item><title>Efficient Sparse Attention needs Adaptive Token Release</title><link>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.02328/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.02328/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.02328">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-핵심-내용">논문의 핵심 내용&lt;/h3>
&lt;p>이 논문은 대형 언어 모델(LLM)의 효율적인 추론을 위해 **ADORE(Adaptive Token Release)**라는 새로운 기법을 제안합니다. 주요 내용은 다음과 같습니다:&lt;/p></description></item><item><title>LOOK-M:Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2406.18139/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2406.18139/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2406.18139">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>문제 정의의 명확성&lt;/strong>:&lt;/p></description></item><item><title>MODEL TELLS YOU WHERE TO MERGE: ADAPTIVE KV CACHE MERGING FOR LLMS ON LONG-CONTEXT TASKS</title><link>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.08454/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-27-paper-2407.08454/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.08454">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>이 논문은 &lt;strong>KV Cache Merging&lt;/strong>이라는 새로운 접근 방식을 통해 LLM(대규모 언어 모델)의 긴 문맥 작업에서 메모리 효율성을 크게 향상시킵니다. 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>Abseil Tip 119 using 선언과 네임스페이스 별칭 사용하기</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-tow119/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-tow119/</guid><description>&lt;h3 id="주간-팁-119-using-선언과-네임스페이스-별칭-사용하기">주간 팁 #119: &lt;code>using&lt;/code> 선언과 네임스페이스 별칭 사용하기&lt;/h3>
&lt;p>원래 게시일: 2016-07-14&lt;br>
작성자: Thomas Köppe &lt;a href="mailto:tkoeppe@google.com">(tkoeppe@google.com)&lt;/a>&lt;/p>
&lt;p>이 팁은 &lt;code>.cc&lt;/code> 파일에서 &lt;code>using&lt;/code> 선언과 네임스페이스 별칭을 작성할 때 발생할 수 있는 미묘한 문제를 피하기 위한 간단하고 견고한 방법을 제공합니다.&lt;/p></description></item><item><title>Abseil Tip 123 absl::optional과 std::unique_ptr</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-tow123/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-tow123/</guid><description>&lt;h3 id="주간-팁-123-absloptional과-stdunique_ptr">주간 팁 #123: &lt;code>absl::optional&lt;/code>과 &lt;code>std::unique_ptr&lt;/code>&lt;/h3>
&lt;p>원래 게시일: 2016-09-06&lt;br>
작성자: Alexey Sokolov &lt;a href="mailto:sokolov@google.com">(sokolov@google.com)&lt;/a>&lt;br>
Etienne Dechamps &lt;a href="mailto:edechamps@google.com">(edechamps@google.com)&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="값을-저장하는-방법">값을 저장하는 방법&lt;/h2>
&lt;p>이 팁에서는 값을 저장하는 여러 가지 방법을 다룹니다. 여기서는 클래스 멤버 변수를 예로 들지만, 아래의 많은 점은 지역 변수에도 적용됩니다.&lt;/p></description></item><item><title>Abseil Tip 130 네임스페이스 이름 지정</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-tow130/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-tow130/</guid><description>&lt;h3 id="주간-팁-130-네임스페이스-이름-지정">주간 팁 #130: 네임스페이스 이름 지정&lt;/h3>
&lt;p>원래 게시일: 2017-02-17&lt;br>
작성자: Titus Winters &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>&lt;/p>
&lt;p>&lt;em>정확한 이름 지정은 보는 이의 독창성을 제한한다.&lt;/em>&lt;br>
— 피에르 보나르(Pierre Bonnard)&lt;/p>
&lt;p>Google C++ 스타일 가이드 초기에는 네임스페이스 이름 지정에 대한 지침이 포함되어 있었습니다. 이 지침은 일반적으로 다음과 같이 요약될 수 있습니다:&lt;/p></description></item><item><title>Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.11550/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.11550/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.11550">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM’s KV Cache Consumption.</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.18003/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.18003/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.18003">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>The uploaded paper is a comprehensive review titled &amp;ldquo;Keep the Cost Down: A Review on Methods to Optimize LLM’s KV Cache Consumption.&amp;rdquo; Here&amp;rsquo;s a detailed breakdown:&lt;/p></description></item><item><title>LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.14057/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.14057/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.14057">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-분석-lazyllm---dynamic-token-pruning-for-efficient-long-context-llm-inference">논문의 분석: LazyLLM - DYNAMIC TOKEN PRUNING FOR EFFICIENT LONG CONTEXT LLM INFERENCE&lt;/h3>
&lt;h4 id="강점-및-독창적인-지점">강점 및 독창적인 지점&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>효율적 인퍼런스&lt;/strong>: LazyLLM은 긴 문맥을 처리하는 LLM의 효율성을 향상시키는 새로운 토큰 제거 방법을 제안합니다. 특히, 첫 번째 토큰을 생성하기 위한 &amp;ldquo;Time-to-First-Token (TTFT)&amp;ldquo;을 줄이는 데 집중하였습니다. 이는 사용자 경험에서 중요한 영향을 미칩니다.&lt;/li>
&lt;li>&lt;strong>동적 토큰 선택&lt;/strong>: 기존의 정적 토큰 제거 방식과 달리, LazyLLM은 각 생성 단계에서 다른 토큰의 중요도를 동적으로 평가하여 필요에 따라 일부 토큰을 &amp;ldquo;부활&amp;quot;시킬 수 있습니다. 이 점은 모델의 정확도를 유지하면서도 속도를 개선하는 데 기여합니다.&lt;/li>
&lt;li>&lt;strong>트레이닝 불필요&lt;/strong>: LazyLLM은 추가적인 모델 재학습 없이 기존의 사전 학습된 LLM에 바로 통합할 수 있습니다.&lt;/li>
&lt;li>&lt;strong>Aux Cache 사용&lt;/strong>: 숨겨진 상태를 효율적으로 캐싱하여 제거된 토큰을 필요 시 재활용할 수 있게 함으로써 중복 계산을 방지합니다.&lt;/li>
&lt;li>&lt;strong>범용성&lt;/strong>: LazyLLM은 모든 트랜스포머 기반 모델과 통합 가능하며 다양한 작업에서 성능 향상을 보였습니다.&lt;/li>
&lt;/ol>
&lt;h4 id="핵심-알고리즘-설명">핵심 알고리즘 설명&lt;/h4>
&lt;p>LazyLLM의 핵심은 **동적 토큰 프루닝(Dynamic Token Pruning)**입니다. 이 과정은 다음 단계로 요약됩니다:&lt;/p></description></item><item><title>PQCache: Product Quantization-based KVCache for Long Context LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.12820/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.12820/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.12820">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-">논문 분석: &lt;strong>&amp;ldquo;PQCache: Product Quantization-based KVCache for Long Context LLM Inference&amp;rdquo;&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h4 id="1-강점-및-독창성">&lt;strong>1. 강점 및 독창성&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>강점&lt;/strong>&lt;/p></description></item><item><title>Pruning in Transformer Decoder</title><link>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.20485/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-26-paper-2407.20485/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.20485">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-및-강점">논문의 주요 내용 및 강점&lt;/h3>
&lt;h4 id="강점-및-독창성">강점 및 독창성:&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>기존 문제점 개선&lt;/strong>:&lt;/p></description></item><item><title>Abseil Tip 109 함수 선언에서 의미 있는 const 사용</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-tow109/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-tow109/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h3 id="tip-of-the-week-109-함수-선언에서-의미-있는">&lt;strong>Tip of the Week #109: 함수 선언에서 의미 있는 &lt;code>const&lt;/code> 사용&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>Greg Miller&lt;/strong> &lt;a href="mailto:jgm@google.com">(jgm@google.com)&lt;/a>&lt;br>
&lt;strong>2016-01-14&lt;/strong>에 totw/109로 최초 게시&lt;/p>
&lt;p>이 문서는 함수 선언에서 &lt;code>const&lt;/code>가 언제 의미가 있고, 언제 의미가 없어 생략하는 것이 최선인지 설명합니다. 먼저, *선언(declaration)*과 *정의(definition)*의 차이를 간단히 살펴보겠습니다.&lt;/p></description></item><item><title>Abseil Tip 126 make_unique는 새로운 new입니다</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-tow126/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-tow126/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h3 id="tip-of-the-week-126">&lt;strong>Tip of the Week #126: &lt;code>make_unique&lt;/code>는 새로운 &lt;code>new&lt;/code>입니다&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>By James Dennett&lt;/strong> &lt;a href="mailto:jdennett@google.com">(jdennett@google.com)&lt;/a>,&lt;br>
&lt;strong>Titus Winters&lt;/strong> &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>의 메일링 리스트 게시글을 기반으로 작성&lt;br>
&lt;strong>2016-12-12&lt;/strong>에 totw/126으로 최초 게시&lt;/p></description></item><item><title>Abseil Tip 99 비멤버 인터페이스 에티켓</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-tow99/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-tow99/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h3 id="tip-of-the-week-99-비멤버-인터페이스-에티켓">&lt;strong>Tip of the Week #99: 비멤버 인터페이스 에티켓&lt;/strong>&lt;/h3>
&lt;p>&lt;strong>2015-06-24&lt;/strong>에 totw/99로 최초 게시&lt;br>
&lt;strong>2017-10-10&lt;/strong> 수정됨&lt;/p>
&lt;p>C++ 클래스의 인터페이스는 클래스 멤버나 정의로만 제한되지 않습니다. API를 평가할 때, 클래스 본문 외부에서 정의된 내용도 클래스의 public 멤버만큼이나 인터페이스의 일부가 될 수 있습니다.&lt;/p></description></item><item><title>NACL: AGeneral and Effective KV Cache Eviction Framework for LLMs at Inference Time</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2408.03675/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2408.03675/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.03675">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-상세-분석">&lt;strong>논문 상세 분석&lt;/strong>&lt;/h3>
&lt;h4 id="제목과-범위">&lt;strong>제목과 범위&lt;/strong>&lt;/h4>
&lt;p>이 논문은 **대규모 언어 모델(LLMs)**의 추론 과정에서 &lt;strong>KV 캐시 관리&lt;/strong>를 최적화하여 메모리 사용량을 줄이고 성능을 유지하기 위한 &lt;strong>NACL&lt;/strong>이라는 프레임워크를 제안합니다. 이 프레임워크는 **프록시 토큰 제거(Proxy-Tokens Eviction)**와 **랜덤 제거(Random Eviction)**를 결합하여 긴 문맥과 짧은 문맥 작업에서 높은 성능을 유지하면서 메모리 사용량을 효과적으로 줄이는 것을 목표로 합니다.&lt;/p></description></item><item><title>Palu: Compressing KV-Cache with Low-Rank Projection</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2407.21118/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2407.21118/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.21118">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문을 분석하고 요청하신 내용을 세부적으로 정리해보겠습니다.&lt;/p></description></item><item><title>Post-Training Sparse Attention with Double Sparsity</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2408.07092/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2408.07092/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2408.07092">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-분석-및-평가">논문의 주요 내용 분석 및 평가&lt;/h3>
&lt;h4 id="논문의-강점과-독창성">&lt;strong>논문의 강점과 독창성&lt;/strong>&lt;/h4>
&lt;p>이 논문은 &lt;strong>Double Sparsity&lt;/strong>라는 새로운 방식의 사후 훈련 희소화 기법을 제안하며, 다음과 같은 독창적인 요소를 포함합니다:&lt;/p></description></item><item><title>RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2409.10516/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2409.10516/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2409.10516">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-및-강점">논문의 주요 내용 및 강점&lt;/h3>
&lt;p>&lt;strong>논문의 목적&lt;/strong>&lt;br>
RetrievalAttention은 대규모 언어 모델(LLM)의 긴 문맥 처리에서 발생하는 연산 복잡성과 GPU 메모리 사용량 문제를 해결하기 위해 제안된 방법입니다. 이 방법은 주의 메커니즘의 동적 희소성을 활용하여, CPU 메모리에 있는 키-값(KV) 벡터를 근사 최근접 이웃 검색(ANNS) 방식으로 검색하여 주목할 만한 KV 벡터만 GPU로 가져와 연산합니다.&lt;/p></description></item><item><title>ThinK: Thinner Key Cache by Query-Driven Pruning</title><link>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2407.21018/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-25-paper-2407.21018/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2407.21018">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문--분석">논문 &lt;strong>&amp;ldquo;THINK: Thinner Key Cache by Query-Driven Pruning&amp;rdquo;&lt;/strong> 분석&lt;/h3>
&lt;hr>
&lt;h4 id="논문의-강점-및-독창성">&lt;strong>논문의 강점 및 독창성&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 메모리 최적화&lt;/strong>:&lt;/p></description></item><item><title>Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction</title><link>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2409.17422/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2409.17422/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2409.17422">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction&amp;rdquo;**은 대규모 언어 모델(LLMs)에서 긴 문맥 입력 처리 시 발생하는 계산 및 메모리 문제를 해결하기 위해 새로운 접근법을 제시합니다. 아래는 논문의 강점, 주요 기여, 한계, 그리고 알고리즘의 단계별 예시에 대한 분석입니다.&lt;/p></description></item><item><title>InfiniPot: Infinite Context Processing on Memory-Constrained LLMs</title><link>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.01518/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.01518/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01518">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-개요">논문 개요&lt;/h3>
&lt;p>&lt;strong>InfiniPot&lt;/strong>는 메모리 제약 환경에서 기존의 대규모 언어 모델(LLM)이 긴 문맥을 처리할 수 있도록 설계된 새로운 Key-Value(KV) 캐시 제어 프레임워크입니다. 추가적인 훈련 없이, &lt;strong>InfiniPot&lt;/strong>은 긴 문맥에서도 핵심 정보를 유지하며 효과적으로 처리할 수 있는 **Continual Context Distillation (CCD)**라는 반복적인 압축 기법을 제안합니다. 이 기법은 과거 및 미래 문맥의 중요도를 계산하는 새로운 메트릭을 통해 정보를 압축 및 유지하며, 모바일 장치와 같은 리소스가 제한된 환경에서도 효율적인 실행을 보장합니다.&lt;/p></description></item><item><title>KV-COMPRESS: Paged KV-Cache Compression with Variable Compression Rates per Attention Head</title><link>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.00161/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.00161/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.00161">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-제목-kv-compress-paged-kv-cache-compression-with-variable-compression-rates-per-attention-head">논문의 제목: &lt;strong>KV-COMPRESS: Paged KV-Cache Compression with Variable Compression Rates per Attention Head&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="1-논문의-강점-및-독창성">&lt;strong>1. 논문의 강점 및 독창성&lt;/strong>&lt;/h3>
&lt;h4 id="11-kv-compress의-주요-기여">1.1 &lt;strong>KV-Compress의 주요 기여&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>PagedAttention 프레임워크 개선&lt;/strong>:&lt;/p></description></item><item><title>Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads</title><link>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.01805/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2410.01805/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.01805">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>이 논문에서는 **LOCRET (Long-context Retaining Heads)**이라는 새로운 프레임워크를 제안하여 긴 문맥을 다루는 대형 언어 모델(LLM)에서 효율적인 메모리 관리와 성능을 제공합니다. 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning</title><link>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2409.13035/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-21-paper-2409.13035/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2409.13035">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning&amp;rdquo;**의 주요 내용을 분석한 결과, 강점, 독창성, 핵심 알고리즘, 그리고 한계점을 아래와 같이 정리했습니다.&lt;/p></description></item><item><title>Abseil Tip 112 emplace vs. push_back</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-tow112/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-tow112/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h2 id="order-112">title: &amp;ldquo;Tip of the Week #112: emplace vs. push_back&amp;rdquo;&lt;br>
layout: tips&lt;br>
sidenav: side-nav-tips.html&lt;br>
published: true&lt;br>
permalink: tips/112&lt;br>
type: markdown&lt;br>
order: &amp;ldquo;112&amp;rdquo;&lt;/h2>
&lt;p>원래 게시 날짜: 2016-02-25, totw/112&lt;br>
작성자: Geoff Romer &lt;a href="mailto:gromer@google.com">(gromer@google.com)&lt;/a>&lt;br>
개정일: 2017-08-30&lt;/p>
&lt;p>&lt;em>&amp;ldquo;우리가 힘을 덜 사용할수록, 우리의 힘은 더 커질 것이다.&amp;rdquo; — 토머스 제퍼슨&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 49 인자 기반 탐색</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-tow49/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-tow49/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h2 id="order-049">title: &amp;ldquo;Tip of the Week #49: Argument-Dependent Lookup&amp;rdquo;&lt;br>
layout: tips&lt;br>
sidenav: side-nav-tips.html&lt;br>
published: true&lt;br>
permalink: tips/49&lt;br>
type: markdown&lt;br>
order: &amp;ldquo;049&amp;rdquo;&lt;/h2>
&lt;p>원래 게시 날짜: 2013-07-14, totw/49&lt;/p>
&lt;p>&lt;em>&amp;quot;&amp;hellip;어떤 법적 용어의 흔적을 따르든 간에 사라져버린 그것&amp;hellip;&amp;quot;&lt;br>
– 안토닌 스칼리아, &lt;a href="http://www.supremecourt.gov/opinions/12pdf/12-307_6j37.pdf">미국 대 Windsor 반대 의견&lt;/a>&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 65 제자리에 넣기</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-tow65/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-tow65/</guid><description>&lt;h3 id="한글-번역">한글 번역&lt;/h3>
&lt;hr>
&lt;h2 id="order-065">title: &amp;ldquo;Tip of the Week #65: 제자리에 요소 넣기(Putting Things in their Place)&amp;rdquo;&lt;br>
layout: tips&lt;br>
sidenav: side-nav-tips.html&lt;br>
published: true&lt;br>
permalink: tips/65&lt;br>
type: markdown&lt;br>
order: &amp;ldquo;065&amp;rdquo;&lt;/h2>
&lt;p>원래 게시 날짜: 2013-12-12, totw/65&lt;br>
작성자: Hyrum Wright &lt;a href="mailto:hyrum@hyrumwright.org">(hyrum@hyrumwright.org)&lt;/a>&lt;/p>
&lt;p>&lt;em>&amp;ldquo;설명하죠. 아니요, 너무 길어요. 요약하겠습니다.&amp;rdquo; — 이니고 몬토야&lt;/em>&lt;/p></description></item><item><title>DUOATTENTION: EFFICIENT LONG-CONTEXT LLM INFERENCE WITH RETRIEVAL AND STREAMING HEADS</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.10819/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.10819/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.10819">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 메모리 사용 및 성능 개선&lt;/strong>:&lt;/p></description></item><item><title>LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.03111/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.03111/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.03111">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>강점&lt;/strong>:&lt;/p></description></item><item><title>SPARSEVLM: VISUAL TOKEN SPARSIFICATION FOR EFFICIENT VISION-LANGUAGE MODEL INFERENCE</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.04417/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.04417/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.04417">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창적인-지점">강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>논문의 강점:&lt;/strong>&lt;/p></description></item><item><title>SWIFTKV: FAST PREFILL-OPTIMIZED INFERENCE WITH KNOWLEDGE-PRESERVING MODEL TRANSFORMATION</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.03960/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.03960/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.03960">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-분석-swiftkv-fast-prefill-optimized-inference-with-knowledge-preserving-model-transformation">논문의 분석: &amp;ldquo;SWIFTKV: FAST PREFILL-OPTIMIZED INFERENCE WITH KNOWLEDGE-PRESERVING MODEL TRANSFORMATION&amp;rdquo;&lt;/h3>
&lt;hr>
&lt;h4 id="논문의-주요-강점과-독창성">&lt;strong>논문의 주요 강점과 독창성&lt;/strong>&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>독창적인 구조 최적화&lt;/strong>:&lt;/p></description></item><item><title>TIDALDECODE: FAST AND ACCURATE LLM DECOD ING WITH POSITION PERSISTENT SPARSE ATTENTION</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.05076/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.05076/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.05076">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="강점-및-독창성">강점 및 독창성&lt;/h3>
&lt;p>&lt;strong>TidalDecode&lt;/strong>의 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training</title><link>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.06511v2/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-20-paper-2410.06511v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.06511v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-제목-torchtitan-one-stop-pytorch-native-solution-for-production-ready-llm-pre-training">논문 제목: &lt;strong>TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="논문의-강점과-독창성">&lt;strong>논문의 강점과 독창성&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>PyTorch 기반 통합 솔루션&lt;/strong>:&lt;/p></description></item><item><title>A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.14442/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.14442/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.14442">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-a-systematic-study-of-cross-layer-kv-sharing-for-efficient-llm-inference">논문 분석: &amp;ldquo;A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference&amp;rdquo;&lt;/h3>
&lt;h4 id="1-논문의-강점과-독창성">1. &lt;strong>논문의 강점과 독창성&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>강점&lt;/strong>&lt;/p></description></item><item><title>In-context KV-Cache Eviction for LLMs via Attention-Gate</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.12876/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.12876/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.12876">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>문제의 명확성&lt;/strong>:&lt;/p></description></item><item><title>Prompt Compression for Large Language Models: A Survey</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.12388/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.12388/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.12388">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-분석-및-요약">&lt;strong>논문 분석 및 요약&lt;/strong>&lt;/h3>
&lt;h4 id="논문-제목">&lt;strong>논문 제목&lt;/strong>:&lt;/h4>
&lt;p>&lt;strong>Prompt Compression for Large Language Models: A Survey&lt;/strong>&lt;/p></description></item><item><title>Scaling Laws for Neural Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2001.08361v1/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2001.08361v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2001.08361v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 제목: &lt;strong>Scaling Laws for Neural Language Models&lt;/strong>&lt;/p></description></item><item><title>SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.13846/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2410.13846/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.13846">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-강점">논문의 강점&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율성 및 간결성&lt;/strong>:&lt;/p></description></item><item><title>Textbooks Are All You Need</title><link>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2306.11644v2/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-19-paper-2306.11644v2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2306.11644v2">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-주요-내용-분석-및-설명">논문의 주요 내용 분석 및 설명&lt;/h3>
&lt;p>이 논문에서는 코드 생성 작업을 위한 &lt;strong>phi-1&lt;/strong>이라는 소형 대규모 언어 모델(LLM)을 제안합니다. 이 모델은 1.3억 개의 파라미터를 가진 Transformer 기반 모델로, 제한된 리소스(8개의 A100 GPU에서 4일간 훈련)를 활용해 높은 성능을 달성했습니다. 주요 기여점은 고품질 데이터(&amp;ldquo;교과서 수준의 데이터&amp;rdquo;)를 사용하여 소규모 데이터셋과 모델 크기에도 불구하고 뛰어난 성능을 보여준다는 점입니다. 주요 결과는 다음과 같습니다:&lt;/p></description></item><item><title>Abseil Tip 101 반환 값, 참조 및 수명</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow101/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow101/</guid><description>&lt;h1 id="주간-팁-101-반환-값-참조-및-수명">주간 팁 #101: 반환 값, 참조 및 수명&lt;/h1>
&lt;p>&lt;strong>Originally posted as totw/101 on 2015-07-29&lt;/strong>&lt;br>
By Titus Winters (&lt;a href="mailto:titus@google.com">titus@google.com&lt;/a>)&lt;/p>
&lt;p>다음의 코드 스니펫을 보겠습니다:&lt;/p>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 C&amp;#43;&amp;#43;
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">obj&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">GetName&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">&lt;span class="n">std&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="n">unique_ptr&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Consumer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">consumer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">Consumer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="p">));&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;p>특히 여기서 &lt;code>&amp;amp;&lt;/code>에 주목해주세요. 이것이 적절할까요? 우리는 무엇을 확인해야 할까요? 어떤 문제가 발생할 수 있을까요? 많은 C++ 프로그래머들이 참조에 대해 완전히 이해하지 못한 상태에서 &amp;ldquo;복사를 피한다&amp;quot;고만 알고 있는 경우가 많습니다. 하지만 C++의 대부분의 문제처럼, 이 문제도 더 복잡합니다.&lt;/p></description></item><item><title>Abseil Tip 107 참조 수명 연장</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow107/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow107/</guid><description>&lt;p>아래는 &amp;ldquo;이번 주의 팁 #107: 참조 수명 확장&amp;quot;에 대한 한글 번역입니다:&lt;/p>
&lt;hr>
&lt;p>&lt;strong>제목:&lt;/strong> &amp;ldquo;이번 주의 팁 #107: 참조 수명 확장&amp;rdquo;&lt;br>
&lt;strong>원문 게시일:&lt;/strong> 2015년 12월 10일&lt;br>
&lt;strong>작성자:&lt;/strong> Titus Winters (&lt;a href="mailto:titus@google.com">titus@google.com&lt;/a>)&lt;/p></description></item><item><title>Abseil Tip 135 계약을 테스트하라, 구현을 테스트하지 마라</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow135/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-tow135/</guid><description>&lt;h1 id="주간-팁-135-계약을-테스트하라-구현을-테스트하지-마라">주간 팁 #135: 계약을 테스트하라, 구현을 테스트하지 마라&lt;/h1>
&lt;p>&lt;strong>Originally posted as TotW #135 on June 5, 2017&lt;/strong>&lt;br>
&lt;em>By &lt;a href="mailto:jdennett@google.com">James Dennett&lt;/a>&lt;/em>&lt;br>
Updated 2020-04-06&lt;/p>
&lt;p>&lt;strong>&amp;ldquo;진정한 친구가 한 명이라도 있다면, 당신은 이미 많은 것을 가진 것이다.&amp;rdquo; — 토마스 풀러(Thomas Fuller)&lt;/strong>&lt;/p></description></item><item><title>MagicPIG: LSH Sampling for Efficient LLM Generation</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2410.16179/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2410.16179/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.16179">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-magicpig-lsh-sampling-for-efficient-llm-generation">논문 요약: &amp;ldquo;MagicPIG: LSH Sampling for Efficient LLM Generation&amp;rdquo;&lt;/h3>
&lt;h4 id="1-논문의-배경-및-문제-정의">1. &lt;strong>논문의 배경 및 문제 정의&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>최근 GPT, Llama 등과 같은 대형 언어 모델(LLM)은 긴 문맥을 활용한 응용에서 주목받고 있습니다. 하지만, 이 모델들은 &lt;strong>키-값(KV) 캐시&lt;/strong>로 인한 병목 현상 때문에 긴 문맥에서 성능이 제한됩니다.&lt;/li>
&lt;li>특히, LLM의 오토레그레시브(Autoregressive) 생성 과정에서 &lt;strong>KV 캐시의 메모리 사용량&lt;/strong>이 모델의 효율을 떨어뜨리고, GPU 자원의 활용을 저해합니다.&lt;/li>
&lt;li>기존의 TopK 기반 접근법은 주로 주의(attention) 계산의 희소성을 이용해 성능을 높이려 했으나, 특정 작업에서는 오히려 성능이 저하되는 문제를 보였습니다.&lt;/li>
&lt;/ul>
&lt;h4 id="2-magicpig의-제안">2. &lt;strong>MagicPIG의 제안&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>논문은 **Locality Sensitive Hashing (LSH)**를 이용한 새로운 샘플링 접근법인 &lt;strong>MagicPIG&lt;/strong>를 제안합니다.&lt;/li>
&lt;li>TopK 접근법의 한계를 극복하기 위해, MagicPIG는 이론적으로 보장된 샘플링을 통해 더 나은 주의 출력 추정치를 제공합니다.&lt;/li>
&lt;li>이 시스템은 GPU에서 해시 계산을 수행하고 CPU에서 주의 계산을 실행하여 더 긴 문맥과 더 큰 배치 크기에서도 높은 정확도를 유지하면서 효율적인 디코딩 성능을 제공합니다.&lt;/li>
&lt;/ul>
&lt;h4 id="3-magicpig의-핵심-알고리즘">3. &lt;strong>MagicPIG의 핵심 알고리즘&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>MagicPIG는 LSH를 사용하여 &lt;strong>주의 점수 분포를 근사&lt;/strong>하고 이를 바탕으로 샘플링을 통해 주의 출력을 추정합니다.&lt;/p></description></item><item><title>Recycled Attention: Efficient inference for long-context language models</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.05787/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.05787/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.05787">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-개요-및-독창성-분석">논문의 개요 및 독창성 분석&lt;/h3>
&lt;p>논문의 제목은 **&amp;ldquo;Recycled Attention: Efficient Inference for Long-Context Language Models&amp;rdquo;**입니다. 이 논문은 **대규모 언어 모델(LLM)**에서 긴 입력 시퀀스를 처리하는 효율성을 높이기 위해 &lt;strong>Recycled Attention&lt;/strong>이라는 새로운 방법을 제안합니다.&lt;/p></description></item><item><title>Squeezed Attention: Accelerating Long Context Length LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.09688/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.09688/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.09688">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문의-요약-및-강점">논문의 요약 및 강점&lt;/h3>
&lt;p>논문의 제목은 **&amp;ldquo;SQUEEZED ATTENTION: Accelerating Long Context Length LLM Inference&amp;rdquo;**입니다. 이 논문은 긴 입력 문맥을 다루는 대형 언어 모델(LLM)에서 &lt;strong>추론 속도를 대폭 개선하기 위해 제안된 새로운 주의 메커니즘&lt;/strong>인 Squeezed Attention을 소개합니다.&lt;/p></description></item><item><title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.02886/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2411.02886/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2411.02886">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;p>논문 **&amp;ldquo;TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection&amp;rdquo;**에 대해 분석한 내용을 아래와 같이 정리하겠습니다.&lt;/p></description></item><item><title>VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration</title><link>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2410.23317/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-18-paper-2410.23317/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.23317">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1>
&lt;h3 id="논문-요약-및-분석">논문 요약 및 분석&lt;/h3>
&lt;p>&lt;strong>논문 제목:&lt;/strong> VL-CACHE: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration&lt;/p></description></item><item><title>Abseil Tip 64 Raw 문자열 리터럴</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-tow64/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-tow64/</guid><description>&lt;hr>
&lt;h2 id="order-064">title: &amp;ldquo;이번 주의 팁 #64: Raw 문자열 리터럴&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/64
type: markdown
order: &amp;ldquo;064&amp;rdquo;&lt;/h2>
&lt;p>원래 totw/64로 2013-12-09에 게시됨&lt;/p>
&lt;p>&lt;em>작성자: Titus Winters &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>2017-10-23 업데이트&lt;/p></description></item><item><title>Abseil Tip 77 임시 객체, 이동, 복사</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-tow77/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-tow77/</guid><description>&lt;h2 id="order-077">title: &amp;ldquo;이번 주의 팁 #77: 임시 객체, 이동, 복사&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/77
type: markdown
order: &amp;ldquo;077&amp;rdquo;&lt;/h2>
&lt;p>원래 totw/77로 2014-07-09에 게시됨&lt;/p>
&lt;p>&lt;em>작성자: Titus Winters &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>&lt;/em>&lt;/p></description></item><item><title>Abseil Tip 86 클래스(enum class)를 활용한 열거형</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-tow86/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-tow86/</guid><description>&lt;hr>
&lt;h2 id="order-086">title: &amp;ldquo;이번 주의 팁 #86: 클래스(enum class)를 활용한 열거형&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/86
type: markdown
order: &amp;ldquo;086&amp;rdquo;&lt;/h2>
&lt;p>원래 totw/86로 2015-01-05에 게시됨&lt;/p></description></item><item><title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-paper-cot/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-paper-cot/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903&lt;/a>&lt;/p>
&lt;p>아래 글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-개요와-강점">논문의 개요와 강점&lt;/h3>
&lt;p>이 논문 **&amp;ldquo;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&amp;rdquo;**는 대규모 언어 모델이 복잡한 추론 작업을 수행할 수 있도록 하는 &lt;strong>Chain-of-Thought (CoT) prompting&lt;/strong> 방법을 제안합니다. 이 기법은 모델이 답을 도출하는 과정에서 중간 추론 단계를 생성하도록 유도하여, 단순한 입력-출력 쌍만을 사용하는 기존의 prompting 방식보다 더 나은 성능을 보여줍니다.&lt;/p></description></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2410.10733v2/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2410.10733v2/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.10733v2">https://arxiv.org/abs/2410.10733v2&lt;/a>&lt;/p>
&lt;p>아래 글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석-및-요약-deep-compression-autoencoder-for-efficient-high-resolution-diffusion-models">논문 분석 및 요약: &amp;ldquo;Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models&amp;rdquo;&lt;/h3>
&lt;h4 id="1-논문의-개요-및-주요-내용">1. &lt;strong>논문의 개요 및 주요 내용&lt;/strong>&lt;/h4>
&lt;p>이 논문은 고해상도 이미지 생성을 위한 효율적인 오토인코더인 **Deep Compression Autoencoder (DC-AE)**를 제안합니다. 기존의 잠재 확산 모델(latent diffusion models)은 오토인코더를 활용하여 고해상도 이미지를 잠재 공간으로 압축하여 계산 비용을 줄이지만, 공간 압축 비율이 높아질수록(예: 64배, 128배) 재구성 정확도가 크게 떨어지는 문제가 있었습니다. DC-AE는 이러한 문제를 해결하기 위해 &lt;strong>Residual Autoencoding&lt;/strong>과 &lt;strong>Decoupled High-Resolution Adaptation&lt;/strong>이라는 두 가지 핵심 기술을 도입했습니다.&lt;/p></description></item><item><title>HART Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2410.10812v1/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2410.10812v1/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.10812">https://arxiv.org/abs/2410.10812&lt;/a>&lt;/p>
&lt;p>아래 글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 Hybrid Autoregressive Transformer (HART)라는 새로운 이미지 생성 모델을 제안하며, 이 모델은 특히 높은 효율성을 자랑합니다. HART는 1024x1024 해상도의 이미지를 직접 생성할 수 있으며, 기존의 확산 모델과 비교하여 다음과 같은 독창적인 강점이 있습니다:&lt;/p></description></item><item><title>Learning Transferable Visual Models From Natural Language Supervision</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-paper-clip/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-paper-clip/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020&lt;/a>&lt;/p>
&lt;p>아래 글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문은 &amp;ldquo;Learning Transferable Visual Models From Natural Language Supervision&amp;quot;이라는 제목을 가진 CLIP (Contrastive Language-Image Pre-training) 모델에 대한 연구입니다.&lt;/p></description></item><item><title>The CoT Collection Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning</title><link>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2305.14045v2/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-14-paper-2305.14045v2/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2305.14045">https://arxiv.org/abs/2305.14045&lt;/a>&lt;/p>
&lt;p>아래 글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석-the-cot-collection-improving-zero-shot-and-few-shot-learning-of-language-models-via-chain-of-thought-fine-tuning">논문 분석: &lt;strong>The COT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h2 id="1-논문의-강점-및-독창적인-지점">1. &lt;strong>논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h2>
&lt;h3 id="1-문제-정의-및-기여">(1) &lt;strong>문제 정의 및 기여&lt;/strong>&lt;/h3>
&lt;p>이 논문은 **소형 언어 모델(100억 개 미만의 파라미터)**이 대규모 모델처럼 체인-오브-생각(Chain-of-Thought, CoT) 추론을 수행하지 못하는 문제를 해결하고자 합니다. 이를 위해, 저자들은 &lt;strong>COT COLLECTION&lt;/strong>이라는 새로운 데이터셋을 구축하여 &lt;strong>Flan-T5 모델&lt;/strong>을 CoT 방식으로 파인튜닝했습니다. 이 방법은 &lt;strong>제로샷(zero-shot)&lt;/strong> 및 &lt;strong>퓨샷(few-shot)&lt;/strong> 성능을 향상시켰습니다.&lt;/p></description></item><item><title>Abseil Tip 122 테스트 픽스처, 명확성, 그리고 데이터 흐름</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-tow122/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-tow122/</guid><description>&lt;hr>
&lt;h2 id="order-122">title: &amp;ldquo;이번 주의 팁 #122: 테스트 픽스처, 명확성, 그리고 데이터 흐름&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/122
type: markdown
order: &amp;ldquo;122&amp;rdquo;&lt;/h2>
&lt;p>원래 totw/122로 2016-08-30에 게시됨&lt;/p></description></item><item><title>Abseil Tip 55 이름 개수 세기와 unique_ptr</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-tow55/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-tow55/</guid><description>&lt;hr>
&lt;h2 id="order-055">title: &amp;ldquo;이번 주의 팁 #55: 이름 개수 세기와 &lt;code>unique_ptr&lt;/code>&amp;rdquo;
layout: tips
sidenav: side-nav-tips.html
published: true
permalink: tips/55
type: markdown
order: &amp;ldquo;055&amp;rdquo;&lt;/h2>
&lt;p>원래 totw/55로 2013-09-12에 게시됨&lt;/p>
&lt;p>&lt;em>작성자: Titus Winters &lt;a href="mailto:titus@google.com">(titus@google.com)&lt;/a>&lt;/em>&lt;/p>
&lt;p>2017-10-20 업데이트됨&lt;/p>
&lt;p>빠른 링크: &lt;a href="https://abseil.io/tips/55">abseil.io/tips/55&lt;/a>&lt;/p></description></item><item><title>Condition-Aware Neural Network for Controlled Image Generation</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2404.01143v1/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2404.01143v1/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2404.01143">https://arxiv.org/abs/2404.01143&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-요약-및-분석">논문의 요약 및 분석&lt;/h3>
&lt;h4 id="1-논문의-개요">1. 논문의 개요&lt;/h4>
&lt;p>논문 **&amp;ldquo;Condition-Aware Neural Network (CAN)&amp;rdquo;**은 이미지 생성 모델에 제어 기능을 추가하기 위해 새로운 접근 방식을 제안합니다. 이 모델은 기존의 조건 제어 방법과는 달리, &lt;strong>조건에 따라 동적으로 네트워크의 가중치를 조정&lt;/strong>하여 이미지 생성 과정을 제어합니다. 이 방식은 기존의 GAN, Transformer 기반의 이미지 생성 모델에서 주로 사용되던 &lt;strong>특징 공간 조작 대신 가중치 공간을 조작&lt;/strong>하는 것을 목표로 합니다.&lt;/p></description></item><item><title>DistriFusion Distributed Parallel Inference for High-Resolution Diffusion Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2402.19481v4/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2402.19481v4/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2402.19481">https://arxiv.org/abs/2402.19481&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-개요-및-독창성">논문의 개요 및 독창성&lt;/h3>
&lt;p>논문 &amp;ldquo;DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models&amp;quot;은 고해상도 이미지 생성에서 발생하는 연산 병목을 해결하기 위해 다중 GPU를 활용하여 Diffusion 모델의 추론 속도를 대폭 개선하는 알고리즘을 제안합니다. 기존의 Diffusion 모델은 대규모의 연산량으로 인해 고해상도 이미지 생성 시 실시간 응용에 적합하지 않았습니다. DistriFusion은 고해상도 이미지를 여러 패치(patch)로 나누어 각 GPU에서 병렬로 처리하면서도, 패치 간 상호작용 문제를 해결하기 위해 비동기 통신과 이전 단계의 특징 맵을 재활용하는 접근 방식을 도입합니다.&lt;/p></description></item><item><title>FastComposer Tuning-Free Multi-Subject Image Generation with Localized Attention</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2305.10431v2/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2305.10431v2/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2305.10431">https://arxiv.org/abs/2305.10431&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-요약-및-주요-내용-분석">논문 요약 및 주요 내용 분석&lt;/h3>
&lt;p>&lt;strong>논문 제목&lt;/strong>: &amp;ldquo;FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention&amp;rdquo;&lt;br>
&lt;strong>저자&lt;/strong>: Guangxuan Xiao, Tianwei Yin, William T. Freeman, Frédo Durand, Song Han (MIT)&lt;/p></description></item><item><title>VILA On Pre-training for Visual Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2312.07533v4/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2312.07533v4/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2312.07533">https://arxiv.org/abs/2312.07533&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석-vila-on-pre-training-for-visual-language-models">논문 분석: &lt;strong>VILA: On Pre-training for Visual Language Models&lt;/strong>&lt;/h3>
&lt;h4 id="1-논문의-배경-및-목표">&lt;strong>1. 논문의 배경 및 목표&lt;/strong>&lt;/h4>
&lt;p>이 논문은 **Visual Language Model (VLM)**을 개선하기 위한 &lt;strong>사전 학습(pre-training)&lt;/strong> 기법을 연구합니다. 최근 **대규모 언어 모델(LLM)**의 성공을 기반으로, 시각적 입력을 처리할 수 있는 &lt;strong>멀티모달 모델&lt;/strong>이 주목받고 있습니다. 그러나 기존 연구는 주로 **시각적 언어 지시 조정(visual instruction tuning)**에 집중되어 있었으며, &lt;strong>사전 학습 단계&lt;/strong>에 대한 심층적인 연구는 부족했습니다.&lt;/p></description></item><item><title>VILA-U a Unified Foundation Model Integrating Visual Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2409.04429v2/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-13-paper-2409.04429v2/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2409.04429">https://arxiv.org/abs/2409.04429&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-요약-및-강점-분석">논문 요약 및 강점 분석&lt;/h3>
&lt;p>논문 제목: &lt;strong>VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation&lt;/strong>&lt;/p></description></item><item><title>Abseil Tip 1 string_view의 활용 방법과 이점</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-tow1/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-tow1/</guid><description>&lt;h3 id="abseil-tip-1">&lt;strong>Abseil Tip #1: &lt;code>string_view&lt;/code>의 활용 방법과 이점&lt;/strong>&lt;/h3>
&lt;h4 id="string_view">&lt;strong>&lt;code>string_view&lt;/code>란 무엇이고, 왜 중요한가?&lt;/strong>&lt;/h4>
&lt;p>C++에서 문자열을 인자로 받는 함수를 작성할 때, 주로 다음 세 가지 방식 중 하나를 사용합니다:&lt;/p></description></item><item><title>Batch Calibration Rethinking Calibration for In-Context Learning and Prompt Engineering</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-paper-BatchCalibration/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-paper-BatchCalibration/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2309.17249">https://arxiv.org/abs/2309.17249&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점과-독창적인-지점">논문의 강점과 독창적인 지점&lt;/h3>
&lt;p>이 논문은 **Batch Calibration (BC)**이라는 새로운 기법을 제안하여 대형 언어 모델(LLMs)에서 **In-Context Learning (ICL)**과 &lt;strong>프롬프트 엔지니어링&lt;/strong>에서 발생하는 다양한 &lt;strong>바이어스 문제를 해결&lt;/strong>합니다. 이 방법은 기존의 교정 기법들이 가지는 한계를 극복하며, &lt;strong>제로샷(zero-shot)&lt;/strong> 및 &lt;strong>추론 시점에서만 적용되는(inference-only)&lt;/strong> 접근 방식으로 &lt;strong>추가적인 학습 비용 없이&lt;/strong> 성능을 향상시킬 수 있습니다.&lt;/p></description></item><item><title>LaRS Latent Reasoning Skills for Chain-of-Thought Reasoning</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-paper-LaRS/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-paper-LaRS/</guid><description>&lt;p>논문 : &lt;a href="https://aclanthology.org/2024.findings-emnlp.206/">https://aclanthology.org/2024.findings-emnlp.206/&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석-강점-독창적인-지점-핵심-알고리즘-설명-및-한계점">&lt;strong>논문 분석: 강점, 독창적인 지점, 핵심 알고리즘 설명 및 한계점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 **Latent Reasoning Skills (LaRS)**라는 새로운 접근법을 제안하여, &lt;strong>Chain-of-Thought (CoT)&lt;/strong> 추론을 위한 &lt;strong>효율적인 데모 선택 방법&lt;/strong>을 개발했습니다. 특히, LaRS는 &lt;strong>기존의 CoT 기반 접근법의 한계&lt;/strong>를 극복하기 위해 설계되었으며, &lt;strong>비지도 학습&lt;/strong>을 활용해 **라티스 기반의 추론 기술(latent reasoning skills)**을 자동으로 학습합니다.&lt;/p></description></item><item><title>LiteMoE Customizing On-device LLM Serving via Proxy Submodel Tuning</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-paper-LiteMoE/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-paper-LiteMoE/</guid><description>&lt;p>논문 : &lt;a href="https://dl.acm.org/doi/10.1145/3666025.3699355">https://dl.acm.org/doi/10.1145/3666025.3699355&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 &lt;strong>LiteMoE (Lightweight Mixture of Experts)&lt;/strong> 프레임워크를 제안하여, &lt;strong>모바일 환경에서 대형 언어 모델(LLM)을 효과적으로 맞춤화하고 배포&lt;/strong>할 수 있도록 하는 방법을 제시합니다. 이 방법은 특히 &lt;strong>모바일 장치의 제한된 리소스&lt;/strong>를 고려하여 설계되었으며, 다음과 같은 강점을 가집니다:&lt;/p></description></item><item><title>Query-Efficient Correlation Clustering with Noisy Oracle</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-paper-Query/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-paper-Query/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2402.01400">https://arxiv.org/abs/2402.01400&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석-강점-독창적인-지점-핵심-알고리즘-설명-및-한계점">&lt;strong>논문 분석: 강점, 독창적인 지점, 핵심 알고리즘 설명 및 한계점&lt;/strong>&lt;/h3>
&lt;p>이 논문은 &lt;strong>Correlation Clustering&lt;/strong> 문제를 다루며, 특히 &lt;strong>노이즈가 있는 환경에서의 효율적인 쿼리 방식&lt;/strong>을 제안합니다. 논문에서 제안된 접근법은 &lt;strong>다중 무장 밴딧(Combinatorial Multi-Armed Bandits, CMAB)&lt;/strong> 문제의 순수 탐색(Pure Exploration) 프레임워크를 활용하여, &lt;strong>효율적인 클러스터링을 수행&lt;/strong>하도록 설계되었습니다.&lt;/p></description></item><item><title>ShadowKV KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-12-paper-ShadowKV/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-12-paper-ShadowKV/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.21465">https://arxiv.org/abs/2410.21465&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점-및-독창적인-지점">논문의 강점 및 독창적인 지점&lt;/h3>
&lt;p>&lt;strong>ShadowKV&lt;/strong>는 긴 문맥을 가진 대형 언어 모델(LLM)에서 고속 추론을 가능하게 하는 시스템으로, 다음과 같은 강점과 독창적인 요소를 갖추고 있습니다:&lt;/p></description></item><item><title>COMET Towards Partical W4A4KV4 LLMs Serving</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-COMET/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-COMET/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.12168">https://arxiv.org/abs/2410.12168&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문 **&amp;ldquo;COMET: Towards Practical W4A4KV4 LLMs Serving&amp;rdquo;**는 대형 언어 모델(LLMs)을 효과적으로 배포하기 위해 양자화(quantization) 기법을 활용하여 GPU 성능을 최적화하는 방법을 제안합니다. 특히 INT4 텐서 코어를 활용해 모델의 메모리 사용량과 계산 효율성을 극대화하는 것이 핵심입니다. 이 논문에서의 주요 내용과 기여점을 설명드리겠습니다.&lt;/p></description></item><item><title>ELICIT LLM Augmentation via External In-Context Capability</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-ELICIT/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-ELICIT/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.09343">https://arxiv.org/abs/2410.09343&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>The paper titled &lt;strong>&amp;ldquo;ELICIT: LLM Augmentation via External In-Context Capability&amp;rdquo;&lt;/strong> introduces a novel framework aimed at enhancing the adaptive capabilities of large language models (LLMs) through a modular and externalized approach to in-context learning. The following provides a summary of its strengths, originality, core algorithm with an illustrative example, and its limitations.&lt;/p></description></item><item><title>EPIC Efficient Position-Independent Context Caching for Serving Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-EPIC/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-EPIC/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.15332">https://arxiv.org/abs/2410.15332&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문 **&amp;ldquo;Epic: Efficient Position-Independent Context Caching for Serving Large Language Models&amp;rdquo;**는 대형 언어 모델(LLM) 추론 속도를 높이기 위한 &lt;strong>위치 독립적 컨텍스트 캐싱(Position-Independent Context Caching, PIC)&lt;/strong> 기법을 도입한 Epic 시스템을 소개하고 있습니다. 주요 내용과 강점, 독창성, 핵심 알고리즘을 예시와 함께 설명한 후, 논문의 한계점에 대해 설명하겠습니다.&lt;/p></description></item><item><title>Foundations of Factor Investing</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-finance1/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-finance1/</guid><description>&lt;p>논문 : &lt;a href="https://www.msci.com/documents/1296102/1336482/Foundations_of_Factor_Investing.pdf">https://www.msci.com/documents/1296102/1336482/Foundations_of_Factor_Investing.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>&lt;strong>논문 분석: &amp;ldquo;Foundations of Factor Investing&amp;rdquo;&lt;/strong>&lt;/p>
&lt;h3 id="1-강점-및-독창적인-지점">1. 강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 &lt;strong>팩터 투자&lt;/strong>(Factor Investing)에 대한 포괄적인 기초를 제공하며, 팩터 기반 인덱스가 어떻게 시장 자본화 가중 인덱스와 비교하여 보다 투명하고 비용 효율적인 접근 방식을 제공할 수 있는지를 설명합니다. 논문의 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>MagicPIG LSH Sampling for Efficient LLM Generation</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-MagicPIG/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-MagicPIG/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.16179">https://arxiv.org/abs/2410.16179&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문 **&amp;ldquo;MagicPIG: LSH Sampling for Efficient LLM Generation&amp;rdquo;**은 대형 언어 모델(LLM)의 효율적 생성 및 디코딩 속도 향상을 목표로 &lt;strong>Locality Sensitive Hashing(LSH) 기반의 샘플링&lt;/strong> 기법을 제안합니다. 이 논문은 특히 TopK 주의(attention) 방법이 특정 작업에서 발생하는 한계를 극복하기 위해 설계되었습니다. 논문은 LSH를 활용하여 보다 정확하고 효율적인 주의 산출을 가능하게 하는 새로운 시스템, MagicPIG를 소개합니다.&lt;/p></description></item><item><title>RAG4ITOps A Supervised Fine-Tunable and Comprehensive RAG Framework for IT Operations and Maintenance</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-RAG4ITOps/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-RAG4ITOps/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.15805v1">https://arxiv.org/abs/2410.15805v1&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문 **&amp;ldquo;RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for IT Operations and Maintenance&amp;rdquo;**는 IT 운영 및 유지보수를 위한 질의응답(QA) 시스템을 개선하기 위한 &lt;strong>Retrieval-Augmented Generation (RAG)&lt;/strong> 프레임워크인 RAG4ITOps를 제안합니다. 다음은 논문의 강점, 독창성, 핵심 알고리즘 예시, 그리고 한계점에 대한 설명입니다.&lt;/p></description></item><item><title>Scientific Beta Multi-Beta Multi-Strategy Indices Implementing Multi-Factor Equity Portfolios with Smart Factor Indices</title><link>https://jaehun.me/ko/ko/posts/2024-11-11-paper-finance2/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-11-paper-finance2/</guid><description>&lt;p>논문 : &lt;a href="https://conferences.pionline.com/uploads/conference_admin/ERI_Scientific_Beta_Publication_Scientific_Beta_Multi-Beta_Multi-Strategy_Indices_Equity_Portfolios.pdf">https://conferences.pionline.com/uploads/conference_admin/ERI_Scientific_Beta_Publication_Scientific_Beta_Multi-Beta_Multi-Strategy_Indices_Equity_Portfolios.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>&lt;strong>논문 분석: &amp;ldquo;Scientific Beta Multi-Beta Multi-Strategy Indices: Implementing Multi-Factor Equity Portfolios with Smart Factor Indices&amp;rdquo;&lt;/strong>&lt;/p>
&lt;h3 id="1-강점-및-독창적인-지점">1. 강점 및 독창적인 지점&lt;/h3>
&lt;p>이 논문은 &lt;strong>팩터 투자 전략&lt;/strong>에서 다중 팩터 접근법을 제시하며, 투자자들이 다양한 팩터에 노출됨으로써 리스크를 줄이고 성과를 극대화할 수 있는 방법을 제안합니다. 논문의 주요 강점은 다음과 같습니다:&lt;/p></description></item><item><title>ALPINE Unveiling the Planning Capability of Autoregressive Learning in Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-ALPINE/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-ALPINE/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2405.09220">https://arxiv.org/abs/2405.09220&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>&lt;strong>논문 분석&lt;/strong>&lt;/p>
&lt;h3 id="강점과-독창적인-지점">강점과 독창적인 지점&lt;/h3>
&lt;p>이 논문 &amp;ldquo;ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models&amp;quot;에서는 &lt;strong>Transformer 기반 언어 모델&lt;/strong>의 경로 탐색 및 계획 능력을 분석하고 확장 가능한 알고리즘을 제안합니다. 이 연구의 주요 강점과 독창성은 다음과 같습니다:&lt;/p></description></item><item><title>Can Graph Learning Improve Planning in LLM-based Agents?</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-Can/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-Can/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2405.19119">https://arxiv.org/abs/2405.19119&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석">&lt;strong>논문 분석: &amp;ldquo;Can Graph Learning Improve Planning in LLM-based Agents?&amp;rdquo;&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="강점과-독창적인-지점">&lt;strong>강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>언어 모델과 그래프 학습의 통합&lt;/strong>:&lt;/p></description></item><item><title>Capital asset prices A theory of market equilibrium under conditions of risk</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance1/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance1/</guid><description>&lt;p>논문 : &lt;a href="https://www.jstor.org/stable/2977928">https://www.jstor.org/stable/2977928&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문 **&amp;ldquo;Capital Asset Prices: A Theory of Market Equilibrium under Conditions of Risk&amp;rdquo;**를 분석한 결과, 아래와 같이 논문의 강점, 독창적인 지점, 알고리즘의 전체 과정, 그리고 한계점을 설명하겠습니다.&lt;/p></description></item><item><title>DynamoLLM Designing LLM Inference Clusters for Performance and Energy Efficiency</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-DynamoLLM/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-DynamoLLM/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2408.00741">https://arxiv.org/abs/2408.00741&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석">&lt;strong>논문 분석: &amp;ldquo;DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency&amp;rdquo;&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="1-논문의-강점-및-독창적인-지점">&lt;strong>1. 논문의 강점 및 독창적인 지점&lt;/strong>&lt;/h3>
&lt;h4 id="강점">&lt;strong>강점:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>에너지 효율적인 LLM 인프라&lt;/strong>: 이 논문은 대규모 LLM 추론 클러스터에서 &lt;strong>에너지 소비를 최적화&lt;/strong>하고 &lt;strong>운영 비용을 줄이기 위해 DynamoLLM이라는 시스템&lt;/strong>을 제안합니다. 이 시스템은 LLM 추론의 다양한 특성을 활용하여 &lt;strong>에너지 효율성을 53% 개선&lt;/strong>하고, &lt;strong>운영 비용을 61% 절감&lt;/strong>하며 &lt;strong>탄소 배출량을 38% 줄였습니다&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>동적 재구성 능력&lt;/strong>: DynamoLLM은 &lt;strong>워크로드 변화에 따라 동적으로 재구성&lt;/strong>할 수 있는 시스템으로, &lt;strong>서버 인스턴스 수, 모델 병렬화, GPU 주파수 조정&lt;/strong> 등을 자동으로 조정합니다. 이를 통해 **성능 목표(SLO)**를 충족하면서도 에너지 소비를 최소화합니다.&lt;/li>
&lt;li>&lt;strong>다양한 LLM 요청에 대응&lt;/strong>: 이 시스템은 &lt;strong>입력 및 출력 토큰 길이, 모델의 특성, SLO 요구 사항&lt;/strong>에 따라 서로 다른 요청을 &lt;strong>다양한 서버 풀로 분류하고 최적화된 설정으로 처리&lt;/strong>합니다. 이를 통해 요청 유형에 따라 &lt;strong>다양한 에너지-성능 프로파일&lt;/strong>을 적용하여 효율을 높입니다.&lt;/li>
&lt;/ul>
&lt;h4 id="독창적인-지점">&lt;strong>독창적인 지점:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>계층적 제어 아키텍처&lt;/strong>: DynamoLLM은 &lt;strong>계층적 제어 시스템&lt;/strong>을 도입하여 클러스터, 풀, 인스턴스 레벨에서 각각 최적화를 수행합니다. 각 레벨에서 독립적으로 조정을 수행하며, 상위 레벨에서 전달된 제약 조건을 준수하면서 에너지 효율성을 극대화합니다.&lt;/li>
&lt;li>&lt;strong>빠른 재구성 및 스케일링&lt;/strong>: 서버 인스턴스, 모델 병렬화, GPU 주파수를 &lt;strong>빠르게 조정&lt;/strong>하여 &lt;strong>재구성 오버헤드를 최소화&lt;/strong>합니다. 이를 통해 변동하는 워크로드에도 빠르게 적응할 수 있습니다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="2-핵심-알고리즘-설명-및-예시">&lt;strong>2. 핵심 알고리즘 설명 및 예시&lt;/strong>&lt;/h3>
&lt;h4 id="입력-예시">&lt;strong>입력 예시&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>사용자가 다음과 같은 요청을 보낸다고 가정합니다:
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&amp;#34;Llama2-70B 모델을 사용하여 100,000개의 입력 토큰을 분석하고 요약을 생성해 주세요.&amp;#34;&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;/li>
&lt;/ul>
&lt;h4 id="dynamollm의-추론-과정-inference-process">&lt;strong>DynamoLLM의 추론 과정 (Inference Process)&lt;/strong>&lt;/h4>
&lt;p>&lt;strong>Step 1: 입력 분석 및 워크로드 분류&lt;/strong>&lt;/p></description></item><item><title>HYSYNTH Context-Free LLM Approximation for Guiding Program Synthesis</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-HYSYNTH/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-HYSYNTH/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2405.15880v2">https://arxiv.org/abs/2405.15880v2&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>&lt;strong>논문 분석: &amp;ldquo;HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis&amp;rdquo;&lt;/strong>&lt;/p>
&lt;h3 id="1-강점과-독창적인-지점">1. &lt;strong>강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>하이브리드 접근법&lt;/strong>: 이 논문은 **대형 언어 모델(LLMs)**과 &lt;strong>하향식 및 상향식 탐색&lt;/strong> 방식을 결합한 하이브리드 방법론을 제안합니다. 기존에는 순수 신경망 모델이 도메인 특화 언어(DSL)에서 정확한 프로그램을 생성하는 데 어려움이 있었지만, 이 논문에서는 **LLM의 생성 결과를 기반으로 확률적 문맥 자유 문법(PCFG)**을 학습하여 &lt;strong>상향식 프로그램 합성&lt;/strong>을 안내합니다.&lt;/p></description></item><item><title>MInference 1.0 Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-MInference/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-MInference/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2407.02490">https://arxiv.org/abs/2407.02490&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문-분석">&lt;strong>논문 분석: &amp;ldquo;MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention&amp;rdquo;&lt;/strong>&lt;/h3>
&lt;hr>
&lt;h3 id="강점과-독창적인-지점">&lt;strong>강점과 독창적인 지점&lt;/strong>&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>효율적인 긴 문맥 처리&lt;/strong>:&lt;/p></description></item><item><title>Portfolio Selection</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance2/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance2/</guid><description>&lt;p>논문 : &lt;a href="https://www.jstor.org/stable/2975974">https://www.jstor.org/stable/2975974&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문 **&amp;ldquo;Portfolio Selection&amp;rdquo; (Harry Markowitz, 1952)**를 분석한 결과를 토대로, 논문의 주요 강점, 독창성, 핵심 알고리즘 설명 및 예시, 그리고 한계점을 정리하여 설명하겠습니다.&lt;/p></description></item><item><title>The Cross-Section of Expected Stock Returns</title><link>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance3/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-10-paper-finance3/</guid><description>&lt;p>논문 : &lt;a href="https://www.jstor.org/stable/2329112">https://www.jstor.org/stable/2329112&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문 **&amp;ldquo;The Cross-Section of Expected Stock Returns&amp;rdquo;**를 분석한 결과를 토대로, 논문의 강점, 독창성, 핵심 알고리즘 설명 및 예시, 그리고 한계점을 정리하여 설명하겠습니다.&lt;/p></description></item><item><title>Efficient Streaming Language Models with Attention Sinks</title><link>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Efficient/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Efficient/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>네, 물론입니다.&lt;/p>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>어텐션 싱크(Attention Sinks) 개념 도입&lt;/strong>: 이 논문은 “어텐션 싱크”라는 새로운 개념을 도입하여, 초기 토큰이 중요한 의미를 갖지 않더라도 어텐션 점수를 흡수하여 모델의 성능을 안정적으로 유지하는 역할을 한다고 설명합니다. 이는 긴 문장에서도 성능 저하 없이 모델이 예측을 유지하도록 돕는 독창적인 접근입니다.&lt;/p></description></item><item><title>Enabling Tensor Language Model to Assist in Generating High-Performance Tensor Programs for Deep Learning</title><link>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Enabling/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Enabling/</guid><description>&lt;p>논문 : &lt;a href="https://www.usenix.org/system/files/osdi24-zhai.pdf">https://www.usenix.org/system/files/osdi24-zhai.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문은 고성능 텐서 프로그램을 효율적으로 생성하기 위해 **TLM (Tensor Language Model)**을 사용하는 프레임워크를 제안하고 있습니다. TLM은 텐서 프로그램의 탐색 공간을 확장하면서도, 효율적인 최적화를 통해 컴파일 성능을 향상시키기 위한 접근 방식을 사용합니다.&lt;/p></description></item><item><title>Meta Large Language Model Compiler Foundation Models of Compiler Optimization</title><link>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Meta/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Meta/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2407.02524">https://arxiv.org/abs/2407.02524&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문은 Meta AI에서 제안한 &lt;strong>LLM Compiler&lt;/strong>라는 모델을 설명합니다. 이는 컴파일러 최적화 작업을 위해 설계된 대규모 언어 모델(LLM)로, 코드 최적화 및 디스어셈블리와 같은 컴파일러 관련 작업에서 높은 성능을 발휘하도록 훈련되었습니다.&lt;/p></description></item><item><title>Model Tells You What to Discard Adaptive KV Cache Compression for LLMs</title><link>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Discard/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Discard/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2310.01801">https://arxiv.org/abs/2310.01801&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>논문은 &lt;strong>FastGen&lt;/strong>이라는 적응형 KV 캐시 압축 기법을 제안합니다. 이 방법은 대규모 언어 모델(LLM)의 생성 추론에서 메모리 사용량을 줄이고, 추가적인 모델 재훈련 없이도 고성능을 유지할 수 있도록 설계되었습니다.&lt;/p></description></item><item><title>Transformers are Multi-State RNNs</title><link>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Transformers/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-07-paper-Transformers/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2401.06104">https://arxiv.org/abs/2401.06104&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>이 논문은 **Transformers를 Multi-State RNN(MSRNN)으로 재해석하고, TOVA(Token Omission Via Attention)**라는 새로운 KV 캐시 압축 정책을 제안합니다. 이를 통해, Transformer 모델의 메모리 사용을 최적화하면서도 성능을 유지할 수 있습니다.&lt;/p></description></item><item><title>BUZZ Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-06-paper-buzz/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-06-paper-buzz/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.23079">https://arxiv.org/abs/2410.23079&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Beehive 구조와 Segmented Heavy Hitter 기반의 효율적 KV 캐시 관리&lt;/strong>: BUZZ는 일반적인 KV 캐시 방식의 메모리 문제를 해결하기 위해 &lt;strong>벌집(beehive) 구조&lt;/strong>와 &lt;strong>세분화된 Heavy Hitter&lt;/strong>를 결합한 새로운 KV 캐싱 방법을 제안합니다. 이를 통해 중요하지 않은 토큰을 효과적으로 제거하고 메모리 사용량을 줄이면서도 중요한 정보는 유지하는 방식입니다. 기존의 KV 캐시 방식과 달리 BUZZ는 &lt;strong>유사한 정보를 군집화하여 유지할 토큰을 선택&lt;/strong>하며, 이는 KV 캐시의 효율성을 극대화합니다.&lt;/p></description></item><item><title>CDMPP:ADevice-Model Agnostic Framework for Latency Prediction of Tensor Programs</title><link>https://jaehun.me/ko/ko/posts/2024-11-06-paper-CDMPP/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-06-paper-CDMPP/</guid><description>&lt;p>논문 : &lt;a href="https://i.cs.hku.hk/~cwu/papers/hphu-eurosys24.pdf">https://i.cs.hku.hk/~cwu/papers/hphu-eurosys24.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점-및-독창성">논문의 강점 및 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>장치 및 모델 독립적 지연 예측&lt;/strong>: 이 논문은 다양한 장치와 DNN 모델에 대해 지연 시간을 예측할 수 있는 &lt;strong>CDMPP (Cross-Device and Cross-Model Performance Prediction)&lt;/strong> 프레임워크를 제안합니다. 기존 방법들이 특정 장치나 모델에 대한 성능 예측에 국한된 반면, CDMPP는 다양한 DNN 모델과 장치에서 작동하도록 설계된 점에서 독창적입니다.&lt;/p></description></item><item><title>Don't Look Twice Faster Video Transformers with Run-Length Tokenization</title><link>https://jaehun.me/ko/ko/posts/2024-11-06-paper-Don/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-06-paper-Don/</guid><description>&lt;p>논문 : &lt;a href="https://openreview.net/pdf/e7782b237ab632c467717143b2b7ef283d71c282.pdf">https://openreview.net/pdf/e7782b237ab632c467717143b2b7ef283d71c282.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Run-Length Tokenization (RLT) 기법&lt;/strong>: 이 논문은 비디오의 정적인 패치를 효율적으로 처리하여 비디오 트랜스포머 모델의 속도를 크게 향상시키기 위해 &lt;strong>Run-Length Tokenization (RLT)&lt;/strong> 기법을 제안합니다. RLT는 비디오에서 시간이 지나도 변화가 없는 패치(정적 패치)를 찾아내어 중복된 패치를 단일 토큰으로 대체하며, 이러한 토큰에 &amp;lsquo;길이 인코딩&amp;rsquo;을 추가하여 변화 없는 영역을 효율적으로 표현합니다. 기존 방식은 모든 프레임을 동일하게 처리해 비효율적이었지만, RLT는 &lt;strong>콘텐츠 인식 방식&lt;/strong>을 통해 토큰 수를 줄이고, 이를 통해 학습 및 추론 속도를 크게 향상시킵니다【35†source】.&lt;/p></description></item><item><title>FLUX Fast Software-based Communication Overlap On GPUs Through Kernel Fusion</title><link>https://jaehun.me/ko/ko/posts/2024-11-06-paper-FLUX/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-06-paper-FLUX/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.06858">https://arxiv.org/abs/2406.06858&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;p>물론입니다. 논문의 주요 내용과 강점, 알고리즘 과정, 한계점에 대해 한국어로 설명드리겠습니다.&lt;/p></description></item><item><title>KVSharer Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing</title><link>https://jaehun.me/ko/ko/posts/2024-11-06-paper-KVSHARER/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-06-paper-KVSHARER/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2410.18517">https://arxiv.org/abs/2410.18517&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h3 id="논문의-강점과-독창성">논문의 강점과 독창성&lt;/h3>
&lt;p>&lt;strong>1. 독창적인 KVSharer 방식&lt;/strong>: 이 논문은 LLM(대형 언어 모델)에서 메모리 사용량을 줄이기 위해 &lt;strong>KVSharer&lt;/strong>라는 새로운 계층 단위의 KV(키-값) 캐시 공유 기법을 제안합니다. 기존 방식은 유사한 KV 캐시를 공유하여 메모리를 절약하려고 했지만, KVSharer는 &lt;strong>비유사한 계층 간 KV 캐시를 공유&lt;/strong>하는 방법이 성능을 더 잘 유지한다는 &lt;strong>역설적이고 직관에 반하는 현상&lt;/strong>을 발견했습니다.&lt;/p></description></item><item><title>Efficient Generative LLM Inference Using Phase Splitting</title><link>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Efficient/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Efficient/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2311.18677v2">https://arxiv.org/abs/2311.18677v2&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>논문 **&amp;ldquo;Splitwise: Efficient Generative LLM Inference Using Phase Splitting&amp;rdquo;**는 대형 언어 모델(LLM)의 추론을 효율적으로 수행하기 위해서 &lt;strong>추론 단계를 분리하는 Splitwise&lt;/strong>라는 기법을 제안합니다. 이 논문은 주로 대형 모델의 추론 요청 시 발생하는 비효율성을 개선하고자 합니다. Splitwise는 추론 과정의 &lt;strong>프롬프트 계산 단계&lt;/strong>와 &lt;strong>토큰 생성 단계&lt;/strong>를 분리하여 각 단계에 맞는 최적의 하드웨어 자원을 할당함으로써 성능과 비용을 동시에 개선하는 시스템입니다. 이 논문의 주요 강점, 독창적인 지점, 핵심 알고리즘의 예시, 그리고 한계점은 다음과 같습니다.&lt;/p></description></item><item><title>KernelGPT Enhanced Kernel Fuzzing via Large Language Models</title><link>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Kernalgpt/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Kernalgpt/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2401.00563">https://arxiv.org/abs/2401.00563&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>&lt;strong>KernelGPT&lt;/strong> 논문은 운영체제 커널에서의 결함 탐지를 위해 **대형 언어 모델(LLM)**을 활용한 자동화된 시스템 호출(specification) 생성 방법을 제안합니다. 기존의 커널 퍼징 도구 Syzkaller의 제약을 해결하기 위해 LLM을 활용하여 Syzkaller가 필요로 하는 system call의 사양을 자동으로 생성, 검증, 수정하는 방식을 도입했습니다. 이를 통해 기존의 수작업 기반 방식보다 더 높은 커버리지와 효율적인 퍼징을 달성할 수 있습니다.&lt;/p></description></item><item><title>Magicoder Empowering Code Generation with OSS-Instruct</title><link>https://jaehun.me/ko/ko/posts/2024-11-05-paper-magicoder/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-05-paper-magicoder/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2312.02120">https://arxiv.org/abs/2312.02120&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>논문 **&amp;ldquo;Magicoder: Empowering Code Generation with OSS-INSTRUCT&amp;rdquo;**는 대형 언어 모델(LLM)을 위한 오픈소스 기반 코드 생성 모델 &lt;strong>Magicoder&lt;/strong>를 제안하며, 이를 통해 LLM의 코드 생성 성능을 크게 향상시키는 방법을 소개합니다. 이 논문의 주요 강점, 독창적인 지점, 핵심 알고리즘의 예시, 그리고 한계점은 다음과 같습니다.&lt;/p></description></item><item><title>Optimal Kernel Orchestration for Tensor Programs with Korch</title><link>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Optimal/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-05-paper-Optimal/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.09465">https://arxiv.org/abs/2406.09465&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>업로드된 논문은 DNN(딥 뉴럴 네트워크)의 성능을 개선하기 위해 커널 오케스트레이션을 최적화하는 텐서 프로그램 최적화 도구인 Korch에 대해 다루고 있습니다. 아래는 논문의 강점, 독창성, 핵심 알고리즘 예시와 한계점에 대한 설명입니다.&lt;/p></description></item><item><title>SpotServe Serving Generative Large Language Models on Preemptible Instances</title><link>https://jaehun.me/ko/ko/posts/2024-11-05-paper-SpotServe/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-05-paper-SpotServe/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2311.15566">https://arxiv.org/abs/2311.15566&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>논문 **&amp;ldquo;SpotServe: Serving Generative Large Language Models on Preemptible Instances&amp;rdquo;**는 대형 언어 모델(LLM)을 경제적인 프리엠티브 인스턴스에서 효율적으로 서빙할 수 있는 새로운 분산 시스템 SpotServe를 제안합니다. 이 논문의 주요 강점과 독창적인 지점, 핵심 알고리즘 예시, 그리고 한계점을 다음과 같이 설명드리겠습니다.&lt;/p></description></item><item><title>GraphPipe Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism</title><link>https://jaehun.me/ko/ko/posts/2024-11-04-paper-GraphPipe/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-04-paper-GraphPipe/</guid><description>&lt;p>개인의견 : pipeline Parallelism에 관련된 논문을 몇개 봤는데 그 중에서 가장 재밋는 논문인것 같네요. graph의 toplogy-aware Pipeline Parallelism이라는 당연한 개념을 포함하여 효과적인 결과를 내는것 같습니다. 다만 구현에 대한 오버헤드는 쫌 있을꺼같군요&lt;/p></description></item><item><title>Helix Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs</title><link>https://jaehun.me/ko/ko/posts/2024-11-04-paper-Helix/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-04-paper-Helix/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.01566v1">https://arxiv.org/abs/2406.01566v1&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>논문에서는 &lt;strong>Helix&lt;/strong>라는 시스템을 소개하며, 이 시스템은 이기종(heterogeneous) GPU 클러스터 상에서 대규모 언어 모델(LLM)을 분산적으로 제공하기 위해 설계되었습니다. Helix의 주요 강점과 독창성, 핵심 알고리즘 예시, 한계점을 다음과 같이 정리해드립니다.&lt;/p></description></item><item><title>Memory Bounds for the Experts Problem</title><link>https://jaehun.me/ko/ko/posts/2024-11-04-paper-MemBound/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-04-paper-MemBound/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2204.09837">https://arxiv.org/abs/2204.09837&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer:&lt;/h1>
&lt;p>이 논문, **&amp;ldquo;Memory Bounds for the Experts Problem&amp;rdquo;**는 전문가 조언을 이용한 온라인 학습 문제에 대한 메모리 제약을 다루고 있습니다. 전통적으로 전문가 예측에 기반한 알고리즘은 매일 예측을 처리하고, 최고의 전문가와 비교하여 손실을 최소화하도록 설계됩니다. 본 논문에서는 이러한 문제를 스트리밍 모델 내에서 다루며, 메모리 사용을 최소화하면서도 최상의 성능을 낼 수 있는 알고리즘을 연구하고 상하 한계를 제시합니다. 주요 내용과 강점, 독창성, 핵심 알고리즘의 과정, 그리고 한계점에 대해 설명드리겠습니다.&lt;/p></description></item><item><title>Sequoia Scalable, Robust, and Hardware-aware Speculative Decoding</title><link>https://jaehun.me/ko/ko/posts/2024-11-04-paper-Sequoia/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-04-paper-Sequoia/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2402.12374">https://arxiv.org/abs/2402.12374&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer:&lt;/h1>
&lt;p>논문 **&amp;ldquo;Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding&amp;rdquo;**는 대형 언어 모델(LLM)에서 효율적인 추론(추론 속도 향상)을 위한 &lt;strong>추론 속도 개선 알고리즘인 Sequoia&lt;/strong>를 제안합니다. 기존의 추론 속도 가속화 기법들이 규모 확장성, 하이퍼파라미터 변화에 대한 견고성, 다양한 하드웨어 환경에 맞춘 적응성에서 한계를 보이는 반면, Sequoia는 이를 개선하여 더 빠르고 효율적인 추론이 가능하도록 설계되었습니다. 다음은 논문의 강점, 독창적인 기여점, 핵심 알고리즘의 예시와 과정, 그리고 논문의 한계점에 대한 설명입니다.&lt;/p></description></item><item><title>SpecExec Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</title><link>https://jaehun.me/ko/ko/posts/2024-11-04-paper-SpecExec/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-04-paper-SpecExec/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.02532">https://arxiv.org/abs/2406.02532&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p>
&lt;h1 id="q--논문의-핵심-내용과-강점-알고리즘-설명-그리고-한계점">Q : 논문의 핵심 내용과 강점, 알고리즘 설명, 그리고 한계점&lt;/h1>
&lt;h1 id="answer">Answer&lt;/h1>
&lt;p>논문 *&amp;ldquo;SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices&amp;rdquo;*의 주요 강점과 독창적인 점, 핵심 알고리즘 예시, 한계점 등을 한국어로 설명드리겠습니다.&lt;/p></description></item><item><title>Breaking the Curse of Quality Saturation with User-Centric Ranking</title><link>https://jaehun.me/ko/ko/posts/2024-11-03-paper-Breaking/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-03-paper-Breaking/</guid><description>&lt;p>개인의견 : 잘모르는 분야이지만 meta publication을 보다가 흥미로워 보여서 선택한 논문.(내가 잘몰라서 질문을 못하겠다 ㅎㅎ;)&lt;/p>
&lt;p>논문 : &lt;a href="https://research.facebook.com/file/640730811107621/Breaking-the-Curse-of-Quality-Saturation-with-User-Centric-Ranking.pdf">https://research.facebook.com/file/640730811107621/Breaking-the-Curse-of-Quality-Saturation-with-User-Centric-Ranking.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p></description></item><item><title>MEGABYTE Predicting Million-byte Sequences with Multiscale Transformers</title><link>https://jaehun.me/ko/ko/posts/2024-11-03-paper-MEGABYTE/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-03-paper-MEGABYTE/</guid><description>&lt;p>개인의견 : 패치 기반의 병렬 연산과 Cross-Attention을 통한 글로벌-로컬 상호작용, 그리고 토크나이저 free? 한 신기한 논문으로 보인다.&lt;/p>
&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2305.07185">https://arxiv.org/abs/2305.07185&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p></description></item><item><title>Reasoning over Public and Private Data in Retrieval-Based Systems</title><link>https://jaehun.me/ko/ko/posts/2024-11-03-paper-HardDiskDrive/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-03-paper-HardDiskDrive/</guid><description>&lt;p>개인의견 : 잘모르는 분야이지만 meta publication을 보다가 흥미로워 보여서 선택한 논문.(내가 잘몰라서 질문을 못하겠다 ㅎㅎ;)&lt;/p>
&lt;p>논문 : &lt;a href="https://research.facebook.com/file/3641873812712342/Hard-Disk-Drive-Failure-Analysis-and-Prediction---An-Industry-View.pdf">https://research.facebook.com/file/3641873812712342/Hard-Disk-Drive-Failure-Analysis-and-Prediction---An-Industry-View.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p></description></item><item><title>Reasoning over Public and Private Data in Retrieval-Based Systems</title><link>https://jaehun.me/ko/ko/posts/2024-11-03-paper-reasoning/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-03-paper-reasoning/</guid><description>&lt;p>개인의견 : 잘모르는 분야이지만 meta publication을 보다가 흥미로워 보여서 선택한 논문.(내가 잘몰라서 질문을 못하겠다 ㅎㅎ;)&lt;/p>
&lt;p>논문 : &lt;a href="https://research.facebook.com/file/960671338467090/Reasoning-over-Public-and-Private-Data-in-Retrieval-Based-Systems.pdf">https://research.facebook.com/file/960671338467090/Reasoning-over-Public-and-Private-Data-in-Retrieval-Based-Systems.pdf&lt;/a>&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p></description></item><item><title>FlexGen High-Throughput Generative Inference of Large Language Models with a Single GPU</title><link>https://jaehun.me/ko/ko/posts/2024-11-01-paper-FlexGen/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-01-paper-FlexGen/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2303.06865">https://arxiv.org/abs/2303.06865&lt;/a>&lt;/p>
&lt;p>개인의견 : 타겟은 다르지만 deepspeed(zero) 오프로딩을 한다는 측면에서는 비슷한 느낌을 받았다 그리고 정말 높은 throughtput을 위해서 quantization과 선형계획법으로 효율적으로 디바이스 사용을 가능하게 하는 방법으로 보인다. 다만 한계점으로 말하듯 TTFT(Time to First Token)가 증가하거나 quantization으로 인한 에러가 발생할 수 있는 구조로 생각이 된다.&lt;/p></description></item><item><title>KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches</title><link>https://jaehun.me/ko/ko/posts/2024-11-01-paper-KVCacheCompression/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-01-paper-KVCacheCompression/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2407.01527v1">https://arxiv.org/abs/2407.01527v1&lt;/a>&lt;/p>
&lt;p>개인의견 : 다양한 경량화 방법의 효과를 long context를 다루는 LLM에서 어떤효과를 주는지 분석을 통하여 인사이트를 주고있다.&lt;/p>
&lt;p>아래글은 Scholar GPT와 대화를 나눈 내용입니다.&lt;/p></description></item><item><title>Quest Query-Aware Sparsity for Efficient Long-Context LLM Inference</title><link>https://jaehun.me/ko/ko/posts/2024-11-01-paper-quest/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-01-paper-quest/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.10774">https://arxiv.org/abs/2406.10774&lt;/a>&lt;/p>
&lt;p>개인의견 : 역시 songhan.. KV cache는 항상 볼때마다 압축을 하거나 줄이고 싶지만 그 특성상 query와 관계를 알아야 이게 중요한 KV였다는걸 알 수 있어 고육지책으로 들고다녀야하는 부담이라고 생각이 드는데 이걸 참 재밋게 풀어 경량화를 하는거 같아서 흥미롭게 읽었습니다.&lt;/p></description></item><item><title>Teola Towards End-to-End Optimization of LLM-based Applications</title><link>https://jaehun.me/ko/ko/posts/2024-11-01-paper-teola/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-01-paper-teola/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2407.00326">https://arxiv.org/abs/2407.00326&lt;/a>&lt;/p>
&lt;p>개인의견 : 결국 프리미티브라고 하는 작은 단위로 나눈다는 개념을 도입하는거 같고 이게 LLM이 아닌 경우도 고려해서 한번에 serving을 한다는것이라고 이해를 했는데 vllm의 continuous batching + sequence를 unit한 단위로 나누어서 스케줄링하는거에 대비 얼마만큼의 이득이 있을까 궁금하긴하다.&lt;/p></description></item><item><title>What Matters in Transformers? Not All Attention is Needed Fusion</title><link>https://jaehun.me/ko/ko/posts/2024-11-01-paper-Not_All_Attention_is_Needed/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-11-01-paper-Not_All_Attention_is_Needed/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2406.15786v1">https://arxiv.org/abs/2406.15786v1&lt;/a>&lt;/p>
&lt;p>개인의견 : 내가 잘 이해를 한건지는 모르겠지만 결국 입력/출력의 cosine similarity를 보고 attention layer를 pruning 하겠다는것 같은데 이게 데이터셋의 분포 혹은 성향이 달라졌을때 잘 동작할지는 납득이 쉽게 되지 았았다. 하지만 최근LLM의 크기가 점점 커져가고 있는 가운데 이렇게 실제로 필요하지 않은 layer들이 있을 수 있다는 시사점을 주는 논문이라고 이해를 하였다.&lt;/p></description></item><item><title>Better &amp; Faster Large Language Models via Multi-token Prediction</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-paper-multi-token-prediction/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-paper-multi-token-prediction/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2404.19737">https://arxiv.org/abs/2404.19737&lt;/a>&lt;/p>
&lt;p>개인의견 : auto-regressive한 현재 LLM은 구조적으로 생성되는 토큰이 많아지면 많은 I/O를 동반하게 됩니다.
그렇기 때문에 memory-bound 로 빠지게 되어서 디바이스의 효율이 극악으로 떨어지게 됩니다. 이러한 문제를 해결하기 위한 많은 방법이 있겠지만 알고리즘적으로 여러개의 token을 생성하여 이러한 문제를 접근하는 논문인것 같습니다.&lt;/p></description></item><item><title>CacheBlend Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-paper-cache-blend/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-paper-cache-blend/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2405.16444">https://arxiv.org/abs/2405.16444&lt;/a>&lt;/p>
&lt;p>개인의견 : rag가 앞으로 정말 많이 사용될거 같은데 어떻게보면 이러한 방법론을 사용하여 kv cache를 partial하게 사용하고 update를 하는 방법에 대한 논문이군요&lt;/p></description></item><item><title>Keyformer KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-paper-keyformer/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-paper-keyformer/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2403.09054">https://arxiv.org/abs/2403.09054&lt;/a>&lt;/p>
&lt;p>개인의견 : 예전부터 KV cache중에 핵심이 되는 내용만 잘 선택하면 오버헤드를 줄일수 있지 않을까라고 생각했는데 핵심 토큰만 가지고 이러한 방법을 구현한 논문입니다.
이러한 논문을 볼때 마다 항상 과연 정확도에 얼마나 영향을 줄까라는 의심이 생길수 밖에 없지만 이렇게 알고리즘쪽에서 경량화를 많이 해주면 좋겠네요&lt;/p></description></item><item><title>Prompt Cache Modular Attention Reuse for Low-Latency Inference</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-paper-promt-cache/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-paper-promt-cache/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2311.04934">https://arxiv.org/abs/2311.04934&lt;/a>&lt;/p>
&lt;p>개인의견 : 일부 반복되는 입력이 같은 위치에 들어오는 task에서(prompt에서 전처리가 있을시에 더욱) prefill시에 효과적으로 caching이 될거 같아 간단하게 효과적인 caching을 할 수 있는 방법일것 같으나 효과적이지 않은 상황이 생길 수 도 있을것 같습니다. 때문에 PML이라는 언어를 기술하여 효과적으로 재사용을 하려고 하지 않나 생각됩니다.&lt;/p></description></item><item><title>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-paper-Taming/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-paper-Taming/</guid><description>&lt;p>논문 : &lt;a href="https://arxiv.org/abs/2403.02310">https://arxiv.org/abs/2403.02310&lt;/a>&lt;/p>
&lt;p>개인의견 : 서비스를 하는 수준에서 생각하면 continuous batching같은 테크닉을 사용하면 prefill을 스케줄을 어떠한 방식으로 할지가 궁금했는데 이러한 방법으로 stall을 줄이다니 재밋는 논문이네요. 다만 prefill의 stall을 줄이기 위해서 decode는 불필요하게 sequence length를 늘려야해서 서비스 관점에서는 필요한 테크닉이지면 생각해볼 점은 있을거 같습니다.&lt;/p></description></item><item><title>블로그 다시 시작..</title><link>https://jaehun.me/ko/ko/posts/2024-10-31-daily_1/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2024-10-31-daily_1/</guid><description>&lt;p>처음 블로그를 만든 날짜를 보니 포항에서 열심히 대학원을 다닐떄 였군요
아마 첫논문을 작성할 때 쯤부터 너무 정신이 없어서 간간히 작성하던 블로그가 멈춰있었던것 같습니다.&lt;/p></description></item><item><title>LLVM (clang) build and install (ubuntu 18.04)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-llvm-clang/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-llvm-clang/</guid><description>&lt;h1 id="clone-llvm-repo">clone llvm repo&lt;/h1>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">git clone -b llvmorg-10.0.0 https://github.com/llvm/llvm-project.git llvm10&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h1 id="configure">configure&lt;/h1>
&lt;p>Ninja를 사용하면 컴파일 시간을 많이 단축할 수 있다.&lt;/p></description></item><item><title>LLVM loop unroll and jam pass and view-cfg</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-llvm-unrollandjam/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-llvm-unrollandjam/</guid><description>&lt;p>
&lt;figure class="image-figure not-prose my-8" 
 data-lightbox-enabled="false"
 data-gallery-type="auto">
 &lt;div class="image-container">
 
 &lt;img
 src="https://jaehun.me/assets/images/llvm.jpeg"
 alt=""
 
 loading="lazy"
 decoding="async"
 data-gallery-src="https://jaehun.me/assets/images/llvm.jpeg"
 data-gallery-alt=""
 data-gallery-title="" />&lt;/div>

 &lt;/figure>
대학원 컴파일러 수업에서 ML을 이용하여 unroll and jam을 판별하는 모델을 학습을 하는 term project를 진행하였다.
unroll and jam pass는 이름에서 알 수 있듯이 loop 최적화에 관련된 pass로 unroll 과 jam을 수행하여 innermost loop body의 병렬성을 증가시켜서 제한된 resource의 utilization을 증가시키는 최적화이다.
내 기억이 맞다면 O2 이상의 최적화 부터 적용되는데 opt의 debug를 통하여 볼때 생각보다 잘? 사용이 안된다.
LLVM code를 보면 대부분 loop unroll과 loop fusion pass를 재활용하며 검사 정도만 하는데 이 때문에 da,lcssa,loop simplify가 조건을 만족하여도 unroll and jam pass가 동작되지 않는 경우가 많다.&lt;/p></description></item><item><title>간단논문 정리 DARTS DIFFERENTIABLE ARCHITECTURE SEARCH (ICLR 2019)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-DARTS/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-DARTS/</guid><description>&lt;h1 id="제목">제목&lt;/h1>
&lt;p>DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Hanxiao Liu,Karen Simonyan,Yiming Yang&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>기존 NAS가 상당수의 시간 혹은 cost가 필요(2000 GPU days of reinforcement learning, 3150 GPU days of evolution)이러한 원인 중 하나가 discrete domain, which leads to a large number of architecture evaluations required 때문이라고 분석. 물론 이전에도 filter size와 같은 것들을 연속적으로 학습 했으나 해당 논문은 블록, 그래프 토플로지 까지 학습하는 것을 목표로 함&lt;/p></description></item><item><title>간단논문 정리 End-to-End Deep Learning of Optimization Heuristics (PACT 17)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-End-to-EndDeepLearning-of-pptimization-heuristics/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-End-to-EndDeepLearning-of-pptimization-heuristics/</guid><description>&lt;h1 id="제목">제목&lt;/h1>
&lt;p>End-to-End Deep Learning of Optimization Heuristics&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Chris Cummins ; Pavlos Petoumenos ; Zheng Wang ; Hugh Leather&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>기존 머신러닝을 이용한 compiler optimizaion 방법에서는 human experts를 이용한 feature engineering 이 필요&lt;/p>
&lt;h1 id="contribution">Contribution&lt;/h1>
&lt;p>논문에서 제안하는 Source Rewriter &amp;amp; Language model을 이용하여 RAW PROGRAM CODE을 직접 이용하여 compiler optimizaion을 수행 아울어 transfer learning 을 이용하여 small number of program 에서도 학습을 수행&lt;/p></description></item><item><title>간단논문 정리 Fast and Effective Orchestration of Compiler Optimizations(Zhelong Pan,Rudolf Eigenmann;Purdue University ;CGO’06)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-Fast-and-EffectiveOrchestrationofCompilerOptimizations/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-Fast-and-EffectiveOrchestrationofCompilerOptimizations/</guid><description>&lt;h1 id="제목">제목&lt;/h1>
&lt;p>Fast and Effective Orchestration of Compiler Optimizations&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Zhelong Pan,Rudolf Eigenmann&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>compile-time optimizations 은 전반적으로 프로그램 성능을 향상시키지만 일부 기법은 성능 하락을 야기한다.
입력프로그램와 target architecture에 대한 불충분한 정보는 컴파일 시간에 정확도 향상을 향상 시키는 모델의 한계를 만든다.&lt;/p></description></item><item><title>간단논문 정리 TVM An Automated End-to-End Optimizing Compiler for Deep Learning (OSDI 18)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-tvm/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-tvm/</guid><description>&lt;p>
&lt;figure class="image-figure not-prose my-8" 
 data-lightbox-enabled="false"
 data-gallery-type="auto">
 &lt;div class="image-container">
 
 &lt;img
 src="https://jaehun.me/assets/images/tvm1.png"
 alt=""
 
 loading="lazy"
 decoding="async"
 data-gallery-src="https://jaehun.me/assets/images/tvm1.png"
 data-gallery-alt=""
 data-gallery-title="" />&lt;/div>

 &lt;/figure>&lt;/p>
&lt;h1 id="제목">제목&lt;/h1>
&lt;p>TVM: An Automated End-to-End Optimizing Compiler for Deep Learning&lt;/p>
&lt;h1 id="tvm">TVM?&lt;/h1>
&lt;p>해당논문은 머신러닝용 컴파일러중에 대표적인 TVM에 대한 paper입니다. 현재는 apache에서 관리 하고 있으며 graph level IR 을 통한 target-independent optimization,
autotune을 통한 target-dependent optimization 을 지원하며 llvm 및 vta를 통하여 cpu,gpu뿐만 아니라 FPGA를 backend로 지원합니다.&lt;/p></description></item><item><title>논문 정리 Chameleon Adaptive Code Optimization for Expedited Deep Neural Network Compilation(ICLR 2020)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-chameleon/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-chameleon/</guid><description>&lt;p>
&lt;figure class="image-figure not-prose my-8" 
 data-lightbox-enabled="false"
 data-gallery-type="auto">
 &lt;div class="image-container">
 
 &lt;img
 src="https://jaehun.me/assets/images/chameleon1.jpg"
 alt=""
 
 loading="lazy"
 decoding="async"
 data-gallery-src="https://jaehun.me/assets/images/chameleon1.jpg"
 data-gallery-alt=""
 data-gallery-title="" />&lt;/div>

 &lt;/figure>&lt;/p>
&lt;h1 id="제목">제목&lt;/h1>
&lt;p>Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh, Hadi Esmaeilzadeh&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>The current approaches are oblivious to the patterns in the design space of schedules that are available for exploitation, and causes inefficient search or even converges to solutions that may even be suboptimal.
Current solutions that rely on greedy sampling lead to significant fractions of the candidate configurations being redundant over iterations(long compilation time)&lt;/p></description></item><item><title>논문 정리 LLVM A Compilation Framework for Lifelong Program Analysis &amp; Transformation(CGO 04)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-llvm/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-llvm/</guid><description>&lt;h1 id="제목">제목&lt;/h1>
&lt;p>LLVM: A Compilation Framework for Lifelong Program Analysis &amp;amp; Transformation&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Chris Lattner Vikram Adve&lt;/p>
&lt;h1 id="개인적으로-느끼는-논문의-insight">개인적으로 느끼는 논문의 insight&lt;/h1>
&lt;p>Lifelong Program Analysis개념을 도입하여 Front-end를 제외한 부분에서 전체적인 최적화를 수행,SSA,machine-independent optimization
논문에서 제시된 개념이 지금의 llvm과 정확하게 일치하지는 않지만 대단하다..&lt;/p></description></item><item><title>논문 정리 NeuroVectorizer End-to-End Vectorization with Deep Reinforcement Learning (CGO 20)</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-paper-NeuroVectorizer/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-paper-NeuroVectorizer/</guid><description>&lt;p>
&lt;figure class="image-figure not-prose my-8" 
 data-lightbox-enabled="false"
 data-gallery-type="auto">
 &lt;div class="image-container">
 
 &lt;img
 src="https://jaehun.me/assets/images/nv1.png"
 alt=""
 
 loading="lazy"
 decoding="async"
 data-gallery-src="https://jaehun.me/assets/images/nv1.png"
 data-gallery-alt=""
 data-gallery-title="" />&lt;/div>

 &lt;/figure>&lt;/p>
&lt;h1 id="제목">제목&lt;/h1>
&lt;p>NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning&lt;/p>
&lt;h1 id="저자">저자&lt;/h1>
&lt;p>Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Sophia Shao, Krste Asanovic, Ion Stoica&lt;/p>
&lt;h1 id="motivation">Motivation&lt;/h1>
&lt;p>Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions
The vectorization is critical to enhancing the performance of compute-intensive workloads in modern computers.&lt;/p></description></item><item><title>자주쓰는 파이썬 스크립트 패턴</title><link>https://jaehun.me/ko/ko/posts/2021-2-12-python_script/</link><pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-12-python_script/</guid><description>&lt;p>개인적으로 shell보다는 python를 자주 사용하기 때문에 자주 사용하는 패턴들을 간단하게 정리&lt;/p>
&lt;h2 id="globos">glob,os&lt;/h2>
&lt;p>glob는 유닉스 스타일 경로명 패턴 확장 라이브러입니다. 이것과 os 라이브러리를 이용하면 간단하게 파일을 찾거나 바꿀수 있습니다.
개인적으로 실험 결과를 파싱할때 많이 사용하는 라이브러리 입니다.&lt;/p></description></item><item><title>apt-get source 바꾸기</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-apt-get/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-apt-get/</guid><description>&lt;p>우분투 저장소 한국 미러(kr.archive.ubuntu.com)가 한번씩 죽는 경우가 있어서 다른곳으로 source를 바꾸어 보았다.&lt;/p>
&lt;p>아래 3가지 방법중 1개를 택하자&lt;/p></description></item><item><title>docker tag 검색하기</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-docker-tag/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-docker-tag/</guid><description>&lt;p>도커에서 이미지를 검색할시는 아래처럼 도커의 내장명령어를 사용하면 된다&lt;/p>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">docker search images&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;p>그런데 tag검색은 당연히 있을줄 알았는데 존재하지 않아서 당황스러웠다. 다행히도 찾는 방법이 존재하였다.&lt;/p></description></item><item><title>docker 로 gitlab만들기</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-docker-gitlab/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-docker-gitlab/</guid><description>&lt;h1 id="도커-이미지-다운-및-실행">도커 이미지 다운 및 실행&lt;/h1>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="n">docker&lt;/span> &lt;span class="n">run&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">detach&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">hostname&lt;/span> &lt;span class="n">gitlab&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">example&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">com&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">publish&lt;/span> &lt;span class="mi">443&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">443&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">publish&lt;/span> &lt;span class="mi">80&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">80&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">publish&lt;/span> &lt;span class="mi">22&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">22&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">name&lt;/span> &lt;span class="n">gitlab&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">restart&lt;/span> &lt;span class="n">always&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">6&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">srv&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">etc&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">7&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">srv&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">logs&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="k">var&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">log&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">8&lt;/span>&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">volume&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">srv&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="k">var&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">opt&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">9&lt;/span>&lt;span class="cl"> &lt;span class="n">gitlab&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">gitlab&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">ce&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">latest&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;p>ip가 있을시에 gitlab.example.com 가 hostname이 된다. http접속을 위한 80번 포트 https 를 위한 443포트 ssh 접속을 위한 22번 포트를 열어준다. 데이터 백업을 위하여 config,logs,data에 볼륨을 백업한다&lt;/p></description></item><item><title>ElementryOS mouch pad Using it like a Mac Touch Gestures (Loki,Juno)</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-elementry-os/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-elementry-os/</guid><description>&lt;h1 id="adding-gestures">Adding Gestures&lt;/h1>
&lt;h2 id="add-yourself-to-the-input-group">Add yourself to the input group&lt;/h2>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">sudo gpasswd -a $USER input &lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h2 id="install-dependencies-sudo-apt-get-install-xdotool-wmctrl-libinput-tools-and-clone-and-install">Install dependencies sudo apt-get install xdotool wmctrl libinput-tools and Clone and install&lt;/h2>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-1"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-1"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-1">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">git clone http://github.com/bulletmark/libinput-gestures 
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">cd libinput-gestures 
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl">sudo ./libinput-gestures-setup install&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-1';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h2 id="create-a-custom-configuration-file">Create a custom configuration file&lt;/h2>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-2"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-2"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-2">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">vim ~/.config/libinput-gestures.conf&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-2';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-3"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-3"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-3">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">up&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">super&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Up&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">down&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">super&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Down&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">super&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Right&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">super&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Left&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">6&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">alt&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Left&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">7&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">alt&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Right&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">8&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">up&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">ctrl&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Page_Down&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">9&lt;/span>&lt;span class="cl">&lt;span class="n">gesture&lt;/span> &lt;span class="n">swipe&lt;/span> &lt;span class="n">down&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">xdotool&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="n">ctrl&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">Page_Up&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-3';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script></description></item><item><title>ElementryOS mouch pad Using it like a Mac Touch Gestures (Loki,Juno)</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-virtualenv/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-virtualenv/</guid><description>&lt;h1 id="about">About&lt;/h1>
&lt;p>가끔 파이썬 환경을 격리 할 필요가 있다. 이런 상황에서 virtualenv는 큰 도움이 된다.&lt;/p>
&lt;h1 id="install">Install&lt;/h1>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">sudo pip install virtualenv&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h1 id="usage">Usage&lt;/h1>
&lt;p>&lt;strong>가상환경 생성&lt;/strong>&lt;/p></description></item><item><title>inplace_swap</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-inplace_swap/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-inplace_swap/</guid><description>&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">void inplace_swap(int *x,int *y)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">{
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl"> *y=*x^*y;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl"> *x=*x^*y;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">5&lt;/span>&lt;span class="cl"> *y=*x^*y;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">6&lt;/span>&lt;span class="cl"> }&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script></description></item><item><title>jupyter notebook 다른python이 실행될 시</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-jupyter-kernel/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-jupyter-kernel/</guid><description>&lt;p>jupyter notebook에서 python3 를 실행하였는데 python2 커널이 계속 실행되었는데 아래외 같이 해결하면 된다.&lt;/p>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">ipython3 kernel install&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;p>references&lt;/p></description></item><item><title>Logitech MX anywhere 2s 우분투에서 제스쳐 사용하기</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-ubuntu-mxanywhere2s/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-ubuntu-mxanywhere2s/</guid><description>&lt;p>Logitech 마우스는 options 라는 소프트웨어를 기본적으로 제공하여 키맵변경 logitech flow 등 많은 기능을 사용할 수 있다. 그러나 options 가 mac과 windows에서만 제공하는 바람에 9만원이나 하는 마우스가 그냥 돌덩이가 되어버렸다. 그러나 역시 리눅스에서 사용할 수 있는 방법이 있었다.&lt;/p></description></item><item><title>ubuntu 16.04 python3 opencv 3.4 설치</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-opencv-3.4/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-opencv-3.4/</guid><description>&lt;h1 id="기존-설치된-패키지-업데이트">기존 설치된 패키지 업데이트&lt;/h1>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-0"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-0">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">sudo apt-get update
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">sudo apt-get upgrade&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-0';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h1 id="설치에-필요한-프로그램-설치">설치에 필요한 프로그램 설치&lt;/h1>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-1"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-1"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-1">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">sudo apt-get install build-essential cmake pkg-config libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev libavcodec-dev libavformat-dev libswscale-dev libxvidcore-dev libx264-dev libxine2-dev libv4l-dev v4l-utils libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libqt4-dev mesa-utils libgl1-mesa-dri libqt4-opengl-dev libatlas-base-dev gfortran libeigen3-dev python2.7-dev python3-dev python-numpy python3-numpy&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-1';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h1 id="opencv-설치">OpenCV 설치&lt;/h1>
&lt;h2 id="opencv-다운">opencv 다운&lt;/h2>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-2"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-2"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-2">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">mkdir opencv &amp;amp;&amp;amp; cd opencv
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.4.2.zip &amp;amp;&amp;amp; unzip opencv.zip
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">3&lt;/span>&lt;span class="cl">wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.4.2.zip&amp;amp;&amp;amp;unzip opencv_contrib.zip
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">4&lt;/span>&lt;span class="cl">rm -rf opencv.zip opencv_contrib.zip&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-2';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;h3 id="opencv-빌드">opencv 빌드&lt;/h3>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-3"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-3"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-3">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln">1&lt;/span>&lt;span class="cl">cd opencv-3.4.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">2&lt;/span>&lt;span class="cl">mkdir build &amp;amp;&amp;amp; cd build&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-3';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;div
 class="code-block-container border-border bg-card my-6 overflow-hidden rounded-xl border shadow-sm transition-all duration-200 ease-out hover:-translate-y-0.5 hover:shadow-md">
 
 &lt;div
 class="code-block-header bg-muted/30 border-border flex items-center justify-between border-b px-4 py-3">
 
 &lt;div class="flex items-center gap-2">
 &lt;div class="text-muted-foreground flex-shrink-0">
 
 &lt;svg class="h-4 w-4"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 20l4-16m4 4l4 4-4 4M6 16l-4-4 4-4" />
&lt;/svg>
 &lt;/div>
 &lt;span class="text-muted-foreground text-sm font-medium">
 PLAINTEXT
 &lt;/span>
 &lt;/div>

 
 &lt;div class="flex items-center gap-2">
 &lt;button
 class="collapse-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-4"
 data-default-state="expanded"
 data-collapsed="false"
 data-auto-collapse-lines="30"
 data-auto-collapse-height="400"
 data-collapsed-height="120"
 title="접기"
 aria-label="접기">
 &lt;span class="collapse-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z"/>&lt;/svg>
 &lt;/span>
 &lt;span class="collapse-text hidden sm:inline"
 >접기&lt;/span
 >
 &lt;/button>
 &lt;button
 class="copy-code-btn text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex items-center gap-1.5 rounded-md px-2 py-1 text-xs font-medium transition-all duration-200 ease-out focus:ring-2 focus:outline-none"
 data-code-id="code-4"
 title="복사"
 aria-label="복사">
 &lt;span class="copy-icon">
 
 &lt;svg class="h-3 w-3"
 fill="none"
 stroke="currentColor"
 viewBox="0 0 24 24">&lt;path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z" />
&lt;/svg>
 &lt;/span>
 &lt;span class="copy-text hidden sm:inline"
 >복사&lt;/span
 >
 &lt;/button>
 &lt;/div>
 &lt;/div>

 
 &lt;div class="code-block-content relative" id="code-4">
 &lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="ln"> 1&lt;/span>&lt;span class="cl">cmake -D CMAKE_BUILD_TYPE=RELEASE \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 2&lt;/span>&lt;span class="cl">-D CMAKE_INSTALL_PREFIX=/usr/local \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 3&lt;/span>&lt;span class="cl">-D WITH_TBB=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 4&lt;/span>&lt;span class="cl">-D WITH_IPP=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 5&lt;/span>&lt;span class="cl">-D WITH_1394=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 6&lt;/span>&lt;span class="cl">-D BUILD_WITH_DEBUG_INFO=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 7&lt;/span>&lt;span class="cl">-D BUILD_DOCS=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 8&lt;/span>&lt;span class="cl">-D INSTALL_C_EXAMPLES=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln"> 9&lt;/span>&lt;span class="cl">-D INSTALL_PYTHON_EXAMPLES=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">10&lt;/span>&lt;span class="cl">-D BUILD_EXAMPLES=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">11&lt;/span>&lt;span class="cl">-D BUILD_TESTS=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">12&lt;/span>&lt;span class="cl">-D BUILD_PERF_TESTS=OFF \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">13&lt;/span>&lt;span class="cl">-D WITH_QT=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">14&lt;/span>&lt;span class="cl">-D WITH_OPENGL=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">15&lt;/span>&lt;span class="cl">-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib-3.4.2/modules \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">16&lt;/span>&lt;span class="cl">-D WITH_V4L=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">17&lt;/span>&lt;span class="cl">-D WITH_FFMPEG=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">18&lt;/span>&lt;span class="cl">-D WITH_XINE=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">19&lt;/span>&lt;span class="cl">-D BUILD_NEW_PYTHON_SUPPORT=ON \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="ln">20&lt;/span>&lt;span class="cl">../&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
 
 &lt;div
 class="collapse-overlay to-card/90 pointer-events-none absolute inset-0 bg-gradient-to-b from-transparent via-transparent opacity-0 transition-opacity duration-300">
 &lt;div
 class="text-muted-foreground bg-card/80 border-border/50 hover:bg-primary/10 hover:text-primary hover:border-primary/30 absolute bottom-4 left-1/2 -translate-x-1/2 cursor-pointer rounded-full border px-3 py-1.5 text-xs backdrop-blur-sm transition-all duration-200">
 클릭하여 더 보기
 &lt;/div>
 &lt;/div>
 &lt;/div>
&lt;/div>


&lt;script>
(function() {
 const codeId = 'code-4';
 const copyBtn = document.querySelector('.copy-code-btn[data-code-id="' + codeId + '"]');
 const collapseBtn = document.querySelector('.collapse-code-btn[data-code-id="' + codeId + '"]');
 const codeContainer = document.getElementById(codeId);

 if (!codeContainer) return;

 
 if (copyBtn) {
 const copyIcon = copyBtn.querySelector('.copy-icon');
 const copyText = copyBtn.querySelector('.copy-text');

 copyBtn.addEventListener('click', async function() {
 try {
 
 let codeText = '';

 
 const codeTableCell = codeContainer.querySelector('.lntd:last-child code');
 if (codeTableCell) {
 codeText = codeTableCell.textContent || codeTableCell.innerText;
 } else {
 
 const codeElement = codeContainer.querySelector('code');
 if (codeElement) {
 
 const hasInlineLineNumbers = codeElement.querySelector('.ln');
 if (hasInlineLineNumbers) {
 
 const codeLines = codeElement.querySelectorAll('.cl');
 if (codeLines.length > 0) {
 codeText = Array.from(codeLines)
 .map(line => {
 const text = line.textContent || line.innerText;
 
 return text.replace(/\n+$/, '');
 })
 .join('\n')
 .replace(/\n+$/, ''); 
 } else {
 
 const allText = codeElement.textContent || codeElement.innerText;
 codeText = allText.replace(/^\d+/gm, '').replace(/^\s+/gm, '');
 }
 } else {
 
 codeText = codeElement.textContent || codeElement.innerText;
 }
 } else {
 
 codeText = codeContainer.textContent || codeContainer.innerText;
 }
 }

 
 codeText = codeText.trim();

 
 await navigator.clipboard.writeText(codeText);

 
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M5 13l4 4L19 7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사됨';
 }
 copyBtn.classList.add('text-green-600');

 
 setTimeout(() => {
 copyIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (copyText) {
 copyText.textContent = '복사';
 }
 copyBtn.classList.remove('text-green-600');
 }, 2000);

 } catch (err) {
 console.error('复制失败:', err);

 
 const range = document.createRange();
 const codeElement = codeContainer.querySelector('code') || codeContainer;
 range.selectNodeContents(codeElement);
 const selection = window.getSelection();
 selection.removeAllRanges();
 selection.addRange(range);

 
 if (copyText) {
 copyText.textContent = '선택됨';
 }

 setTimeout(() => {
 if (copyText) {
 copyText.textContent = '복사';
 }
 selection.removeAllRanges();
 }, 2000);
 }
 });
 }

 
 if (collapseBtn) {
 const collapseIcon = collapseBtn.querySelector('.collapse-icon');
 const collapseText = collapseBtn.querySelector('.collapse-text');
 const collapseOverlay = codeContainer.querySelector('.collapse-overlay');

 
 let codeElement = codeContainer.querySelector('pre.chroma');
 if (!codeElement) {
 codeElement = codeContainer.querySelector('pre');
 }

 const defaultState = collapseBtn.dataset.defaultState || 'expanded';
 const isCollapsedAttr = collapseBtn.dataset.collapsed === 'true';
 const autoCollapseLines = parseInt(collapseBtn.dataset.autoCollapseLines) || 30;
 const autoCollapseHeight = parseInt(collapseBtn.dataset.autoCollapseHeight) || 400;
 const collapsedHeight = parseInt(collapseBtn.dataset.collapsedHeight) || 120;

 let isCollapsed = false;

 
 function initCollapse() {
 
 const shouldCollapse = isCollapsedAttr ||
 defaultState === 'collapsed' ||
 shouldAutoCollapse();

 if (shouldCollapse) {
 setCollapsed(true, false); 
 }
 }

 function shouldAutoCollapse() {
 
 if (codeElement) {
 const lines = codeElement.querySelectorAll('.line, .cl');
 const height = codeElement.offsetHeight;
 return lines.length > autoCollapseLines || height > autoCollapseHeight;
 }

 
 const containerHeight = codeContainer.offsetHeight;
 if (containerHeight > autoCollapseHeight) {
 return true;
 }

 
 const textContent = codeContainer.textContent || codeContainer.innerText || '';
 const estimatedLines = textContent.split('\n').length;
 return estimatedLines > autoCollapseLines;
 }

 function setCollapsed(collapsed, animate = true) {
 if (!collapseOverlay) return;

 isCollapsed = collapsed;

 if (collapsed) {
 
 codeContainer.style.maxHeight = collapsedHeight + 'px';
 codeContainer.style.overflow = 'hidden';
 collapseOverlay.style.opacity = '1';
 collapseOverlay.style.pointerEvents = 'auto';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath stroke-linecap=\u0022round\u0022 stroke-linejoin=\u0022round\u0022 stroke-width=\u00222\u0022 d=\u0022M19 9l-7 7-7-7\u0022 \/\u003e\n\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '펼치기';
 }
 collapseBtn.title = '펼치기';

 } else {
 
 codeContainer.style.maxHeight = '';
 codeContainer.style.overflow = '';
 collapseOverlay.style.opacity = '0';
 collapseOverlay.style.pointerEvents = 'none';

 
 collapseIcon.innerHTML = `\n \u003csvg class=\u0022h-3 w-3\u0022\n fill=\u0022none\u0022\n stroke=\u0022currentColor\u0022\n viewBox=\u00220 0 24 24\u0022\u003e\u003cpath fill=\u0022currentColor\u0022 d=\u0022M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6l-6 6z\u0022\/\u003e\u003c\/svg\u003e`;
 if (collapseText) {
 collapseText.textContent = '접기';
 }
 collapseBtn.title = '접기';
 }

 
 if (animate) {
 codeContainer.style.transition = 'max-height 0.3s ease-out';
 setTimeout(() => {
 codeContainer.style.transition = '';
 }, 300);
 }
 }

 function toggleCollapse() {
 setCollapsed(!isCollapsed, true);
 }

 
 collapseBtn.addEventListener('click', toggleCollapse);

 
 if (collapseOverlay) {
 collapseOverlay.addEventListener('click', () => {
 if (isCollapsed) {
 setCollapsed(false, true);
 }
 });
 }

 
 initCollapse();
 }
})();
&lt;/script>
&lt;p>컴파일을 한다 밑의 4는 예시일뿐 본인의 코어수를 입력하면 된다.&lt;/p></description></item><item><title>Ubuntu pdf 를 이미지로 변환</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-pdftoimage/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-pdftoimage/</guid><description>&lt;h2 id="about">About&lt;/h2>
&lt;p>Pdf 를 이미지로 변환해야할 일이 종종 있어 구글에서 검색되는 사이트를 이용하곤 하지만 많은 양의 pdf 를 변환해야 하거나 개인정보가 있는 데이터를 서버에 올리기 꺼림직 한 경우가 있다 이럴때 터미널에서 직접 pdf 를 변환해보자.
pdftoppm 를 이용할 것이며 다음과 같이 변환 하면 된다.&lt;/p></description></item><item><title>ubuntu에서 parallel gzip사용하여 빠르게 압축하기(pigz)</title><link>https://jaehun.me/ko/ko/posts/2021-2-11-pigz/</link><pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-2-11-pigz/</guid><description>&lt;h1 id="pigz">Pigz?&lt;/h1>
&lt;p>최근 많은 디바이스가 멀티코어를 사용하며 많은 프로그램이 멀티코어, 멀티프로세서를 이용하도록 작성되고있다. 이러한 흐름에 맞추어 압축프로그램인 &lt;code>gzip&lt;/code> 을 멀티코어를 사용하도록 개발하는 프로젝트인 &lt;code>pigz&lt;/code>를 이용하면 빠른 압축을 진행 할 수 있다.&lt;/p></description></item><item><title>Linux ubuntu nvidia-docker 설치 및 자주 쓰는 명령어</title><link>https://jaehun.me/ko/ko/posts/2021-1-8-docker/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-1-8-docker/</guid><description>&lt;h1 id="도커란-">도커란 ?&lt;/h1>
&lt;p>최근 많은 어플리케이션등은 컨테이너(container)를 통하여 관리 되고 있습니다. 컨테이너에는 코드와 실행에 필요한 소프트웨어 패키지가 담겨져 있으며 어떠한 환경이든 빠르고 안전하게(격리를 통하여) 실행할 수 있습니다.&lt;/p></description></item><item><title>Linux ubuntu Zsh 및 oh-my-zsh 설치</title><link>https://jaehun.me/ko/ko/posts/2021-1-3-zsh/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-1-3-zsh/</guid><description>&lt;h2 id="zsh-이란">zsh 이란?&lt;/h2>
&lt;p>sh의 기능을 확장한 많은 종류의 shell들(ex.. csh,bash,tsh&amp;hellip;) 이 있지만 zsh은 그중에서 강력한 셀중 하나이며 많은 플러그인이 존재합니다.&lt;/p>
&lt;p>오늘은 linux에서 zsh의 설치법을 알아보고자 합니다.&lt;/p></description></item><item><title>블로그 시작..</title><link>https://jaehun.me/ko/en/posts/2021-1-3-daily_1/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/en/posts/2021-1-3-daily_1/</guid><description>&lt;p>지금것 wordpress를 이용하여 블로그를 운영했으나, 2021년 부터 Github blog로 이전을 합니다.
글을 쓰는 빈도가 높지 않았는데 열심히 써보도록 하곘습니다.&lt;/p></description></item><item><title>블로그 시작..</title><link>https://jaehun.me/ko/ko/posts/2021-1-3-daily_1/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2021-1-3-daily_1/</guid><description>&lt;p>지금것 wordpress를 이용하여 블로그를 운영했으나, 2021년 부터 Github blog로 이전을 합니다.
글을 쓰는 빈도가 높지 않았는데 열심히 써보도록 하곘습니다.&lt;/p></description></item></channel></rss>