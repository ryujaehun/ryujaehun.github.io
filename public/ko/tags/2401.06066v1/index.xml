<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2401.06066v1 on Jaehun's Blog</title><link>https://jaehun.me/ko/tags/2401.06066v1/</link><description>Recent content in 2401.06066v1 on Jaehun's Blog</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Sun, 29 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehun.me/ko/tags/2401.06066v1/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title><link>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.06066v1/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-06-29-paper-2401.06066v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2401.06066v1">논문 링크&lt;/a>&lt;/p>
&lt;h1 id="deepseekmoe-정리--dense-성능을-40--flops로-끌어낸-세분화-전문가-트릭">DeepSeekMoE 정리 – Dense 성능을 &lt;strong>40 % FLOPs&lt;/strong>로 끌어낸 ‘세분화-전문가’ 트릭&lt;/h1>
&lt;h2 id="한-줄-요약-tldr">한 줄 요약 (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Fine-Grained Expert Segmentation (FGES) + Shared Experts (SEI)&lt;/strong> 로 FFN-MoE를 재설계한 &lt;strong>DeepSeekMoE&lt;/strong>는
동일 FLOPs에서 &lt;strong>Dense 상한선의 95 %↑&lt;/strong> 성능을 달성하고, 16 B 모델 기준 &lt;strong>LLaMA-2 7 B와 동급 품질을 연산량 0.4×&lt;/strong> 로 구현한다.&lt;/p></description></item></channel></rss>