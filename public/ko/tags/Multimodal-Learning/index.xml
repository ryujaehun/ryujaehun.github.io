<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multimodal Learning on Jaehun's Blog</title><link>https://jaehun.me/ko/tags/Multimodal-Learning/</link><description>Recent content in Multimodal Learning on Jaehun's Blog</description><generator>Hugo</generator><language>ko-kr</language><lastBuildDate>Sun, 06 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jaehun.me/ko/tags/Multimodal-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Janus-Pro: UnifiedMultimodalUnderstanding and Generation with Data and Model Scaling</title><link>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.17811v1/</link><pubDate>Sun, 06 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-06-paper-2501.17811v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2501.17811v1">ë…¼ë¬¸ ë§í¬&lt;/a>&lt;/p>
&lt;h1 id="janus-pro-7b-dual-encoder-multimodal-llm-that-outsmarts-bigger-models">&lt;strong>Janus-Pro 7B: Dual-Encoder Multimodal LLM That Outsmarts Bigger Models&lt;/strong>&lt;/h1>
&lt;h2 id="í•œ-ì¤„-ìš”ì•½-tldr">í•œ ì¤„ ìš”ì•½ (TL;DR)&lt;/h2>
&lt;p>&lt;em>SigLIP ì´í•´ ì¸ì½”ë” + VQ ìƒì„± ì¸ì½”ë”&lt;/em>ë¥¼ ì™„ì „ížˆ ë¶„ë¦¬í•œ ë’¤ &lt;strong>7 B íŒŒë¼ë¯¸í„° LLM&lt;/strong>ì— ë¶™ì´ëŠ” â€˜Dual-Encoder + Adapterâ€™ ì„¤ê³„ë¡œ, 13 B í†µí•© ëª¨ë¸(TokenFlow-XL)ì„ &lt;strong>ì´í•´Â·ìƒì„± ì–‘ìª½ì—ì„œ ë™ì‹œì— ì œì¹˜ëŠ”&lt;/strong> ì²« ì‚¬ë¡€. â€“ &lt;strong>MMBench 79.2 (+10.3 pt) / GenEval 0.80 (+45 %)&lt;/strong>&lt;/p></description></item><item><title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title><link>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.10302v1/</link><pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-05-paper-2412.10302v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2412.10302v1">ë…¼ë¬¸ ë§í¬&lt;/a>&lt;/p>
&lt;h1 id="deepseek-vl2--ìž‘ê³ -ë¹ ë¥´ë©´ì„œ-ê³ í•´ìƒë„ê¹Œì§€-ì •í™•í•œ-ë©€í‹°ëª¨ë‹¬-llm">DeepSeek-VL2 â€” â€œìž‘ê³  ë¹ ë¥´ë©´ì„œ ê³ í•´ìƒë„ê¹Œì§€ ì •í™•í•œâ€ ë©€í‹°ëª¨ë‹¬ LLM&lt;/h1>
&lt;h2 id="í•œ-ì¤„-ìš”ì•½-tldr">í•œ ì¤„ ìš”ì•½ (TL;DR)&lt;/h2>
&lt;p>Dynamic Tiling Ã— MLA-MoE Ã— 800 B VL ë°ì´í„°ë¼ëŠ” ì„¸ ì¶•ì˜ ì„¤ê³„ë¡œ, &lt;strong>4.5 B í™œì„± íŒŒë¼ë¯¸í„°&lt;/strong> ëª¨ë¸ì´ &lt;strong>DocVQA 92.3 / MMStar 61.3&lt;/strong> ë“± ë™ê¸‰ ê³µê°œ VLM ëŒ€ë¹„ â–²2â€Šâ€“â€Šâ–²10 pt ì„±ëŠ¥ì„ ë‚´ë©´ì„œë„ &lt;strong>ë‹¨ì¼ 10 GB GPU&lt;/strong>ì—ì„œ ê³ í•´ìƒë„(ìµœëŒ€ 3.5 KÂ²) ì´ë¯¸ì§€ë¥¼ ì‹¤ì‹œê°„ ì²˜ë¦¬í•œë‹¤.&lt;/p></description></item><item><title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2410.13848v1/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://jaehun.me/ko/ko/posts/2025-07-02-paper-2410.13848v1/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2410.13848v1">ë…¼ë¬¸ ë§í¬&lt;/a>&lt;/p>
&lt;h1 id="-janus-ë‘-ì–¼êµ´ì˜-ì¸ì½”ë”ë¡œ-ë©€í‹°ëª¨ë‹¬-ì´í•´--ìƒì„±ì˜-ë”œë ˆë§ˆë¥¼-ê¹¬-13-b-ëª¨ë¸">ðŸ“ Janus: &lt;strong>ë‘ ì–¼êµ´ì˜ ì¸ì½”ë”&lt;/strong>ë¡œ ë©€í‹°ëª¨ë‹¬ ì´í•´ + ìƒì„±ì˜ ë”œë ˆë§ˆë¥¼ ê¹¬ 1.3 B ëª¨ë¸&lt;/h1>
&lt;h2 id="í•œ-ì¤„-ìš”ì•½-tldr">í•œ ì¤„ ìš”ì•½ (TL;DR)&lt;/h2>
&lt;p>&lt;strong>Janus&lt;/strong>ëŠ” ë©€í‹°ëª¨ë‹¬ ì´í•´ì™€ ì´ë¯¸ì§€ ìƒì„±ì— ìš”êµ¬ë˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ â€• ê³ ì°¨ì› semantic vs. ì €ì°¨ì› texture â€• ì˜ â€œí‘œí˜„ ì¶©ëŒâ€ì„ &lt;strong>ë“€ì–¼ ë¹„ì „ ì¸ì½”ë”&lt;/strong>(SigLIP + VQ Tokenizer)ë¡œ ë¶„ë¦¬í•¨ìœ¼ë¡œì¨, ë‹¨ 1.3 B íŒŒë¼ë¯¸í„° ëª¨ë¸ì´ ê¸°ì¡´ ë‹¨ì¼-ì¸ì½”ë” í†µí•© ëª¨ë¸ë³´ë‹¤ **ì´í•´ +41 %, FID âˆ’8 %**ë¥¼ ë™ì‹œì— ë‹¬ì„±í•œ ì²« ì‚¬ë¡€ë‹¤.&lt;/p></description></item></channel></rss>