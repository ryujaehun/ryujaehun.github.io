<!doctype html>
<html
  lang="ko-kr"
  data-theme="claude">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
<meta
  name="viewport"
  content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />

<title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | Jaehun&#39;s Blog</title>

<meta name="description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 요약 및 결론 이 논문,..." />
<meta name="keywords" content="개발, 컴파일러, 프로그래밍, 에세이" />
<meta name="author" content="" />
<meta name="language" content="ko-kr" />

<meta name="robots" content="index, follow" />
<link rel="canonical" href="http://192.168.1.11:1314/posts/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/" />



  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />



<link rel="manifest" href="/site.webmanifest" />
<meta property="og:url" content="http://192.168.1.11:1314/posts/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/">
  <meta property="og:site_name" content="Jaehun&#39;s Blog">
  <meta property="og:title" content="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness">
  <meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 요약 및 결론 이 논문, “FlashAttention”, 은 Transformer 모델의 self-attention 모듈에서 메모리 및 계산 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 이를 통해 긴 시퀀스를 다루는 Transformer의 병목을 해소하며 기존 방법 대비 속도와 메모리 사용에서 큰 개선을 이룹니다.">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-18T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-18T00:00:00+00:00">
    <meta property="og:image" content="http://192.168.1.11:1314/images/og-default.avif">


  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://192.168.1.11:1314/images/og-default.avif">
  <meta name="twitter:title" content="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness">
  <meta name="twitter:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 요약 및 결론 이 논문, “FlashAttention”, 은 Transformer 모델의 self-attention 모듈에서 메모리 및 계산 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 이를 통해 긴 시퀀스를 다루는 Transformer의 병목을 해소하며 기존 방법 대비 속도와 메모리 사용에서 큰 개선을 이룹니다.">


  <meta itemprop="name" content="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness">
  <meta itemprop="description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 요약 및 결론 이 논문, “FlashAttention”, 은 Transformer 모델의 self-attention 모듈에서 메모리 및 계산 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 이를 통해 긴 시퀀스를 다루는 Transformer의 병목을 해소하며 기존 방법 대비 속도와 메모리 사용에서 큰 개선을 이룹니다.">
  <meta itemprop="datePublished" content="2024-12-18T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-12-18T00:00:00+00:00">
  <meta itemprop="wordCount" content="12710">
  <meta itemprop="image" content="http://192.168.1.11:1314/images/og-default.avif">
  <meta itemprop="keywords" content="Paper-Review,With-Gpt">







<script>
  window.HUGO_SEARCH_CONFIG = {
    searchIndexURL: "\"/index.json\""
  };
</script>


































<script>

  window.HUGO_GALLERY_CONFIG = {
    justified_gallery:  false ,
    lightbox:  false ,
    justified: "{}",
    lightbox_options: "{}"
  };
</script>




  
    <link rel="stylesheet" href="/css/compiled.css" />
  





  
    <link rel="stylesheet" href="/css/chroma.css" />
  












      <script src="/js/main.js"></script>



  <script src="/js/gumshoe.polyfills.min.js"></script>



        <script src="/js/toc.js" defer></script>



      <script src="/js/search.js" defer></script>



      <script src="/js/dock.js" defer></script>











  











    <script>
      
      (function() {
        const theme = localStorage.getItem('theme') || 'system';
        const colorScheme = localStorage.getItem('colorScheme') || 'claude';

        
        document.documentElement.setAttribute('data-theme', colorScheme);

        
        function applyTheme() {
          if (theme === 'dark' || (theme === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark');
          } else {
            document.documentElement.classList.remove('dark');
          }
        }

        applyTheme();

        
        if (theme === 'system') {
          window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', applyTheme);
        }
      })();
    </script>
  </head>

  <body class="bg-background text-foreground min-h-screen antialiased">
    
    
  <div
    id="reading-progress-container"
    class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out"
    data-height="3"
    data-smooth-scroll="true"
    data-hide-on-complete="false">
    
    <div class="reading-progress-bg w-full"></div>

    
    <div
      id="reading-progress-bar"
      class="from-primary to-primary/80 reading-progress-bar 
        transition-all duration-150 ease-out
       absolute top-0 left-0 w-0 bg-gradient-to-r"></div>
  </div>

  <script>
    (function () {
      "use strict";

      const progressContainer = document.getElementById(
        "reading-progress-container",
      );
      const progressBar = document.getElementById("reading-progress-bar");

      if (!progressContainer || !progressBar) return;

      
      const config = {
        smoothScroll: progressContainer.dataset.smoothScroll === "true",
        hideOnComplete: progressContainer.dataset.hideOnComplete === "true",
      };

      let isVisible = true;
      let gumshoeInstance = null;

      
      function calculateProgress() {
        const scrollTop =
          window.pageYOffset || document.documentElement.scrollTop;
        const documentHeight = document.documentElement.scrollHeight;
        const windowHeight = window.innerHeight;
        const scrollableHeight = documentHeight - windowHeight;

        if (scrollableHeight <= 0) return 0;

        return Math.min(Math.max((scrollTop / scrollableHeight) * 100, 0), 100);
      }

      
      function updateProgress() {
        const progress = calculateProgress();
        progressBar.style.width = progress + "%";

        
        if (config.hideOnComplete && progress >= 99.5) {
          if (isVisible) {
            progressContainer.style.opacity = "0";
            isVisible = false;
          }
        } else if (!isVisible) {
          progressContainer.style.opacity = "1";
          isVisible = true;
        }
      }

      
      let ticking = false;
      function requestTick() {
        if (!ticking) {
          requestAnimationFrame(() => {
            updateProgress();
            ticking = false;
          });
          ticking = true;
        }
      }

      
      function init() {
        
        window.addEventListener("scroll", requestTick, { passive: true });
        window.addEventListener("resize", requestTick, { passive: true });
        document.addEventListener("visibilitychange", () => {
          if (!document.hidden) requestTick();
        });

        
        requestTick();

        
        window.addEventListener("beforeunload", () => {
          window.removeEventListener("scroll", requestTick);
          window.removeEventListener("resize", requestTick);
        });
      }

      
      if (document.readyState === "loading") {
        document.addEventListener("DOMContentLoaded", init);
      } else {
        init();
      }
    })();
  </script>

    <header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6">
  <div
    class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm">
    
    <div class="hidden w-full items-center md:flex">
      
      <div class="flex items-center">
        
          
          <a
            href="/"
            class="bg-primary text-primary-foreground hover:bg-primary/90 flex h-10 w-10 items-center justify-center rounded-full text-lg font-bold transition-transform duration-200 hover:scale-105"
            aria-label="Jaehun&#39;s Blog">
            J
          </a>
        
      </div>

      
      <nav class="mx-8 flex flex-1 items-center justify-center">
        <div class="flex items-center space-x-1">
          
          
            
            
            
              
                
              
            
            
            <a
              href="/posts/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.posts">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
              
              <span
                >
                  게시글
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/categories/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.categories">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10" />

</svg>
              
              <span
                >
                  카테고리
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/tags/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.tags">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" />

</svg>
              
              <span
                >
                  태그
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/archives/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.archives">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4" />

</svg>
              
              <span
                >
                  아카이브
                </span
              >
            </a>
          
        </div>
      </nav>

      
      <div class="flex items-center space-x-2">
        
        
<div class="relative">
  <button id="language-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="language"
    aria-label="언어 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="언어 변경">
	<path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8l6 6m-7 0l6-6l2-3M2 5h12M7 2h1m14 20l-5-10l-5 10m2-4h6" />
</svg></button>

  
  <div id="language-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="language"
    role="menu"
    aria-labelledby="language-toggle"><a href="/ko/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem" aria-current="true"><span class="font-medium">한국어</span></a><a href="/en/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem"><span class="font-medium">English</span></a></div>
</div>

        
        
<div class="relative">
  <button id="color-scheme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="color-scheme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="테마 변경">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z" />

</svg></button>

  
  <div id="color-scheme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="color-scheme"><button data-color-scheme="claude" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme="default" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme="bumblebee" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme="emerald" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme="nord" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme="sunset" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme="abyss" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme="dracula" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme="amethyst" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme="slate" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme="twitter" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div>
</div>

        
        
<div class="relative">
  <button id="theme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="theme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
      
  <svg class="h-5 w-5 sun-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg>
      
      
  <svg class="h-5 w-5 moon-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg>
      
      
  <svg class="h-5 w-5 system-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg></button>

  
  <div id="theme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="theme"><button data-theme="light" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg><span>라이트</span></button><button data-theme="dark" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg><span>다크</span></button><button data-theme="system" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg><span>시스템</span></button></div>
</div>
      </div>
    </div>

    
    <div class="flex w-full items-center justify-between md:hidden">
      
      
<div class="relative">
  <button
    id="mobile-menu-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200"
    data-dropdown-type="mobile-menu"
    aria-label="메뉴"
    aria-expanded="false"
    aria-haspopup="true">
    
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="메뉴">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />

</svg>
  </button>

  
  <div
    id="mobile-menu"
    class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden"
    data-dropdown-type="mobile-menu"
    role="menu"
    aria-labelledby="mobile-menu-toggle">
    <nav class="flex flex-col">
      
      
        
        
        
          
            
          
        
        
        <a
          href="/posts/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.posts">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
          
          <span
            >
              게시글
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/categories/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.categories">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10" />

</svg>
          
          <span
            >
              카테고리
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/tags/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.tags">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" />

</svg>
          
          <span
            >
              태그
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/archives/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.archives">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4" />

</svg>
          
          <span
            >
              아카이브
            </span
          >
        </a>
      
    </nav>
  </div>
</div>


      
      <div class="flex items-center space-x-2">
        
        
<div class="relative">
  <button id="language-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="language"
    aria-label="언어 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="언어 변경">
	<path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8l6 6m-7 0l6-6l2-3M2 5h12M7 2h1m14 20l-5-10l-5 10m2-4h6" />
</svg></button>

  
  <div id="language-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="language"
    role="menu"
    aria-labelledby="language-toggle"><a href="/ko/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem" aria-current="true"><span class="font-medium">한국어</span></a><a href="/en/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem"><span class="font-medium">English</span></a></div>
</div>

        
        
<div class="relative">
  <button id="color-scheme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="color-scheme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="테마 변경">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z" />

</svg></button>

  
  <div id="color-scheme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="color-scheme"><button data-color-scheme="claude" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme="default" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme="bumblebee" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme="emerald" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme="nord" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme="sunset" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme="abyss" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme="dracula" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme="amethyst" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme="slate" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme="twitter" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div>
</div>

        
        
<div class="relative">
  <button id="theme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="theme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
      
  <svg class="h-5 w-5 sun-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg>
      
      
  <svg class="h-5 w-5 moon-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg>
      
      
  <svg class="h-5 w-5 system-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg></button>

  
  <div id="theme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="theme"><button data-theme="light" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg><span>라이트</span></button><button data-theme="dark" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg><span>다크</span></button><button data-theme="system" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg><span>시스템</span></button></div>
</div>
      </div>
    </div>
  </div>
</header>


    <main class="mx-auto max-w-4xl px-4 py-6">
      
  
  <nav
  class="breadcrumb mb-4 md:mb-6 py-1"
  aria-label="경로">
  <ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1">
    
    <li class="flex-shrink-0">
      <a
        href="/"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="홈">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11l2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6" />

</svg>
        <span class="max-w-[4rem] md:max-w-none truncate">홈</span>
      </a>
    </li>

    
    
      <li class="flex items-center gap-1 md:gap-2 min-w-0">
        <span class="text-muted-foreground/50 flex-shrink-0"
          >
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg>
        </span>

        
          
          <div class="flex items-center gap-1 md:gap-2 min-w-0">
            
              
              <a
                href="/posts"
                class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0">
                
                  
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
                  <span class="max-w-[3rem] md:max-w-none truncate">게시글</span>
                
              </a>
              <span class="text-muted-foreground/50 flex-shrink-0"
                >
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg></span
              >
            
            
            <span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0">
              
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
</svg>
              <span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</span>
            </span>
          </div>
        
      </li>
    

  </ol>
</nav>



  


  
  <header class="mb-8">
    


<div class="post-meta">
  
  <h1 class="text-foreground mb-6 text-3xl leading-tight font-bold md:text-4xl">
    FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
  </h1>

  
  


  
  <div class="bg-card border-border flex flex-col gap-4 rounded-xl border p-6">
    
    <div
      class="text-muted-foreground flex flex-wrap items-center gap-4 text-sm">
      
      <div class="flex items-center gap-2">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="작성일"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
        <time datetime="2024-12-18">
          2024년 12월 18일
        </time>
      </div>

      
      


      
      
        <div class="flex items-center gap-2">
          
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="읽는 시간"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
</svg>
          <span
            >26 분</span
          >
        </div>
      


      
      
        <div class="flex items-center gap-2">
          
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="단어 수"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7" />
</svg>
          <span>12710 단어</span>
        </div>
      

    </div>

    
    
      <div class="flex flex-wrap items-center gap-4">
        
        
          <div class="flex items-center gap-2">
            
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="카테고리"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5a2 2 0 00-2 2z" />
</svg>
            <span class="text-muted-foreground mr-2 text-sm"
              >카테고리:</span
            >
            <div class="flex flex-wrap gap-2">
              
                <a
                  href="/categories/paper-review/"
                  class="bg-primary/10 text-primary hover:bg-primary/20 focus:ring-primary/20 inline-flex items-center rounded-full px-3 py-1 text-sm font-medium transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 hover:shadow-md focus:ring-2 focus:outline-none">
                  paper-review
                </a>
              
                <a
                  href="/categories/with-gpt/"
                  class="bg-primary/10 text-primary hover:bg-primary/20 focus:ring-primary/20 inline-flex items-center rounded-full px-3 py-1 text-sm font-medium transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 hover:shadow-md focus:ring-2 focus:outline-none">
                  with-gpt
                </a>
              
            </div>
          </div>
        


        
        

      </div>
    


    
    

  </div>
</div>

  </header>

  
  <article class="prose prose-neutral dark:prose-invert mb-12 max-w-none">
    <p><a href="https://arxiv.org/abs/2205.14135v2">논문 링크</a></p>
<h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1>
<h3 id="요약-및-결론">요약 및 결론</h3>
<p>이 논문, <strong>&ldquo;FlashAttention&rdquo;</strong>, 은 Transformer 모델의 self-attention 모듈에서 메모리 및 계산 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 이를 통해 긴 시퀀스를 다루는 Transformer의 병목을 해소하며 기존 방법 대비 속도와 메모리 사용에서 큰 개선을 이룹니다.</p>
<ol>
<li>
<p><strong>주요 강점</strong>:</p>
<ul>
<li><strong>IO 효율성</strong>: GPU 메모리 계층 구조(HBM와 SRAM)를 활용하여 데이터 이동을 최적화.</li>
<li><strong>속도</strong>: GPT-2 모델에서 3배 이상의 속도 향상 및 BERT-large 모델에서 15% 속도 증가.</li>
<li><strong>메모리 효율성</strong>: 메모리 사용량이 시퀀스 길이에 선형적으로 증가하며 기존 PyTorch 대비 최대 20배 메모리 절약.</li>
<li><strong>확장 가능성</strong>: 시퀀스 길이가 64K까지 확장 가능.</li>
</ul>
</li>
<li>
<p><strong>독창성</strong>:</p>
<ul>
<li><strong>Recomputation 활용</strong>: 중간 결과를 재사용하여 메모리 요구 사항을 줄임.</li>
<li><strong>블록 기반 Sparse Attention</strong>: Sparse Attention 방법론과 결합하여 대규모 시퀀스에 효율적으로 적용 가능.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="핵심-알고리즘-설명">핵심 알고리즘 설명</h3>
<p>FlashAttention의 핵심 아이디어는 <strong>블록 기반 타일링(tiled approach)</strong> 과 <strong>재계산(recomputation)</strong> 을 통해 메모리 이동을 줄이는 것입니다. 아래는 예시 입력과 함께 알고리즘의 주요 단계입니다:</p>
<h4 id="입력-데이터">입력 데이터:</h4>
<ul>
<li>Q (Query): ( \mathbb{R}^{1024 \times 64} )</li>
<li>K (Key): ( \mathbb{R}^{1024 \times 64} )</li>
<li>V (Value): ( \mathbb{R}^{1024 \times 64} )</li>
</ul>
<h4 id="과정">과정:</h4>
<ol>
<li>
<p><strong>블록 분할</strong>:</p>
<ul>
<li>Q, K, V를 SRAM에 적합한 크기(예: 128x64 블록)로 분할.</li>
</ul>
</li>
<li>
<p><strong>블록 계산</strong>:</p>
<ul>
<li>첫 번째 블록의 ( Q_1 ) 와 ( K_1 ) 를 SRAM으로 로드.</li>
<li>( S_1 = Q_1 K_1^\top ) 를 계산하여 부분 attention 행렬 ( S_1 ) 생성.</li>
<li>( P_1 = \text{softmax}(S_1) ) 계산 후 ( V_1 ) 와 결합하여 ( O_1 ) 를 생성.</li>
</ul>
</li>
<li>
<p><strong>출력 축적</strong>:</p>
<ul>
<li>각 블록의 결과를 합산하여 최종 ( O ) 생성.</li>
</ul>
</li>
<li>
<p><strong>역전파</strong>:</p>
<ul>
<li>forward 계산 중 저장된 normalization 통계와 ( O ) 를 활용해 backward에서 메모리 소모를 줄임.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="강점과-독창성">강점과 독창성</h3>
<h4 id="강점">강점:</h4>
<ol>
<li>
<p><strong>성능 향상</strong>:</p>
<ul>
<li>실험에서 FlashAttention은 BERT-large의 학습 속도를 15% 단축시키고, GPT-2의 경우 3배 속도를 기록.</li>
<li>대규모 문서(16K-64K 토큰)에 대해 효율적으로 적용 가능.</li>
</ul>
</li>
<li>
<p><strong>높은 정확도</strong>:</p>
<ul>
<li>긴 문맥 학습에서 기존 Transformer보다 더 나은 perplexity와 문서 분류 성능을 보임.</li>
</ul>
</li>
</ol>
<h4 id="독창성">독창성:</h4>
<ol>
<li>기존 self-attention의 메모리 병목 문제를 근본적으로 해결.</li>
<li><strong>타일링과 재계산</strong>의 결합을 통해 sub-quadratic I/O 복잡도를 달성.</li>
</ol>
<hr>
<h3 id="한계점">한계점</h3>
<ol>
<li><strong>CUDA 종속성</strong>:
<ul>
<li>구현이 GPU 구조에 강하게 의존하며, 다른 하드웨어 플랫폼으로의 이식성 부족.</li>
</ul>
</li>
<li><strong>복잡성 증가</strong>:
<ul>
<li>타일링과 재계산으로 인해 알고리즘 구현이 표준 PyTorch보다 복잡.</li>
</ul>
</li>
<li><strong>정확도 제한</strong>:
<ul>
<li>일부 희소 블록 기반 변형에서 성능 손실 가능성 존재.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="결론">결론</h3>
<p>FlashAttention은 GPU 메모리 계층 구조를 효과적으로 활용하여 Transformer 모델의 속도와 효율성을 대폭 향상시키는 혁신적인 알고리즘입니다. 특히, 긴 시퀀스의 처리 성능을 크게 개선하면서도 정확도를 유지하는 점에서 뛰어난 가치를 보여줍니다.</p>
<h1 id="q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘">Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1>
<h3 id="핵심-알고리즘-flashattention">핵심 알고리즘: FlashAttention</h3>
<p>FlashAttention은 <strong>GPU 메모리 계층 구조</strong>를 활용하여 기존 self-attention의 병목 문제(메모리 소모 및 계산량)를 해결합니다. 이 알고리즘의 핵심은 <strong>타일링(Tiling)</strong> 과 <strong>재계산(Recomputation)</strong> 기법을 적용해 대규모 데이터 처리 시 효율성을 극대화하는 것입니다.</p>
<hr>
<h3 id="예시를-통한-알고리즘-단계별-설명">예시를 통한 알고리즘 단계별 설명</h3>
<h4 id="가정된-입력">가정된 입력:</h4>
<ul>
<li>( Q ): Query 행렬 (( 8 \times 64 )) – 시퀀스 길이 8, 차원 64.</li>
<li>( K ): Key 행렬 (( 8 \times 64 ))</li>
<li>( V ): Value 행렬 (( 8 \times 64 ))</li>
<li><strong>블록 크기</strong>: ( 4 \times 64 ) (GPU SRAM 용량에 맞춤).</li>
</ul>
<h4 id="전체-계산-목표">전체 계산 목표:</h4>
<p>[ O = \text{softmax}(QK^\top)V ]</p>
<hr>
<h3 id="단계-1-블록-분할-tiling">단계 1: 블록 분할 (Tiling)</h3>
<ul>
<li>( Q, K, V ) 행렬을 GPU SRAM에 적합한 크기의 <strong>블록 단위로 분할</strong>:
<ul>
<li>( Q_1, Q_2 ): ( 4 \times 64 ) 크기의 두 블록.</li>
<li>( K_1, K_2 ): ( 4 \times 64 ) 크기의 두 블록.</li>
<li>( V_1, V_2 ): ( 4 \times 64 ) 크기의 두 블록.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="단계-2-블록-기반-attention-계산">단계 2: 블록 기반 Attention 계산</h3>
<h4 id="1-첫-번째-블록--q_1-와--k_1-v_1-">(1) 첫 번째 블록 (( Q_1 )와 ( K_1, V_1 ))</h4>
<ol>
<li>GPU SRAM으로 ( Q_1 ), ( K_1 ), ( V_1 )을 로드.</li>
<li>( S_1 = Q_1 K_1^\top ) 계산:
<ul>
<li>( S_1 ): ( 4 \times 4 ) 행렬.</li>
<li>예: ( Q_1 = [[1, 2], [3, 4], &hellip;] ), ( K_1 = [[5, 6], [7, 8], &hellip;] )인 경우,
<ul>
<li>( S_1[1,1] = 1 \cdot 5 + 2 \cdot 7 ), &hellip;</li>
</ul>
</li>
</ul>
</li>
<li>( P_1 = \text{softmax}(S_1) ): 행 단위 소프트맥스.
<ul>
<li>예: ( S_1[1,:] = [10, 20, 30, 40] )이라면,
[
P_1[1,:] = \frac{[e^{10}, e^{20}, e^{30}, e^{40}]}{\sum e^{S_1[1,:]}}
]</li>
</ul>
</li>
<li>( O_1 = P_1 V_1 ): 결과는 ( O_1 \in \mathbb{R}^{4 \times 64} ).</li>
</ol>
<h4 id="2-두-번째-블록--q_2-와--k_1-v_1-">(2) 두 번째 블록 (( Q_2 )와 ( K_1, V_1 ))</h4>
<ul>
<li>GPU SRAM으로 ( Q_2 ), ( K_1 ), ( V_1 ) 로드.</li>
<li>위와 동일한 방식으로 계산:
[
S_2 = Q_2 K_1^\top, \quad P_2 = \text{softmax}(S_2), \quad O_2 = P_2 V_1
]</li>
</ul>
<hr>
<h3 id="단계-3-중간-결과-축적-및-재계산">단계 3: 중간 결과 축적 및 재계산</h3>
<ul>
<li>
<p><strong>정규화 및 최적화</strong>:</p>
<ul>
<li>( \text{softmax} ) 계산 시 최대값 ( m )과 합 ( \ell )을 각 블록에서 저장.</li>
<li>이후 단계에서 메모리 과부하를 피하기 위해 결과를 GPU HBM에 순차적으로 저장.</li>
</ul>
</li>
<li>
<p><strong>후속 블록 계산</strong>:</p>
<ul>
<li>( K_2, V_2 )와 ( Q_1, Q_2 ) 간의 상호작용 계산.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="단계-4-최종-결과-조합">단계 4: 최종 결과 조합</h3>
<ul>
<li>각 블록의 ( O_1, O_2 )를 합쳐 최종 ( O ) 생성.</li>
</ul>
<hr>
<h3 id="전체-과정의-시간-및-메모리-효율">전체 과정의 시간 및 메모리 효율</h3>
<table>
  <thead>
      <tr>
          <th><strong>단계</strong></th>
          <th>기존 Attention</th>
          <th>FlashAttention</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>메모리 사용량</td>
          <td>( O(N^2) )</td>
          <td>( O(N) )</td>
      </tr>
      <tr>
          <td>계산 복잡도</td>
          <td>( O(N^2 \cdot d) )</td>
          <td>( O(N \cdot d \cdot M^{-1}) )</td>
      </tr>
  </tbody>
</table>
<ul>
<li><strong>( N )</strong>: 시퀀스 길이</li>
<li><strong>( d )</strong>: 차원</li>
<li><strong>( M )</strong>: SRAM 크기</li>
</ul>
<hr>
<h3 id="정리">정리</h3>
<p>FlashAttention은 다음과 같은 주요 특징을 통해 효율성을 크게 향상합니다:</p>
<ol>
<li>
<p><strong>타일링 기반 처리</strong>:</p>
<ul>
<li>시퀀스를 작은 블록으로 나누어 각 블록을 SRAM에서 처리.</li>
</ul>
</li>
<li>
<p><strong>재계산 전략</strong>:</p>
<ul>
<li>역전파 시 불필요한 중간 결과 저장을 피하고 재계산을 활용.</li>
</ul>
</li>
<li>
<p><strong>입출력(IO) 최적화</strong>:</p>
<ul>
<li>메모리 이동을 줄여 GPU의 병목을 해소.</li>
</ul>
</li>
</ol>
<p>FlashAttention은 기존 Transformer 모델에서의 병목 문제를 해결하며, 긴 시퀀스 처리에서 특히 강력한 성능을 제공합니다.</p>
<h1 id="q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘">Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1>
<h3 id="논문-결과-분석-flashattention의-성과">논문 결과 분석: FlashAttention의 성과</h3>
<p>FlashAttention은 <strong>속도, 메모리 사용량, 모델 정확도</strong>의 측면에서 기존 방법론을 압도합니다. 논문에서 제시한 실험 결과를 정리하고, 기존 방법론과의 비교, 결과를 뒷받침하는 기법, 그리고 제 생각을 포함해 설명하겠습니다.</p>
<hr>
<h3 id="1-실험-결과">1. <strong>실험 결과</strong></h3>
<h4 id="1-속도-학습-및-추론-시간-단축">(1) <strong>속도</strong>: 학습 및 추론 시간 단축</h4>
<ul>
<li><strong>BERT-large 모델</strong>:
<ul>
<li>FlashAttention은 <strong>MLPerf 1.1</strong> 기록 대비 <strong>15% 더 빠르게</strong> 학습.</li>
<li>표준 PyTorch Attention 대비 2~4배 더 빠른 속도.</li>
</ul>
</li>
<li><strong>GPT-2 모델</strong>:
<ul>
<li>HuggingFace 및 Megatron-LM 대비 최대 <strong>3배 빠르게 학습</strong>.</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>모델</strong></th>
          <th>기존 구현 (시간)</th>
          <th>FlashAttention (시간)</th>
          <th>속도 향상</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>BERT-large</td>
          <td>20.0분</td>
          <td>17.4분</td>
          <td><strong>15%</strong></td>
      </tr>
      <tr>
          <td>GPT-2 (512 길이)</td>
          <td>9.5일</td>
          <td>2.7일</td>
          <td><strong>3.5배</strong></td>
      </tr>
  </tbody>
</table>
<h4 id="2-메모리-사용량">(2) <strong>메모리 사용량</strong></h4>
<ul>
<li>FlashAttention은 메모리 사용량이 <strong>시퀀스 길이에 선형적으로 증가</strong>.</li>
<li>기존 PyTorch Attention 대비 최대 <strong>20배 메모리 절약</strong>.</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>시퀀스 길이</strong></th>
          <th>PyTorch (GB)</th>
          <th>FlashAttention (GB)</th>
          <th>절약 비율</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>16K</td>
          <td>메모리 초과</td>
          <td>3.3</td>
          <td>-</td>
      </tr>
      <tr>
          <td>64K</td>
          <td>메모리 초과</td>
          <td>13.4</td>
          <td>-</td>
      </tr>
  </tbody>
</table>
<h4 id="3-정확도-개선">(3) <strong>정확도 개선</strong></h4>
<ul>
<li><strong>GPT-2</strong>:
<ul>
<li>시퀀스 길이를 4배(1K → 4K) 확장하면서도 Megatron-LM보다 <strong>30% 빠르고 0.7 낮은 perplexity</strong> 달성.</li>
</ul>
</li>
<li><strong>긴 문서 분류 (MIMIC-III, ECtHR 데이터셋)</strong>:
<ul>
<li>시퀀스 길이 증가로 각각 <strong>4.3점</strong>, <strong>8.5점 F1 점수 개선</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-특출난-점">2. <strong>특출난 점</strong></h3>
<h4 id="1-긴-시퀀스-처리-능력">(1) <strong>긴 시퀀스 처리 능력</strong></h4>
<ul>
<li>기존 Attention은 (O(N^2)) 메모리 복잡도를 가져 시퀀스 길이 4K 이상에서 사용이 비효율적이거나 불가능.</li>
<li>FlashAttention은 메모리 사용량을 (O(N))로 줄여 <strong>64K 시퀀스 길이</strong>에서도 학습 가능.</li>
<li>예: <strong>Path-X</strong> 및 <strong>Path-256</strong>(16K 및 64K 시퀀스 길이)에서 Transformer 최초로 랜덤 성능 이상 달성.</li>
</ul>
<h4 id="2-효율성과-정확도의-균형">(2) <strong>효율성과 정확도의 균형</strong></h4>
<ul>
<li>일부 Sparse 또는 Approximate Attention은 속도는 개선되지만 정확도 손실 발생.</li>
<li>FlashAttention은 정확도를 유지하며 속도와 메모리 사용량을 동시에 최적화.</li>
</ul>
<h4 id="3-확장성">(3) <strong>확장성</strong></h4>
<ul>
<li>FlashAttention은 <strong>CUDA 커널 최적화</strong>를 통해 다양한 GPU 아키텍처에서 효율적으로 작동하며, Sparse Attention과의 결합도 가능.</li>
</ul>
<hr>
<h3 id="3-flashattention의-기여-요인">3. <strong>FlashAttention의 기여 요인</strong></h3>
<h4 id="1-타일링-기반-설계">(1) <strong>타일링 기반 설계</strong></h4>
<ul>
<li>전체 Attention 행렬을 저장하지 않고, 입력 데이터를 GPU SRAM에 적합한 <strong>작은 블록 단위로 분할 및 처리</strong>.</li>
<li>이로 인해 메모리 이동을 최소화하고, GPU의 병목 문제 해소.</li>
</ul>
<h4 id="2-recomputation-재계산">(2) <strong>Recomputation (재계산)</strong></h4>
<ul>
<li>역전파 시 저장된 중간 값 대신, 필요한 값을 재계산하여 메모리 사용량 절감.</li>
<li><strong>정규화 통계</strong>(( \text{rowmax}, \text{rowsum} )) 만 저장하여 정확도 손실 없이 복잡도 감소.</li>
</ul>
<h4 id="3-gpu-io-계층-최적화">(3) <strong>GPU IO 계층 최적화</strong></h4>
<ul>
<li>고속 SRAM을 최대한 활용하고, HBM과의 데이터 이동을 줄여 <strong>속도 개선</strong>.</li>
<li>IO 복잡도: 기존 ( O(N^2) )에서 ( O(N \cdot d / M) )로 개선.</li>
</ul>
<hr>
<h3 id="4-논문에서-제시한-이유와-내-의견">4. <strong>논문에서 제시한 이유와 내 의견</strong></h3>
<h4 id="논문에서-제시한-이유">논문에서 제시한 이유:</h4>
<ol>
<li><strong>Transformer 모델의 병목</strong>:
<ul>
<li>GPU에서 연산은 빠르지만, 메모리 접근(IO)이 상대적으로 느려, 대부분의 시간 소모가 메모리 이동에 있음.</li>
</ul>
</li>
<li><strong>메모리-계산 간 균형 부족</strong>:
<ul>
<li>기존 Approximate Attention은 계산량은 줄였지만 메모리 병목을 해결하지 못함.</li>
</ul>
</li>
<li><strong>GPU 최적화 부재</strong>:
<ul>
<li>PyTorch 및 기존 라이브러리는 GPU 메모리 계층 활용 최적화가 부족.</li>
</ul>
</li>
</ol>
<h4 id="내-의견">내 의견:</h4>
<p>FlashAttention은 단순히 알고리즘의 계산량(FLOP)을 줄이는 것에 그치지 않고, <strong>입출력(IO)</strong> 문제를 해결한 점에서 혁신적입니다. 특히:</p>
<ul>
<li>현대 GPU에서 <strong>SRAM-HBM 계층 구조</strong>를 활용한 접근은 다양한 GPU 기반 ML 모델에 확장 가능성이 높습니다.</li>
<li>다만, <strong>CUDA 커널 수준의 구현 복잡성</strong>은 개발자의 접근성을 낮추는 요인으로 작용할 수 있습니다. 이를 PyTorch 같은 고수준 프레임워크로 추상화하는 후속 연구가 필요할 것입니다.</li>
</ul>
<hr>
<h3 id="결론-1">결론</h3>
<p>FlashAttention은 Transformer 모델의 속도, 메모리 효율성, 긴 시퀀스 학습 성능에서 독보적인 성과를 보였습니다. 이 결과는 <strong>GPU 메모리 계층 활용, 타일링, 재계산</strong> 기법의 조화로 이루어졌습니다. 이러한 접근은 다른 메모리-계산 집약적 작업에도 충분히 확장 가능하며, Transformer를 넘어선 <strong>모델 최적화 연구</strong>에도 중요한 이정표가 될 것입니다.</p>
<h1 id="q--이-논문에서-제시된-방법을-실제로-사용하려면-기존-방법에서-어떠한-구현이-추가적으로-필요하고-이러한-구현에-소모되는-공수-및-연산에-필요한-컴퓨팅-자원의-크기에-대해서-계산해줄-수-있겠니">Q : 이 논문에서 제시된 방법을 실제로 사용하려면 기존 방법에서 어떠한 구현이 추가적으로 필요하고 이러한 구현에 소모되는 공수 및 연산에 필요한 컴퓨팅 자원의 크기에 대해서 계산해줄 수 있겠니?</h1>
<h3 id="flashattention-적용을-위한-구현-추가-사항과-요구-리소스-분석">FlashAttention 적용을 위한 구현 추가 사항과 요구 리소스 분석</h3>
<p>FlashAttention을 기존 Transformer 모델에 통합하려면 <strong>기존의 Attention 구현</strong>과 비교하여 몇 가지 추가적인 구현 작업이 필요합니다. 이러한 작업은 주로 <strong>GPU 최적화</strong>, <strong>커널 설계</strong>, <strong>타일링 처리</strong>, <strong>재계산 알고리즘</strong>에 중점을 둡니다. 아래에 필요한 구현, 예상 공수, 추가 연산 및 컴퓨팅 자원 요구 사항을 정리하겠습니다.</p>
<hr>
<h3 id="1-기존-방법에서의-구현-대비-추가-작업">1. 기존 방법에서의 구현 대비 추가 작업</h3>
<h4 id="1-cuda-기반-커널-구현">(1) <strong>CUDA 기반 커널 구현</strong></h4>
<ul>
<li>
<p>FlashAttention은 PyTorch 또는 TensorFlow와 같은 고수준 프레임워크에서 구현된 표준 Attention과 달리, <strong>CUDA 커널 수준의 최적화</strong>가 필요합니다.</p>
</li>
<li>
<p><strong>필요 작업</strong>:</p>
<ol>
<li>Attention 연산을 한 번의 GPU 커널 호출로 병합(Fused kernel).</li>
<li>GPU <strong>SRAM-타일링</strong> 구조를 활용하여 메모리 이동을 최소화.</li>
<li>Forward 및 Backward pass에서 중간 값을 저장하는 대신 재계산하도록 설계.</li>
</ol>
</li>
<li>
<p><strong>예상 공수</strong>:</p>
<ul>
<li>CUDA 최적화 경험이 있는 개발자를 기준으로 <strong>1~2개월의 엔지니어링 작업</strong> 필요.</li>
<li>메모리 관리와 커널 병합에 대한 숙련된 이해 필요.</li>
</ul>
</li>
</ul>
<h4 id="2-recomputation-알고리즘-구현">(2) <strong>Recomputation 알고리즘 구현</strong></h4>
<ul>
<li>
<p>중간 Attention 행렬을 저장하지 않고, 역전파 시 필요할 때 재계산.</p>
</li>
<li>
<p><strong>필요 작업</strong>:</p>
<ol>
<li>Softmax의 row-wise 최대값 ( m )과 합 ( \ell ) 저장 및 활용.</li>
<li>Backward에서 재계산을 통해 GPU 메모리 사용량 최소화.</li>
</ol>
</li>
<li>
<p><strong>예상 공수</strong>:</p>
<ul>
<li>기존 Gradient Checkpointing 기술을 참고하여 설계하면 약 <strong>2~3주</strong> 소요.</li>
</ul>
</li>
</ul>
<h4 id="3-타일링-전략-설계-및-테스트">(3) <strong>타일링 전략 설계 및 테스트</strong></h4>
<ul>
<li>
<p>( Q, K, V ) 행렬을 GPU SRAM에 맞게 타일로 나누고, 병렬 처리.</p>
</li>
<li>
<p><strong>필요 작업</strong>:</p>
<ol>
<li>타일 크기(( M ))를 GPU SRAM 용량에 맞게 동적으로 설정.</li>
<li>각 타일 처리 결과를 결합하는 루프 설계.</li>
<li>다양한 시퀀스 길이와 배치 크기에 대한 테스트.</li>
</ol>
</li>
<li>
<p><strong>예상 공수</strong>:</p>
<ul>
<li>GPU 아키텍처(SRAM 크기, 대역폭 등)에 따라 최적화를 반복해야 하므로 <strong>1~2주 테스트 및 튜닝</strong> 필요.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-추가-연산과-컴퓨팅-자원-요구-사항">2. 추가 연산과 컴퓨팅 자원 요구 사항</h3>
<p>FlashAttention의 설계는 <strong>연산량(FLOPs)</strong> 측면에서 기존 Attention보다 약간 증가할 수 있지만, <strong>메모리 이동(IO)</strong> 을 대폭 줄임으로써 전체적인 효율성을 높입니다.</p>
<h4 id="연산량-flops">연산량 (FLOPs)</h4>
<ol>
<li>
<p><strong>Forward Pass</strong>:</p>
<ul>
<li>기존: ( O(N^2 \cdot d) )</li>
<li>FlashAttention: 약 ( O(N^2 \cdot d) ) + ( O(N \cdot d \cdot M^{-1}) ) (재계산)</li>
<li>차이: 추가적인 재계산 연산(softmax normalization)으로 약 10~15%의 FLOP 증가.</li>
</ul>
</li>
<li>
<p><strong>Backward Pass</strong>:</p>
<ul>
<li>기존: ( O(N^2 \cdot d) )</li>
<li>FlashAttention: ( O(N^2 \cdot d) ) + ( O(N \cdot d \cdot M^{-1}) )</li>
<li>차이: 재계산으로 인해 약 10~15%의 FLOP 증가.</li>
</ul>
</li>
</ol>
<h4 id="메모리-사용량">메모리 사용량</h4>
<p>FlashAttention은 <strong>시퀀스 길이에 선형적으로 증가하는 메모리 사용량</strong>을 가지며, GPU의 SRAM 크기와 병렬 처리 용량에 따라 크게 차이가 납니다.</p>
<ul>
<li>GPU HBM 사용량: 기존 대비 약 <strong>20~30% 감소</strong>.</li>
<li>GPU SRAM 사용량: 타일 크기 ( M ) 에 따라 최대 100KB~1MB 필요.</li>
</ul>
<h4 id="io-복잡도">IO 복잡도</h4>
<p>FlashAttention은 IO 복잡도를 크게 줄입니다:</p>
<ul>
<li>기존 Attention: ( O(N^2) ) IO.</li>
<li>FlashAttention: ( O(N \cdot d / M) ) IO.</li>
</ul>
<hr>
<h3 id="3-필요-컴퓨팅-자원-계산">3. 필요 컴퓨팅 자원 계산</h3>
<h4 id="시뮬레이션-환경">시뮬레이션 환경</h4>
<ul>
<li>모델: GPT-2 (시퀀스 길이 ( N = 1024 ), 차원 ( d = 64 ))</li>
<li>GPU: NVIDIA A100 (HBM: 40GB, SRAM: 19TB/s, 약 40MB)</li>
</ul>
<h4 id="자원-요구량">자원 요구량</h4>
<table>
  <thead>
      <tr>
          <th><strong>항목</strong></th>
          <th>기존 Attention</th>
          <th>FlashAttention</th>
          <th>비고</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>FLOPs (Forward)</strong></td>
          <td>( \sim10^{12} )</td>
          <td>( \sim1.1 \times 10^{12} )</td>
          <td>10% 증가</td>
      </tr>
      <tr>
          <td><strong>FLOPs (Backward)</strong></td>
          <td>( \sim2 \times 10^{12} )</td>
          <td>( \sim2.2 \times 10^{12} )</td>
          <td>10% 증가</td>
      </tr>
      <tr>
          <td><strong>HBM 이동량</strong></td>
          <td>( 1.5 \times 10^{11} )</td>
          <td>( 2 \times 10^{10} )</td>
          <td>최대 7.5배 감소</td>
      </tr>
      <tr>
          <td><strong>SRAM 사용량</strong></td>
          <td>-</td>
          <td>( 20 \sim 100 ) KB</td>
          <td>블록 크기에 따라 결정</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="4-제-의견">4. 제 의견</h3>
<p>FlashAttention의 구현은 추가 엔지니어링 공수와 컴퓨팅 자원을 필요로 하지만, <strong>긴 시퀀스 학습</strong>에서의 성능 향상이 이 요구사항을 상쇄한다고 봅니다. 특히:</p>
<ol>
<li><strong>긴 시퀀스 모델링</strong>:
<ul>
<li>기존 방법으로는 불가능한 64K 이상의 시퀀스 처리에 필수적.</li>
</ul>
</li>
<li><strong>실제 비용 효율성</strong>:
<ul>
<li>재계산으로 FLOPs는 약간 증가하지만, GPU 메모리 사용량 절감으로 인해 더 적은 GPU 리소스에서 대규모 모델 학습 가능.</li>
</ul>
</li>
</ol>
<p>단, CUDA 최적화 및 하드웨어 의존성을 줄이기 위해 PyTorch 또는 TensorFlow의 고수준 라이브러리 지원이 필요할 것입니다. 이는 FlashAttention의 보급을 크게 촉진할 것입니다.</p>
<h1 id="q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘">Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1>
<h3 id="flashattention의-입력-데이터-및-추론-과정">FlashAttention의 입력 데이터 및 추론 과정</h3>
<p>FlashAttention의 핵심은 Transformer 모델의 <strong>self-attention</strong> 모듈에 적용됩니다. 이를 기반으로 입력 데이터, 추론 과정, 모델 아키텍처, 메모리 및 컴퓨팅 요구량을 예를 들어 설명하겠습니다.</p>
<hr>
<h3 id="1-입력-데이터">1. <strong>입력 데이터</strong></h3>
<h4 id="1-데이터-정의">(1) 데이터 정의</h4>
<p>Transformer의 self-attention 모듈의 입력은 세 개의 행렬 ( Q ), ( K ), ( V ) 로 구성됩니다.</p>
<ul>
<li>( Q ) (Query): 현재 토큰이 다른 토큰과 얼마나 관련이 있는지를 판단하기 위한 질의.</li>
<li>( K ) (Key): 각 토큰의 특징 표현.</li>
<li>( V ) (Value): 각 토큰의 실제 값 표현.</li>
</ul>
<h4 id="2-데이터-크기">(2) 데이터 크기</h4>
<ul>
<li>( N ): 시퀀스 길이 (예: 문장의 토큰 수)</li>
<li>( d ): Attention Head 차원</li>
<li><strong>예시 입력</strong>:
<ul>
<li>( Q \in \mathbb{R}^{N \times d} )</li>
<li>( K \in \mathbb{R}^{N \times d} )</li>
<li>( V \in \mathbb{R}^{N \times d} )</li>
<li>시퀀스 길이 ( N = 1024 ), 차원 ( d = 64 ) 라고 가정하면:
<ul>
<li>( Q, K, V )는 각각 ( 1024 \times 64 ) 행렬.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-추론-과정">2. <strong>추론 과정</strong></h3>
<p>FlashAttention의 추론 과정은 일반적인 Transformer의 self-attention 계산을 기반으로 하지만, <strong>타일링(tiled approach)</strong> 과 <strong>재계산(recomputation)</strong> 을 적용합니다.</p>
<h4 id="기본-수식">기본 수식</h4>
<p>Self-attention의 계산:</p>
<ol>
<li>
<p><strong>Similarity 계산</strong>:
[
S = Q K^\top \quad (S \in \mathbb{R}^{N \times N})
]</p>
<ul>
<li>각 토큰 간 유사도를 계산.</li>
<li>( S[i, j] ): ( i )-번째와 ( j )-번째 토큰 간의 유사도.</li>
</ul>
</li>
<li>
<p><strong>Softmax 적용</strong>:
[
P = \text{softmax}(S) \quad (P \in \mathbb{R}^{N \times N})
]</p>
<ul>
<li>각 토큰의 중요도를 확률로 변환.</li>
</ul>
</li>
<li>
<p><strong>가중합 계산</strong>:
[
O = PV \quad (O \in \mathbb{R}^{N \times d})
]</p>
<ul>
<li>중요도를 기반으로 Value를 합산.</li>
</ul>
</li>
</ol>
<h4 id="flashattention의-최적화-과정">FlashAttention의 최적화 과정</h4>
<ol>
<li>
<p><strong>타일링 처리</strong>:</p>
<ul>
<li>( Q, K, V ) 를 GPU <strong>SRAM</strong>에 적합한 크기(( M ))로 블록화.</li>
<li>예: ( 1024 \times 64 )를 ( 128 \times 64 ) 블록으로 분할.</li>
</ul>
</li>
<li>
<p><strong>블록별 계산</strong>:</p>
<ul>
<li>각 블록 ( Q_i, K_i, V_i ) 에 대해:
<ul>
<li>( S_i = Q_i K_i^\top )</li>
<li>( P_i = \text{softmax}(S_i) )</li>
<li>( O_i = P_i V_i )</li>
</ul>
</li>
<li>( O_i ) 는 HBM(Higher Bandwidth Memory)에 저장.</li>
</ul>
</li>
<li>
<p><strong>재계산</strong>:</p>
<ul>
<li>Backward pass에서 ( S ), ( P ) 전체를 저장하지 않고, 필요한 값을 <strong>Softmax normalization 통계</strong>로 재계산.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-모델-아키텍처-구성">3. <strong>모델 아키텍처 구성</strong></h3>
<p>FlashAttention은 Transformer 모델의 self-attention 블록을 대체합니다. 이를 포함한 Transformer의 전체 아키텍처:</p>
<ol>
<li>
<p><strong>Input Embedding</strong>:</p>
<ul>
<li>입력 단어를 ( d )-차원의 벡터로 변환.</li>
</ul>
</li>
<li>
<p><strong>Multi-Head Attention</strong>:</p>
<ul>
<li>FlashAttention으로 구성된 self-attention 모듈.</li>
<li>입력 ( Q, K, V )는 여러 헤드로 나뉘어 병렬 처리.</li>
</ul>
</li>
<li>
<p><strong>Feed-Forward Layer</strong>:</p>
<ul>
<li>Attention 출력에 비선형 변환 적용.</li>
</ul>
</li>
<li>
<p><strong>Layer Normalization 및 Skip Connection</strong>:</p>
<ul>
<li>각 Layer의 출력을 안정화.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4-연산-및-메모리-요구량">4. <strong>연산 및 메모리 요구량</strong></h3>
<h4 id="1-연산량-flops">(1) 연산량 (FLOPs)</h4>
<p>FlashAttention은 타일링 및 재계산으로 기존 Attention 대비 FLOPs가 약간 증가:</p>
<ul>
<li>기존 Attention:
<ul>
<li>( O(N^2 \cdot d) ): Similarity 계산, Softmax, 가중합.</li>
</ul>
</li>
<li>FlashAttention:
<ul>
<li>( O(N^2 \cdot d) + O(N \cdot d / M) ): 재계산 연산 추가.</li>
</ul>
</li>
</ul>
<h4 id="예시-연산량-계산-gpt-2-모델-기준"><strong>예시 연산량 계산</strong> (GPT-2 모델 기준)</h4>
<ul>
<li>( N = 1024 ), ( d = 64 ), ( M = 128 ):
<ul>
<li>기존: ( \sim1.3 \times 10^9 ) FLOPs.</li>
<li>FlashAttention: ( \sim1.4 \times 10^9 ) FLOPs (10% 증가).</li>
</ul>
</li>
</ul>
<h4 id="2-메모리-요구량">(2) 메모리 요구량</h4>
<p>FlashAttention은 메모리 이동을 최적화하여 사용량을 감소:</p>
<ul>
<li>기존: ( O(N^2) ) (HBM에 전체 Attention 행렬 ( S ) 저장).</li>
<li>FlashAttention: ( O(N \cdot d) + O(M) ).</li>
</ul>
<h4 id="예시-메모리-요구량-a100-gpu-기준"><strong>예시 메모리 요구량</strong> (A100 GPU 기준)</h4>
<ul>
<li>( N = 1024 ), ( d = 64 ):
<ul>
<li>기존 Attention: 약 9MB (HBM에 ( S, P ) 저장).</li>
<li>FlashAttention: 약 1.6MB (SRAM에서 타일 기반 처리).</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-컴퓨팅-자원-요구량">5. <strong>컴퓨팅 자원 요구량</strong></h3>
<p>FlashAttention은 GPU의 SRAM을 활용하며, GPU 아키텍처에 따라 성능이 달라집니다.</p>
<h4 id="a100-gpu-예시">A100 GPU 예시</h4>
<ul>
<li><strong>SRAM 용량</strong>: 약 40MB.</li>
<li><strong>HBM 대역폭</strong>: 1.5TB/s.</li>
<li><strong>최적 타일 크기</strong>: ( M = 128 )일 때 가장 빠른 성능.</li>
<li><strong>추론 속도</strong>:
<ul>
<li>기존 Attention 대비 2~4배 빠른 성능.</li>
</ul>
</li>
</ul>
<h4 id="t4-gpu-예시">T4 GPU 예시</h4>
<ul>
<li><strong>SRAM 용량</strong>: 약 8MB.</li>
<li>SRAM 크기가 작아 타일 크기를 줄여야 하며, 성능 개선폭은 상대적으로 낮음(1.5~2배 속도 향상).</li>
</ul>
<hr>
<h3 id="요약">요약</h3>
<p>FlashAttention의 입력 데이터 및 추론 과정은 기존 Attention과 유사하나, 타일링 및 재계산을 통해 메모리 사용량과 IO 복잡도를 대폭 줄입니다. 이는 긴 시퀀스 처리에서 특히 효과적이며, GPU 자원을 효율적으로 활용해 성능을 최적화합니다. <strong>추론 과정 최적화와 효율적인 메모리 사용</strong>은 긴 시퀀스 학습의 새로운 가능성을 열어줍니다.</p>
<h1 id="q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘">Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1>
<h3 id="flashattention의-한계와-이를-극복하기-위한-연구-흐름">FlashAttention의 한계와 이를 극복하기 위한 연구 흐름</h3>
<p>FlashAttention은 Transformer 모델의 self-attention에서 메모리 사용량과 연산 효율성을 크게 개선했지만, 여전히 몇 가지 한계가 존재합니다. 이를 극복하기 위한 연구 흐름은 크게 <strong>구현 상의 제약</strong>, <strong>장기적 확장성</strong>, <strong>하드웨어 종속성</strong>의 측면에서 논의될 수 있습니다.</p>
<hr>
<h3 id="1-한계점-요약">1. <strong>한계점 요약</strong></h3>
<h4 id="1-cuda-커널-의존">(1) <strong>CUDA 커널 의존</strong></h4>
<ul>
<li>FlashAttention은 GPU의 CUDA 커널 수준 최적화를 필요로 하며, <strong>하드웨어 의존적 구현</strong>으로 인해:
<ul>
<li>다른 플랫폼(TPU, CPU)에서 사용하기 어렵다.</li>
<li>PyTorch/TensorFlow의 고수준 API로 추상화되지 않음.</li>
</ul>
</li>
</ul>
<h4 id="2-짧은-타일-크기의-성능-제한">(2) <strong>짧은 타일 크기의 성능 제한</strong></h4>
<ul>
<li>GPU의 <strong>SRAM 크기</strong>가 제한되어 타일 크기를 작게 설정해야 하며, 이로 인해:
<ul>
<li>SRAM 용량이 적은 하드웨어(A100 이하)에서는 성능 개선 폭이 제한.</li>
<li>매우 큰 시퀀스 처리에서도 블록 병렬화에 따른 IO 병목 가능성.</li>
</ul>
</li>
</ul>
<h4 id="3-초대형-모델-및-멀티-gpu-확장성">(3) <strong>초대형 모델 및 멀티-GPU 확장성</strong></h4>
<ul>
<li>FlashAttention은 단일 GPU에서 최적화되었으나, <strong>멀티-GPU 환경에서 IO 효율성 최적화 부족</strong>.</li>
<li>초대형 모델에서는 노드 간 통신 병목이 발생할 가능성이 있음.</li>
</ul>
<h4 id="4-희소-attention과의-결합-제한">(4) <strong>희소 Attention과의 결합 제한</strong></h4>
<ul>
<li>희소 Attention(block-sparse, global-sparse)과의 결합에서 효율성과 정확도 간 균형이 완벽히 해결되지 않음.</li>
</ul>
<hr>
<h3 id="2-한계를-극복하기-위한-연구-흐름">2. <strong>한계를 극복하기 위한 연구 흐름</strong></h3>
<h4 id="1-고수준-api로-추상화된-구현">(1) <strong>고수준 API로 추상화된 구현</strong></h4>
<p>FlashAttention의 CUDA 기반 최적화는 강력하지만, 일반 연구자들에게 접근성이 낮습니다. 이를 해결하기 위한 연구 방향:</p>
<ul>
<li><strong>자동 커널 생성</strong>:
<ul>
<li>Halide (이미지 처리 컴파일러)와 같이 <strong>자동으로 CUDA 커널을 생성</strong>하는 시스템 개발.</li>
<li>PyTorch/XLA 및 JAX와 통합하여 플랫폼 독립적인 사용 가능.</li>
</ul>
</li>
<li><strong>Dynamic Kernel Fusion</strong>:
<ul>
<li>동적 그래프 컴파일러(TensorRT, TVM)를 활용하여, Attention 연산과 FlashAttention을 통합.</li>
<li>메모리 이동과 연산을 자동으로 병합.</li>
</ul>
</li>
</ul>
<p><strong>예시 연구 흐름</strong>:</p>
<ul>
<li>NVIDIA의 Apex, PyTorch의 TorchScript를 확장해 FlashAttention을 통합.</li>
<li>TensorFlow XLA에서의 IO-aware 컴파일 지원.</li>
</ul>
<hr>
<h4 id="2-sram-활용-극대화">(2) <strong>SRAM 활용 극대화</strong></h4>
<p>GPU의 SRAM 크기가 제한적이므로 이를 극복하기 위한 메모리 및 연산 최적화 연구:</p>
<ul>
<li><strong>Adaptive Tiling</strong>:
<ul>
<li><strong>SRAM 용량에 따라 타일 크기를 동적으로 조정</strong>.</li>
<li>타일별 메모리 사용량을 예측하여 SRAM 및 HBM 간 최적의 데이터 이동 설계.</li>
</ul>
</li>
<li><strong>Hierarchical Attention</strong>:
<ul>
<li>중요한 부분만 SRAM에 캐싱하고 나머지는 희소 접근 방식 적용.</li>
</ul>
</li>
</ul>
<p><strong>예시 연구 흐름</strong>:</p>
<ul>
<li><strong>Learned Cache Strategy</strong>:
<ul>
<li>SRAM에 어떤 데이터를 우선 캐싱할지 학습하는 방식.</li>
<li>메모리-연산 균형을 유지.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-멀티-gpu-io-aware-attention">(3) <strong>멀티-GPU IO-aware Attention</strong></h4>
<p>FlashAttention은 단일 GPU에서의 IO 최적화에 중점을 두었으나, <strong>분산 환경</strong>에서 최적화를 위해 다음과 같은 방향이 제안됩니다:</p>
<ul>
<li><strong>Cross-GPU Memory Sharing</strong>:
<ul>
<li>노드 간 통신 병목을 줄이기 위해, GPU HBM 간 데이터 공유 최적화.</li>
</ul>
</li>
<li><strong>Pipeline Parallelism 통합</strong>:
<ul>
<li>Attention 연산을 단계적으로 분산 처리하여 <strong>메모리 요구량 분산</strong>.</li>
</ul>
</li>
</ul>
<p><strong>예시 연구 흐름</strong>:</p>
<ul>
<li>NVIDIA Megatron-LM과 같은 초대형 모델 프레임워크에 FlashAttention 확장.</li>
<li><strong>Sharded Attention</strong>:
<ul>
<li>Attention 연산을 GPU 그룹 간 나누어 처리.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-희소-attention과의-완벽한-결합">(4) <strong>희소 Attention과의 완벽한 결합</strong></h4>
<p>FlashAttention은 block-sparse Attention을 지원하지만, 희소성 적용 시 정확도 손실 가능성 존재:</p>
<ul>
<li><strong>Learnable Sparsity</strong>:
<ul>
<li>중요한 Attention 패턴을 학습하여 동적으로 희소성을 설정.</li>
</ul>
</li>
<li><strong>Hybrid Attention</strong>:
<ul>
<li>FlashAttention의 타일링 기반 접근과 희소 Attention을 결합하여 희소성 영역 최적화.</li>
</ul>
</li>
</ul>
<p><strong>예시 연구 흐름</strong>:</p>
<ul>
<li>Performer와 FlashAttention 결합 연구.</li>
<li>Longformer와 같은 sparse+dense 혼합 접근에 FlashAttention 적용.</li>
</ul>
<hr>
<h3 id="3-멀티-플랫폼-지원-및-범용성-강화">3. <strong>멀티 플랫폼 지원 및 범용성 강화</strong></h3>
<p>FlashAttention이 GPU에 최적화되어 있지만, TPU, CPU, 기타 AI 가속기로 확장 가능성을 연구하는 흐름:</p>
<ul>
<li><strong>TPU/CPU용 구현</strong>:
<ul>
<li>TPU 메모리 계층 구조에 맞게 재구성.</li>
<li>메모리 효율성을 극대화하는 XLA 기반 최적화.</li>
</ul>
</li>
<li><strong>FPGA 및 ASIC 지원</strong>:
<ul>
<li>고정 연산 구조를 가진 FPGA/ASIC용 FlashAttention 설계.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="요약-flashattention-극복-연구-흐름">요약: FlashAttention 극복 연구 흐름</h3>
<table>
  <thead>
      <tr>
          <th><strong>한계</strong></th>
          <th><strong>연구 흐름</strong></th>
          <th><strong>예시 접근</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>CUDA 종속성</td>
          <td>고수준 API 통합, 자동 커널 생성</td>
          <td>Halide-like 시스템, PyTorch TorchScript 확장</td>
      </tr>
      <tr>
          <td>SRAM 크기 제한</td>
          <td>Adaptive Tiling, Hierarchical Attention</td>
          <td>SRAM 캐싱 학습, 다중 타일 크기 최적화</td>
      </tr>
      <tr>
          <td>멀티-GPU 확장성</td>
          <td>Cross-GPU Memory Sharing, Pipeline Parallel</td>
          <td>Sharded Attention, 분산 처리 최적화</td>
      </tr>
      <tr>
          <td>희소 Attention 결합</td>
          <td>Learnable Sparsity, Hybrid Attention</td>
          <td>Performer와 FlashAttention 결합</td>
      </tr>
      <tr>
          <td>멀티 플랫폼 지원</td>
          <td>TPU/FPGA/ASIC 확장</td>
          <td>TPU의 XLA 기반 최적화, FPGA 전용 FlashAttention 설계</td>
      </tr>
  </tbody>
</table>
<hr>
<p>FlashAttention은 현대 Transformer 모델의 병목 문제를 해결한 중요한 기여를 했지만, 더 넓은 적용성과 범용성을 위해 위와 같은 연구 흐름이 필요합니다. <strong>SRAM 활용 최적화, 멀티 플랫폼 지원, 희소성 적용 개선</strong>이 핵심 연구 방향이 될 것입니다.</p>
<h1 id="q-flashattention에서-사용하는-타일링tiling과-재계산recomputation-기법이-기존-self-attention의-메모리-및-연산-복잡도를-어떻게-구체적으로-줄이는지-설명할-수-있는가">Q: FlashAttention에서 사용하는 타일링(Tiling)과 재계산(Recomputation) 기법이 기존 self-attention의 메모리 및 연산 복잡도를 어떻게 구체적으로 줄이는지 설명할 수 있는가?</h1>
<h1 id="a">A:</h1>
<h3 id="flashattention의-타일링tiling과-재계산recomputation-기법이-메모리-및-연산-복잡도를-줄이는-방식">FlashAttention의 타일링(Tiling)과 재계산(Recomputation) 기법이 메모리 및 연산 복잡도를 줄이는 방식</h3>
<p>FlashAttention은 self-attention의 메모리 사용량과 연산 복잡도를 최적화하기 위해 <strong>타일링(Tiling)</strong> 과 <strong>재계산(Recomputation)</strong> 기법을 활용합니다. 이 두 가지 기법이 각각 메모리와 연산 측면에서 개선을 이루는 과정을 구체적으로 설명하겠습니다.</p>
<hr>
<h3 id="1-타일링tiling-기법-메모리-최적화">1. 타일링(Tiling) 기법: 메모리 최적화</h3>
<h4 id="기존-방법의-문제점">기존 방법의 문제점</h4>
<ul>
<li>기존 self-attention은 전체 시퀀스 ( Q, K, V ) 행렬을 곱해 ( S = QK^\top ) 를 계산합니다.</li>
<li>이 ( S ) 행렬은 크기가 ( N \times N )으로, 시퀀스 길이 ( N )이 커질수록 <strong>메모리 사용량이 ( O(N^2) )</strong> 으로 증가.</li>
<li>중간 결과(( S ), ( P = \text{softmax}(S) ))를 저장하는 데도 많은 메모리를 소모.</li>
</ul>
<h4 id="타일링의-해결-방법">타일링의 해결 방법</h4>
<ul>
<li><strong>블록 단위 계산</strong>: ( Q, K, V )를 GPU SRAM에 적합한 크기(( M ))로 블록화하여 처리.
<ul>
<li>예: ( Q, K, V ) 각각 ( N \times d ) 행렬을 ( M \times d ) 블록으로 분할.</li>
<li>각 블록(( Q_i, K_j, V_j ))에 대해 부분 결과를 계산.
[
S_{i,j} = Q_i K_j^\top, \quad P_{i,j} = \text{softmax}(S_{i,j}), \quad O_{i,j} = P_{i,j} V_j
]</li>
<li>계산된 ( O_{i,j} )는 HBM(Higher Bandwidth Memory)에 저장.</li>
</ul>
</li>
<li><strong>장점</strong>:
<ul>
<li>( N \times N ) 크기의 전체 ( S ) 행렬을 한 번에 생성할 필요가 없어 메모리 사용량이 <strong>( O(N) )</strong> 로 감소.</li>
<li>( Q, K, V ) 블록만 SRAM에 올리므로 메모리 병목 해소.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-재계산recomputation-기법-역전파-시-메모리-최적화">2. 재계산(Recomputation) 기법: 역전파 시 메모리 최적화</h3>
<h4 id="기존-방법의-문제점-1">기존 방법의 문제점</h4>
<ul>
<li>역전파 과정에서 ( S )와 ( P ) 같은 중간 결과를 저장하여, 이를 기반으로 그래디언트를 계산.</li>
<li>중간 값 저장으로 인해 추가 메모리 사용량 발생.</li>
</ul>
<h4 id="재계산의-해결-방법">재계산의 해결 방법</h4>
<ul>
<li><strong>중간 값 재생성</strong>:
<ul>
<li>( S )와 ( P )를 저장하지 않고, 필요한 경우 <strong>Softmax 정규화 통계</strong>만을 활용해 재계산.</li>
<li>Forward pass 동안 ( \text{rowmax} )와 ( \text{rowsum} ) 통계를 저장.
[
m(x) = \max(x), \quad \ell(x) = \sum e^{x - m(x)}
]</li>
<li>역전파에서 ( S_{i,j} )와 ( P_{i,j} )를 재생성하여 사용.
<ul>
<li>저장된 ( m(x), \ell(x) )를 기반으로 Softmax를 재계산.</li>
</ul>
</li>
</ul>
</li>
<li><strong>장점</strong>:
<ul>
<li>중간 결과 저장에 필요한 메모리 감소.</li>
<li>메모리 사용량은 ( O(N \cdot d) ) 로 감소.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-메모리-및-연산-복잡도-비교">3. 메모리 및 연산 복잡도 비교</h3>
<table>
  <thead>
      <tr>
          <th><strong>항목</strong></th>
          <th>기존 Attention</th>
          <th>FlashAttention</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>메모리 복잡도</td>
          <td>( O(N^2) )</td>
          <td>( O(N) )</td>
      </tr>
      <tr>
          <td>IO 복잡도</td>
          <td>( O(N^2) )</td>
          <td>( O(N \cdot d / M) )</td>
      </tr>
      <tr>
          <td>연산 복잡도(FLOPs)</td>
          <td>( O(N^2 \cdot d) )</td>
          <td>( O(N^2 \cdot d) + O(N \cdot d / M) )</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="4-주요-성과">4. 주요 성과</h3>
<h4 id="1-메모리-사용량-감소">(1) 메모리 사용량 감소</h4>
<ul>
<li>( N = 1024 ), ( d = 64 ), ( M = 128 ) 기준:
<ul>
<li>기존 Attention: 약 9MB의 HBM 사용.</li>
<li>FlashAttention: 약 1.6MB로 감소.</li>
</ul>
</li>
</ul>
<h4 id="2-연산량-증가">(2) 연산량 증가</h4>
<ul>
<li>재계산 과정에서 Softmax 연산 추가로 약 10~15%의 연산량 증가.</li>
</ul>
<h4 id="3-io-효율성">(3) IO 효율성</h4>
<ul>
<li>HBM과 SRAM 간 데이터 이동량을 최대 <strong>7.5배 감소</strong>.</li>
</ul>
<hr>
<h3 id="요약-1">요약</h3>
<p>FlashAttention의 타일링은 <strong>전체 Attention 행렬 생성 및 저장을 피하며</strong> 메모리 사용량을 줄이고, 재계산 기법은 <strong>중간 값을 저장하지 않음으로써 역전파 시 추가 메모리 소모를 최소화</strong>합니다. 이러한 두 가지 기법의 조화는 Transformer 모델의 긴 시퀀스 학습에서 탁월한 성능을 발휘하게 합니다.</p>
<h1 id="q-flashattention이-기존-sparse-attention-또는-approximate-attention과-비교하여-정확도와-속도에서-차별화되는-이유는-무엇인가">Q: FlashAttention이 기존 Sparse Attention 또는 Approximate Attention과 비교하여 정확도와 속도에서 차별화되는 이유는 무엇인가?</h1>
<h1 id="a-1">A:</h1>
<p>FlashAttention은 <strong>기존 Sparse Attention</strong> 및 <strong>Approximate Attention</strong>과 비교하여 정확도와 속도에서 차별화된 강점을 가지며, 이를 뒷받침하는 이유를 구체적으로 설명할 수 있습니다.</p>
<hr>
<h3 id="1-정확도에서의-차별화">1. 정확도에서의 차별화</h3>
<h4 id="기존-sparse-또는-approximate-attention의-문제점">기존 Sparse 또는 Approximate Attention의 문제점</h4>
<ol>
<li><strong>희소화(Sparsity)에 따른 정보 손실</strong>:
<ul>
<li>Sparse Attention은 특정 패턴(예: 지역 패턴)만 유지하고 나머지를 제거하여 <strong>정확도가 손실</strong>될 가능성이 있음.</li>
<li>예: Longformer나 BigBird는 긴 시퀀스 처리에 적합하지만, 전체 시퀀스를 고려하지 않아 세부 정보 손실 발생.</li>
</ul>
</li>
<li><strong>근사 계산(Approximation)의 한계</strong>:
<ul>
<li>Low-rank Approximation(Performer, Linformer)은 Attention 행렬을 근사화하여 계산량을 줄이지만, 긴 시퀀스에서 <strong>근사화 오류가 누적</strong>되어 모델 성능이 저하.</li>
</ul>
</li>
</ol>
<h4 id="flashattention의-정확도-보장">FlashAttention의 정확도 보장</h4>
<ol>
<li><strong>정확한 Attention 계산</strong>:
<ul>
<li>FlashAttention은 Sparse Attention과 달리 <strong>정확한 Attention 행렬</strong>을 계산하여 정보 손실 없이 정확도를 유지.</li>
<li>Approximate Attention과 달리 근사화 없이 모든 ( QK^\top ) 항목을 정확히 계산.</li>
</ul>
</li>
<li><strong>장기적 의존성 학습</strong>:
<ul>
<li>긴 시퀀스에서도 전체 정보(글로벌 컨텍스트)를 유지하므로 모델이 더 긴 문맥과 복잡한 의존성을 학습 가능.</li>
<li>Path-X(16K 길이) 및 Path-256(64K 길이)에서 <strong>Transformer 최초로 랜덤 성능을 초과</strong>.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-속도에서의-차별화">2. 속도에서의 차별화</h3>
<h4 id="기존-sparse-또는-approximate-attention의-속도-한계">기존 Sparse 또는 Approximate Attention의 속도 한계</h4>
<ol>
<li>
<p><strong>메모리 이동 병목</strong>:</p>
<ul>
<li>Sparse Attention은 ( O(N \cdot \text{sparsity}) ) 복잡도로 계산량을 줄이지만, 메모리 이동(IO) 최적화 부족으로 실제 속도 향상이 제한적.</li>
<li>Approximate Attention은 FLOP를 줄였지만, HBM과 SRAM 간의 데이터 이동이 많아 <strong>실제 벽시계 시간(wall-clock time)에서는 개선이 미미</strong>.</li>
<li>예: Performer, Linformer는 긴 시퀀스에서 계산 효율성은 높지만, IO 병목으로 인해 속도가 제한됨.</li>
</ul>
</li>
<li>
<p><strong>비효율적인 연산 순서</strong>:</p>
<ul>
<li>Sparse Attention은 희소 패턴을 적용하기 위해 추가 연산이 필요하며, 실제로는 단순한 계산 병렬화보다 느릴 수 있음.</li>
</ul>
</li>
</ol>
<h4 id="flashattention의-속도-개선">FlashAttention의 속도 개선</h4>
<ol>
<li><strong>IO 복잡도 최적화</strong>:
<ul>
<li>FlashAttention은 타일링(Tiling)을 통해 ( Q, K, V ) 블록을 GPU SRAM에서 처리하여 <strong>HBM과 SRAM 간 데이터 이동량을 줄임</strong>.</li>
<li>IO 복잡도를 기존 ( O(N^2) )에서 ( O(N \cdot d / M) )로 개선.</li>
</ul>
</li>
<li><strong>CUDA 커널 병합(Fused Kernel)</strong>:
<ul>
<li>Attention 계산, Softmax, Dropout 등을 단일 CUDA 커널에서 수행.</li>
<li>데이터 이동 및 커널 호출 오버헤드를 최소화.</li>
</ul>
</li>
<li><strong>FLOP 효율성 유지</strong>:
<ul>
<li>Sparse/Approximate Attention과 달리 정확한 Attention 계산을 유지하면서도 연산 최적화로 실제 속도를 개선.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-성능-비교-정확도와-속도">3. 성능 비교: 정확도와 속도</h3>
<table>
  <thead>
      <tr>
          <th><strong>기법</strong></th>
          <th><strong>시간 복잡도</strong></th>
          <th><strong>메모리 복잡도</strong></th>
          <th><strong>정확도</strong></th>
          <th><strong>주요 한계</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Sparse Attention</strong></td>
          <td>( O(N \cdot k) )</td>
          <td>( O(N \cdot k) )</td>
          <td>희소 패턴으로 정보 손실 발생 가능</td>
          <td>희소성에 따른 정보 손실, IO 병목 문제</td>
      </tr>
      <tr>
          <td><strong>Approximate Attention</strong></td>
          <td>( O(N \cdot d) )</td>
          <td>( O(N \cdot d) )</td>
          <td>근사화에 따른 정확도 손실 발생</td>
          <td>긴 시퀀스에서 근사화 오류 누적, IO 병목</td>
      </tr>
      <tr>
          <td><strong>FlashAttention</strong></td>
          <td>( O(N^2) )</td>
          <td>( O(N) )</td>
          <td>정확한 Attention 계산으로 정보 손실 없음</td>
          <td>CUDA 최적화 필요, 짧은 시퀀스에서는 효율성 제한</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="4-실험-결과로-본-차별화된-성능">4. 실험 결과로 본 차별화된 성능</h3>
<h4 id="정확도">정확도</h4>
<ul>
<li><strong>GPT-2</strong>:
<ul>
<li>FlashAttention은 시퀀스 길이 1K에서 4K로 확장하면서도 <strong>Perplexity가 0.7 감소</strong>.</li>
<li>Performer 등 Sparse/Approximate Attention에서는 동일한 확장 시 정확도 손실 발생.</li>
</ul>
</li>
</ul>
<h4 id="속도">속도</h4>
<ul>
<li><strong>GPT-2 학습 속도</strong>:
<ul>
<li>FlashAttention은 HuggingFace 대비 <strong>3배</strong>, Megatron-LM 대비 <strong>1.7배 빠른 속도</strong>.</li>
<li>Approximate Attention(Linformer 등)은 실제 속도 개선폭이 제한적.</li>
</ul>
</li>
</ul>
<h4 id="긴-시퀀스-처리">긴 시퀀스 처리</h4>
<ul>
<li><strong>Path-X 및 Path-256</strong>:
<ul>
<li>FlashAttention은 Transformer 최초로 랜덤 성능 초과(16K, 64K 시퀀스 길이).</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-결론-flashattention의-차별화-이유">5. 결론: FlashAttention의 차별화 이유</h3>
<p>FlashAttention이 정확도와 속도 모두에서 기존 Sparse 또는 Approximate Attention과 차별화되는 이유는 다음과 같습니다:</p>
<ol>
<li><strong>정확한 Attention 계산</strong>:
<ul>
<li>Approximation 없이 모든 Attention 계산을 수행해 정확도를 보장.</li>
</ul>
</li>
<li><strong>IO 최적화 기반 속도 개선</strong>:
<ul>
<li>타일링 및 CUDA 최적화를 통해 실제 벽시계 시간에서 뛰어난 성능.</li>
</ul>
</li>
<li><strong>긴 시퀀스 처리 능력</strong>:
<ul>
<li>Sparse Attention보다 높은 정보 유지력, Approximate Attention보다 높은 정확도.</li>
</ul>
</li>
</ol>
<p>FlashAttention은 기존의 정확도-효율성 트레이드오프를 극복하여 긴 시퀀스와 대규모 데이터에서 Transformer 모델의 성능을 새로운 수준으로 끌어올렸습니다.</p>
<h1 id="q-flashattention을-멀티-gpu-환경에서-효율적으로-확장하기-위해-어떤-추가적인-io-최적화-또는-알고리즘-변경이-필요한가">Q: FlashAttention을 멀티-GPU 환경에서 효율적으로 확장하기 위해 어떤 추가적인 IO 최적화 또는 알고리즘 변경이 필요한가?</h1>
<h1 id="a-2">A:</h1>
<p>FlashAttention을 멀티-GPU 환경에서 효율적으로 확장하려면 <strong>노드 간 데이터 이동</strong>과 <strong>GPU 간 작업 병렬화</strong>에서 IO 병목을 최소화해야 합니다. 이를 달성하기 위해 다음과 같은 <strong>추가적인 IO 최적화</strong> 및 <strong>알고리즘 변경</strong>이 필요합니다.</p>
<hr>
<h3 id="1-멀티-gpu-환경에서의-주요-과제">1. 멀티-GPU 환경에서의 주요 과제</h3>
<h4 id="1-노드-간-통신-병목">(1) <strong>노드 간 통신 병목</strong></h4>
<ul>
<li>멀티-GPU 환경에서는 각 GPU가 HBM에서 데이터를 처리하며, GPU 간 통신을 위해 <strong>인터커넥트</strong>(NVLink, PCIe)를 사용합니다.</li>
<li>FlashAttention은 타일링을 통해 개별 GPU에서 효율적으로 동작하지만, GPU 간 데이터를 교환할 때 <strong>메모리 대역폭과 네트워크 병목</strong>이 발생할 수 있음.</li>
</ul>
<h4 id="2-작업-병렬화">(2) <strong>작업 병렬화</strong></h4>
<ul>
<li>Attention 계산은 ( Q, K, V ) 블록을 분할하여 처리하지만, 멀티-GPU 환경에서는 <strong>블록 간 의존성</strong> 때문에 GPU 사용률이 낮아질 수 있음.</li>
<li>GPU 간의 균등한 작업 분배가 필요.</li>
</ul>
<hr>
<h3 id="2-멀티-gpu-환경을-위한-추가-io-최적화">2. 멀티-GPU 환경을 위한 추가 IO 최적화</h3>
<h4 id="1-cross-gpu-memory-sharing-gpu-간-메모리-공유">(1) <strong>Cross-GPU Memory Sharing (GPU 간 메모리 공유)</strong></h4>
<ul>
<li>GPU 간 HBM 메모리를 효율적으로 공유하도록 설계.</li>
<li><strong>방법</strong>:
<ul>
<li>( Q, K, V ) 데이터와 계산 중간 결과를 각 GPU에 분산 저장.</li>
<li>GPU A가 계산한 ( S_{i,j} ), ( P_{i,j} ) 결과를 GPU B와 공유하는 구조.</li>
</ul>
</li>
<li><strong>추가 최적화</strong>:
<ul>
<li>GPU HBM과 NVLink를 활용해 GPU 간 통신 병목 최소화.</li>
<li><strong>비동기 데이터 전송</strong>(Asynchronous Data Transfer)을 통해 통신과 연산 중첩.</li>
</ul>
</li>
</ul>
<h4 id="2-io-aware-partitioning-io-중심-분할">(2) <strong>IO-aware Partitioning (IO 중심 분할)</strong></h4>
<ul>
<li>( Q, K, V ) 블록을 IO 비용을 고려해 GPU 간 균등하게 분할.</li>
<li><strong>방법</strong>:
<ul>
<li>타일 크기 및 데이터 위치를 GPU 간 통신 비용을 최소화하도록 동적으로 조정.</li>
<li>GPU 간 통신량이 최소화되도록 ( Q ), ( K ), ( V )를 분산.</li>
</ul>
</li>
</ul>
<h4 id="3-pipeline-parallelism">(3) <strong>Pipeline Parallelism</strong></h4>
<ul>
<li>GPU 간 계산을 파이프라인 형태로 나눠 처리.</li>
<li><strong>방법</strong>:
<ul>
<li>각 GPU는 Attention 계산의 일부 단계를 담당.
<ul>
<li>GPU A: ( Q \times K^\top ) 계산.</li>
<li>GPU B: ( P = \text{softmax}(S) ).</li>
<li>GPU C: ( O = PV ).</li>
</ul>
</li>
<li>각 단계는 비동기적으로 처리하여 통신 대기 시간을 줄임.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-알고리즘-변경">3. 알고리즘 변경</h3>
<h4 id="1-sharded-attention">(1) <strong>Sharded Attention</strong></h4>
<ul>
<li>( Q, K, V )를 각 GPU에 분할하여 저장하고 연산.</li>
<li><strong>방법</strong>:
<ul>
<li>각 GPU가 시퀀스의 일부를 담당하고, 필요한 데이터만 노드 간 전송.</li>
<li>각 GPU에서 ( Q ), ( K )의 로컬 부분을 처리한 후 결과를 교환.</li>
</ul>
</li>
<li><strong>장점</strong>:
<ul>
<li>전체 메모리 사용량 감소.</li>
<li>노드 간 통신 최소화.</li>
</ul>
</li>
</ul>
<h4 id="2-sparse-attention과-결합">(2) <strong>Sparse Attention과 결합</strong></h4>
<ul>
<li>희소 Attention 기법을 사용해 멀티-GPU 환경에서 불필요한 데이터 전송을 줄임.</li>
<li><strong>방법</strong>:
<ul>
<li>Sparse Attention을 FlashAttention의 타일링에 결합.</li>
<li>중요한 ( Q, K ) 블록만 GPU 간 교환.</li>
</ul>
</li>
<li><strong>효과</strong>:
<ul>
<li>매우 긴 시퀀스에서도 노드 간 통신량 감소.</li>
</ul>
</li>
</ul>
<h4 id="3-hierarchical-attention">(3) <strong>Hierarchical Attention</strong></h4>
<ul>
<li>GPU 간 작업을 계층적으로 분리하여 병렬화.</li>
<li><strong>방법</strong>:
<ul>
<li>1단계: 각 GPU가 로컬 Attention 계산.</li>
<li>2단계: GPU 간 Attention 결과를 결합.</li>
</ul>
</li>
<li><strong>장점</strong>:
<ul>
<li>GPU 별로 독립적인 작업 수행 가능.</li>
<li>교환 단계에서 최소한의 데이터만 전송.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-멀티-gpu-환경에서-flashattention의-최적화-효과-분석">4. 멀티-GPU 환경에서 FlashAttention의 최적화 효과 분석</h3>
<h4 id="io-병목-개선">IO 병목 개선</h4>
<ul>
<li><strong>Cross-GPU Memory Sharing</strong>:
<ul>
<li>NVLink 대역폭 활용 시 ( 10 \sim 20 %) 통신 비용 감소.</li>
</ul>
</li>
<li><strong>Sharded Attention</strong>:
<ul>
<li>데이터 전송량을 희소 패턴으로 줄여 통신 병목 해소.</li>
</ul>
</li>
</ul>
<h4 id="작업-병렬화-개선">작업 병렬화 개선</h4>
<ul>
<li><strong>Pipeline Parallelism</strong>:
<ul>
<li>GPU 사용률을 ( 80 % ) 이상으로 유지.</li>
</ul>
</li>
<li><strong>IO-aware Partitioning</strong>:
<ul>
<li>GPU 간 균등 작업 배분으로 성능 향상.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-flashattention-확장-적용-실험-결과-예측">5. FlashAttention 확장 적용 실험 결과 예측</h3>
<table>
  <thead>
      <tr>
          <th><strong>방법</strong></th>
          <th><strong>추가 IO 비용 감소율</strong></th>
          <th><strong>GPU 사용률</strong></th>
          <th><strong>추론 속도 향상</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>기본 FlashAttention</td>
          <td>-</td>
          <td>( \sim70 %)</td>
          <td>-</td>
      </tr>
      <tr>
          <td>Cross-GPU Memory Sharing</td>
          <td>( 10 %)</td>
          <td>( \sim80 %)</td>
          <td>( \sim1.3\times )</td>
      </tr>
      <tr>
          <td>Sharded Attention</td>
          <td>( 20 %)</td>
          <td>( \sim85 %)</td>
          <td>( \sim1.5\times )</td>
      </tr>
      <tr>
          <td>Pipeline Parallelism</td>
          <td>( 15 %)</td>
          <td>( \sim90 %)</td>
          <td>( \sim1.8\times )</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="결론-2">결론</h3>
<p>FlashAttention을 멀티-GPU 환경에서 효율적으로 확장하려면, GPU 간 통신 최적화(Cross-GPU Memory Sharing), Sharded Attention 설계, 그리고 Pipeline Parallelism을 결합해야 합니다. 이러한 최적화는 긴 시퀀스 학습 시 통신 병목을 줄이고, 연산 자원을 최대한 활용하며, 멀티-GPU 환경에서의 Transformer 모델 학습을 크게 가속화할 수 있습니다.</p>

  </article>

  
  
  <div class="post-license bg-muted/30 border-border mb-8 rounded-xl border p-6">
    <div class="flex items-start gap-4">
      <div
        class="bg-primary/10 flex h-12 w-12 flex-shrink-0 items-center justify-center rounded-lg">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이선스"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m5.618-4.016A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 3.04A12.02 12.02 0 003 9c0 5.591 3.824 10.29 9 11.622 5.176-1.332 9-6.03 9-11.622 0-1.042-.133-2.052-.382-3.016z" />
</svg>
      </div>

      <div class="flex-1">
        <h3 class="text-foreground mb-2 text-lg font-semibold">
          라이선스
        </h3>

        <div class="text-muted-foreground space-y-2 text-sm">
          
          <p>
            <strong>저작자:</strong>
            류재훈
          </p>
          

          <p>
            <strong>링크:</strong>
            <a
              href="http://192.168.1.11:1314/posts/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/"
              class="text-primary hover:text-primary/80 transition-colors duration-200">
              http://192.168.1.11:1314/posts/FlashAttention-Fast-and-Memory-Efficient-Exact-Attention-with-IO-Awareness/
            </a>
          </p>

          <p>
            <strong>라이선스:</strong>
            <a
              href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
              target="_blank"
              rel="noopener noreferrer"
              class="text-primary hover:text-primary/80 transition-colors duration-200">
              CC BY-NC-SA 4.0
            </a>
          </p>

          <p class="text-xs">
            이 저작물은 크리에이티브 커먼즈 저작자표시-비영리-동일조건변경허락 4.0 국제 라이선스에 따라 이용할 수 있습니다. 출처를 표시하고, 비영리 목적으로만 사용하며, 동일한 라이선스를 유지해야 합니다.
          </p>
        </div>
      </div>
    </div>
  </div>


  
  





  <nav
    class="post-navigation mb-12"
    aria-label="게시글 네비게이션">
    <div class="grid grid-cols-1 gap-4 md:grid-cols-2">
      
      <div class="nav-item">
        
          <a
            href="/posts/Infinite-LLM-Efficient-LLM-Service-for-Long-Context-with-DistAttention-and-Distributed-KVCache/"
            class="group bg-card border-border hover:bg-primary/5 hover:border-primary/20 focus:ring-primary/20 flex min-h-[140px] flex-col justify-between rounded-xl border p-6 transition-all duration-300 ease-out hover:-translate-y-1 hover:scale-[1.02] hover:shadow-lg focus:ring-2 focus:outline-none">
            <div>
              <div class="mb-3 flex items-center gap-3">
                
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7" />
</svg>
                <span class="text-muted-foreground text-sm font-medium">
                  이전
                </span>
              </div>

              <h3
                class="text-foreground group-hover:text-primary mb-3 line-clamp-2 text-lg leading-tight font-semibold transition-colors duration-200">
                Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache
              </h3>
            </div>

            <div
              class="text-muted-foreground mt-auto flex items-center gap-2 text-xs">
              
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
              <time datetime="2024-12-18">
                12月18日
              </time>
            </div>
          </a>
        
      </div>

      
      <div class="nav-item">
        
          <a
            href="/posts/FlashAttention-3-Fast-and-Accurate-Attention-with-Asynchrony-and-Low-precision/"
            class="group bg-card border-border hover:bg-primary/5 hover:border-primary/20 focus:ring-primary/20 flex min-h-[140px] flex-col justify-between rounded-xl border p-6 transition-all duration-300 ease-out hover:-translate-y-1 hover:scale-[1.02] hover:shadow-lg focus:ring-2 focus:outline-none">
            <div>
              <div class="mb-3 flex items-center justify-end gap-3">
                <span class="text-muted-foreground text-sm font-medium">
                  다음
                </span>
                
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg>
              </div>

              <h3
                class="text-foreground group-hover:text-primary mb-3 line-clamp-2 text-right text-lg leading-tight font-semibold transition-colors duration-200">
                FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
              </h3>
            </div>

            <div
              class="text-muted-foreground mt-auto flex items-center justify-end gap-2 text-xs">
              <time datetime="2024-12-18">
                12月18日
              </time>
              
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
            </div>
          </a>
        
      </div>
    </div>
  </nav>




  
  










  
  


    </main>

    <footer class="mx-auto max-w-4xl px-4 py-8">
  <div class="px-6 py-6">
    
    
      <nav class="mb-6">
        <div class="flex flex-wrap items-center justify-center gap-1">
          
            
            
            
            <a
              href="/index.xml"
              
              class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="RSS Feed">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18 0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z" />

</svg>
              
              <span
                >
                  RSS Feed
                </span
              >
            </a>
          
        </div>
      </nav>
    


    
    
      <div class="border-border mb-6 border-t"></div>
    


    
    <div class="flex flex-col items-center justify-between gap-4 md:flex-row">
      
      <div class="text-muted-foreground text-sm">
        <p>
          &copy; 2025 Jaehun&#39;s Blog.
          모든 권리 보유.
        </p>
      </div>

      
      
        <div class="flex items-center gap-3">
          
            <a
              href="https://github.com/ryujaehun"
              
                target="_blank" rel="noopener noreferrer"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="GitHub"
              aria-label="GitHub">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5c.08-1.25-.27-2.48-1-3.5c.28-1.15.28-2.35 0-3.5c0 0-1 0-3 1.5c-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.4 5.4 0 0 0 4 9c0 3.5 3 5.5 6 5.5c-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
              
            </a>
          
            <a
              href="https://www.linkedin.com/in/jaehunryu/"
              
                target="_blank" rel="noopener noreferrer"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="Linkedin"
              aria-label="Linkedin">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2a2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
              
            </a>
          
            <a
              href="mailto:jaehunryu@icloud.com"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="Email"
              aria-label="Email">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="Email">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg>
              
            </a>
          
        </div>
      

    </div>
  </div>
</footer>


    
    



<div
  id="dock"
  class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0"
  role="toolbar"
  aria-label="바로가기 도구 모음">
  <nav
    class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3">
    
    
      <button
        id="dock-back"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
        title="뒤로"
        aria-label="뒤로">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path d="m12 19-7-7 7-7"/>
  <path d="M19 12H5"/>

</svg>
        <span class="hidden sm:inline"
          >뒤로</span
        >
      </button>

      
      <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>
    


    
    
      <button
        id="dock-toc"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
        title="목차"
        aria-label="목차">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <line x1="8" x2="21" y1="6" y2="6"/>
  <line x1="8" x2="21" y1="12" y2="12"/>
  <line x1="8" x2="21" y1="18" y2="18"/>
  <line x1="3" x2="3.01" y1="6" y2="6"/>
  <line x1="3" x2="3.01" y1="12" y2="12"/>
  <line x1="3" x2="3.01" y1="18" y2="18"/>

</svg>
        <span class="hidden sm:inline"
          >목차</span
        >
      </button>

      
      <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>
    


    
    <button
      id="dock-search"
      class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2"
      title="검색"
      aria-label="검색">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
      <span class="hidden md:inline"
        >검색</span
      >
    </button>

    
    <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>

    
    
    


    
    <button
      id="dock-top"
      class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
      title="맨 위로"
      aria-label="맨 위로">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path d="m18 15-6-6-6 6"/>

</svg>
      <span class="hidden sm:inline"
        >맨 위로</span
      >
    </button>
  </nav>
</div>


    
      



<div
  id="toc-overlay"
  class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300"
  role="dialog"
  aria-modal="true"
  aria-labelledby="toc-title"></div>


<div
  id="toc-card"
  class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2"
  role="dialog"
  aria-modal="true"
  aria-labelledby="toc-title">
  <div
    class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md">
    
    <div
      class="border-border bg-muted/30 flex items-center justify-between border-b p-4">
      <div class="flex items-center gap-3">
        
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <line x1="8" x2="21" y1="6" y2="6"/>
  <line x1="8" x2="21" y1="12" y2="12"/>
  <line x1="8" x2="21" y1="18" y2="18"/>
  <line x1="3" x2="3.01" y1="6" y2="6"/>
  <line x1="3" x2="3.01" y1="12" y2="12"/>
  <line x1="3" x2="3.01" y1="18" y2="18"/>

</svg>
        <h2 id="toc-title" class="text-foreground text-lg font-semibold">
          목차
        </h2>
      </div>

      
      <button
        id="toc-close"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"
        title="닫기"
        aria-label="닫기">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
      </button>
    </div>

    
    <div class="max-h-96 overflow-y-auto p-4">
      <nav
        id="toc-nav"
        class="toc-scrollbar"
        aria-label="글 목차">
          
          <div id="toc-content">
            <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#요약-및-결론">요약 및 결론</a></li>
        <li><a href="#핵심-알고리즘-설명">핵심 알고리즘 설명</a>
          <ul>
            <li><a href="#입력-데이터">입력 데이터:</a></li>
            <li><a href="#과정">과정:</a></li>
          </ul>
        </li>
        <li><a href="#강점과-독창성">강점과 독창성</a>
          <ul>
            <li><a href="#강점">강점:</a></li>
            <li><a href="#독창성">독창성:</a></li>
          </ul>
        </li>
        <li><a href="#한계점">한계점</a></li>
        <li><a href="#결론">결론</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#핵심-알고리즘-flashattention">핵심 알고리즘: FlashAttention</a></li>
        <li><a href="#예시를-통한-알고리즘-단계별-설명">예시를 통한 알고리즘 단계별 설명</a>
          <ul>
            <li><a href="#가정된-입력">가정된 입력:</a></li>
            <li><a href="#전체-계산-목표">전체 계산 목표:</a></li>
          </ul>
        </li>
        <li><a href="#단계-1-블록-분할-tiling">단계 1: 블록 분할 (Tiling)</a></li>
        <li><a href="#단계-2-블록-기반-attention-계산">단계 2: 블록 기반 Attention 계산</a>
          <ul>
            <li><a href="#1-첫-번째-블록--q_1-와--k_1-v_1-">(1) 첫 번째 블록 (( Q_1 )와 ( K_1, V_1 ))</a></li>
            <li><a href="#2-두-번째-블록--q_2-와--k_1-v_1-">(2) 두 번째 블록 (( Q_2 )와 ( K_1, V_1 ))</a></li>
          </ul>
        </li>
        <li><a href="#단계-3-중간-결과-축적-및-재계산">단계 3: 중간 결과 축적 및 재계산</a></li>
        <li><a href="#단계-4-최종-결과-조합">단계 4: 최종 결과 조합</a></li>
        <li><a href="#전체-과정의-시간-및-메모리-효율">전체 과정의 시간 및 메모리 효율</a></li>
        <li><a href="#정리">정리</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#논문-결과-분석-flashattention의-성과">논문 결과 분석: FlashAttention의 성과</a></li>
        <li><a href="#1-실험-결과">1. <strong>실험 결과</strong></a>
          <ul>
            <li><a href="#1-속도-학습-및-추론-시간-단축">(1) <strong>속도</strong>: 학습 및 추론 시간 단축</a></li>
            <li><a href="#2-메모리-사용량">(2) <strong>메모리 사용량</strong></a></li>
            <li><a href="#3-정확도-개선">(3) <strong>정확도 개선</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-특출난-점">2. <strong>특출난 점</strong></a>
          <ul>
            <li><a href="#1-긴-시퀀스-처리-능력">(1) <strong>긴 시퀀스 처리 능력</strong></a></li>
            <li><a href="#2-효율성과-정확도의-균형">(2) <strong>효율성과 정확도의 균형</strong></a></li>
            <li><a href="#3-확장성">(3) <strong>확장성</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-flashattention의-기여-요인">3. <strong>FlashAttention의 기여 요인</strong></a>
          <ul>
            <li><a href="#1-타일링-기반-설계">(1) <strong>타일링 기반 설계</strong></a></li>
            <li><a href="#2-recomputation-재계산">(2) <strong>Recomputation (재계산)</strong></a></li>
            <li><a href="#3-gpu-io-계층-최적화">(3) <strong>GPU IO 계층 최적화</strong></a></li>
          </ul>
        </li>
        <li><a href="#4-논문에서-제시한-이유와-내-의견">4. <strong>논문에서 제시한 이유와 내 의견</strong></a>
          <ul>
            <li><a href="#논문에서-제시한-이유">논문에서 제시한 이유:</a></li>
            <li><a href="#내-의견">내 의견:</a></li>
          </ul>
        </li>
        <li><a href="#결론-1">결론</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#flashattention-적용을-위한-구현-추가-사항과-요구-리소스-분석">FlashAttention 적용을 위한 구현 추가 사항과 요구 리소스 분석</a></li>
        <li><a href="#1-기존-방법에서의-구현-대비-추가-작업">1. 기존 방법에서의 구현 대비 추가 작업</a>
          <ul>
            <li><a href="#1-cuda-기반-커널-구현">(1) <strong>CUDA 기반 커널 구현</strong></a></li>
            <li><a href="#2-recomputation-알고리즘-구현">(2) <strong>Recomputation 알고리즘 구현</strong></a></li>
            <li><a href="#3-타일링-전략-설계-및-테스트">(3) <strong>타일링 전략 설계 및 테스트</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-추가-연산과-컴퓨팅-자원-요구-사항">2. 추가 연산과 컴퓨팅 자원 요구 사항</a>
          <ul>
            <li><a href="#연산량-flops">연산량 (FLOPs)</a></li>
            <li><a href="#메모리-사용량">메모리 사용량</a></li>
            <li><a href="#io-복잡도">IO 복잡도</a></li>
          </ul>
        </li>
        <li><a href="#3-필요-컴퓨팅-자원-계산">3. 필요 컴퓨팅 자원 계산</a>
          <ul>
            <li><a href="#시뮬레이션-환경">시뮬레이션 환경</a></li>
            <li><a href="#자원-요구량">자원 요구량</a></li>
          </ul>
        </li>
        <li><a href="#4-제-의견">4. 제 의견</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#flashattention의-입력-데이터-및-추론-과정">FlashAttention의 입력 데이터 및 추론 과정</a></li>
        <li><a href="#1-입력-데이터">1. <strong>입력 데이터</strong></a>
          <ul>
            <li><a href="#1-데이터-정의">(1) 데이터 정의</a></li>
            <li><a href="#2-데이터-크기">(2) 데이터 크기</a></li>
          </ul>
        </li>
        <li><a href="#2-추론-과정">2. <strong>추론 과정</strong></a>
          <ul>
            <li><a href="#기본-수식">기본 수식</a></li>
            <li><a href="#flashattention의-최적화-과정">FlashAttention의 최적화 과정</a></li>
          </ul>
        </li>
        <li><a href="#3-모델-아키텍처-구성">3. <strong>모델 아키텍처 구성</strong></a></li>
        <li><a href="#4-연산-및-메모리-요구량">4. <strong>연산 및 메모리 요구량</strong></a>
          <ul>
            <li><a href="#1-연산량-flops">(1) 연산량 (FLOPs)</a></li>
            <li><a href="#예시-연산량-계산-gpt-2-모델-기준"><strong>예시 연산량 계산</strong> (GPT-2 모델 기준)</a></li>
            <li><a href="#2-메모리-요구량">(2) 메모리 요구량</a></li>
            <li><a href="#예시-메모리-요구량-a100-gpu-기준"><strong>예시 메모리 요구량</strong> (A100 GPU 기준)</a></li>
          </ul>
        </li>
        <li><a href="#5-컴퓨팅-자원-요구량">5. <strong>컴퓨팅 자원 요구량</strong></a>
          <ul>
            <li><a href="#a100-gpu-예시">A100 GPU 예시</a></li>
            <li><a href="#t4-gpu-예시">T4 GPU 예시</a></li>
          </ul>
        </li>
        <li><a href="#요약">요약</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#flashattention의-한계와-이를-극복하기-위한-연구-흐름">FlashAttention의 한계와 이를 극복하기 위한 연구 흐름</a></li>
        <li><a href="#1-한계점-요약">1. <strong>한계점 요약</strong></a>
          <ul>
            <li><a href="#1-cuda-커널-의존">(1) <strong>CUDA 커널 의존</strong></a></li>
            <li><a href="#2-짧은-타일-크기의-성능-제한">(2) <strong>짧은 타일 크기의 성능 제한</strong></a></li>
            <li><a href="#3-초대형-모델-및-멀티-gpu-확장성">(3) <strong>초대형 모델 및 멀티-GPU 확장성</strong></a></li>
            <li><a href="#4-희소-attention과의-결합-제한">(4) <strong>희소 Attention과의 결합 제한</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-한계를-극복하기-위한-연구-흐름">2. <strong>한계를 극복하기 위한 연구 흐름</strong></a>
          <ul>
            <li><a href="#1-고수준-api로-추상화된-구현">(1) <strong>고수준 API로 추상화된 구현</strong></a></li>
            <li><a href="#2-sram-활용-극대화">(2) <strong>SRAM 활용 극대화</strong></a></li>
            <li><a href="#3-멀티-gpu-io-aware-attention">(3) <strong>멀티-GPU IO-aware Attention</strong></a></li>
            <li><a href="#4-희소-attention과의-완벽한-결합">(4) <strong>희소 Attention과의 완벽한 결합</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-멀티-플랫폼-지원-및-범용성-강화">3. <strong>멀티 플랫폼 지원 및 범용성 강화</strong></a></li>
        <li><a href="#요약-flashattention-극복-연구-흐름">요약: FlashAttention 극복 연구 흐름</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#flashattention의-타일링tiling과-재계산recomputation-기법이-메모리-및-연산-복잡도를-줄이는-방식">FlashAttention의 타일링(Tiling)과 재계산(Recomputation) 기법이 메모리 및 연산 복잡도를 줄이는 방식</a></li>
        <li><a href="#1-타일링tiling-기법-메모리-최적화">1. 타일링(Tiling) 기법: 메모리 최적화</a>
          <ul>
            <li><a href="#기존-방법의-문제점">기존 방법의 문제점</a></li>
            <li><a href="#타일링의-해결-방법">타일링의 해결 방법</a></li>
          </ul>
        </li>
        <li><a href="#2-재계산recomputation-기법-역전파-시-메모리-최적화">2. 재계산(Recomputation) 기법: 역전파 시 메모리 최적화</a>
          <ul>
            <li><a href="#기존-방법의-문제점-1">기존 방법의 문제점</a></li>
            <li><a href="#재계산의-해결-방법">재계산의 해결 방법</a></li>
          </ul>
        </li>
        <li><a href="#3-메모리-및-연산-복잡도-비교">3. 메모리 및 연산 복잡도 비교</a></li>
        <li><a href="#4-주요-성과">4. 주요 성과</a>
          <ul>
            <li><a href="#1-메모리-사용량-감소">(1) 메모리 사용량 감소</a></li>
            <li><a href="#2-연산량-증가">(2) 연산량 증가</a></li>
            <li><a href="#3-io-효율성">(3) IO 효율성</a></li>
          </ul>
        </li>
        <li><a href="#요약-1">요약</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#1-정확도에서의-차별화">1. 정확도에서의 차별화</a>
          <ul>
            <li><a href="#기존-sparse-또는-approximate-attention의-문제점">기존 Sparse 또는 Approximate Attention의 문제점</a></li>
            <li><a href="#flashattention의-정확도-보장">FlashAttention의 정확도 보장</a></li>
          </ul>
        </li>
        <li><a href="#2-속도에서의-차별화">2. 속도에서의 차별화</a>
          <ul>
            <li><a href="#기존-sparse-또는-approximate-attention의-속도-한계">기존 Sparse 또는 Approximate Attention의 속도 한계</a></li>
            <li><a href="#flashattention의-속도-개선">FlashAttention의 속도 개선</a></li>
          </ul>
        </li>
        <li><a href="#3-성능-비교-정확도와-속도">3. 성능 비교: 정확도와 속도</a></li>
        <li><a href="#4-실험-결과로-본-차별화된-성능">4. 실험 결과로 본 차별화된 성능</a>
          <ul>
            <li><a href="#정확도">정확도</a></li>
            <li><a href="#속도">속도</a></li>
            <li><a href="#긴-시퀀스-처리">긴 시퀀스 처리</a></li>
          </ul>
        </li>
        <li><a href="#5-결론-flashattention의-차별화-이유">5. 결론: FlashAttention의 차별화 이유</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#1-멀티-gpu-환경에서의-주요-과제">1. 멀티-GPU 환경에서의 주요 과제</a>
          <ul>
            <li><a href="#1-노드-간-통신-병목">(1) <strong>노드 간 통신 병목</strong></a></li>
            <li><a href="#2-작업-병렬화">(2) <strong>작업 병렬화</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-멀티-gpu-환경을-위한-추가-io-최적화">2. 멀티-GPU 환경을 위한 추가 IO 최적화</a>
          <ul>
            <li><a href="#1-cross-gpu-memory-sharing-gpu-간-메모리-공유">(1) <strong>Cross-GPU Memory Sharing (GPU 간 메모리 공유)</strong></a></li>
            <li><a href="#2-io-aware-partitioning-io-중심-분할">(2) <strong>IO-aware Partitioning (IO 중심 분할)</strong></a></li>
            <li><a href="#3-pipeline-parallelism">(3) <strong>Pipeline Parallelism</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-알고리즘-변경">3. 알고리즘 변경</a>
          <ul>
            <li><a href="#1-sharded-attention">(1) <strong>Sharded Attention</strong></a></li>
            <li><a href="#2-sparse-attention과-결합">(2) <strong>Sparse Attention과 결합</strong></a></li>
            <li><a href="#3-hierarchical-attention">(3) <strong>Hierarchical Attention</strong></a></li>
          </ul>
        </li>
        <li><a href="#4-멀티-gpu-환경에서-flashattention의-최적화-효과-분석">4. 멀티-GPU 환경에서 FlashAttention의 최적화 효과 분석</a>
          <ul>
            <li><a href="#io-병목-개선">IO 병목 개선</a></li>
            <li><a href="#작업-병렬화-개선">작업 병렬화 개선</a></li>
          </ul>
        </li>
        <li><a href="#5-flashattention-확장-적용-실험-결과-예측">5. FlashAttention 확장 적용 실험 결과 예측</a></li>
        <li><a href="#결론-2">결론</a></li>
      </ul>
    </li>
  </ul>
</nav>
          </div>
      </nav>
    </div>

    
    <div class="border-border bg-muted/20 border-t px-4 py-3">
      <div class="text-muted-foreground text-center text-xs">
        <span>제목을 클릭하면 해당 위치로 이동합니다</span>
      </div>
    </div>
  </div>
</div>

    


  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
    integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP"
    crossorigin="anonymous" />
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
    integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6"
    crossorigin="anonymous"></script>
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
    integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
    crossorigin="anonymous"></script>

<script>
    
    function initKaTeX() {
        if (typeof renderMathInElement === 'undefined') {
            setTimeout(initKaTeX, 100);
            return;
        }

        var delimiters = [{"display":true,"left":"$$","right":"$$"},{"display":false,"left":"$","right":"$"}];

        renderMathInElement(document.body, {
            delimiters: delimiters,
            throwOnError: false,
            errorColor: '#cc0000',
            fleqn: false,
            leqno: false,
            trust: false
        });
    }

    
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', initKaTeX);
    } else {
        initKaTeX();
    }
</script>
    



    
    
<div
  id="search-overlay"
  class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div>


<div
  id="search-modal"
  class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300">
  
  <div class="border-border flex items-center gap-3 border-b p-4">
    
    <div class="text-muted-foreground h-5 w-5 flex-shrink-0">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
    </div>

    
    <div class="relative flex-1">
      
      <button
        id="search-clear"
        class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200"
        title="지우기"
        aria-label="지우기">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
      </button>

      
      <input
        id="search-input"
        type="text"
        placeholder="게시글 검색..."
        class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none"
        autocomplete="off"
        spellcheck="false" />
    </div>

    
    <button
      id="search-close"
      class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200"
      title="닫기"
      aria-label="닫기">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
    </button>
  </div>

  
  <div id="search-results" class="max-h-96 overflow-y-auto">
    
    <div
      id="search-empty"
      class="flex flex-col items-center justify-center py-12 text-center">
      <div
        class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
      </div>
      <h3 class="text-foreground mb-2 text-lg font-semibold">
        검색 시작
      </h3>
      <p class="text-muted-foreground text-sm">
        검색어를 입력하세요
      </p>
    </div>

    
    <div
      id="search-loading"
      class="flex hidden items-center justify-center py-8">
      <div
        class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div>
      <span class="text-muted-foreground"
        >검색 중...</span
      >
    </div>

    
    <div
      id="search-no-results"
      class="flex hidden flex-col items-center justify-center py-12 text-center">
      <div
        class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7" />
</svg>
      </div>
      <h3 class="text-foreground mb-2 text-lg font-semibold">
        검색 결과 없음
      </h3>
      <p class="text-muted-foreground text-sm">
        다른 검색어로 시도해보세요
      </p>
    </div>

    
    <div id="search-results-list" class="hidden">
      
      <div
        id="search-stats"
        class="text-muted-foreground border-border border-b px-4 py-3 text-sm"
        data-template="%d개의 결과 찾음">
      </div>

      
      <div id="search-items" class="divide-border divide-y">
        
      </div>
    </div>
  </div>

  
  <div class="border-border bg-muted/20 border-t px-4 py-3">
    <div
      class="text-muted-foreground flex items-center justify-between text-xs">
      <div class="flex items-center gap-2 md:gap-4">
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >↑↓</kbd
          >
          <span class="hidden sm:inline"
            >이동</span
          >
        </div>
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >↵</kbd
          >
          <span class="hidden sm:inline"
            >선택</span
          >
        </div>
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >ESC</kbd
          >
          <span class="hidden sm:inline"
            >닫기</span
          >
        </div>
      </div>
      <div class="search-hint-desktop flex items-center gap-1">
        <kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
          >⌘K</kbd
        >
        <span>단축키</span>
      </div>
    </div>
  </div>
</div>

  </body>
</html>
