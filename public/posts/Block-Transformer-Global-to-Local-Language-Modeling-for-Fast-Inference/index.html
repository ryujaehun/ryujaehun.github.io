<!doctype html>
<html
  lang="ko-kr"
  data-theme="claude">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8" />
<meta
  name="viewport"
  content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />

<title>Block Transformer: Global-to-Local Language Modeling for Fast Inference | Jaehun&#39;s Blog</title>

<meta name="description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창적 지점 강점:
효율적 성능 개선: Block..." />
<meta name="keywords" content="개발, 컴파일러, 프로그래밍, 에세이" />
<meta name="author" content="" />
<meta name="language" content="ko-kr" />

<meta name="robots" content="index, follow" />
<link rel="canonical" href="http://192.168.1.11:1314/posts/Block-Transformer-Global-to-Local-Language-Modeling-for-Fast-Inference/" />



  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />



<link rel="manifest" href="/site.webmanifest" />
<meta property="og:url" content="http://192.168.1.11:1314/posts/Block-Transformer-Global-to-Local-Language-Modeling-for-Fast-Inference/">
  <meta property="og:site_name" content="Jaehun&#39;s Blog">
  <meta property="og:title" content="Block Transformer: Global-to-Local Language Modeling for Fast Inference">
  <meta property="og:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창적 지점 강점:">
  <meta property="og:locale" content="ko_kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-15T00:00:00+00:00">
    <meta property="og:image" content="http://192.168.1.11:1314/images/og-default.avif">


  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://192.168.1.11:1314/images/og-default.avif">
  <meta name="twitter:title" content="Block Transformer: Global-to-Local Language Modeling for Fast Inference">
  <meta name="twitter:description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창적 지점 강점:">


  <meta itemprop="name" content="Block Transformer: Global-to-Local Language Modeling for Fast Inference">
  <meta itemprop="description" content="논문 링크
Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘 논문의 강점과 독창적 지점 강점:">
  <meta itemprop="datePublished" content="2025-01-15T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-01-15T00:00:00+00:00">
  <meta itemprop="wordCount" content="14467">
  <meta itemprop="image" content="http://192.168.1.11:1314/images/og-default.avif">
  <meta itemprop="keywords" content="Paper-Review,With-Gpt">







<script>
  window.HUGO_SEARCH_CONFIG = {
    searchIndexURL: "\"/index.json\""
  };
</script>


































<script>

  window.HUGO_GALLERY_CONFIG = {
    justified_gallery:  false ,
    lightbox:  false ,
    justified: "{}",
    lightbox_options: "{}"
  };
</script>




  
    <link rel="stylesheet" href="/css/compiled.css" />
  





  
    <link rel="stylesheet" href="/css/chroma.css" />
  












      <script src="/js/main.js"></script>



  <script src="/js/gumshoe.polyfills.min.js"></script>



        <script src="/js/toc.js" defer></script>



      <script src="/js/search.js" defer></script>



      <script src="/js/dock.js" defer></script>











  











    <script>
      
      (function() {
        const theme = localStorage.getItem('theme') || 'system';
        const colorScheme = localStorage.getItem('colorScheme') || 'claude';

        
        document.documentElement.setAttribute('data-theme', colorScheme);

        
        function applyTheme() {
          if (theme === 'dark' || (theme === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.classList.add('dark');
          } else {
            document.documentElement.classList.remove('dark');
          }
        }

        applyTheme();

        
        if (theme === 'system') {
          window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', applyTheme);
        }
      })();
    </script>
  </head>

  <body class="bg-background text-foreground min-h-screen antialiased">
    
    
  <div
    id="reading-progress-container"
    class="reading-progress-container pointer-events-none fixed top-0 right-0 left-0 z-50 transition-opacity duration-300 ease-out"
    data-height="3"
    data-smooth-scroll="true"
    data-hide-on-complete="false">
    
    <div class="reading-progress-bg w-full"></div>

    
    <div
      id="reading-progress-bar"
      class="from-primary to-primary/80 reading-progress-bar 
        transition-all duration-150 ease-out
       absolute top-0 left-0 w-0 bg-gradient-to-r"></div>
  </div>

  <script>
    (function () {
      "use strict";

      const progressContainer = document.getElementById(
        "reading-progress-container",
      );
      const progressBar = document.getElementById("reading-progress-bar");

      if (!progressContainer || !progressBar) return;

      
      const config = {
        smoothScroll: progressContainer.dataset.smoothScroll === "true",
        hideOnComplete: progressContainer.dataset.hideOnComplete === "true",
      };

      let isVisible = true;
      let gumshoeInstance = null;

      
      function calculateProgress() {
        const scrollTop =
          window.pageYOffset || document.documentElement.scrollTop;
        const documentHeight = document.documentElement.scrollHeight;
        const windowHeight = window.innerHeight;
        const scrollableHeight = documentHeight - windowHeight;

        if (scrollableHeight <= 0) return 0;

        return Math.min(Math.max((scrollTop / scrollableHeight) * 100, 0), 100);
      }

      
      function updateProgress() {
        const progress = calculateProgress();
        progressBar.style.width = progress + "%";

        
        if (config.hideOnComplete && progress >= 99.5) {
          if (isVisible) {
            progressContainer.style.opacity = "0";
            isVisible = false;
          }
        } else if (!isVisible) {
          progressContainer.style.opacity = "1";
          isVisible = true;
        }
      }

      
      let ticking = false;
      function requestTick() {
        if (!ticking) {
          requestAnimationFrame(() => {
            updateProgress();
            ticking = false;
          });
          ticking = true;
        }
      }

      
      function init() {
        
        window.addEventListener("scroll", requestTick, { passive: true });
        window.addEventListener("resize", requestTick, { passive: true });
        document.addEventListener("visibilitychange", () => {
          if (!document.hidden) requestTick();
        });

        
        requestTick();

        
        window.addEventListener("beforeunload", () => {
          window.removeEventListener("scroll", requestTick);
          window.removeEventListener("resize", requestTick);
        });
      }

      
      if (document.readyState === "loading") {
        document.addEventListener("DOMContentLoaded", init);
      } else {
        init();
      }
    })();
  </script>

    <header class="sticky top-0 z-50 mx-auto max-w-4xl px-4 py-6">
  <div
    class="border-border bg-card/80 flex items-center rounded-xl border px-6 py-4 shadow-sm backdrop-blur-sm">
    
    <div class="hidden w-full items-center md:flex">
      
      <div class="flex items-center">
        
          
          <a
            href="/"
            class="bg-primary text-primary-foreground hover:bg-primary/90 flex h-10 w-10 items-center justify-center rounded-full text-lg font-bold transition-transform duration-200 hover:scale-105"
            aria-label="Jaehun&#39;s Blog">
            J
          </a>
        
      </div>

      
      <nav class="mx-8 flex flex-1 items-center justify-center">
        <div class="flex items-center space-x-1">
          
          
            
            
            
              
                
              
            
            
            <a
              href="/posts/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.posts">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
              
              <span
                >
                  게시글
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/categories/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.categories">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10" />

</svg>
              
              <span
                >
                  카테고리
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/tags/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.tags">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" />

</svg>
              
              <span
                >
                  태그
                </span
              >
            </a>
          
            
            
            
              
                
              
            
            
            <a
              href="/archives/"
              class="nav-link 
                text-muted-foreground hover:text-primary hover:bg-primary/10
               focus:ring-primary/20 relative flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.archives">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4" />

</svg>
              
              <span
                >
                  아카이브
                </span
              >
            </a>
          
        </div>
      </nav>

      
      <div class="flex items-center space-x-2">
        
        
<div class="relative">
  <button id="language-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="language"
    aria-label="언어 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="언어 변경">
	<path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8l6 6m-7 0l6-6l2-3M2 5h12M7 2h1m14 20l-5-10l-5 10m2-4h6" />
</svg></button>

  
  <div id="language-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="language"
    role="menu"
    aria-labelledby="language-toggle"><a href="/ko/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem" aria-current="true"><span class="font-medium">한국어</span></a><a href="/en/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem"><span class="font-medium">English</span></a></div>
</div>

        
        
<div class="relative">
  <button id="color-scheme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="color-scheme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="테마 변경">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z" />

</svg></button>

  
  <div id="color-scheme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="color-scheme"><button data-color-scheme="claude" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme="default" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme="bumblebee" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme="emerald" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme="nord" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme="sunset" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme="abyss" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme="dracula" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme="amethyst" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme="slate" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme="twitter" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div>
</div>

        
        
<div class="relative">
  <button id="theme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="theme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
      
  <svg class="h-5 w-5 sun-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg>
      
      
  <svg class="h-5 w-5 moon-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg>
      
      
  <svg class="h-5 w-5 system-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg></button>

  
  <div id="theme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="theme"><button data-theme="light" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg><span>라이트</span></button><button data-theme="dark" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg><span>다크</span></button><button data-theme="system" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg><span>시스템</span></button></div>
</div>
      </div>
    </div>

    
    <div class="flex w-full items-center justify-between md:hidden">
      
      
<div class="relative">
  <button
    id="mobile-menu-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:bg-accent hover:text-foreground flex h-10 w-10 items-center justify-center rounded-lg border transition-colors duration-200"
    data-dropdown-type="mobile-menu"
    aria-label="메뉴"
    aria-expanded="false"
    aria-haspopup="true">
    
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="메뉴">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />

</svg>
  </button>

  
  <div
    id="mobile-menu"
    class="mobile-menu dropdown-menu border-border bg-popover/95 absolute top-12 left-0 z-[60] hidden w-80 max-w-[calc(100vw-2rem)] rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out md:hidden"
    data-dropdown-type="mobile-menu"
    role="menu"
    aria-labelledby="mobile-menu-toggle">
    <nav class="flex flex-col">
      
      
        
        
        
          
            
          
        
        
        <a
          href="/posts/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.posts">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
          
          <span
            >
              게시글
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/categories/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.categories">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10" />

</svg>
          
          <span
            >
              카테고리
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/tags/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.tags">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 7h.01M7 3h5c.512 0 1.024.195 1.414.586l7 7a2 2 0 010 2.828l-7 7a2 2 0 01-2.828 0l-7-7A1.994 1.994 0 013 12V7a4 4 0 014-4z" />

</svg>
          
          <span
            >
              태그
            </span
          >
        </a>
      
        
        
        
          
            
          
        
        
        <a
          href="/archives/"
          class="nav-link 
            text-muted-foreground hover:text-primary hover:bg-primary/10
           focus:ring-primary/20 relative flex items-center gap-3 rounded-md px-4 py-3 text-sm font-medium transition-all duration-200 ease-out hover:translate-x-1 focus:ring-2 focus:outline-none"
          role="menuitem"
          >
          
            
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="nav.archives">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 8h14M5 8a2 2 0 110-4h14a2 2 0 110 4M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8m-9 4h4" />

</svg>
          
          <span
            >
              아카이브
            </span
          >
        </a>
      
    </nav>
  </div>
</div>


      
      <div class="flex items-center space-x-2">
        
        
<div class="relative">
  <button id="language-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="language"
    aria-label="언어 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="언어 변경">
	<path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m5 8l6 6m-7 0l6-6l2-3M2 5h12M7 2h1m14 20l-5-10l-5 10m2-4h6" />
</svg></button>

  
  <div id="language-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="language"
    role="menu"
    aria-labelledby="language-toggle"><a href="/ko/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem" aria-current="true"><span class="font-medium">한국어</span></a><a href="/en/" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none" role="menuitem"><span class="font-medium">English</span></a></div>
</div>

        
        
<div class="relative">
  <button id="color-scheme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="color-scheme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
  <svg class="h-5 w-5 relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="테마 변경">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zM21 5a2 2 0 00-2-2h-4a2 2 0 00-2 2v6a2 2 0 002 2h4a2 2 0 002-2V5z" />

</svg></button>

  
  <div id="color-scheme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-44 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="color-scheme"><button data-color-scheme="claude" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Default</span></button><button data-color-scheme="default" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>White</span></button><button data-color-scheme="bumblebee" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Bumblebee</span></button><button data-color-scheme="emerald" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Emerald</span></button><button data-color-scheme="nord" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Nord</span></button><button data-color-scheme="sunset" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Sunset</span></button><button data-color-scheme="abyss" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Abyss</span></button><button data-color-scheme="dracula" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Dracula</span></button><button data-color-scheme="amethyst" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Amethyst</span></button><button data-color-scheme="slate" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Slate</span></button><button data-color-scheme="twitter" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none"><span>Twitter</span></button></div>
</div>

        
        
<div class="relative">
  <button id="theme-toggle"
    class="dropdown-toggle border-border bg-background text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 flex h-10 w-10 items-center justify-center rounded-lg border transition-all duration-300 ease-out hover:scale-105 focus:ring-2 focus:outline-none active:scale-95"
    data-dropdown-type="theme"
    aria-label="테마 변경"
    aria-expanded="false"
    aria-haspopup="true">
      
      
  <svg class="h-5 w-5 sun-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg>
      
      
  <svg class="h-5 w-5 moon-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg>
      
      
  <svg class="h-5 w-5 system-icon hidden relative z-10"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg></button>

  
  <div id="theme-dropdown"
    class="dropdown-menu border-border bg-popover/95 absolute top-12 right-0 z-[60] hidden w-40 rounded-lg border p-1 shadow-lg backdrop-blur-sm transition-all duration-200 ease-out"
    data-dropdown-type="theme"><button data-theme="light" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이트">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />

</svg><span>라이트</span></button><button data-theme="dark" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="다크">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />

</svg><span>다크</span></button><button data-theme="system" class="text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground flex w-full items-center rounded-md px-4 py-2 text-sm transition-all duration-200 ease-out focus:outline-none">
  <svg class="h-5 w-5 mr-3 h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="시스템">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg><span>시스템</span></button></div>
</div>
      </div>
    </div>
  </div>
</header>


    <main class="mx-auto max-w-4xl px-4 py-6">
      
  
  <nav
  class="breadcrumb mb-4 md:mb-6 py-1"
  aria-label="경로">
  <ol class="text-muted-foreground flex items-center space-x-1 md:space-x-2 text-sm overflow-x-auto whitespace-nowrap scrollbar-hide py-1">
    
    <li class="flex-shrink-0">
      <a
        href="/"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02]">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="홈">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m3 12 2-2m0 0 7-7 7 7M5 10v10a1 1 0 001 1h3m10-11l2 2m-2-2v10a1 1 0 01-1 1h-3m-6 0a1 1 0 001-1v-4a1 1 0 011-1h2a1 1 0 011 1v4a1 1 0 001 1m-6 0h6" />

</svg>
        <span class="max-w-[4rem] md:max-w-none truncate">홈</span>
      </a>
    </li>

    
    
      <li class="flex items-center gap-1 md:gap-2 min-w-0">
        <span class="text-muted-foreground/50 flex-shrink-0"
          >
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg>
        </span>

        
          
          <div class="flex items-center gap-1 md:gap-2 min-w-0">
            
              
              <a
                href="/posts"
                class="text-muted-foreground hover:text-primary hover:bg-primary/10 flex items-center gap-0.5 md:gap-1 rounded-lg px-1 md:px-3 py-0.5 md:py-1.5 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-[1.02] flex-shrink-0">
                
                  
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z" />

</svg>
                  <span class="max-w-[3rem] md:max-w-none truncate">게시글</span>
                
              </a>
              <span class="text-muted-foreground/50 flex-shrink-0"
                >
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg></span
              >
            
            
            <span class="text-foreground flex items-center gap-0.5 md:gap-1 font-medium min-w-0">
              
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
</svg>
              <span class="max-w-[8rem] md:max-w-[16rem] lg:max-w-none truncate">Block Transformer: Global-to-Local Language Modeling for Fast Inference</span>
            </span>
          </div>
        
      </li>
    

  </ol>
</nav>



  


  
  <header class="mb-8">
    


<div class="post-meta">
  
  <h1 class="text-foreground mb-6 text-3xl leading-tight font-bold md:text-4xl">
    Block Transformer: Global-to-Local Language Modeling for Fast Inference
  </h1>

  
  


  
  <div class="bg-card border-border flex flex-col gap-4 rounded-xl border p-6">
    
    <div
      class="text-muted-foreground flex flex-wrap items-center gap-4 text-sm">
      
      <div class="flex items-center gap-2">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="작성일"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
        <time datetime="2025-01-15">
          2025년 01월 15일
        </time>
      </div>

      
      


      
      
        <div class="flex items-center gap-2">
          
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="읽는 시간"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
</svg>
          <span
            >29 분</span
          >
        </div>
      


      
      
        <div class="flex items-center gap-2">
          
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="단어 수"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7" />
</svg>
          <span>14467 단어</span>
        </div>
      

    </div>

    
    
      <div class="flex flex-wrap items-center gap-4">
        
        
          <div class="flex items-center gap-2">
            
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="카테고리"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 7v10a2 2 0 002 2h14a2 2 0 002-2V9a2 2 0 00-2-2h-6l-2-2H5a2 2 0 00-2 2z" />
</svg>
            <span class="text-muted-foreground mr-2 text-sm"
              >카테고리:</span
            >
            <div class="flex flex-wrap gap-2">
              
                <a
                  href="/categories/paper-review/"
                  class="bg-primary/10 text-primary hover:bg-primary/20 focus:ring-primary/20 inline-flex items-center rounded-full px-3 py-1 text-sm font-medium transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 hover:shadow-md focus:ring-2 focus:outline-none">
                  paper-review
                </a>
              
                <a
                  href="/categories/with-gpt/"
                  class="bg-primary/10 text-primary hover:bg-primary/20 focus:ring-primary/20 inline-flex items-center rounded-full px-3 py-1 text-sm font-medium transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 hover:shadow-md focus:ring-2 focus:outline-none">
                  with-gpt
                </a>
              
            </div>
          </div>
        


        
        

      </div>
    


    
    

  </div>
</div>

  </header>

  
  <article class="prose prose-neutral dark:prose-invert mb-12 max-w-none">
    <p><a href="https://arxiv.org/abs/2406.02657v2">논문 링크</a></p>
<h1 id="q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘">Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘</h1>
<h3 id="논문의-강점과-독창적-지점">논문의 강점과 독창적 지점</h3>
<p><strong>강점:</strong></p>
<ol>
<li><strong>효율적 성능 개선:</strong> Block Transformer는 기존 Transformer의 가장 큰 병목 현상인 Key-Value(KV) 캐시 관련 I/O 부담을 크게 완화하여, 최대 10~20배 높은 추론 처리량을 달성합니다.</li>
<li><strong>효율적인 글로벌-로컬 구조:</strong> 하위 계층은 글로벌 문맥을 효율적으로 모델링하고, 상위 계층은 블록 단위로 로컬 디테일을 처리하여 메모리 요구 사항을 크게 줄입니다.</li>
<li><strong>손쉬운 업트레이닝:</strong> 기존 Transformer 모델을 Block Transformer로 간단히 변환(uptraining)할 수 있어, 이미 학습된 모델을 효과적으로 활용합니다.</li>
<li><strong>모듈식 설계:</strong> 글로벌(block decoder) 및 로컬(token decoder) 컴포넌트의 역할을 분리하여 병렬 처리와 메모리 최적화 가능성을 제공합니다.</li>
</ol>
<p><strong>독창적 지점:</strong></p>
<ol>
<li><strong>KV 캐시 I/O 최적화:</strong> KV 캐시 메모리 접근을 블록 단위로 제한하여, 메모리 사용량과 I/O를 각각 최대 256배까지 감소시켰습니다.</li>
<li><strong>두 계층 구조 통합:</strong> 글로벌과 로컬 처리를 명확히 분리한 계층적 설계로 추론 효율성을 극대화했습니다.</li>
<li><strong>플렉시블 블록 크기:</strong> 블록 크기와 글로벌-로컬 컴포넌트의 파라미터 비율을 동적으로 조정해 성능과 처리량을 최적화합니다.</li>
</ol>
<hr>
<h3 id="핵심-알고리즘-설명-예시-입력을-통한-과정">핵심 알고리즘 설명: 예시 입력을 통한 과정</h3>
<p>예를 들어, <code>L=12</code>개의 토큰으로 구성된 시퀀스에서 <code>LB=4</code>인 블록을 사용할 경우:</p>
<ol>
<li>
<p><strong>Embedder 단계:</strong></p>
<ul>
<li>입력 토큰 <code>[A, B, C, D, E, F, G, H, I, J, K, L]</code>를 블록 단위로 묶어 각 블록을 하나의 임베딩으로 압축합니다.</li>
<li>결과: 블록 임베딩 <code>[Block1: ABCD, Block2: EFGH, Block3: IJKL]</code>.</li>
</ul>
</li>
<li>
<p><strong>Block Decoder 단계:</strong></p>
<ul>
<li>블록 간 관계를 학습합니다. Block1의 임베딩이 Block2의 예측에 사용되고, Block2는 Block3을 지원합니다.</li>
<li>글로벌 문맥: Block1 → Block2 → Block3.</li>
</ul>
</li>
<li>
<p><strong>Token Decoder 단계:</strong></p>
<ul>
<li>각 블록을 개별적으로 풀어내 세부적인 토큰 간 관계를 모델링합니다.</li>
<li>예: Block2 (<code>EFGH</code>)의 세부 토큰은 <code>Block1</code>의 글로벌 정보를 기반으로 복원됩니다.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="논문의-한계">논문의 한계</h3>
<ol>
<li><strong>훈련 비용 증가:</strong> Block Transformer는 기존 Transformer보다 파라미터 수와 FLOP(계산량)이 많아 훈련 시간이 길어질 수 있습니다.</li>
<li><strong>모바일 및 로컬 환경 제한:</strong> 큰 메모리 사용량은 메모리가 제한된 장치에서는 구현하기 어려울 수 있습니다.</li>
<li><strong>정교한 하이퍼파라미터 최적화 필요:</strong> 블록 크기와 컴포넌트 간 파라미터 비율을 설정하는 데 추가적인 튜닝 비용이 요구됩니다.</li>
</ol>
<hr>
<h3 id="요약-비교-vanilla-transformer-vs-block-transformer">요약 비교 (Vanilla Transformer vs Block Transformer)</h3>
<table>
  <thead>
      <tr>
          <th><strong>모델</strong></th>
          <th><strong>KV 캐시 I/O</strong></th>
          <th><strong>추론 처리량</strong></th>
          <th><strong>훈련 비용</strong></th>
          <th><strong>글로벌 문맥</strong></th>
          <th><strong>로컬 문맥</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Vanilla Transformer</td>
          <td>O(n^2)</td>
          <td>느림</td>
          <td>적음</td>
          <td>우수</td>
          <td>제한적</td>
      </tr>
      <tr>
          <td>Block Transformer</td>
          <td>O(n/LB)</td>
          <td>10~20배 빠름</td>
          <td>많음</td>
          <td>우수</td>
          <td>매우 우수</td>
      </tr>
  </tbody>
</table>
<p>Block Transformer는 추론 속도와 효율성을 크게 개선하며, 특히 긴 문맥을 처리할 때 매우 유리합니다.</p>
<h1 id="q--논문의-핵심알고리즘을-예시를-들어-아주-자세하게-설명해주고-정리해-줘">Q : 논문의 핵심알고리즘을 예시를 들어 아주 자세하게 설명해주고 정리해 줘</h1>
<h3 id="핵심-알고리즘-block-transformer의-구조와-작동-원리">핵심 알고리즘: Block Transformer의 구조와 작동 원리</h3>
<p>Block Transformer의 알고리즘은 글로벌 문맥을 포착하는 <strong>Block Decoder</strong>와 로컬 문맥을 모델링하는 <strong>Token Decoder</strong>로 나뉩니다. 이 두 단계는 입력 토큰을 계층적으로 처리하여 효율성을 높입니다.</p>
<hr>
<h3 id="1-알고리즘-전체-과정-요약">1. <strong>알고리즘 전체 과정 요약</strong></h3>
<h4 id="단계-1-입력-토큰을-블록으로-분할">단계 1: <strong>입력 토큰을 블록으로 분할</strong></h4>
<ul>
<li>입력 시퀀스를 고정된 크기 <code>LB</code>의 블록으로 분할합니다.</li>
<li>각 블록은 여러 토큰으로 구성됩니다. 예를 들어:
<ul>
<li>입력: <code>[A, B, C, D, E, F, G, H, I, J, K, L]</code></li>
<li>블록 크기(<code>LB</code>): 4</li>
<li>블록으로 분할: <code>[Block1: [A, B, C, D], Block2: [E, F, G, H], Block3: [I, J, K, L]]</code></li>
</ul>
</li>
</ul>
<h4 id="단계-2-embedder로-블록-임베딩-생성">단계 2: <strong>Embedder로 블록 임베딩 생성</strong></h4>
<ul>
<li>각 블록을 단일 벡터로 압축합니다.
<ul>
<li><code>Embedder</code>: <code>[Block1] → [E1]</code>, <code>[Block2] → [E2]</code>, <code>[Block3] → [E3]</code></li>
<li>결과: <code>[E1, E2, E3]</code> (블록 임베딩)</li>
</ul>
</li>
</ul>
<h4 id="단계-3-block-decoder로-글로벌-문맥-학습">단계 3: <strong>Block Decoder로 글로벌 문맥 학습</strong></h4>
<ul>
<li>블록 단위로 Transformer를 적용해 블록 간 관계를 모델링합니다.</li>
<li><code>E1</code>이 <code>E2</code>를, <code>E2</code>가 <code>E3</code>을 지원하며 글로벌 문맥을 학습합니다.
<ul>
<li>결과: <code>[C1, C2, C3]</code> (컨텍스트 임베딩)</li>
</ul>
</li>
</ul>
<h4 id="단계-4-token-decoder로-로컬-문맥-복원">단계 4: <strong>Token Decoder로 로컬 문맥 복원</strong></h4>
<ul>
<li>각 블록을 세부적으로 모델링하며, 현재 블록의 세부 토큰 관계를 학습합니다.</li>
<li>블록 단위로 독립적으로 처리하므로 병렬화 가능.
<ul>
<li>예: <code>[C2] → [E, F, G, H]</code></li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-예시를-통한-상세-설명">2. <strong>예시를 통한 상세 설명</strong></h3>
<h4 id="입력-시퀀스"><strong>입력 시퀀스</strong>:</h4>
<p><code>L=12</code>개의 토큰 <code>[A, B, C, D, E, F, G, H, I, J, K, L]</code>, 블록 크기 <code>LB=4</code>.</p>
<h4 id="step-1-블록-생성"><strong>Step 1: 블록 생성</strong></h4>
<ul>
<li>토큰을 4개씩 나눕니다:
<ul>
<li><code>[Block1: A, B, C, D], [Block2: E, F, G, H], [Block3: I, J, K, L]</code>.</li>
</ul>
</li>
</ul>
<h4 id="step-2-블록-임베딩"><strong>Step 2: 블록 임베딩</strong></h4>
<ul>
<li>각 블록을 단일 벡터로 압축:
<ul>
<li><code>Block1 → E1</code>, <code>Block2 → E2</code>, <code>Block3 → E3</code>.</li>
<li><code>Embedder</code>는 블록 내의 토큰을 통합하여 고차원 임베딩(<code>d_model</code>)을 생성.</li>
</ul>
</li>
</ul>
<h4 id="step-3-block-decoder"><strong>Step 3: Block Decoder</strong></h4>
<ul>
<li>블록 간 관계를 학습해 컨텍스트를 생성:
<ul>
<li><code>E1</code> → <code>C1</code>, <code>E2</code> → <code>C2</code>, <code>E3</code> → <code>C3</code>.</li>
<li><code>C2</code>는 <code>[E1]</code>을 참고해 <code>Block2</code>의 글로벌 문맥 정보를 담습니다.</li>
</ul>
</li>
</ul>
<h4 id="step-4-token-decoder"><strong>Step 4: Token Decoder</strong></h4>
<ul>
<li><code>C2</code>로부터 <code>[E, F, G, H]</code> 복원:
<ul>
<li>블록 단위로 독립적이며, 로컬 토큰 간 관계를 모델링.</li>
<li><code>E</code>는 <code>F, G, H</code>와의 관계를 통해 <code>Block2</code>를 완전하게 디코딩.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-알고리즘-특징">3. <strong>알고리즘 특징</strong></h3>
<ol>
<li>
<p><strong>병목 현상 제거</strong>:</p>
<ul>
<li>Block Decoder는 입력 시퀀스 길이를 <code>LB</code>로 줄여 글로벌 문맥 계산 비용을 크게 절감합니다.</li>
<li>Token Decoder는 로컬 KV 캐시만 사용해 I/O 부담을 최소화합니다.</li>
</ul>
</li>
<li>
<p><strong>효율적인 계층 구조</strong>:</p>
<ul>
<li>글로벌 문맥은 Block Decoder에서, 세부 관계는 Token Decoder에서 분리 처리.</li>
<li>계층적 설계를 통해 병렬 처리가 가능.</li>
</ul>
</li>
<li>
<p><strong>KV 캐시 최적화</strong>:</p>
<ul>
<li>KV 캐시 크기를 <code>1/LB</code>로 줄이며, I/O 접근은 <code>1/(LB^2)</code>로 감소.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4-요약-표">4. <strong>요약 표</strong></h3>
<table>
  <thead>
      <tr>
          <th>단계</th>
          <th>역할</th>
          <th>입력</th>
          <th>출력</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>블록 분할</strong></td>
          <td>입력 토큰을 고정 크기 블록으로 나눔</td>
          <td><code>[A, B, C, D, E, ...]</code></td>
          <td><code>[Block1, Block2, Block3]</code></td>
      </tr>
      <tr>
          <td><strong>Embedder</strong></td>
          <td>블록을 단일 벡터로 압축</td>
          <td><code>[Block1, Block2, Block3]</code></td>
          <td><code>[E1, E2, E3]</code></td>
      </tr>
      <tr>
          <td><strong>Block Decoder</strong></td>
          <td>글로벌 문맥 학습</td>
          <td><code>[E1, E2, E3]</code></td>
          <td><code>[C1, C2, C3]</code></td>
      </tr>
      <tr>
          <td><strong>Token Decoder</strong></td>
          <td>블록별 로컬 관계 복원</td>
          <td><code>[C1, C2, C3]</code></td>
          <td><code>[A, B, C, ..., L]</code></td>
      </tr>
  </tbody>
</table>
<p>Block Transformer는 글로벌-로컬 문맥의 분리와 계층적 설계를 통해 추론 속도를 크게 향상시키는 효율적인 알고리즘입니다.</p>
<h1 id="q--글로벌-문맥-학습을-하는-방법을-아주-자세하게-알려줘">Q : 글로벌 문맥 학습을 하는 방법을 아주 자세하게 알려줘</h1>
<h3 id="글로벌-문맥-학습-방법-block-transformer의-block-decoder">글로벌 문맥 학습 방법: Block Transformer의 Block Decoder</h3>
<p>Block Transformer의 <strong>Block Decoder</strong>는 입력 시퀀스를 블록 단위로 나누어 <strong>글로벌 문맥</strong>을 학습합니다. 이는 각 블록이 다른 블록들과의 관계를 이해하도록 돕는 역할을 합니다. 아래는 Block Decoder가 글로벌 문맥을 학습하는 전체적인 과정과 그 세부적인 구현 방식을 설명합니다.</p>
<hr>
<h3 id="1-block-decoder의-구조">1. <strong>Block Decoder의 구조</strong></h3>
<p>Block Decoder는 전형적인 Transformer의 <strong>Self-Attention 메커니즘</strong>을 블록 단위로 적용한 모델입니다. 여기서는 다음과 같은 주요 컴포넌트가 사용됩니다:</p>
<ol>
<li><strong>입력</strong>: 블록 임베딩(<code>E1, E2, ..., EN</code>)으로 표현된 시퀀스.</li>
<li><strong>Self-Attention</strong>: 블록 간 관계를 모델링.</li>
<li><strong>Feedforward Network (FFN)</strong>: 블록 임베딩의 표현력을 강화.</li>
</ol>
<hr>
<h3 id="2-글로벌-문맥-학습-과정">2. <strong>글로벌 문맥 학습 과정</strong></h3>
<h4 id="step-1-블록-임베딩-준비"><strong>Step 1: 블록 임베딩 준비</strong></h4>
<ul>
<li>입력 시퀀스의 각 블록(<code>Block1, Block2, ..., BlockN</code>)은 <code>Embedder</code>를 통해 고차원 벡터(<code>E1, E2, ..., EN</code>)로 압축됩니다.</li>
<li>예: 입력 <code>[A, B, C, D]</code> → 블록 임베딩 <code>E1</code>.</li>
</ul>
<h4 id="step-2-self-attention을-통한-블록-간-관계-학습"><strong>Step 2: Self-Attention을 통한 블록 간 관계 학습</strong></h4>
<p>Block Decoder의 Self-Attention은 <strong>블록 간의 종속성</strong>을 모델링하여 각 블록 임베딩을 업데이트합니다.</p>
<ol>
<li>
<p><strong>Query, Key, Value 계산</strong>:</p>
<ul>
<li>각 블록 임베딩 <code>Ei</code>는 Query(<code>Qi</code>), Key(<code>Ki</code>), Value(<code>Vi</code>)로 변환됩니다:
[
Q = E \cdot W_Q, \quad K = E \cdot W_K, \quad V = E \cdot W_V
]
여기서 ( W_Q, W_K, W_V )는 학습 가능한 매트릭스.</li>
</ul>
</li>
<li>
<p><strong>Attention Score 계산</strong>:</p>
<ul>
<li>Query와 Key를 사용해 각 블록 간의 연관성을 계산합니다:
[
\text{Attention}(Q, K) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right)
]
여기서 ( d_k )는 Key의 차원.</li>
</ul>
</li>
<li>
<p><strong>Value의 가중 평균 계산</strong>:</p>
<ul>
<li>각 블록이 다른 블록들과의 상호작용을 반영하여 Value를 결합합니다:
[
\text{Output} = \text{Attention}(Q, K) \cdot V
]</li>
</ul>
</li>
<li>
<p><strong>컨텍스트 임베딩 생성</strong>:</p>
<ul>
<li>이 과정을 통해 각 블록은 다른 블록의 문맥 정보를 포함한 새로운 임베딩(<code>C1, C2, ..., CN</code>)을 생성합니다.</li>
</ul>
</li>
</ol>
<h4 id="step-3-feedforward-network로-표현력-강화"><strong>Step 3: Feedforward Network로 표현력 강화</strong></h4>
<ul>
<li>Self-Attention의 출력을 FFN에 통과시켜 블록 임베딩의 표현력을 강화합니다:
[
\text{FFN}(x) = \text{ReLU}(x \cdot W_1 + b_1) \cdot W_2 + b_2
]
여기서 ( W_1, W_2, b_1, b_2 )는 학습 가능한 매개변수.</li>
</ul>
<h4 id="step-4-residual-connection과-layer-normalization"><strong>Step 4: Residual Connection과 Layer Normalization</strong></h4>
<ul>
<li>각 레이어는 Residual Connection과 Layer Normalization을 통해 안정성을 유지합니다:
[
x_{\text{out}} = \text{LayerNorm}(x_{\text{in}} + \text{Self-Attention}(x_{\text{in}}) + \text{FFN}(x_{\text{in}}))
]</li>
</ul>
<hr>
<h3 id="3-예시를-통한-글로벌-문맥-학습">3. <strong>예시를 통한 글로벌 문맥 학습</strong></h3>
<h4 id="입력">입력:</h4>
<ul>
<li>3개의 블록으로 나뉜 입력: <code>[Block1: A, B, C, D]</code>, <code>[Block2: E, F, G, H]</code>, <code>[Block3: I, J, K, L]</code>.</li>
</ul>
<h4 id="과정">과정:</h4>
<ol>
<li>
<p><strong>블록 임베딩 생성</strong>:</p>
<ul>
<li><code>Block1 → E1</code>, <code>Block2 → E2</code>, <code>Block3 → E3</code>.</li>
</ul>
</li>
<li>
<p><strong>Self-Attention 계산</strong>:</p>
<ul>
<li>Query: ( Q1, Q2, Q3 )</li>
<li>Key: ( K1, K2, K3 )</li>
<li>Value: ( V1, V2, V3 )</li>
<li>Attention Score:
[
\text{Attention}(Q1, K2) = \text{softmax}\left(\frac{Q1 \cdot K2^\top}{\sqrt{d_k}}\right)
]</li>
<li>블록 간의 상호작용:
<ul>
<li><code>C1</code>은 <code>E2</code>, <code>E3</code>와의 관계를 포함.</li>
<li><code>C2</code>는 <code>E1</code>, <code>E3</code>와의 관계를 포함.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>FFN 적용</strong>:</p>
<ul>
<li>각 블록 임베딩의 정보가 강화됨:
<ul>
<li>( C1 \rightarrow C1&rsquo; ), ( C2 \rightarrow C2&rsquo; ), ( C3 \rightarrow C3&rsquo; ).</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>결과</strong>:</p>
<ul>
<li>업데이트된 컨텍스트 임베딩: <code>[C1', C2', C3']</code>.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4-block-decoder의-최적화">4. <strong>Block Decoder의 최적화</strong></h3>
<h4 id="kv-캐시-최적화"><strong>KV 캐시 최적화</strong>:</h4>
<ul>
<li>Self-Attention은 블록 단위로만 작동하여, 캐시 크기와 I/O 비용을 줄입니다:
<ul>
<li>블록 수가 <code>LB</code>로 줄어들어, 비용이 기존 대비 ( 1/LB^2 )로 감소.</li>
</ul>
</li>
</ul>
<h4 id="컴퓨팅-효율"><strong>컴퓨팅 효율</strong>:</h4>
<ul>
<li>전체 시퀀스 대신 블록 간의 관계만 모델링하므로 계산 비용이 크게 줄어듭니다.</li>
</ul>
<h4 id="병렬-처리"><strong>병렬 처리</strong>:</h4>
<ul>
<li>블록별로 독립적이므로 병렬 처리 가능.</li>
</ul>
<hr>
<h3 id="요약-block-decoder의-글로벌-문맥-학습">요약: Block Decoder의 글로벌 문맥 학습</h3>
<table>
  <thead>
      <tr>
          <th>단계</th>
          <th>세부 과정</th>
          <th>역할</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>블록 임베딩 준비</td>
          <td>블록별 입력을 단일 벡터로 압축</td>
          <td>입력을 간단히 하여 효율적 학습 준비.</td>
      </tr>
      <tr>
          <td>Self-Attention</td>
          <td>블록 간의 연관성을 학습</td>
          <td>글로벌 문맥 정보를 학습하여 각 블록 임베딩 업데이트.</td>
      </tr>
      <tr>
          <td>Feedforward</td>
          <td>블록 임베딩의 표현력 강화</td>
          <td>정보를 강화하고 복잡한 관계를 학습.</td>
      </tr>
      <tr>
          <td>KV 캐시 최적화</td>
          <td>블록 단위로 연산 수행</td>
          <td>메모리 I/O와 계산 비용 최소화.</td>
      </tr>
  </tbody>
</table>
<p>Block Decoder는 입력 시퀀스를 효율적으로 축약하고, 중요한 글로벌 문맥 정보를 통합하여 Token Decoder가 세부 관계를 모델링할 수 있도록 돕습니다.</p>
<h1 id="q--이-논문에서-사용하는-방법을-학습하기-위하여-어떤-데이터셋이-필요할까-그리고-어떻게-학습을-진행하면-될지-예시를-들어서-아주-자세하게-설명해줘">Q : 이 논문에서 사용하는 방법을 학습하기 위하여 어떤 데이터셋이 필요할까? 그리고 어떻게 학습을 진행하면 될지 예시를 들어서 아주 자세하게 설명해줘</h1>
<h3 id="학습에-필요한-데이터셋과-학습-프로세스">학습에 필요한 데이터셋과 학습 프로세스</h3>
<p>Block Transformer는 <strong>언어 모델링</strong>을 목적으로 설계되었기 때문에, 대규모의 텍스트 데이터셋이 필요합니다. 이러한 데이터는 모델이 글로벌 문맥과 로컬 문맥을 모두 학습할 수 있도록 다양한 토큰 관계를 포함해야 합니다.</p>
<hr>
<h3 id="1-필요한-데이터셋">1. <strong>필요한 데이터셋</strong></h3>
<p>Block Transformer 학습에 적합한 데이터셋은 다음과 같은 특징을 가져야 합니다:</p>
<h4 id="1-대규모-텍스트-데이터셋">(1) <strong>대규모 텍스트 데이터셋</strong></h4>
<ul>
<li><strong>다양한 문맥</strong>: 문학, 과학 논문, 뉴스, 대화 데이터 등 다양한 도메인의 텍스트를 포함해야 합니다.</li>
<li><strong>길이의 다양성</strong>: 짧은 문장부터 긴 문서까지 포함하여 글로벌-로컬 문맥을 모두 학습할 수 있어야 합니다.</li>
</ul>
<h4 id="2-대표적인-데이터셋">(2) <strong>대표적인 데이터셋</strong></h4>
<ul>
<li><strong>The Pile</strong>: 약 800GB 크기의 영어 텍스트 데이터셋으로, 학술 논문, 코드, 뉴스, 웹 문서 등 다양한 도메인을 포함.</li>
<li><strong>OpenWebText</strong>: 웹 텍스트로 구성된 대규모 데이터셋.</li>
<li><strong>C4 (Colossal Clean Crawled Corpus)</strong>: 웹에서 크롤링한 텍스트를 필터링하여 정제된 데이터셋.</li>
<li><strong>PG19</strong>: 긴 문서로 구성된 문학 데이터셋으로, 글로벌 문맥 학습에 적합.</li>
</ul>
<hr>
<h3 id="2-학습-프로세스-설계">2. <strong>학습 프로세스 설계</strong></h3>
<p>Block Transformer를 학습하기 위해 필요한 단계는 다음과 같습니다:</p>
<hr>
<h4 id="step-1-데이터-전처리"><strong>Step 1: 데이터 전처리</strong></h4>
<ol>
<li>
<p><strong>토큰화</strong>:</p>
<ul>
<li>데이터셋을 Byte Pair Encoding(BPE) 또는 SentencePiece로 토큰화.</li>
<li>동일한 단어를 일관되게 처리하고 서브워드 단위로 학습 가능.</li>
<li>예:
<ul>
<li>입력 문장: &ldquo;The quick brown fox&rdquo;</li>
<li>토큰화 결과: <code>[The, quick, brown, fox]</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>블록 생성</strong>:</p>
<ul>
<li>각 입력 시퀀스를 고정된 크기(<code>LB</code>)의 블록으로 나눕니다.</li>
<li>예:
<ul>
<li>입력 문장: <code>[A, B, C, D, E, F, G, H, I, J, K, L]</code></li>
<li>블록 크기: <code>LB=4</code></li>
<li>결과: <code>[Block1: A, B, C, D], [Block2: E, F, G, H], [Block3: I, J, K, L]</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>패딩 및 정렬</strong>:</p>
<ul>
<li>문장의 길이가 블록 크기에 맞지 않을 경우 패딩을 추가:
<ul>
<li>예: <code>[A, B, C]</code> → <code>[A, B, C, &lt;PAD&gt;]</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="step-2-모델-학습"><strong>Step 2: 모델 학습</strong></h4>
<ol>
<li>
<p><strong>모델 초기화</strong>:</p>
<ul>
<li>Block Transformer의 파라미터 초기화를 수행.</li>
<li>Block Decoder와 Token Decoder를 포함.</li>
</ul>
</li>
<li>
<p><strong>학습 과정</strong>:</p>
<ul>
<li><strong>목표</strong>: 언어 모델링(다음 토큰 예측) 또는 문맥 이해.</li>
<li><strong>손실 함수</strong>: 크로스 엔트로피 손실(<code>Cross Entropy Loss</code>)을 사용하여 출력 토큰 분포와 실제 다음 토큰 분포 간의 차이를 최소화.</li>
<li><strong>학습 데이터 배치</strong>:
<ul>
<li>각 배치마다 블록과 토큰 단위로 모델에 입력.</li>
</ul>
</li>
<li><strong>옵티마이저</strong>:
<ul>
<li>AdamW를 사용하여 학습률 감쇠(Learning Rate Decay)를 적용.</li>
</ul>
</li>
<li><strong>학습률 스케줄링</strong>:
<ul>
<li>Warm-up 단계를 포함하여 학습률을 선형적으로 증가시킨 뒤 감소.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>학습 예시</strong>:</p>
<ul>
<li><strong>입력</strong>: <code>[A, B, C, D]</code></li>
<li><strong>Block Decoder 출력</strong>: <code>E1 → C1</code> (글로벌 문맥 학습)</li>
<li><strong>Token Decoder 출력</strong>: <code>[A, B, C, D]</code>의 다음 토큰 예측.</li>
</ul>
</li>
</ol>
<hr>
<h4 id="step-3-검증"><strong>Step 3: 검증</strong></h4>
<ol>
<li>
<p><strong>검증 데이터 준비</strong>:</p>
<ul>
<li>학습 데이터와 분리된 데이터셋 사용.</li>
<li>예: PG19 데이터셋을 사용해 긴 문맥에서의 모델 성능 평가.</li>
</ul>
</li>
<li>
<p><strong>지표</strong>:</p>
<ul>
<li><strong>Perplexity</strong>: 모델의 언어 예측 능력을 평가.</li>
<li><strong>Zero-shot 성능</strong>: 학습하지 않은 과제에서의 모델 성능 확인.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-학습-예시">3. <strong>학습 예시</strong></h3>
<h4 id="예-the-quick-brown-fox를-학습하는-과정">예: &ldquo;The quick brown fox&quot;를 학습하는 과정</h4>
<ol>
<li>
<p><strong>입력 데이터</strong>:</p>
<ul>
<li>텍스트: <code>&quot;The quick brown fox jumps over the lazy dog&quot;</code></li>
<li>토큰화: <code>[The, quick, brown, fox, jumps, over, the, lazy, dog]</code></li>
<li>블록화: <code>[Block1: The, quick, brown, fox], [Block2: jumps, over, the, lazy], [Block3: dog, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;]</code>.</li>
</ul>
</li>
<li>
<p><strong>Block Decoder 학습</strong>:</p>
<ul>
<li><strong>입력</strong>: <code>[E1, E2, E3]</code> (블록 임베딩)</li>
<li><strong>출력</strong>: <code>[C1, C2, C3]</code> (글로벌 문맥 임베딩)</li>
<li><code>C1</code>은 <code>E2</code>, <code>E3</code>와의 관계를 학습.</li>
</ul>
</li>
<li>
<p><strong>Token Decoder 학습</strong>:</p>
<ul>
<li><strong>입력</strong>: <code>[C1]</code></li>
<li><strong>출력</strong>: <code>[The, quick, brown, fox]</code>의 다음 토큰 예측.</li>
</ul>
</li>
<li>
<p><strong>손실 계산</strong>:</p>
<ul>
<li>각 토큰에 대해 크로스 엔트로피 손실을 계산:
[
\text{Loss} = -\sum_i p_{\text{model}}(y_i | x) \cdot \log(p_{\text{true}}(y_i))
]</li>
</ul>
</li>
<li>
<p><strong>역전파 및 업데이트</strong>:</p>
<ul>
<li>손실을 최소화하도록 모델 파라미터 업데이트.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4-요약-학습-프로세스">4. <strong>요약: 학습 프로세스</strong></h3>
<table>
  <thead>
      <tr>
          <th>단계</th>
          <th>세부 내용</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>데이터 전처리</strong></td>
          <td>토큰화 → 블록화 → 패딩 및 정렬</td>
      </tr>
      <tr>
          <td><strong>Block Decoder</strong></td>
          <td>글로벌 문맥 학습 (블록 간 관계 학습)</td>
      </tr>
      <tr>
          <td><strong>Token Decoder</strong></td>
          <td>로컬 문맥 학습 (블록 내 토큰 간 관계 학습)</td>
      </tr>
      <tr>
          <td><strong>손실 계산</strong></td>
          <td>크로스 엔트로피 손실로 다음 토큰 예측 성능 최적화</td>
      </tr>
      <tr>
          <td><strong>검증 및 테스트</strong></td>
          <td>Perplexity 및 Zero-shot 성능으로 모델 성능 평가</td>
      </tr>
  </tbody>
</table>
<p>Block Transformer의 학습은 전형적인 언어 모델 학습과 유사하지만, 입력을 블록 단위로 나누고, 글로벌-로컬 문맥을 계층적으로 학습한다는 점에서 차별화됩니다.</p>
<h1 id="q--이-논문에서-제시한-결과를-자세하게-보고-다른-방법론에-비하여-특출난-점과-논문에서-제기하는-어떠한-방법이-이러한-결과를-도출하게-되었는지-논문에서-제시하는-이유와-너의-생각을-알려줘">Q : 이 논문에서 제시한 결과를 자세하게 보고 다른 방법론에 비하여 특출난 점과 논문에서 제기하는 어떠한 방법이 이러한 결과를 도출하게 되었는지 논문에서 제시하는 이유와 너의 생각을 알려줘</h1>
<h3 id="논문에서-제시한-결과와-비교">논문에서 제시한 결과와 비교</h3>
<p>Block Transformer는 기존 Transformer에 비해 **추론 처리량(Throughput)**과 <strong>효율성</strong>에서 특출난 결과를 보여줍니다. 아래는 논문에서 제시한 주요 결과를 기반으로 한 Block Transformer의 특출난 점과 이를 가능하게 한 방법론을 정리한 내용입니다.</p>
<hr>
<h3 id="1-결과-분석-block-transformer의-성능">1. <strong>결과 분석: Block Transformer의 성능</strong></h3>
<h4 id="1-추론-처리량">(1) <strong>추론 처리량</strong></h4>
<ul>
<li><strong>최대 10~20배 높은 처리량</strong>:
<ul>
<li>기존 Transformer는 긴 문맥(large context)에서 KV 캐시의 메모리 I/O로 인해 병목이 발생합니다.</li>
<li>Block Transformer는 KV 캐시 크기를 줄이고, 메모리 접근을 줄임으로써 처리량을 크게 개선했습니다.</li>
</ul>
</li>
<li>결과:
<ul>
<li><strong>Prefill 단계</strong>에서 최대 10배 처리량 증가.</li>
<li><strong>Decode 단계</strong>에서 최대 20배 처리량 증가.</li>
</ul>
</li>
</ul>
<h4 id="2-메모리-사용량-최적화">(2) <strong>메모리 사용량 최적화</strong></h4>
<ul>
<li>KV 캐시 크기가 블록 크기(<code>LB</code>)에 따라 선형적으로 감소:
<ul>
<li>( 1/LB )로 메모리 사용량을 줄임.</li>
</ul>
</li>
<li>결과적으로 <strong>긴 문맥 처리</strong>(e.g., 2048~8192 tokens)에 뛰어난 성능을 보임.</li>
</ul>
<h4 id="3-언어-모델링-성능-perplexity">(3) <strong>언어 모델링 성능 (Perplexity)</strong></h4>
<ul>
<li>Zero-shot 및 Perplexity 성능에서 기존 Transformer와 <strong>동등한 수준</strong>:
<ul>
<li>예: LAMBADA, HellaSwag, PG19 등 여러 벤치마크에서 기존 성능을 유지.</li>
</ul>
</li>
<li>이는 모델이 글로벌 문맥과 로컬 문맥을 동시에 학습할 수 있는 구조적 장점을 보여줍니다.</li>
</ul>
<hr>
<h3 id="2-block-transformer의-특출난-점">2. <strong>Block Transformer의 특출난 점</strong></h3>
<table>
  <thead>
      <tr>
          <th>특징</th>
          <th>Block Transformer의 이점</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>추론 처리량</strong></td>
          <td>기존 Transformer 대비 최대 20배 증가. 긴 문맥 처리에 적합.</td>
      </tr>
      <tr>
          <td><strong>KV 캐시 최적화</strong></td>
          <td>KV 캐시 크기를 줄여 메모리 I/O 병목 제거. 블록 크기 (LB)에 따라 선형적으로 감소.</td>
      </tr>
      <tr>
          <td><strong>병렬 처리 가능</strong></td>
          <td>글로벌 문맥(Block Decoder)과 로컬 문맥(Token Decoder)을 분리하여 병렬 처리가 가능.</td>
      </tr>
      <tr>
          <td><strong>적응형 블록 크기</strong></td>
          <td>블록 크기 및 모델 파라미터 할당 비율을 동적으로 조정해 성능과 효율성 사이의 균형 조절 가능.</td>
      </tr>
      <tr>
          <td><strong>학습 효율성</strong></td>
          <td>기존 Transformer를 업트레이닝(uptraining)해 비용 절감 가능.</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="3-논문에서-제시한-방법론의-기여와-논문에서-언급한-이유">3. <strong>논문에서 제시한 방법론의 기여와 논문에서 언급한 이유</strong></h3>
<h4 id="1-글로벌-로컬-구조-채택">(1) <strong>글로벌-로컬 구조 채택</strong></h4>
<ul>
<li>Block Transformer는 글로벌 문맥을 학습하는 <strong>Block Decoder</strong>와 로컬 문맥을 학습하는 <strong>Token Decoder</strong>로 나누어 작업을 분리했습니다.</li>
<li>논문의 설명:
<ul>
<li>Block Decoder는 긴 문맥을 처리하는 데 필요한 글로벌 정보를 모델링.</li>
<li>Token Decoder는 블록 내 토큰 간 세부 관계를 학습.</li>
<li>이를 통해 KV 캐시 크기를 크게 줄이고, 추론 속도를 비약적으로 향상.</li>
</ul>
</li>
</ul>
<h4 id="2-블록-단위의-계산">(2) <strong>블록 단위의 계산</strong></h4>
<ul>
<li>긴 문맥을 처리할 때, 기존 Transformer는 모든 이전 토큰의 KV 캐시를 참조해야 했습니다.</li>
<li>Block Transformer는 각 블록에서만 로컬 KV 캐시를 참조:
<ul>
<li>( L )이 전체 문맥 길이라면, 기존 방식의 I/O 비용은 ( O(L^2) ).</li>
<li>Block Transformer는 ( O(L/LB) )로 감소.</li>
</ul>
</li>
<li>논문에서 제시한 이유:
<ul>
<li>이는 KV 캐시의 I/O 병목을 제거하며, 실제 하드웨어에서의 처리량을 대폭 개선.</li>
</ul>
</li>
</ul>
<h4 id="3-업트레이닝-가능성">(3) <strong>업트레이닝 가능성</strong></h4>
<ul>
<li>기존의 학습된 Transformer를 Block Transformer로 변환 가능:
<ul>
<li>블록 크기 (LB = 1)로 시작한 뒤, 점진적으로 증가시키는 방식.</li>
</ul>
</li>
<li>논문에서 제시한 이유:
<ul>
<li>비용을 줄이면서 기존 모델의 학습된 지식을 유지할 수 있어 실용적.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-이-결과를-가능하게-한-구체적인-방법과-내-생각">4. <strong>이 결과를 가능하게 한 구체적인 방법과 내 생각</strong></h3>
<h4 id="1-방법-블록화된-문맥-학습">(1) <strong>방법: 블록화된 문맥 학습</strong></h4>
<ul>
<li>기존 Transformer의 병목은 모든 이전 토큰을 참조하는 전역적 Self-Attention에 있습니다.</li>
<li>Block Transformer는 이를 블록 단위로 축소하여 전역적 문맥 학습 비용을 줄였습니다.</li>
</ul>
<h5 id="내-생각-블록화의-장점"><strong>내 생각: 블록화의 장점</strong></h5>
<ol>
<li>메모리 사용량 감소와 I/O 병목 해결:
<ul>
<li>이는 GPU 메모리 제약이 큰 환경에서 매우 효과적입니다.</li>
</ul>
</li>
<li>추론 효율성 향상:
<ul>
<li>글로벌-로컬 분리로 병렬 처리가 가능하며, 긴 문맥 처리에서도 처리량이 유지됩니다.</li>
</ul>
</li>
</ol>
<h4 id="2-방법-블록-단위의-kv-캐시-최적화">(2) <strong>방법: 블록 단위의 KV 캐시 최적화</strong></h4>
<ul>
<li>기존 Transformer는 ( O(L^2) )로 스케일링하는 KV 캐시 I/O 비용이 가장 큰 문제였습니다.</li>
<li>Block Transformer는 블록 크기 (LB)를 도입하여 이 비용을 ( O(L/LB^2) )로 감소시켰습니다.</li>
</ul>
<h5 id="내-생각-kv-캐시-최적화의-효과"><strong>내 생각: KV 캐시 최적화의 효과</strong></h5>
<ol>
<li>긴 문맥에서도 메모리 병목 없이 효율적.</li>
<li>실제 하드웨어에서 병렬 처리를 통해 GPU 활용률을 높여 처리량 증가.</li>
</ol>
<h4 id="3-방법-학습과-추론의-분리">(3) <strong>방법: 학습과 추론의 분리</strong></h4>
<ul>
<li>학습 단계에서는 블록 크기를 자유롭게 조정하여 성능을 극대화.</li>
<li>추론 단계에서는 로컬 문맥을 효율적으로 처리.</li>
</ul>
<h5 id="내-생각-학습-추론-최적화"><strong>내 생각: 학습-추론 최적화</strong></h5>
<ol>
<li>학습 시 다양한 크기의 블록 실험을 통해 최적의 성능을 찾을 수 있음.</li>
<li>추론 단계에서 블록 크기와 토큰 디코더를 동적으로 조정해 더 높은 처리량 확보 가능.</li>
</ol>
<hr>
<h3 id="5-block-transformer와-기존-방법론의-비교">5. <strong>Block Transformer와 기존 방법론의 비교</strong></h3>
<table>
  <thead>
      <tr>
          <th><strong>방법론</strong></th>
          <th><strong>Perplexity</strong></th>
          <th><strong>추론 처리량 (Throughput)</strong></th>
          <th><strong>KV 캐시 I/O</strong></th>
          <th><strong>학습 효율성</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Vanilla Transformer</strong></td>
          <td>우수</td>
          <td>낮음</td>
          <td>( O(L^2) )</td>
          <td>일반적</td>
      </tr>
      <tr>
          <td><strong>Block Transformer</strong></td>
          <td>동등 또는 약간 우수</td>
          <td>10~20배 증가</td>
          <td>( O(L/LB^2) )</td>
          <td>업트레이닝 가능</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="결론">결론</h3>
<p>Block Transformer는 글로벌-로컬 구조와 블록 단위 처리 방식을 도입하여, 기존 Transformer의 병목을 해결했습니다. 특히, <strong>KV 캐시 I/O 최적화</strong>와 <strong>글로벌-로컬 분리 설계</strong>는 높은 추론 처리량과 효율성을 가능하게 했습니다. 이러한 설계는 긴 문맥 처리와 병렬화를 요구하는 현대의 언어 모델링에서 매우 유리하다고 판단됩니다.</p>
<h1 id="q--이-논문의-입력데이터와-추론-과정에-대해서-예시를-들어-아주-자세하게-설명해-주겠니-추가적으로-모델아키텍처의-구성-및-모델을-구성하는-연산과-메모리-요구량-컴퓨팅-요구량-대해서도-설명해줘">Q : 이 논문의 입력데이터와 추론 과정에 대해서 예시를 들어 아주 자세하게 설명해 주겠니? 추가적으로 모델아키텍처의 구성 및 모델을 구성하는 연산과 메모리 요구량 컴퓨팅 요구량 대해서도 설명해줘</h1>
<h3 id="block-transformer의-입력-데이터와-추론-과정">Block Transformer의 입력 데이터와 추론 과정</h3>
<hr>
<h4 id="1-입력-데이터-처리-과정"><strong>1. 입력 데이터 처리 과정</strong></h4>
<p>Block Transformer의 입력 데이터는 블록 단위로 처리됩니다. 이는 모델의 글로벌 및 로컬 문맥 학습을 최적화하고 메모리 및 컴퓨팅 효율성을 높이기 위함입니다.</p>
<p><strong>예시:</strong></p>
<ul>
<li><strong>문장</strong>: <code>&quot;The quick brown fox jumps over the lazy dog&quot;</code></li>
<li><strong>토큰화</strong>: <code>[The, quick, brown, fox, jumps, over, the, lazy, dog]</code></li>
<li><strong>블록화</strong>:
<ul>
<li>블록 크기((LB)) = 4</li>
<li>블록 분할:
<ul>
<li><code>Block1: [The, quick, brown, fox]</code></li>
<li><code>Block2: [jumps, over, the, lazy]</code></li>
<li><code>Block3: [dog, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;]</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>전처리 요약:</strong></p>
<ul>
<li>데이터는 고정된 크기의 블록으로 나뉘며, 각 블록은 필요한 경우 패딩으로 채워집니다.</li>
<li>블록은 모델의 Embedder를 통해 고차원 벡터로 변환됩니다.</li>
</ul>
<hr>
<h4 id="2-추론-과정"><strong>2. 추론 과정</strong></h4>
<p>Block Transformer는 두 가지 주요 컴포넌트, 즉 <strong>Block Decoder</strong>와 <strong>Token Decoder</strong>를 사용하여 추론을 진행합니다.</p>
<hr>
<p><strong>단계 1: 블록 임베딩 생성 (Embedder)</strong></p>
<ul>
<li>각 블록의 토큰을 단일 벡터로 압축하여 입력 블록 임베딩을 생성합니다.
<ul>
<li>예:
[
Block1 → E1, Block2 → E2, Block3 → E3
]</li>
</ul>
</li>
</ul>
<hr>
<p><strong>단계 2: 글로벌 문맥 학습 (Block Decoder)</strong></p>
<ul>
<li>블록 임베딩 간의 관계를 모델링하여 글로벌 문맥을 학습합니다.
<ul>
<li>Self-Attention 메커니즘을 사용하여 각 블록이 다른 블록과의 상호작용을 학습:
[
Attention(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
]</li>
<li>출력: 블록 임베딩의 글로벌 문맥 버전 ([C1, C2, C3]).</li>
</ul>
</li>
</ul>
<hr>
<p><strong>단계 3: 로컬 문맥 학습 (Token Decoder)</strong></p>
<ul>
<li>각 블록을 독립적으로 디코딩하여 세부적인 토큰 간 관계를 학습합니다.
<ul>
<li>입력: <code>C1</code> (글로벌 문맥 정보).</li>
<li>출력: <code>Block1</code>의 토큰 복원 ([The, quick, brown, fox]).</li>
</ul>
</li>
<li>블록별로 독립적이므로 병렬 처리가 가능합니다.</li>
</ul>
<hr>
<p><strong>단계 4: 출력 생성</strong></p>
<ul>
<li>모든 블록을 디코딩한 후, 결과를 결합하여 최종 출력 문장을 생성합니다.
<ul>
<li>출력: <code>[The, quick, brown, fox, jumps, over, the, lazy, dog]</code>.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-모델-아키텍처의-구성"><strong>3. 모델 아키텍처의 구성</strong></h4>
<p>Block Transformer는 다음과 같은 주요 컴포넌트로 구성됩니다:</p>
<table>
  <thead>
      <tr>
          <th><strong>컴포넌트</strong></th>
          <th><strong>역할</strong></th>
          <th><strong>설명</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Embedder</strong></td>
          <td>입력 블록 임베딩 생성</td>
          <td>각 블록을 단일 벡터로 압축.</td>
      </tr>
      <tr>
          <td><strong>Block Decoder</strong></td>
          <td>글로벌 문맥 학습</td>
          <td>블록 간 관계를 Self-Attention으로 학습.</td>
      </tr>
      <tr>
          <td><strong>Token Decoder</strong></td>
          <td>로컬 문맥 학습</td>
          <td>각 블록 내의 토큰 관계를 학습.</td>
      </tr>
      <tr>
          <td><strong>Residual 연결</strong></td>
          <td>안정성 및 학습 효율성 증가</td>
          <td>각 계층의 입력과 출력을 더해 모델 안정성을 유지.</td>
      </tr>
      <tr>
          <td><strong>Layer Normalization</strong></td>
          <td>학습 안정성 보장</td>
          <td>각 계층의 출력을 정규화하여 학습을 최적화.</td>
      </tr>
  </tbody>
</table>
<hr>
<h4 id="4-연산-요구량-computation-requirements"><strong>4. 연산 요구량 (Computation Requirements)</strong></h4>
<p>연산 요구량은 주로 Self-Attention과 Feedforward 연산에 의해 결정됩니다.</p>
<ul>
<li>
<p><strong>Self-Attention 연산</strong></p>
<ul>
<li>기존 Transformer:
<ul>
<li>전체 토큰 수 (L)에서 Self-Attention 비용: (O(L^2)).</li>
</ul>
</li>
<li>Block Transformer:
<ul>
<li>블록 크기 (LB)에 따라 블록 단위로 처리:
[
O\left(\frac{L}{LB}\right) \text{(블록 개수)} \times O(LB^2) \text{(각 블록 내 연산)} = O\left(L \cdot LB\right)
]</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Feedforward Network</strong></p>
<ul>
<li>각 블록 내 연산:
[
O(LB \cdot d_{model} \cdot d_{ff})
]</li>
</ul>
</li>
</ul>
<hr>
<h4 id="5-메모리-요구량-memory-requirements"><strong>5. 메모리 요구량 (Memory Requirements)</strong></h4>
<p>메모리 사용량은 주로 <strong>KV 캐시</strong>에 의해 결정됩니다.</p>
<ul>
<li>
<p>기존 Transformer:</p>
<ul>
<li>KV 캐시 크기:
[
O(L \cdot d_{model})
]</li>
<li>전체 메모리 I/O 비용:
[
O(L^2 \cdot d_{model})
]</li>
</ul>
</li>
<li>
<p>Block Transformer:</p>
<ul>
<li>블록 단위로 처리:
[
O\left(\frac{L}{LB} \cdot LB \cdot d_{model}\right) = O(L \cdot d_{model})
]</li>
<li>KV 캐시 I/O 비용은 (LB)에 따라 선형적으로 감소:
[
O\left(\frac{L}{LB^2} \cdot d_{model}\right)
]</li>
</ul>
</li>
</ul>
<hr>
<h4 id="6-block-transformer의-효율성-요약"><strong>6. Block Transformer의 효율성 요약</strong></h4>
<table>
  <thead>
      <tr>
          <th><strong>측면</strong></th>
          <th><strong>기존 Transformer</strong></th>
          <th><strong>Block Transformer</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Self-Attention 비용</strong></td>
          <td>(O(L^2))</td>
          <td>(O(L \cdot LB))</td>
      </tr>
      <tr>
          <td><strong>KV 캐시 메모리</strong></td>
          <td>(O(L^2))</td>
          <td>(O(L/LB^2))</td>
      </tr>
      <tr>
          <td><strong>처리량 (Throughput)</strong></td>
          <td>낮음</td>
          <td>최대 20배 증가</td>
      </tr>
      <tr>
          <td><strong>병렬 처리 가능성</strong></td>
          <td>제한적</td>
          <td>블록 단위 병렬 처리 가능</td>
      </tr>
  </tbody>
</table>
<p>Block Transformer는 <strong>글로벌 문맥 학습을 블록 단위로 축소</strong>하고, <strong>로컬 문맥 학습은 병렬로 처리</strong>함으로써 기존 Transformer보다 훨씬 효율적입니다. 이는 긴 문맥을 다루는 작업에서 큰 장점을 제공합니다.</p>
<h1 id="q--이-논문의-한계를-극복하기-위한-방법으로-어떤-연구흐름이-있는지-정리해서-자세하게-설명해-줘">Q : 이 논문의 한계를 극복하기 위한 방법으로 어떤 연구흐름이 있는지 정리해서 자세하게 설명해 줘</h1>
<h3 id="block-transformer의-한계와-이를-극복하기-위한-연구-흐름">Block Transformer의 한계와 이를 극복하기 위한 연구 흐름</h3>
<p>Block Transformer는 효율적인 글로벌-로컬 문맥 학습을 통해 추론 속도와 메모리 효율성을 크게 개선했지만, 몇 가지 한계점이 존재합니다. 이를 해결하기 위해 현재까지의 연구 흐름과 발전 가능성을 정리합니다.</p>
<hr>
<h3 id="1-block-transformer의-주요-한계">1. <strong>Block Transformer의 주요 한계</strong></h3>
<h4 id="1-훈련-비용-증가">(1) <strong>훈련 비용 증가</strong></h4>
<ul>
<li>Block Transformer는 블록 구조와 계층적 설계로 인해 추가적인 학습 비용이 필요합니다.</li>
<li>각 블록에 대해 독립적인 처리 과정(Embedder, Block Decoder, Token Decoder)이 포함되어 모델 복잡성이 증가합니다.</li>
</ul>
<h4 id="2-모바일-및-경량-환경에서의-제약">(2) <strong>모바일 및 경량 환경에서의 제약</strong></h4>
<ul>
<li>큰 메모리 요구량과 높은 FLOP 비용은 메모리가 제한된 환경(예: 모바일 장치)에서 적용하기 어렵습니다.</li>
</ul>
<h4 id="3-정적-블록-크기의-한계">(3) <strong>정적 블록 크기의 한계</strong></h4>
<ul>
<li>블록 크기(<code>LB</code>)는 고정되어 있어, 입력 데이터의 복잡성이나 문맥 길이에 따라 유연하게 조정되지 않습니다.</li>
<li>모든 데이터에 동일한 블록 크기를 적용하면, 효율성이 떨어질 수 있습니다.</li>
</ul>
<h4 id="4-글로벌-정보-손실-가능성">(4) <strong>글로벌 정보 손실 가능성</strong></h4>
<ul>
<li>글로벌 문맥 정보를 단일 컨텍스트 임베딩으로 압축하는 과정에서 일부 중요한 정보를 손실할 가능성이 있습니다.</li>
</ul>
<hr>
<h3 id="2-한계를-극복하기-위한-연구-흐름">2. <strong>한계를 극복하기 위한 연구 흐름</strong></h3>
<h4 id="1-동적-블록-크기-및-적응형-모델링"><strong>(1) 동적 블록 크기 및 적응형 모델링</strong></h4>
<ul>
<li><strong>연구 흐름</strong>:
<ul>
<li>입력 데이터의 복잡성 또는 길이에 따라 블록 크기를 동적으로 조정하는 방법론 개발.</li>
<li>블록 크기를 학습 과정에서 자동으로 최적화하는 메커니즘 도입.</li>
</ul>
</li>
<li><strong>기술적 접근</strong>:
<ul>
<li><strong>Adaptive Block Length</strong>:
<ul>
<li>블록 크기를 데이터의 &ldquo;난이도&quot;에 따라 동적으로 할당.</li>
<li>예: 간단한 문장은 작은 블록 크기, 복잡한 문장은 큰 블록 크기.</li>
</ul>
</li>
<li><strong>Dynamic Token Pooling</strong>:
<ul>
<li>이전 레이어에서 중요하지 않은 토큰을 제거하거나 병합하여 다음 레이어로 전달.</li>
<li>참고 연구: Dynamic Token Pooling (Nawrot et al., 2023).</li>
</ul>
</li>
</ul>
</li>
<li><strong>기대 효과</strong>:
<ul>
<li>데이터에 따라 최적화된 블록 크기를 사용해 효율성 및 성능 개선.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-kv-캐시-압축-및-최적화"><strong>(2) KV 캐시 압축 및 최적화</strong></h4>
<ul>
<li><strong>연구 흐름</strong>:
<ul>
<li>KV 캐시 메모리 사용량을 줄이기 위해 압축 알고리즘 도입.</li>
<li>불필요한 KV 캐시를 삭제하거나, 필요한 정보만 유지.</li>
</ul>
</li>
<li><strong>기술적 접근</strong>:
<ul>
<li><strong>Attention Sink</strong>:
<ul>
<li>중요하지 않은 KV 캐시를 제거하고, 핵심 토큰만 유지.</li>
<li>참고 연구: StreamingLLM (Xiao et al., 2023).</li>
</ul>
</li>
<li><strong>Multi-query Attention (MQA)</strong>:
<ul>
<li>다수의 Query가 하나의 KV 쌍을 공유하도록 설계.</li>
<li>참고 연구: Multi-query Attention (Shazeer, 2019).</li>
</ul>
</li>
<li><strong>Grouped-query Attention (GQA)</strong>:
<ul>
<li>KV 쌍을 그룹화하여 메모리 요구량 감소.</li>
</ul>
</li>
</ul>
</li>
<li><strong>기대 효과</strong>:
<ul>
<li>KV 캐시 저장 공간과 I/O 비용을 줄여 경량화 및 추론 속도 향상.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-글로벌-정보-보존-강화"><strong>(3) 글로벌 정보 보존 강화</strong></h4>
<ul>
<li><strong>연구 흐름</strong>:
<ul>
<li>블록 간 글로벌 정보가 손실되지 않도록 보존하는 메커니즘 도입.</li>
<li>컨텍스트 임베딩이 충분한 정보를 유지하도록 설계 개선.</li>
</ul>
</li>
<li><strong>기술적 접근</strong>:
<ul>
<li><strong>Cross-layer Attention</strong>:
<ul>
<li>블록 디코더와 토큰 디코더 간 교차 주의를 도입해 글로벌 정보와 로컬 정보의 상호작용 강화.</li>
</ul>
</li>
<li><strong>Hierarchical Memory</strong>:
<ul>
<li>블록 디코더에서 글로벌 문맥 정보를 계층적으로 축적.</li>
<li>참고 연구: Big Bird (Zaheer et al., 2020).</li>
</ul>
</li>
</ul>
</li>
<li><strong>기대 효과</strong>:
<ul>
<li>글로벌 정보를 더욱 효과적으로 활용해 성능 손실 방지.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-경량화-모델-설계"><strong>(4) 경량화 모델 설계</strong></h4>
<ul>
<li><strong>연구 흐름</strong>:
<ul>
<li>블록 Transformer를 경량화하여 모바일 환경에서도 사용 가능하도록 최적화.</li>
</ul>
</li>
<li><strong>기술적 접근</strong>:
<ul>
<li><strong>Parameter Pruning</strong>:
<ul>
<li>중요하지 않은 파라미터를 제거해 모델 크기 축소.</li>
</ul>
</li>
<li><strong>Quantization</strong>:
<ul>
<li>16-bit 또는 8-bit 정밀도로 모델을 압축.</li>
<li>참고 연구: Quantization-aware Training.</li>
</ul>
</li>
<li><strong>Knowledge Distillation</strong>:
<ul>
<li>큰 Block Transformer 모델에서 지식을 추출하여 경량 모델로 전이.</li>
</ul>
</li>
</ul>
</li>
<li><strong>기대 효과</strong>:
<ul>
<li>모델의 파라미터 크기와 계산량을 줄여 모바일 및 IoT 환경에서 적용 가능.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="5-멀티스케일-학습"><strong>(5) 멀티스케일 학습</strong></h4>
<ul>
<li><strong>연구 흐름</strong>:
<ul>
<li>여러 크기의 블록을 동시에 처리하여 글로벌 및 로컬 문맥을 멀티스케일로 학습.</li>
</ul>
</li>
<li><strong>기술적 접근</strong>:
<ul>
<li><strong>Multiscale Transformers</strong>:
<ul>
<li>블록 크기가 다른 여러 계층을 구성하여 다양한 스케일에서 정보를 통합.</li>
<li>참고 연구: MEGABYTE (Yu et al., 2024).</li>
</ul>
</li>
<li><strong>Hierarchical Transformer</strong>:
<ul>
<li>입력 시퀀스를 계층적으로 다운샘플링 및 업샘플링.</li>
<li>참고 연구: Hierarchical Transformer (Nawrot et al., 2022).</li>
</ul>
</li>
</ul>
</li>
<li><strong>기대 효과</strong>:
<ul>
<li>다양한 문맥 수준에서의 성능 개선과 더 나은 추론 효율성.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-한계-극복을-위한-기술-요약">3. <strong>한계 극복을 위한 기술 요약</strong></h3>
<table>
  <thead>
      <tr>
          <th>한계</th>
          <th>극복 기술</th>
          <th>기대 효과</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>훈련 비용 증가</strong></td>
          <td>Parameter Pruning, Knowledge Distillation</td>
          <td>모델 크기와 학습 비용 감소</td>
      </tr>
      <tr>
          <td><strong>메모리 및 경량 환경 제약</strong></td>
          <td>KV 캐시 압축 (MQA, GQA), Quantization</td>
          <td>메모리 효율성 증가, 모바일 환경 적용 가능</td>
      </tr>
      <tr>
          <td><strong>정적 블록 크기의 한계</strong></td>
          <td>Adaptive Block Length, Dynamic Token Pooling</td>
          <td>블록 크기 최적화로 데이터별 성능 극대화</td>
      </tr>
      <tr>
          <td><strong>글로벌 정보 손실</strong></td>
          <td>Cross-layer Attention, Hierarchical Memory</td>
          <td>글로벌 문맥 정보 보존 및 성능 개선</td>
      </tr>
      <tr>
          <td><strong>멀티스케일 정보 부족</strong></td>
          <td>Multiscale Transformers, Hierarchical Models</td>
          <td>글로벌 및 로컬 문맥 통합 성능 향상</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="4-결론">4. <strong>결론</strong></h3>
<p>Block Transformer는 효율성을 극대화한 설계로 많은 장점을 제공하지만, 훈련 비용, 메모리 사용량, 정적 블록 크기 등의 한계가 존재합니다. 이를 극복하기 위한 연구는 <strong>동적 블록 크기</strong>, <strong>KV 캐시 최적화</strong>, <strong>글로벌 정보 보존</strong> 등 여러 방향으로 확장되고 있으며, 향후 Block Transformer를 더 널리 활용 가능하게 만들 잠재력을 가지고 있습니다.</p>
<h1 id="q--block-transformer에서-글로벌-문맥-학습을-담당하는-block-decoder와-로컬-문맥-학습을-담당하는-token-decoder의-역할과-상호작용은-어떻게-설계되어-있으며-이를-통해-기존-transformer의-병목-문제를-어떻게-해결했는가">Q : Block Transformer에서 글로벌 문맥 학습을 담당하는 Block Decoder와 로컬 문맥 학습을 담당하는 Token Decoder의 역할과 상호작용은 어떻게 설계되어 있으며, 이를 통해 기존 Transformer의 병목 문제를 어떻게 해결했는가?</h1>
<h3 id="block-decoder와-token-decoder의-역할-및-상호작용">Block Decoder와 Token Decoder의 역할 및 상호작용</h3>
<p>Block Transformer는 글로벌 문맥과 로컬 문맥을 분리하여 학습하며, 각각 <strong>Block Decoder</strong>와 <strong>Token Decoder</strong>가 이를 담당합니다. 이 설계를 통해 기존 Transformer의 병목 문제를 해결합니다.</p>
<hr>
<h4 id="1-block-decoder-글로벌-문맥-학습">1. <strong>Block Decoder: 글로벌 문맥 학습</strong></h4>
<p>Block Decoder는 입력 시퀀스를 블록 단위로 분리한 후, 각 블록 간의 상호작용을 모델링하여 <strong>글로벌 문맥</strong>을 학습합니다.</p>
<ul>
<li>
<p><strong>역할</strong>:</p>
<ul>
<li>블록 간 관계를 Self-Attention으로 모델링하여 전체 시퀀스의 전역 정보를 학습.</li>
<li>각 블록을 단일 임베딩(<code>Context Embedding</code>)으로 압축.</li>
</ul>
</li>
<li>
<p><strong>작동 방식</strong>:</p>
<ol>
<li><strong>입력</strong>: 각 블록의 임베딩(<code>E1, E2, ..., EN</code>).</li>
<li><strong>Self-Attention 계산</strong>:
<ul>
<li>각 블록 임베딩을 Query, Key, Value로 변환.</li>
<li>블록 간 상호작용을 통해 Attention 값을 계산:
[
Attention(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
]</li>
</ul>
</li>
<li><strong>출력</strong>: 각 블록의 글로벌 문맥을 포함한 컨텍스트 임베딩(<code>C1, C2, ..., CN</code>).</li>
</ol>
</li>
<li>
<p><strong>효과</strong>:</p>
<ul>
<li>블록 단위로 글로벌 문맥을 처리하여 메모리 사용량을 크게 줄임.</li>
<li>기존 Transformer의 KV 캐시 I/O 병목을 완화.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="2-token-decoder-로컬-문맥-학습">2. <strong>Token Decoder: 로컬 문맥 학습</strong></h4>
<p>Token Decoder는 각 블록을 독립적으로 처리하며, 해당 블록 내의 세부적인 토큰 간 관계를 학습합니다.</p>
<ul>
<li>
<p><strong>역할</strong>:</p>
<ul>
<li>각 블록의 세부 토큰 간 상호작용을 모델링.</li>
<li>Block Decoder에서 생성된 글로벌 문맥 정보를 활용.</li>
</ul>
</li>
<li>
<p><strong>작동 방식</strong>:</p>
<ol>
<li><strong>입력</strong>: 현재 블록의 컨텍스트 임베딩(<code>Ci</code>)과 해당 블록의 토큰.</li>
<li><strong>Self-Attention 계산</strong>:
<ul>
<li>블록 내에서만 로컬 Attention을 수행:
[
Attention(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
]</li>
<li>블록 간의 KV 캐시는 참조하지 않음.</li>
</ul>
</li>
<li><strong>출력</strong>: 해당 블록의 세부 토큰 정보(<code>T1, T2, ..., TLB</code>).</li>
</ol>
</li>
<li>
<p><strong>효과</strong>:</p>
<ul>
<li>블록 간 의존성을 제거하여 독립적으로 처리.</li>
<li>메모리 I/O 병목 없이 고속 병렬 처리 가능.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-block-decoder와-token-decoder의-상호작용">3. <strong>Block Decoder와 Token Decoder의 상호작용</strong></h4>
<ul>
<li>
<p><strong>흐름</strong>:</p>
<ol>
<li>Block Decoder는 글로벌 문맥 정보를 학습하고, 이를 각 블록에 전달.</li>
<li>Token Decoder는 전달받은 글로벌 문맥 정보를 활용하여 블록 내 토큰 간의 관계를 학습.</li>
<li>각 블록은 독립적으로 처리되므로 병렬 처리가 가능.</li>
</ol>
</li>
<li>
<p><strong>글로벌-로컬 구조</strong>:</p>
<ul>
<li>Block Decoder는 블록 간 상호작용을 처리하여 전역 문맥을 압축.</li>
<li>Token Decoder는 블록 내 세부 정보를 복원하여 로컬 문맥을 학습.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="4-기존-transformer의-병목-문제-해결">4. <strong>기존 Transformer의 병목 문제 해결</strong></h4>
<p>Block Transformer의 설계는 기존 Transformer의 두 가지 주요 병목 문제를 해결합니다:</p>
<ol>
<li>
<p><strong>KV 캐시 메모리 I/O 병목</strong>:</p>
<ul>
<li>기존 Transformer:
<ul>
<li>전체 시퀀스 길이 (L)에 대해 KV 캐시를 참조.</li>
<li>I/O 비용: (O(L^2)).</li>
</ul>
</li>
<li>Block Transformer:
<ul>
<li>KV 캐시 크기를 블록 크기 (LB)로 줄여 I/O 비용 감소:
[
O(L/LB^2)
]</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>긴 시퀀스 처리 비용</strong>:</p>
<ul>
<li>기존 Transformer:
<ul>
<li>모든 토큰 간의 상호작용을 계산하여 비용 증가.</li>
</ul>
</li>
<li>Block Transformer:
<ul>
<li>블록 단위 처리로 계산 비용을 (O(L \cdot LB))로 줄임.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="5-block-transformer의-효율성-요약">5. <strong>Block Transformer의 효율성 요약</strong></h4>
<table>
  <thead>
      <tr>
          <th><strong>구조</strong></th>
          <th><strong>역할</strong></th>
          <th><strong>효과</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Block Decoder</strong></td>
          <td>글로벌 문맥 학습</td>
          <td>블록 단위로 문맥 처리 → KV 캐시 I/O 감소</td>
      </tr>
      <tr>
          <td><strong>Token Decoder</strong></td>
          <td>로컬 문맥 학습</td>
          <td>블록 내 토큰 처리 → 병렬 처리 가능</td>
      </tr>
      <tr>
          <td><strong>상호작용</strong></td>
          <td>글로벌 정보를 로컬 처리에 전달</td>
          <td>글로벌-로컬 통합으로 효율성과 성능 동시 개선</td>
      </tr>
  </tbody>
</table>
<p>Block Decoder와 Token Decoder의 설계는 메모리 효율성을 높이고 추론 속도를 크게 향상시켜, 기존 Transformer가 직면한 병목 문제를 효과적으로 해결합니다.</p>
<h1 id="q--블록-크기lb를-선택하는-기준과-블록-크기에-따른-성능perplexity-및-추론-처리량throughput-간의-관계는-무엇이며-다양한-데이터-길이와-복잡성을-다룰-때-적합한-블록-크기를-어떻게-결정할-수-있는가">Q : 블록 크기(LB)를 선택하는 기준과 블록 크기에 따른 성능(Perplexity) 및 추론 처리량(Throughput) 간의 관계는 무엇이며, 다양한 데이터 길이와 복잡성을 다룰 때 적합한 블록 크기를 어떻게 결정할 수 있는가?</h1>
<h3 id="블록-크기lb-선택-기준과-성능처리량-간의-관계">블록 크기(LB) 선택 기준과 성능/처리량 간의 관계</h3>
<p>블록 크기((LB))는 Block Transformer의 성능과 처리량에 큰 영향을 미치는 중요한 하이퍼파라미터입니다. 블록 크기는 글로벌 문맥 학습의 <strong>효율성</strong>과 로컬 문맥의 <strong>정확성</strong> 사이에서 균형을 맞추는 데 핵심적인 역할을 합니다.</p>
<hr>
<h3 id="1-블록-크기lb의-선택-기준">1. <strong>블록 크기(LB)의 선택 기준</strong></h3>
<p>블록 크기를 선택할 때 고려해야 할 주요 기준은 다음과 같습니다:</p>
<h4 id="1-글로벌-문맥-처리-요구">(1) <strong>글로벌 문맥 처리 요구</strong></h4>
<ul>
<li>긴 문맥(Global Context)이 중요한 경우:
<ul>
<li>작은 블록 크기 ((LB))가 적합.</li>
<li>이유: 블록이 작을수록 글로벌 문맥 정보가 더 세밀하게 반영됩니다.</li>
</ul>
</li>
<li>짧은 문맥(Local Context) 위주인 경우:
<ul>
<li>큰 블록 크기 ((LB))가 적합.</li>
<li>이유: 글로벌 정보를 상대적으로 적게 참조하므로 블록 간 처리 비용이 줄어듭니다.</li>
</ul>
</li>
</ul>
<h4 id="2-추론-효율성">(2) <strong>추론 효율성</strong></h4>
<ul>
<li>큰 블록 크기는 KV 캐시 크기와 I/O를 줄여 **추론 처리량(Throughput)**을 높입니다.</li>
<li>작은 블록 크기는 계산 비용이 증가하지만, <strong>로컬 정보 처리</strong>에서의 성능(Perplexity)을 개선합니다.</li>
</ul>
<h4 id="3-데이터-복잡성">(3) <strong>데이터 복잡성</strong></h4>
<ul>
<li>복잡한 데이터나 긴 시퀀스(예: 문학, 논문)에서는 작은 블록 크기가 필요합니다.</li>
<li>간단한 데이터나 짧은 시퀀스(예: 뉴스 헤드라인)에서는 큰 블록 크기를 활용해 효율성을 극대화할 수 있습니다.</li>
</ul>
<h4 id="4-하드웨어-제한">(4) <strong>하드웨어 제한</strong></h4>
<ul>
<li>GPU 메모리 제한이 있는 경우:
<ul>
<li>큰 블록 크기로 메모리 사용량을 줄임.</li>
</ul>
</li>
<li>계산량에 여유가 있는 경우:
<ul>
<li>작은 블록 크기를 사용해 모델 성능 최적화.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-블록-크기와-성능perplexity-및-처리량throughput의-관계">2. <strong>블록 크기와 성능(Perplexity) 및 처리량(Throughput)의 관계</strong></h3>
<p>블록 크기는 성능과 처리량에 트레이드오프(trade-off)를 만듭니다.</p>
<h4 id="1-성능perplexity">(1) 성능(Perplexity)</h4>
<ul>
<li><strong>작은 블록 크기</strong>:
<ul>
<li>글로벌 문맥이 더 세밀하게 반영되므로 성능(Perplexity)이 향상됩니다.</li>
<li>단점: 블록이 작아질수록 계산량 증가.</li>
</ul>
</li>
<li><strong>큰 블록 크기</strong>:
<ul>
<li>글로벌 정보 손실로 인해 성능이 약간 저하될 수 있음.</li>
</ul>
</li>
</ul>
<h4 id="2-처리량throughput">(2) 처리량(Throughput)</h4>
<ul>
<li><strong>작은 블록 크기</strong>:
<ul>
<li>KV 캐시 참조가 많아지고, I/O 비용 증가로 처리량이 낮아짐.</li>
</ul>
</li>
<li><strong>큰 블록 크기</strong>:
<ul>
<li>KV 캐시 크기 감소 및 블록 단위 병렬 처리로 처리량이 크게 증가.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="3-성능-및-처리량의-상관관계">(3) 성능 및 처리량의 상관관계</h4>
<table>
  <thead>
      <tr>
          <th><strong>블록 크기 (LB)</strong></th>
          <th><strong>글로벌 문맥 반영</strong></th>
          <th><strong>로컬 문맥 처리</strong></th>
          <th><strong>Perplexity</strong></th>
          <th><strong>Throughput</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>작은 값 (예: 1~4)</td>
          <td>매우 세밀</td>
          <td>뛰어남</td>
          <td>높음</td>
          <td>낮음</td>
      </tr>
      <tr>
          <td>중간 값 (예: 8~16)</td>
          <td>균형</td>
          <td>균형</td>
          <td>중간</td>
          <td>중간</td>
      </tr>
      <tr>
          <td>큰 값 (예: 32 이상)</td>
          <td>약함</td>
          <td>적당</td>
          <td>낮음</td>
          <td>높음</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="3-다양한-데이터-길이-및-복잡성에-따른-블록-크기-결정">3. <strong>다양한 데이터 길이 및 복잡성에 따른 블록 크기 결정</strong></h3>
<h4 id="1-데이터-길이에-따른-블록-크기">(1) 데이터 길이에 따른 블록 크기</h4>
<ul>
<li><strong>긴 데이터</strong>:
<ul>
<li>문학, 과학 논문, 코드 등 긴 문맥을 요구하는 데이터.</li>
<li>작은 블록 크기((LB = 4) 또는 (LB = 8))가 적합.</li>
</ul>
</li>
<li><strong>짧은 데이터</strong>:
<ul>
<li>뉴스 헤드라인, 대화 데이터 등 짧은 문맥을 요구하는 데이터.</li>
<li>큰 블록 크기((LB = 16) 이상)가 적합.</li>
</ul>
</li>
</ul>
<h4 id="2-데이터-복잡성에-따른-블록-크기">(2) 데이터 복잡성에 따른 블록 크기</h4>
<ul>
<li><strong>복잡한 데이터</strong>:
<ul>
<li>다양한 토픽과 긴 의존성을 가진 데이터.</li>
<li>작은 블록 크기로 글로벌 문맥 세밀하게 학습.</li>
</ul>
</li>
<li><strong>간단한 데이터</strong>:
<ul>
<li>문맥 의존성이 낮고 반복적인 패턴이 많은 데이터.</li>
<li>큰 블록 크기로 처리량을 극대화.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-결론-및-추천-전략">4. <strong>결론 및 추천 전략</strong></h3>
<h4 id="블록-크기-선택을-위한-추천-전략"><strong>블록 크기 선택을 위한 추천 전략</strong></h4>
<ol>
<li><strong>초기 설정</strong>:
<ul>
<li>(LB = 4) 또는 (LB = 8)로 시작하여 성능과 처리량을 관찰.</li>
</ul>
</li>
<li><strong>데이터 특성 분석</strong>:
<ul>
<li>데이터 길이와 복잡성을 분석하여 블록 크기를 조정.</li>
</ul>
</li>
<li><strong>최적화 실험</strong>:
<ul>
<li>블록 크기를 다양한 값으로 실험하여 Perplexity와 Throughput 간의 최적 균형점을 찾음.</li>
</ul>
</li>
</ol>
<h4 id="요약"><strong>요약</strong></h4>
<ul>
<li>블록 크기는 글로벌 문맥 학습과 처리량 사이의 트레이드오프를 조절하는 핵심 하이퍼파라미터입니다.</li>
<li>데이터 특성, 하드웨어 환경, 그리고 목적(성능 vs 처리량)에 따라 동적으로 블록 크기를 선택하는 것이 중요합니다.</li>
<li>실험적 접근과 데이터 분석을 통해 최적의 블록 크기를 설정하는 것이 바람직합니다.</li>
</ul>
<h1 id="q--block-transformer에서-kv-캐시의-크기와-io-비용을-줄이기-위한-메커니즘이-실제-하드웨어에서의-성능-최적화와-어떻게-연결되며-이를-더욱-개선하기-위해-어떤-추가적인-접근법을-고려할-수-있는가">Q : Block Transformer에서 KV 캐시의 크기와 I/O 비용을 줄이기 위한 메커니즘이 실제 하드웨어에서의 성능 최적화와 어떻게 연결되며, 이를 더욱 개선하기 위해 어떤 추가적인 접근법을 고려할 수 있는가?</h1>
<h3 id="block-transformer에서-kv-캐시-최적화와-하드웨어-성능의-관계">Block Transformer에서 KV 캐시 최적화와 하드웨어 성능의 관계</h3>
<p>Block Transformer는 <strong>KV 캐시의 크기와 I/O 비용을 줄이는 메커니즘</strong>을 통해 하드웨어 성능 최적화를 달성합니다. 이는 추론 과정에서의 <strong>메모리 병목</strong>을 완화하고, 처리량(Throughput)을 증가시키는 데 핵심적인 역할을 합니다.</p>
<hr>
<h3 id="1-kv-캐시의-크기와-io-비용-최적화-메커니즘">1. <strong>KV 캐시의 크기와 I/O 비용 최적화 메커니즘</strong></h3>
<h4 id="1-블록-단위-kv-캐시-사용">(1) <strong>블록 단위 KV 캐시 사용</strong></h4>
<ul>
<li>기존 Transformer는 모든 이전 토큰((L))의 KV 캐시를 참조하며, 비용이 (O(L^2))로 증가합니다.</li>
<li>Block Transformer는 입력 시퀀스를 고정된 크기의 블록((LB))으로 나누고, <strong>각 블록 단위로 로컬 KV 캐시를 참조</strong>합니다.
<ul>
<li>비용 감소:
[
O\left(\frac{L}{LB} \times LB^2\right) = O(L \cdot LB)
]</li>
<li>(LB)가 작을수록 I/O 비용이 줄어듭니다.</li>
</ul>
</li>
</ul>
<h4 id="2-kv-캐시의-글로벌-로컬-분리">(2) <strong>KV 캐시의 글로벌-로컬 분리</strong></h4>
<ul>
<li><strong>Block Decoder</strong>:
<ul>
<li>블록 단위의 KV 캐시만 유지하며, 전체 시퀀스 대신 축약된 글로벌 문맥 정보를 저장.</li>
</ul>
</li>
<li><strong>Token Decoder</strong>:
<ul>
<li>현재 블록에 대한 로컬 KV 캐시만 유지하며, 이전 블록 정보를 참조하지 않음.</li>
<li>이를 통해 필요하지 않은 KV 캐시 참조를 완전히 제거.</li>
</ul>
</li>
</ul>
<h4 id="3-캐시-접근-최적화">(3) <strong>캐시 접근 최적화</strong></h4>
<ul>
<li>KV 캐시는 메모리와 I/O 간 병목이 발생하는 주된 원인입니다.</li>
<li>Block Transformer는 블록 간 독립성을 보장하여, 불필요한 KV 캐시 접근을 줄임.
<ul>
<li>기존 Transformer:
<ul>
<li>모든 레이어에서 전체 KV 캐시에 접근.</li>
</ul>
</li>
<li>Block Transformer:
<ul>
<li>블록 내에서만 로컬 KV 캐시에 접근.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-실제-하드웨어에서의-성능-최적화">2. <strong>실제 하드웨어에서의 성능 최적화</strong></h3>
<h4 id="1-메모리-io-병목-완화">(1) <strong>메모리 I/O 병목 완화</strong></h4>
<ul>
<li>KV 캐시 크기를 (O(L/LB))로 줄임으로써, <strong>GPU 메모리 대역폭 병목</strong>을 완화합니다.
<ul>
<li>예: 블록 크기 (LB = 4)인 경우, I/O 병목을 최대 256배 감소.</li>
</ul>
</li>
<li>이는 GPU 메모리 용량을 최적으로 사용하여 더 많은 요청을 병렬로 처리할 수 있게 합니다.</li>
</ul>
<h4 id="2-처리량-증가">(2) <strong>처리량 증가</strong></h4>
<ul>
<li>줄어든 KV 캐시 I/O로 인해, GPU의 계산 성능을 최대한 활용 가능.
<ul>
<li>기존 Transformer는 I/O 병목으로 인해 GPU의 계산 자원을 제대로 활용하지 못함.</li>
<li>Block Transformer는 KV 캐시 I/O를 줄여, 계산 자원의 활용률(Compute Utilization)을 극대화.</li>
</ul>
</li>
</ul>
<h4 id="3-확장성">(3) <strong>확장성</strong></h4>
<ul>
<li>긴 시퀀스(예: 8192 tokens)에서도 처리량을 유지.
<ul>
<li>기존 Transformer는 시퀀스 길이가 증가하면 처리 속도가 급격히 감소.</li>
<li>Block Transformer는 블록 단위 처리로 동일한 처리량 유지.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-kv-캐시-최적화를-위한-추가적인-접근법">3. <strong>KV 캐시 최적화를 위한 추가적인 접근법</strong></h3>
<h4 id="1-kv-캐시-압축">(1) <strong>KV 캐시 압축</strong></h4>
<ul>
<li><strong>Attention Sink</strong>:
<ul>
<li>중요하지 않은 KV 값을 제거하고, 핵심 정보만 보존.</li>
<li>참고 연구: StreamingLLM (Xiao et al., 2023).</li>
</ul>
</li>
<li><strong>Token Pruning</strong>:
<ul>
<li>이전 단계에서 덜 중요한 토큰을 제거하여 KV 캐시 크기를 줄임.</li>
<li>참고 연구: PyramidInfer (Yang et al., 2024).</li>
</ul>
</li>
<li><strong>Grouped-query Attention (GQA)</strong>:
<ul>
<li>여러 Query가 하나의 KV 쌍을 공유하여 메모리 사용량 감소.</li>
</ul>
</li>
</ul>
<h4 id="2-kv-캐시-재사용">(2) <strong>KV 캐시 재사용</strong></h4>
<ul>
<li><strong>Cross-layer Attention Sharing</strong>:
<ul>
<li>각 레이어가 독립적인 KV 캐시를 유지하지 않고, 상위 레이어와 공유.</li>
<li>참고 연구: Multi-query Attention (Shazeer, 2019).</li>
</ul>
</li>
</ul>
<h4 id="3-캐시-업데이트-동적-제어">(3) <strong>캐시 업데이트 동적 제어</strong></h4>
<ul>
<li>중요도가 낮은 블록의 KV 캐시는 업데이트를 건너뛰거나 병합.</li>
<li>적응형 업데이트로 캐시 효율성 극대화.</li>
</ul>
<h4 id="4-블록-길이-동적-조정">(4) <strong>블록 길이 동적 조정</strong></h4>
<ul>
<li>입력 데이터의 복잡성에 따라 블록 크기를 동적으로 조정.
<ul>
<li>간단한 문장은 큰 블록 크기, 복잡한 문장은 작은 블록 크기.</li>
</ul>
</li>
</ul>
<h4 id="5-하드웨어-최적화">(5) <strong>하드웨어 최적화</strong></h4>
<ul>
<li><strong>Custom Memory Layout</strong>:
<ul>
<li>GPU의 메모리 대역폭을 최적화하기 위해 KV 캐시를 압축된 포맷으로 저장.</li>
</ul>
</li>
<li><strong>Kernel Fusion</strong>:
<ul>
<li>캐시 접근 및 계산을 통합하여 I/O 지연 최소화.</li>
<li>참고 연구: FlashAttention (Dao et al., 2022).</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-block-transformer의-개선-방향-요약">4. <strong>Block Transformer의 개선 방향 요약</strong></h3>
<table>
  <thead>
      <tr>
          <th><strong>기술</strong></th>
          <th><strong>효과</strong></th>
          <th><strong>참고 연구</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>KV 캐시 압축</strong></td>
          <td>메모리 사용량 및 I/O 비용 감소</td>
          <td>StreamingLLM, PyramidInfer</td>
      </tr>
      <tr>
          <td><strong>Cross-layer Sharing</strong></td>
          <td>메모리 중복 제거 및 효율성 증가</td>
          <td>Multi-query Attention</td>
      </tr>
      <tr>
          <td><strong>Dynamic Block Size</strong></td>
          <td>데이터 특성에 따른 최적화</td>
          <td>Adaptive Block Length</td>
      </tr>
      <tr>
          <td><strong>Kernel Fusion</strong></td>
          <td>캐시 접근 및 계산 효율화</td>
          <td>FlashAttention</td>
      </tr>
      <tr>
          <td><strong>Attention Sink</strong></td>
          <td>불필요한 KV 캐시 제거</td>
          <td>H2O, Scissorhands</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="5-결론">5. <strong>결론</strong></h3>
<p>Block Transformer는 블록 단위 KV 캐시 접근 및 크기 감소 메커니즘을 통해 <strong>GPU 메모리 병목을 해결</strong>하고, 추론 처리량을 대폭 향상시켰습니다. 이를 더욱 개선하기 위해 KV 캐시 압축, 크로스 레이어 공유, 동적 블록 크기, 하드웨어 최적화와 같은 추가적인 접근법이 유용하며, 특히 긴 문맥과 복잡한 데이터를 처리할 때 효율성을 극대화할 수 있습니다.</p>

  </article>

  
  
  <div class="post-license bg-muted/30 border-border mb-8 rounded-xl border p-6">
    <div class="flex items-start gap-4">
      <div
        class="bg-primary/10 flex h-12 w-12 flex-shrink-0 items-center justify-center rounded-lg">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="라이선스"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m5.618-4.016A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 3.04A12.02 12.02 0 003 9c0 5.591 3.824 10.29 9 11.622 5.176-1.332 9-6.03 9-11.622 0-1.042-.133-2.052-.382-3.016z" />
</svg>
      </div>

      <div class="flex-1">
        <h3 class="text-foreground mb-2 text-lg font-semibold">
          라이선스
        </h3>

        <div class="text-muted-foreground space-y-2 text-sm">
          
          <p>
            <strong>저작자:</strong>
            류재훈
          </p>
          

          <p>
            <strong>링크:</strong>
            <a
              href="http://192.168.1.11:1314/posts/Block-Transformer-Global-to-Local-Language-Modeling-for-Fast-Inference/"
              class="text-primary hover:text-primary/80 transition-colors duration-200">
              http://192.168.1.11:1314/posts/Block-Transformer-Global-to-Local-Language-Modeling-for-Fast-Inference/
            </a>
          </p>

          <p>
            <strong>라이선스:</strong>
            <a
              href="https://creativecommons.org/licenses/by-nc-sa/4.0/"
              target="_blank"
              rel="noopener noreferrer"
              class="text-primary hover:text-primary/80 transition-colors duration-200">
              CC BY-NC-SA 4.0
            </a>
          </p>

          <p class="text-xs">
            이 저작물은 크리에이티브 커먼즈 저작자표시-비영리-동일조건변경허락 4.0 국제 라이선스에 따라 이용할 수 있습니다. 출처를 표시하고, 비영리 목적으로만 사용하며, 동일한 라이선스를 유지해야 합니다.
          </p>
        </div>
      </div>
    </div>
  </div>


  
  





  <nav
    class="post-navigation mb-12"
    aria-label="게시글 네비게이션">
    <div class="grid grid-cols-1 gap-4 md:grid-cols-2">
      
      <div class="nav-item">
        
          <a
            href="/posts/FLAME-Factuality-Aware-Alignment-for-Large-Language-Models/"
            class="group bg-card border-border hover:bg-primary/5 hover:border-primary/20 focus:ring-primary/20 flex min-h-[140px] flex-col justify-between rounded-xl border p-6 transition-all duration-300 ease-out hover:-translate-y-1 hover:scale-[1.02] hover:shadow-lg focus:ring-2 focus:outline-none">
            <div>
              <div class="mb-3 flex items-center gap-3">
                
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7" />
</svg>
                <span class="text-muted-foreground text-sm font-medium">
                  이전
                </span>
              </div>

              <h3
                class="text-foreground group-hover:text-primary mb-3 line-clamp-2 text-lg leading-tight font-semibold transition-colors duration-200">
                FLAME: Factuality-Aware Alignment for Large Language Models
              </h3>
            </div>

            <div
              class="text-muted-foreground mt-auto flex items-center gap-2 text-xs">
              
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
              <time datetime="2025-01-15">
                01月15日
              </time>
            </div>
          </a>
        
      </div>

      
      <div class="nav-item">
        
          <a
            href="/posts/SANA-EFFICIENT-HIGH-RESOLUTION-IMAGE-SYN-THESIS-WITH-LINEAR-DIFFUSION-TRANSFORMERS/"
            class="group bg-card border-border hover:bg-primary/5 hover:border-primary/20 focus:ring-primary/20 flex min-h-[140px] flex-col justify-between rounded-xl border p-6 transition-all duration-300 ease-out hover:-translate-y-1 hover:scale-[1.02] hover:shadow-lg focus:ring-2 focus:outline-none">
            <div>
              <div class="mb-3 flex items-center justify-end gap-3">
                <span class="text-muted-foreground text-sm font-medium">
                  다음
                </span>
                
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
</svg>
              </div>

              <h3
                class="text-foreground group-hover:text-primary mb-3 line-clamp-2 text-right text-lg leading-tight font-semibold transition-colors duration-200">
                 SANA: EFFICIENT HIGH-RESOLUTION IMAGE SYN THESIS WITH LINEAR DIFFUSION TRANSFORMERS
              </h3>
            </div>

            <div
              class="text-muted-foreground mt-auto flex items-center justify-end gap-2 text-xs">
              <time datetime="2025-01-15">
                01月15日
              </time>
              
  <svg class="h-3 w-3"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 7V3m8 4V3m-9 8h10M5 21h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v12a2 2 0 002 2z" />
</svg>
            </div>
          </a>
        
      </div>
    </div>
  </nav>




  
  










  
  


    </main>

    <footer class="mx-auto max-w-4xl px-4 py-8">
  <div class="px-6 py-6">
    
    
      <nav class="mb-6">
        <div class="flex flex-wrap items-center justify-center gap-1">
          
            
            
            
            <a
              href="/index.xml"
              
              class="nav-link hover:text-primary hover:bg-primary/10 text-muted-foreground flex items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 hover:scale-105">
              
                
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="RSS Feed">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 5c7.18 0 13 5.82 13 13M6 11a7 7 0 017 7m-6 0a1 1 0 11-2 0 1 1 0 012 0z" />

</svg>
              
              <span
                >
                  RSS Feed
                </span
              >
            </a>
          
        </div>
      </nav>
    


    
    
      <div class="border-border mb-6 border-t"></div>
    


    
    <div class="flex flex-col items-center justify-between gap-4 md:flex-row">
      
      <div class="text-muted-foreground text-sm">
        <p>
          &copy; 2025 Jaehun&#39;s Blog.
          모든 권리 보유.
        </p>
      </div>

      
      
        <div class="flex items-center gap-3">
          
            <a
              href="https://github.com/ryujaehun"
              
                target="_blank" rel="noopener noreferrer"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="GitHub"
              aria-label="GitHub">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="GitHub"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5c.08-1.25-.27-2.48-1-3.5c.28-1.15.28-2.35 0-3.5c0 0-1 0-3 1.5c-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.4 5.4 0 0 0 4 9c0 3.5 3 5.5 6 5.5c-.39.49-.68 1.05-.85 1.65S8.93 17.38 9 18v4"/><path d="M9 18c-4.51 2-5-2-7-2"/></g></svg>
              
            </a>
          
            <a
              href="https://www.linkedin.com/in/jaehunryu/"
              
                target="_blank" rel="noopener noreferrer"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="Linkedin"
              aria-label="Linkedin">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="Linkedin"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2a2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6M2 9h4v12H2z"/><circle cx="4" cy="4" r="2"/></g></svg>
              
            </a>
          
            <a
              href="mailto:jaehunryu@icloud.com"
              
              class="text-muted-foreground hover:text-primary focus:ring-primary/20 rounded-lg p-1 transition-all duration-300 ease-out hover:scale-110 focus:ring-2 focus:outline-none"
              title="Email"
              aria-label="Email">
              
                
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24" aria-label="Email">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />

</svg>
              
            </a>
          
        </div>
      

    </div>
  </div>
</footer>


    
    



<div
  id="dock"
  class="pointer-events-none fixed bottom-4 left-1/2 z-[9999] w-fit -translate-x-1/2 translate-y-24 opacity-0 transition-all duration-300 ease-out sm:right-0 sm:left-0 sm:mx-auto sm:translate-x-0"
  role="toolbar"
  aria-label="바로가기 도구 모음">
  <nav
    class="border-border bg-card/80 scrollbar-hide xs:px-3 xs:py-2 mx-auto flex max-w-[calc(100vw-2rem)] min-w-fit items-center justify-center overflow-x-auto rounded-2xl border px-4 py-3 shadow-lg backdrop-blur-sm sm:px-4 sm:py-3">
    
    
      <button
        id="dock-back"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
        title="뒤로"
        aria-label="뒤로">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path d="m12 19-7-7 7-7"/>
  <path d="M19 12H5"/>

</svg>
        <span class="hidden sm:inline"
          >뒤로</span
        >
      </button>

      
      <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>
    


    
    
      <button
        id="dock-toc"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
        title="목차"
        aria-label="목차">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <line x1="8" x2="21" y1="6" y2="6"/>
  <line x1="8" x2="21" y1="12" y2="12"/>
  <line x1="8" x2="21" y1="18" y2="18"/>
  <line x1="3" x2="3.01" y1="6" y2="6"/>
  <line x1="3" x2="3.01" y1="12" y2="12"/>
  <line x1="3" x2="3.01" y1="18" y2="18"/>

</svg>
        <span class="hidden sm:inline"
          >목차</span
        >
      </button>

      
      <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>
    


    
    <button
      id="dock-search"
      class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-3 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-4 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-4 sm:py-2"
      title="검색"
      aria-label="검색">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
      <span class="hidden md:inline"
        >검색</span
      >
    </button>

    
    <div class="bg-border xs:mx-1 mx-2 h-6 w-px sm:mx-2"></div>

    
    
    


    
    <button
      id="dock-top"
      class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 xs:px-2 xs:py-1.5 flex flex-shrink-0 items-center gap-2 rounded-lg px-3 py-2 text-sm font-medium transition-all duration-300 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none sm:px-3 sm:py-2"
      title="맨 위로"
      aria-label="맨 위로">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path d="m18 15-6-6-6 6"/>

</svg>
      <span class="hidden sm:inline"
        >맨 위로</span
      >
    </button>
  </nav>
</div>


    
      



<div
  id="toc-overlay"
  class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 backdrop-blur-sm transition-all duration-300"
  role="dialog"
  aria-modal="true"
  aria-labelledby="toc-title"></div>


<div
  id="toc-card"
  class="pointer-events-none fixed inset-4 z-50 flex w-auto scale-95 items-center justify-center opacity-0 transition-all duration-300 sm:inset-auto sm:top-1/2 sm:left-1/2 sm:block sm:w-full sm:max-w-md sm:-translate-x-1/2 sm:-translate-y-1/2"
  role="dialog"
  aria-modal="true"
  aria-labelledby="toc-title">
  <div
    class="bg-card border-border w-full max-w-sm overflow-hidden rounded-xl border shadow-xl sm:max-w-md">
    
    <div
      class="border-border bg-muted/30 flex items-center justify-between border-b p-4">
      <div class="flex items-center gap-3">
        
  <svg class="h-5 w-5"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <line x1="8" x2="21" y1="6" y2="6"/>
  <line x1="8" x2="21" y1="12" y2="12"/>
  <line x1="8" x2="21" y1="18" y2="18"/>
  <line x1="3" x2="3.01" y1="6" y2="6"/>
  <line x1="3" x2="3.01" y1="12" y2="12"/>
  <line x1="3" x2="3.01" y1="18" y2="18"/>

</svg>
        <h2 id="toc-title" class="text-foreground text-lg font-semibold">
          목차
        </h2>
      </div>

      
      <button
        id="toc-close"
        class="text-muted-foreground hover:text-primary hover:bg-primary/10 focus:ring-primary/20 rounded-lg p-2 transition-all duration-200 ease-out hover:-translate-y-0.5 hover:scale-105 focus:ring-2 focus:outline-none"
        title="닫기"
        aria-label="닫기">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
      </button>
    </div>

    
    <div class="max-h-96 overflow-y-auto p-4">
      <nav
        id="toc-nav"
        class="toc-scrollbar"
        aria-label="글 목차">
          
          <div id="toc-content">
            <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#논문의-강점과-독창적-지점">논문의 강점과 독창적 지점</a></li>
        <li><a href="#핵심-알고리즘-설명-예시-입력을-통한-과정">핵심 알고리즘 설명: 예시 입력을 통한 과정</a></li>
        <li><a href="#논문의-한계">논문의 한계</a></li>
        <li><a href="#요약-비교-vanilla-transformer-vs-block-transformer">요약 비교 (Vanilla Transformer vs Block Transformer)</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#핵심-알고리즘-block-transformer의-구조와-작동-원리">핵심 알고리즘: Block Transformer의 구조와 작동 원리</a></li>
        <li><a href="#1-알고리즘-전체-과정-요약">1. <strong>알고리즘 전체 과정 요약</strong></a>
          <ul>
            <li><a href="#단계-1-입력-토큰을-블록으로-분할">단계 1: <strong>입력 토큰을 블록으로 분할</strong></a></li>
            <li><a href="#단계-2-embedder로-블록-임베딩-생성">단계 2: <strong>Embedder로 블록 임베딩 생성</strong></a></li>
            <li><a href="#단계-3-block-decoder로-글로벌-문맥-학습">단계 3: <strong>Block Decoder로 글로벌 문맥 학습</strong></a></li>
            <li><a href="#단계-4-token-decoder로-로컬-문맥-복원">단계 4: <strong>Token Decoder로 로컬 문맥 복원</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-예시를-통한-상세-설명">2. <strong>예시를 통한 상세 설명</strong></a>
          <ul>
            <li><a href="#입력-시퀀스"><strong>입력 시퀀스</strong>:</a></li>
            <li><a href="#step-1-블록-생성"><strong>Step 1: 블록 생성</strong></a></li>
            <li><a href="#step-2-블록-임베딩"><strong>Step 2: 블록 임베딩</strong></a></li>
            <li><a href="#step-3-block-decoder"><strong>Step 3: Block Decoder</strong></a></li>
            <li><a href="#step-4-token-decoder"><strong>Step 4: Token Decoder</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-알고리즘-특징">3. <strong>알고리즘 특징</strong></a></li>
        <li><a href="#4-요약-표">4. <strong>요약 표</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#글로벌-문맥-학습-방법-block-transformer의-block-decoder">글로벌 문맥 학습 방법: Block Transformer의 Block Decoder</a></li>
        <li><a href="#1-block-decoder의-구조">1. <strong>Block Decoder의 구조</strong></a></li>
        <li><a href="#2-글로벌-문맥-학습-과정">2. <strong>글로벌 문맥 학습 과정</strong></a>
          <ul>
            <li><a href="#step-1-블록-임베딩-준비"><strong>Step 1: 블록 임베딩 준비</strong></a></li>
            <li><a href="#step-2-self-attention을-통한-블록-간-관계-학습"><strong>Step 2: Self-Attention을 통한 블록 간 관계 학습</strong></a></li>
            <li><a href="#step-3-feedforward-network로-표현력-강화"><strong>Step 3: Feedforward Network로 표현력 강화</strong></a></li>
            <li><a href="#step-4-residual-connection과-layer-normalization"><strong>Step 4: Residual Connection과 Layer Normalization</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-예시를-통한-글로벌-문맥-학습">3. <strong>예시를 통한 글로벌 문맥 학습</strong></a>
          <ul>
            <li><a href="#입력">입력:</a></li>
            <li><a href="#과정">과정:</a></li>
          </ul>
        </li>
        <li><a href="#4-block-decoder의-최적화">4. <strong>Block Decoder의 최적화</strong></a>
          <ul>
            <li><a href="#kv-캐시-최적화"><strong>KV 캐시 최적화</strong>:</a></li>
            <li><a href="#컴퓨팅-효율"><strong>컴퓨팅 효율</strong>:</a></li>
            <li><a href="#병렬-처리"><strong>병렬 처리</strong>:</a></li>
          </ul>
        </li>
        <li><a href="#요약-block-decoder의-글로벌-문맥-학습">요약: Block Decoder의 글로벌 문맥 학습</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#학습에-필요한-데이터셋과-학습-프로세스">학습에 필요한 데이터셋과 학습 프로세스</a></li>
        <li><a href="#1-필요한-데이터셋">1. <strong>필요한 데이터셋</strong></a>
          <ul>
            <li><a href="#1-대규모-텍스트-데이터셋">(1) <strong>대규모 텍스트 데이터셋</strong></a></li>
            <li><a href="#2-대표적인-데이터셋">(2) <strong>대표적인 데이터셋</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-학습-프로세스-설계">2. <strong>학습 프로세스 설계</strong></a>
          <ul>
            <li><a href="#step-1-데이터-전처리"><strong>Step 1: 데이터 전처리</strong></a></li>
            <li><a href="#step-2-모델-학습"><strong>Step 2: 모델 학습</strong></a></li>
            <li><a href="#step-3-검증"><strong>Step 3: 검증</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-학습-예시">3. <strong>학습 예시</strong></a>
          <ul>
            <li><a href="#예-the-quick-brown-fox를-학습하는-과정">예: &ldquo;The quick brown fox&quot;를 학습하는 과정</a></li>
          </ul>
        </li>
        <li><a href="#4-요약-학습-프로세스">4. <strong>요약: 학습 프로세스</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#논문에서-제시한-결과와-비교">논문에서 제시한 결과와 비교</a></li>
        <li><a href="#1-결과-분석-block-transformer의-성능">1. <strong>결과 분석: Block Transformer의 성능</strong></a>
          <ul>
            <li><a href="#1-추론-처리량">(1) <strong>추론 처리량</strong></a></li>
            <li><a href="#2-메모리-사용량-최적화">(2) <strong>메모리 사용량 최적화</strong></a></li>
            <li><a href="#3-언어-모델링-성능-perplexity">(3) <strong>언어 모델링 성능 (Perplexity)</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-block-transformer의-특출난-점">2. <strong>Block Transformer의 특출난 점</strong></a></li>
        <li><a href="#3-논문에서-제시한-방법론의-기여와-논문에서-언급한-이유">3. <strong>논문에서 제시한 방법론의 기여와 논문에서 언급한 이유</strong></a>
          <ul>
            <li><a href="#1-글로벌-로컬-구조-채택">(1) <strong>글로벌-로컬 구조 채택</strong></a></li>
            <li><a href="#2-블록-단위의-계산">(2) <strong>블록 단위의 계산</strong></a></li>
            <li><a href="#3-업트레이닝-가능성">(3) <strong>업트레이닝 가능성</strong></a></li>
          </ul>
        </li>
        <li><a href="#4-이-결과를-가능하게-한-구체적인-방법과-내-생각">4. <strong>이 결과를 가능하게 한 구체적인 방법과 내 생각</strong></a>
          <ul>
            <li><a href="#1-방법-블록화된-문맥-학습">(1) <strong>방법: 블록화된 문맥 학습</strong></a></li>
            <li><a href="#2-방법-블록-단위의-kv-캐시-최적화">(2) <strong>방법: 블록 단위의 KV 캐시 최적화</strong></a></li>
            <li><a href="#3-방법-학습과-추론의-분리">(3) <strong>방법: 학습과 추론의 분리</strong></a></li>
          </ul>
        </li>
        <li><a href="#5-block-transformer와-기존-방법론의-비교">5. <strong>Block Transformer와 기존 방법론의 비교</strong></a></li>
        <li><a href="#결론">결론</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#block-transformer의-입력-데이터와-추론-과정">Block Transformer의 입력 데이터와 추론 과정</a>
          <ul>
            <li><a href="#1-입력-데이터-처리-과정"><strong>1. 입력 데이터 처리 과정</strong></a></li>
            <li><a href="#2-추론-과정"><strong>2. 추론 과정</strong></a></li>
            <li><a href="#3-모델-아키텍처의-구성"><strong>3. 모델 아키텍처의 구성</strong></a></li>
            <li><a href="#4-연산-요구량-computation-requirements"><strong>4. 연산 요구량 (Computation Requirements)</strong></a></li>
            <li><a href="#5-메모리-요구량-memory-requirements"><strong>5. 메모리 요구량 (Memory Requirements)</strong></a></li>
            <li><a href="#6-block-transformer의-효율성-요약"><strong>6. Block Transformer의 효율성 요약</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#block-transformer의-한계와-이를-극복하기-위한-연구-흐름">Block Transformer의 한계와 이를 극복하기 위한 연구 흐름</a></li>
        <li><a href="#1-block-transformer의-주요-한계">1. <strong>Block Transformer의 주요 한계</strong></a>
          <ul>
            <li><a href="#1-훈련-비용-증가">(1) <strong>훈련 비용 증가</strong></a></li>
            <li><a href="#2-모바일-및-경량-환경에서의-제약">(2) <strong>모바일 및 경량 환경에서의 제약</strong></a></li>
            <li><a href="#3-정적-블록-크기의-한계">(3) <strong>정적 블록 크기의 한계</strong></a></li>
            <li><a href="#4-글로벌-정보-손실-가능성">(4) <strong>글로벌 정보 손실 가능성</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-한계를-극복하기-위한-연구-흐름">2. <strong>한계를 극복하기 위한 연구 흐름</strong></a>
          <ul>
            <li><a href="#1-동적-블록-크기-및-적응형-모델링"><strong>(1) 동적 블록 크기 및 적응형 모델링</strong></a></li>
            <li><a href="#2-kv-캐시-압축-및-최적화"><strong>(2) KV 캐시 압축 및 최적화</strong></a></li>
            <li><a href="#3-글로벌-정보-보존-강화"><strong>(3) 글로벌 정보 보존 강화</strong></a></li>
            <li><a href="#4-경량화-모델-설계"><strong>(4) 경량화 모델 설계</strong></a></li>
            <li><a href="#5-멀티스케일-학습"><strong>(5) 멀티스케일 학습</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-한계-극복을-위한-기술-요약">3. <strong>한계 극복을 위한 기술 요약</strong></a></li>
        <li><a href="#4-결론">4. <strong>결론</strong></a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#block-decoder와-token-decoder의-역할-및-상호작용">Block Decoder와 Token Decoder의 역할 및 상호작용</a>
          <ul>
            <li><a href="#1-block-decoder-글로벌-문맥-학습">1. <strong>Block Decoder: 글로벌 문맥 학습</strong></a></li>
            <li><a href="#2-token-decoder-로컬-문맥-학습">2. <strong>Token Decoder: 로컬 문맥 학습</strong></a></li>
            <li><a href="#3-block-decoder와-token-decoder의-상호작용">3. <strong>Block Decoder와 Token Decoder의 상호작용</strong></a></li>
            <li><a href="#4-기존-transformer의-병목-문제-해결">4. <strong>기존 Transformer의 병목 문제 해결</strong></a></li>
            <li><a href="#5-block-transformer의-효율성-요약">5. <strong>Block Transformer의 효율성 요약</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#블록-크기lb-선택-기준과-성능처리량-간의-관계">블록 크기(LB) 선택 기준과 성능/처리량 간의 관계</a></li>
        <li><a href="#1-블록-크기lb의-선택-기준">1. <strong>블록 크기(LB)의 선택 기준</strong></a>
          <ul>
            <li><a href="#1-글로벌-문맥-처리-요구">(1) <strong>글로벌 문맥 처리 요구</strong></a></li>
            <li><a href="#2-추론-효율성">(2) <strong>추론 효율성</strong></a></li>
            <li><a href="#3-데이터-복잡성">(3) <strong>데이터 복잡성</strong></a></li>
            <li><a href="#4-하드웨어-제한">(4) <strong>하드웨어 제한</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-블록-크기와-성능perplexity-및-처리량throughput의-관계">2. <strong>블록 크기와 성능(Perplexity) 및 처리량(Throughput)의 관계</strong></a>
          <ul>
            <li><a href="#1-성능perplexity">(1) 성능(Perplexity)</a></li>
            <li><a href="#2-처리량throughput">(2) 처리량(Throughput)</a></li>
            <li><a href="#3-성능-및-처리량의-상관관계">(3) 성능 및 처리량의 상관관계</a></li>
          </ul>
        </li>
        <li><a href="#3-다양한-데이터-길이-및-복잡성에-따른-블록-크기-결정">3. <strong>다양한 데이터 길이 및 복잡성에 따른 블록 크기 결정</strong></a>
          <ul>
            <li><a href="#1-데이터-길이에-따른-블록-크기">(1) 데이터 길이에 따른 블록 크기</a></li>
            <li><a href="#2-데이터-복잡성에-따른-블록-크기">(2) 데이터 복잡성에 따른 블록 크기</a></li>
          </ul>
        </li>
        <li><a href="#4-결론-및-추천-전략">4. <strong>결론 및 추천 전략</strong></a>
          <ul>
            <li><a href="#블록-크기-선택을-위한-추천-전략"><strong>블록 크기 선택을 위한 추천 전략</strong></a></li>
            <li><a href="#요약"><strong>요약</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li><a href="#block-transformer에서-kv-캐시-최적화와-하드웨어-성능의-관계">Block Transformer에서 KV 캐시 최적화와 하드웨어 성능의 관계</a></li>
        <li><a href="#1-kv-캐시의-크기와-io-비용-최적화-메커니즘">1. <strong>KV 캐시의 크기와 I/O 비용 최적화 메커니즘</strong></a>
          <ul>
            <li><a href="#1-블록-단위-kv-캐시-사용">(1) <strong>블록 단위 KV 캐시 사용</strong></a></li>
            <li><a href="#2-kv-캐시의-글로벌-로컬-분리">(2) <strong>KV 캐시의 글로벌-로컬 분리</strong></a></li>
            <li><a href="#3-캐시-접근-최적화">(3) <strong>캐시 접근 최적화</strong></a></li>
          </ul>
        </li>
        <li><a href="#2-실제-하드웨어에서의-성능-최적화">2. <strong>실제 하드웨어에서의 성능 최적화</strong></a>
          <ul>
            <li><a href="#1-메모리-io-병목-완화">(1) <strong>메모리 I/O 병목 완화</strong></a></li>
            <li><a href="#2-처리량-증가">(2) <strong>처리량 증가</strong></a></li>
            <li><a href="#3-확장성">(3) <strong>확장성</strong></a></li>
          </ul>
        </li>
        <li><a href="#3-kv-캐시-최적화를-위한-추가적인-접근법">3. <strong>KV 캐시 최적화를 위한 추가적인 접근법</strong></a>
          <ul>
            <li><a href="#1-kv-캐시-압축">(1) <strong>KV 캐시 압축</strong></a></li>
            <li><a href="#2-kv-캐시-재사용">(2) <strong>KV 캐시 재사용</strong></a></li>
            <li><a href="#3-캐시-업데이트-동적-제어">(3) <strong>캐시 업데이트 동적 제어</strong></a></li>
            <li><a href="#4-블록-길이-동적-조정">(4) <strong>블록 길이 동적 조정</strong></a></li>
            <li><a href="#5-하드웨어-최적화">(5) <strong>하드웨어 최적화</strong></a></li>
          </ul>
        </li>
        <li><a href="#4-block-transformer의-개선-방향-요약">4. <strong>Block Transformer의 개선 방향 요약</strong></a></li>
        <li><a href="#5-결론">5. <strong>결론</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
          </div>
      </nav>
    </div>

    
    <div class="border-border bg-muted/20 border-t px-4 py-3">
      <div class="text-muted-foreground text-center text-xs">
        <span>제목을 클릭하면 해당 위치로 이동합니다</span>
      </div>
    </div>
  </div>
</div>

    


  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
    integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP"
    crossorigin="anonymous" />
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
    integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6"
    crossorigin="anonymous"></script>
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
    integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
    crossorigin="anonymous"></script>

<script>
    
    function initKaTeX() {
        if (typeof renderMathInElement === 'undefined') {
            setTimeout(initKaTeX, 100);
            return;
        }

        var delimiters = [{"display":true,"left":"$$","right":"$$"},{"display":false,"left":"$","right":"$"}];

        renderMathInElement(document.body, {
            delimiters: delimiters,
            throwOnError: false,
            errorColor: '#cc0000',
            fleqn: false,
            leqno: false,
            trust: false
        });
    }

    
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', initKaTeX);
    } else {
        initKaTeX();
    }
</script>
    



    
    
<div
  id="search-overlay"
  class="pointer-events-none fixed inset-0 z-40 bg-black/50 opacity-0 transition-opacity duration-300"></div>


<div
  id="search-modal"
  class="bg-card border-border pointer-events-none fixed top-1/2 left-1/2 z-50 max-h-[80vh] w-full max-w-2xl -translate-x-1/2 -translate-y-1/2 scale-95 transform overflow-hidden rounded-xl border opacity-0 shadow-xl transition-all duration-300">
  
  <div class="border-border flex items-center gap-3 border-b p-4">
    
    <div class="text-muted-foreground h-5 w-5 flex-shrink-0">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
    </div>

    
    <div class="relative flex-1">
      
      <button
        id="search-clear"
        class="text-muted-foreground hover:text-foreground hover:bg-muted/50 pointer-events-none absolute top-1/2 left-0 z-10 h-5 w-5 -translate-y-1/2 rounded opacity-0 transition-all duration-200"
        title="지우기"
        aria-label="지우기">
        
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
      </button>

      
      <input
        id="search-input"
        type="text"
        placeholder="게시글 검색..."
        class="text-foreground placeholder:text-muted-foreground w-full border-none bg-transparent pl-8 text-lg outline-none"
        autocomplete="off"
        spellcheck="false" />
    </div>

    
    <button
      id="search-close"
      class="text-muted-foreground hover:text-foreground hover:bg-muted/50 flex h-6 w-6 items-center justify-center rounded-md p-0.5 transition-all duration-200"
      title="닫기"
      aria-label="닫기">
      
  <svg class="h-4 w-4"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />

</svg>
    </button>
  </div>

  
  <div id="search-results" class="max-h-96 overflow-y-auto">
    
    <div
      id="search-empty"
      class="flex flex-col items-center justify-center py-12 text-center">
      <div
        class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24">
  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m21 21-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />

</svg>
      </div>
      <h3 class="text-foreground mb-2 text-lg font-semibold">
        검색 시작
      </h3>
      <p class="text-muted-foreground text-sm">
        검색어를 입력하세요
      </p>
    </div>

    
    <div
      id="search-loading"
      class="flex hidden items-center justify-center py-8">
      <div
        class="mr-3 h-6 w-6 animate-spin rounded-full border-2 border-current border-t-transparent"></div>
      <span class="text-muted-foreground"
        >검색 중...</span
      >
    </div>

    
    <div
      id="search-no-results"
      class="flex hidden flex-col items-center justify-center py-12 text-center">
      <div
        class="bg-muted/50 mb-4 flex h-16 w-16 items-center justify-center rounded-full">
        
  <svg class="h-6 w-6"
    fill="none"
    stroke="currentColor"
    viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h7" />
</svg>
      </div>
      <h3 class="text-foreground mb-2 text-lg font-semibold">
        검색 결과 없음
      </h3>
      <p class="text-muted-foreground text-sm">
        다른 검색어로 시도해보세요
      </p>
    </div>

    
    <div id="search-results-list" class="hidden">
      
      <div
        id="search-stats"
        class="text-muted-foreground border-border border-b px-4 py-3 text-sm"
        data-template="%d개의 결과 찾음">
      </div>

      
      <div id="search-items" class="divide-border divide-y">
        
      </div>
    </div>
  </div>

  
  <div class="border-border bg-muted/20 border-t px-4 py-3">
    <div
      class="text-muted-foreground flex items-center justify-between text-xs">
      <div class="flex items-center gap-2 md:gap-4">
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >↑↓</kbd
          >
          <span class="hidden sm:inline"
            >이동</span
          >
        </div>
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >↵</kbd
          >
          <span class="hidden sm:inline"
            >선택</span
          >
        </div>
        <div class="flex items-center gap-1">
          <kbd
            class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
            >ESC</kbd
          >
          <span class="hidden sm:inline"
            >닫기</span
          >
        </div>
      </div>
      <div class="search-hint-desktop flex items-center gap-1">
        <kbd class="bg-muted border-border rounded border px-1.5 py-0.5 text-xs"
          >⌘K</kbd
        >
        <span>단축키</span>
      </div>
    </div>
  </div>
</div>

  </body>
</html>
