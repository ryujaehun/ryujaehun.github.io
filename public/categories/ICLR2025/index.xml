<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ICLR2025 on Jaehun&#39;s Blog</title>
    <link>http://192.168.1.11:1314/categories/ICLR2025/</link>
    <description>Recent content in ICLR2025 on Jaehun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Tue, 25 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://192.168.1.11:1314/categories/ICLR2025/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dynamic Diffusion Transformer</title>
      <link>http://192.168.1.11:1314/posts/Dynamic-Diffusion-Transformer/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Dynamic-Diffusion-Transformer/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.03456&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h2 id=&#34;논문의-핵심-요약&#34;&gt;&lt;strong&gt;논문의 핵심 요약&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;이 논문은 **Dynamic Diffusion Transformer (DyDiT)**라는 새로운 모델을 제안하며, 기존 **Diffusion Transformer (DiT)**의 &lt;strong&gt;과도한 연산량 문제&lt;/strong&gt;를 해결하는 것을 목표로 한다. DyDiT는 &lt;strong&gt;시간축(Timestep)과 공간축(Spatial)에서 동적으로 연산을 조정하는 방식&lt;/strong&gt;을 도입하여 효율성을 크게 향상시켰다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>HEXGEN-2: DISAGGREGATED GENERATIVE INFERENCE OF LLMS IN HETEROGENEOUS ENVIRONMENT</title>
      <link>http://192.168.1.11:1314/posts/HEXGEN-2-DISAGGREGATED-GENERATIVE-INFERENCE-OF-LLMS-IN-HETEROGENEOUS-ENVIRONMENT/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/HEXGEN-2-DISAGGREGATED-GENERATIVE-INFERENCE-OF-LLMS-IN-HETEROGENEOUS-ENVIRONMENT/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.07903v1&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-핵심-요약-및-기여점&#34;&gt;&lt;strong&gt;논문의 핵심 요약 및 기여점&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;이 논문은 &lt;strong&gt;HEXGEN-2&lt;/strong&gt;라는 새로운 분산 LLM(대형 언어 모델) 추론 시스템을 제안합니다. 기존 동질적인 고성능 GPU 클러스터를 이용하는 방식과 달리, &lt;strong&gt;이기종 GPU 환경&lt;/strong&gt;에서 &lt;strong&gt;Prefill(입력 처리)과 Decoding(출력 생성) 단계를 분리(disaggregated inference)&lt;/strong&gt; 하여 비용 효율성을 극대화하는 것이 핵심 아이디어입니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding</title>
      <link>http://192.168.1.11:1314/posts/Speculate-then-Collaborate-Fusing-Knowledge-of-Language-Models-during-Decoding/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Speculate-then-Collaborate-Fusing-Knowledge-of-Language-Models-during-Decoding/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.08020v1&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-강점과-독창적인-지점&#34;&gt;&lt;strong&gt;논문의 강점과 독창적인 지점&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;이 논문의 핵심 기여는 &lt;strong&gt;Collaborative Speculative Decoding (CoSD)&lt;/strong&gt; 알고리즘을 제안하여, 훈련 없이도 여러 LLM의 지식을 효과적으로 융합하는 방법을 제시한 점이다. CoSD의 강점은 다음과 같다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>You OnlyPruneOnce: DESIGNING CALIBRATION-FREE MODEL COMPRESSION WITH POLICY LEARNING</title>
      <link>http://192.168.1.11:1314/posts/You-OnlyPruneOnce-DESIGNING-CALIBRATION-FREE-MODEL-COMPRESSION-WITH-POLICY-LEARNING/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/You-OnlyPruneOnce-DESIGNING-CALIBRATION-FREE-MODEL-COMPRESSION-WITH-POLICY-LEARNING/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2501.15296&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-핵심-요약&#34;&gt;&lt;strong&gt;논문의 핵심 요약&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;이 논문은 &lt;strong&gt;PruneNet&lt;/strong&gt;이라는 새로운 모델 압축 기법을 제안하며, 기존 방법들의 한계를 극복하고자 한다. 주요 기여점은 다음과 같다:&lt;/p&gt;</description>
    </item>
    <item>
      <title>FlashMask: Efficient and Rich Mask Extension of FlashAttention</title>
      <link>http://192.168.1.11:1314/posts/FlashMask-Efficient-and-Rich-Mask-Extension-of-FlashAttention/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/FlashMask-Efficient-and-Rich-Mask-Extension-of-FlashAttention/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.01359&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-핵심-요약&#34;&gt;&lt;strong&gt;논문의 핵심 요약&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;논문 **&amp;ldquo;FLASHMASK: Efficient and Rich Mask Extension of FlashAttention&amp;rdquo;**는 기존 FlashAttention이 지원하는 마스크 타입을 확장하고, 메모리 복잡도를 **O(N²)에서 O(N)**으로 줄이면서 성능을 향상시킨 새로운 &lt;strong&gt;column-wise sparse mask&lt;/strong&gt; 표현을 제안한다. 이로 인해 복잡한 마스킹을 처리할 때도 계산량을 줄일 수 있으며, &lt;strong&gt;최신 FlexAttention 대비 12.1%&lt;del&gt;60.7% 향상된 성능(TFLOPs/s)과 1.65x&lt;/del&gt;3.22x 속도 증가&lt;/strong&gt;를 달성했다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TypedThinker: Typed Thinking Improves Large Language Model Reasoning</title>
      <link>http://192.168.1.11:1314/posts/TypedThinker-Typed-Thinking-Improves-Large-Language-Model-Reasoning/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/TypedThinker-Typed-Thinking-Improves-Large-Language-Model-Reasoning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.01952&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h2 id=&#34;논문의-강점과-독창적인-지점&#34;&gt;논문의 강점과 독창적인 지점&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-강점&#34;&gt;1. &lt;strong&gt;강점&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;다양한 추론 방식의 활용&lt;/strong&gt;&lt;br&gt;&#xA;기존 LLM(대형 언어 모델)의 한계를 극복하기 위해 &lt;strong&gt;연역(deductive), 귀납(inductive), 가설(abductive), 유추(analogical)&lt;/strong&gt; 네 가지의 논리적 추론 방식을 적용함.&lt;br&gt;&#xA;각각의 추론 방식이 특정 유형의 문제에서 효과적인지를 실험적으로 분석함.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
