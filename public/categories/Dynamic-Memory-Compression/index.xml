<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dynamic Memory Compression on Jaehun&#39;s Blog</title>
    <link>http://192.168.1.11:1314/categories/Dynamic-Memory-Compression/</link>
    <description>Recent content in Dynamic Memory Compression on Jaehun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Fri, 06 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://192.168.1.11:1314/categories/Dynamic-Memory-Compression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</title>
      <link>http://192.168.1.11:1314/posts/Dynamic-Memory-Compression-Retrofitting-LLMs-for-Accelerated-Inference/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Dynamic-Memory-Compression-Retrofitting-LLMs-for-Accelerated-Inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09636&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문-요약-및-강점-독창성-핵심-알고리즘-설명&#34;&gt;논문 요약 및 강점, 독창성, 핵심 알고리즘 설명&lt;/h3&gt;&#xA;&lt;h4 id=&#34;논문-요약&#34;&gt;논문 요약&lt;/h4&gt;&#xA;&lt;p&gt;논문은 대형 언어 모델(LLM)의 비효율적인 메모리 사용 문제를 해결하기 위해 &lt;strong&gt;Dynamic Memory Compression (DMC)&lt;/strong&gt; 방법을 제안합니다. 이 방법은 모델의 키-값(KV) 캐시를 실시간으로 동적으로 압축하여, 더 적은 메모리로 유사한 성능을 유지하면서도 추론 속도를 증가시킵니다. DMC는 학습 중 각 레이어와 헤드마다 서로 다른 압축 비율을 자동으로 학습하며, 기존 LLM에 새로운 파라미터를 추가하지 않고 간단한 후속 학습으로 적용 가능합니다.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
