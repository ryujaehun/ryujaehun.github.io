<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM-Inference on Jaehun&#39;s Blog</title>
    <link>http://192.168.1.11:1314/categories/LLM-Inference/</link>
    <description>Recent content in LLM-Inference on Jaehun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://192.168.1.11:1314/categories/LLM-Inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</title>
      <link>http://192.168.1.11:1314/posts/CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/CacheGen-KV-Cache-Compression-and-Streaming-for-Fast-Large-Language-Model-Serving/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.07240&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;p&gt;논문 **&amp;ldquo;CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving&amp;rdquo;**에 대한 분석을 아래와 같이 제공합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CHAI: Clustered Head Attention for Efficient LLM Inference</title>
      <link>http://192.168.1.11:1314/posts/CHAI-Clustered-Head-Attention-for-Efficient-LLM-Inference/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/CHAI-Clustered-Head-Attention-for-Efficient-LLM-Inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.08058&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-제목-chai-clustered-head-attention-for-efficient-llm-inference&#34;&gt;논문의 제목: &lt;strong&gt;CHAI: Clustered Head Attention for Efficient LLM Inference&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;논문의-주요-내용-요약&#34;&gt;&lt;strong&gt;논문의 주요 내용 요약&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;이 논문은 대규모 언어 모델(LLM)의 추론 시 발생하는 메모리 및 계산 병목현상을 줄이기 위해 **Clustered Head Attention (CHAI)**를 제안합니다. CHAI는 다중 헤드 어텐션(MHA)에서 주의할 토큰에 대해 유사한 출력을 생성하는 헤드를 클러스터링하여 메모리 및 계산 요구를 줄입니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Compressed Context Memory For Online Language Model Interaction</title>
      <link>http://192.168.1.11:1314/posts/Compressed-Context-Memory-For-Online-Language-Model-Interaction/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Compressed-Context-Memory-For-Online-Language-Model-Interaction/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.03414&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-강점&#34;&gt;논문의 강점&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;효율적 메모리 관리&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</title>
      <link>http://192.168.1.11:1314/posts/LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06839&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-주요-내용-강점-독창성-알고리즘-예시-한계점&#34;&gt;논문의 주요 내용, 강점, 독창성, 알고리즘 예시, 한계점&lt;/h3&gt;&#xA;&lt;h4 id=&#34;논문의-개요&#34;&gt;&lt;strong&gt;논문의 개요&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;논문 &amp;ldquo;LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression&amp;quot;은 대규모 언어 모델(LLM)이 긴 문맥을 처리할 때 발생하는 주요 문제(계산 비용, 성능 저하, 위치 편향)를 해결하기 위해 설계된 LongLLMLingua라는 프롬프트 압축 기법을 제안합니다. 이 방법은 압축을 통해 비용을 절감하고, 성능을 향상시키며, 문맥 위치에 따른 성능 편차를 최소화합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>QAQ: Quality Adaptive Quantization for LLM KV Cache</title>
      <link>http://192.168.1.11:1314/posts/QAQ-Quality-Adaptive-Quantization-for-LLM-KV-Cache/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/QAQ-Quality-Adaptive-Quantization-for-LLM-KV-Cache/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.04643&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-요약-강점-독창성-그리고-한계점&#34;&gt;논문의 요약, 강점, 독창성, 그리고 한계점&lt;/h3&gt;&#xA;&lt;h4 id=&#34;논문-요약&#34;&gt;&lt;strong&gt;논문 요약&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;이 논문은 **QAQ (Quality Adaptive Quantization)**라는 방법을 제안하여 대규모 언어 모델(LLM)의 Key-Value (KV) 캐시 메모리 사용량을 효율적으로 줄이는 방법을 다룹니다. KV 캐시는 LLM의 긴 문맥 처리에서 중요한 메모리 자원이지만, 캐시 크기가 문맥 길이에 비례하여 증가하므로 GPU 메모리에 부담을 줍니다. QAQ는 KV 캐시의 비균일(non-uniform) 양자화 방식을 사용하여 키와 값 벡터의 서로 다른 민감도를 고려하고, 이를 통해 모델 성능을 크게 저하시키지 않으면서 최대 10배의 캐시 압축을 달성합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepCache: Accelerating Diffusion Models for Free</title>
      <link>http://192.168.1.11:1314/posts/DeepCache-Accelerating-Diffusion-Models-for-Free/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/DeepCache-Accelerating-Diffusion-Models-for-Free/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.00858v2&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-강점과-독창성&#34;&gt;논문의 강점과 독창성&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;강점&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference</title>
      <link>http://192.168.1.11:1314/posts/Get-More-with-LESS-Synthesizing-Recurrence-with-KV-Cache-Compression-for-Efficient-LLM-Inference/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Get-More-with-LESS-Synthesizing-Recurrence-with-KV-Cache-Compression-for-Efficient-LLM-Inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.09398&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;p&gt;논문 제목은 **&amp;ldquo;Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference&amp;rdquo;**입니다. 논문에서 제안된 LESS(Low-rank Embedding Sidekick with Sparse policy)는 대형 언어 모델(LLM)의 키-값(KV) 캐시 메모리 병목 문제를 해결하기 위해 Sparse(희소) 정책과 Low-rank(저차원) 캐시를 결합한 새로운 방법론입니다. 이제 논문의 주요 강점과 독창성, 알고리즘, 한계점을 자세히 설명하겠습니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</title>
      <link>http://192.168.1.11:1314/posts/KIVI-A-Tuning-Free-Asymmetric-2bit-Quantization-for-KV-Cache/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/KIVI-A-Tuning-Free-Asymmetric-2bit-Quantization-for-KV-Cache/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.02750&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;강점과-독창성&#34;&gt;강점과 독창성&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;효율적이고 튜닝이 필요 없는 2비트 양자화 알고리즘&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title>
      <link>http://192.168.1.11:1314/posts/KVQuant-Towards-10-Million-Context-Length-LLM-Inference-with-KV-Cache-Quantization/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/KVQuant-Towards-10-Million-Context-Length-LLM-Inference-with-KV-Cache-Quantization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.18079&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;p&gt;논문 **&amp;ldquo;KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization&amp;rdquo;**은 초대형 문맥 창 길이를 가진 대규모 언어 모델(LLM)에서 효율적인 추론을 가능하게 하는 방법론을 제안합니다. 아래는 논문의 강점, 독창적인 지점, 핵심 알고리즘, 한계점 등을 분석한 내용입니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixed Precision Quantization</title>
      <link>http://192.168.1.11:1314/posts/Mixed-Precision-Quantization/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Mixed-Precision-Quantization/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.18096&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문-요약&#34;&gt;논문 요약:&lt;/h3&gt;&#xA;&lt;p&gt;**&amp;ldquo;No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization&amp;rdquo;**는 대규모 언어 모델(LLM)의 추론 시 발생하는 메모리 병목 현상을 해결하기 위한 Key-Value(KV) 캐싱 압축 기법을 제안합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</title>
      <link>http://192.168.1.11:1314/posts/WKVQuant-Quantizing-Weight-and-Key/Value-Cache-for-Large-Language-Models-Gains-More/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/WKVQuant-Quantizing-Weight-and-Key/Value-Cache-for-Large-Language-Models-Gains-More/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.12065&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;강점과-독창적인-지점&#34;&gt;&lt;strong&gt;강점과 독창적인 지점&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;WKVQuant&lt;/strong&gt;은 대규모 언어 모델(LLM)의 메모리 사용 문제를 해결하기 위해 설계된 &lt;strong&gt;Post-Training Quantization (PTQ)&lt;/strong&gt; 프레임워크입니다. 주요 강점과 독창적 특징은 다음과 같습니다:&lt;/p&gt;</description>
    </item>
    <item>
      <title>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</title>
      <link>http://192.168.1.11:1314/posts/PowerInfer-Fast-Large-Language-Model-Serving-with-a-Consumer-grade-GPU/</link>
      <pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/PowerInfer-Fast-Large-Language-Model-Serving-with-a-Consumer-grade-GPU/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.12456v1&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-요약&#34;&gt;논문의 요약&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;PowerInfer&lt;/strong&gt;는 소비자용 GPU로 대형 언어 모델(LLM)을 빠르게 추론할 수 있도록 설계된 시스템입니다. LLM 추론에서 고도 국소성을 활용하여 GPU와 CPU의 협력을 통해 효율성을 극대화합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>QAQ: Quality Adaptive Quantization for LLM KV Cache</title>
      <link>http://192.168.1.11:1314/posts/QAQ-Quality-Adaptive-Quantization-for-LLM-KV-Cache/</link>
      <pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/QAQ-Quality-Adaptive-Quantization-for-LLM-KV-Cache/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.04643&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문-요약-qaq-quality-adaptive-quantization-for-llm-kv-cache&#34;&gt;논문 요약: &lt;strong&gt;QAQ: Quality Adaptive Quantization for LLM KV Cache&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;주요-기여&#34;&gt;주요 기여&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;품질 적응형 양자화(Quality Adaptive Quantization)&lt;/strong&gt;: LLM의 Key-Value(KV) 캐시를 위한 새로운 양자화 방식 제안. 이는 GPU 메모리 사용량을 10배까지 줄이면서 성능 손실을 최소화함.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;이론적 분석 및 실험적 검증&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Key와 Value 캐시는 양자화에 대해 서로 다른 민감도를 보임. 이에 따라 각기 다른 양자화 전략을 설계.&lt;/li&gt;&#xA;&lt;li&gt;기존 중요성 지속성 가설(Persistence of Importance)을 보완해, 예외적인 토큰 중요도 변화를 처리하는 방법 제안.&lt;/li&gt;&#xA;&lt;li&gt;Outliers(이상치)의 중요성을 강조하고 이를 개별적으로 처리하여 성능 손실 최소화.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;우수한 성능&lt;/strong&gt;: 기존 방법 대비 1.6~1.8배 높은 압축률을 달성하면서 성능 손실을 거의 없는 수준으로 유지.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;강점과-독창성&#34;&gt;강점과 독창성&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;독창적 설계&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching</title>
      <link>http://192.168.1.11:1314/posts/ALISA-Accelerating-Large-Language-Model-Inference-via-Sparsity-Aware-KV-Caching/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/ALISA-Accelerating-Large-Language-Model-Inference-via-Sparsity-Aware-KV-Caching/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.17312&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문-요약-alisa-대규모-언어-모델-추론-가속화를-위한-희소성-기반-kv-캐싱&#34;&gt;논문 요약: &lt;strong&gt;ALISA: 대규모 언어 모델 추론 가속화를 위한 희소성 기반 KV 캐싱&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;이 논문은 대규모 언어 모델(LLM)의 추론 속도를 개선하기 위해 &lt;strong&gt;ALISA&lt;/strong&gt;라는 새로운 알고리즘-시스템 공동 설계 방안을 제시합니다. 특히, GPU 메모리가 제한된 환경에서 LLM 추론 효율을 극대화하기 위한 다양한 방법론을 다룹니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</title>
      <link>http://192.168.1.11:1314/posts/Dynamic-Memory-Compression-Retrofitting-LLMs-for-Accelerated-Inference/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/Dynamic-Memory-Compression-Retrofitting-LLMs-for-Accelerated-Inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09636&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문-요약-및-강점-독창성-핵심-알고리즘-설명&#34;&gt;논문 요약 및 강점, 독창성, 핵심 알고리즘 설명&lt;/h3&gt;&#xA;&lt;h4 id=&#34;논문-요약&#34;&gt;논문 요약&lt;/h4&gt;&#xA;&lt;p&gt;논문은 대형 언어 모델(LLM)의 비효율적인 메모리 사용 문제를 해결하기 위해 &lt;strong&gt;Dynamic Memory Compression (DMC)&lt;/strong&gt; 방법을 제안합니다. 이 방법은 모델의 키-값(KV) 캐시를 실시간으로 동적으로 압축하여, 더 적은 메모리로 유사한 성능을 유지하면서도 추론 속도를 증가시킵니다. DMC는 학습 중 각 레이어와 헤드마다 서로 다른 압축 비율을 자동으로 학습하며, 기존 LLM에 새로운 파라미터를 추가하지 않고 간단한 후속 학습으로 적용 가능합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>FASTDECODE: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines</title>
      <link>http://192.168.1.11:1314/posts/FASTDECODE-High-Throughput-GPU-Efficient-LLM-Serving-using-Heterogeneous-Pipelines/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/FASTDECODE-High-Throughput-GPU-Efficient-LLM-Serving-using-Heterogeneous-Pipelines/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.11421&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-강점과-독창적인-지점&#34;&gt;논문의 강점과 독창적인 지점&lt;/h3&gt;&#xA;&lt;p&gt;이 논문은 대규모 언어 모델(LLM) 서빙을 위해 &lt;strong&gt;FASTDECODE&lt;/strong&gt;라는 새로운 시스템을 제안합니다. 주요 강점과 독창성은 다음과 같습니다:&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title>
      <link>http://192.168.1.11:1314/posts/LLMLingua-2-Data-Distillation-for-Efficient-and-Faithful-Task-Agnostic-Prompt-Compression/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/LLMLingua-2-Data-Distillation-for-Efficient-and-Faithful-Task-Agnostic-Prompt-Compression/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12968&#34;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;q--이-논문을-아주-자세하게-읽고-논문의-강점과-독창적인-지점을-설명해주고-핵심-알고리즘을-예시-입력을-들어서-전체적인-과정을-설명해줘-추가적으로-논문의-한계점에-대해서도-알려줘&#34;&gt;Q : 이 논문을 아주 자세하게 읽고 논문의 강점과 독창적인 지점을 설명해주고 핵심 알고리즘을 예시 입력을 들어서 전체적인 과정을 설명해줘 추가적으로 논문의 한계점에 대해서도 알려줘&lt;/h1&gt;&#xA;&lt;h3 id=&#34;논문의-강점과-독창성&#34;&gt;논문의 강점과 독창성&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;강점:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
