<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on Jaehun&#39;s Blog</title>
    <link>http://192.168.1.11:1314/categories/ML/</link>
    <description>Recent content in ML on Jaehun&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Fri, 12 Feb 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://192.168.1.11:1314/categories/ML/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLVM loop unroll and jam pass and view-cfg</title>
      <link>http://192.168.1.11:1314/posts/LLVM-loop-unroll-and-jam-pass-and-view-cfg/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/LLVM-loop-unroll-and-jam-pass-and-view-cfg/</guid>
      <description>&lt;p&gt;&#xA;&lt;figure class=&#34;image-figure not-prose my-8&#34; &#xA;        data-lightbox-enabled=&#34;false&#34;&#xA;        data-gallery-type=&#34;auto&#34;&gt;&#xA;  &lt;div class=&#34;image-container&#34;&gt;&#xA;    &#xA;    &lt;img&#xA;      src=&#34;http://192.168.1.11:1314/assets/images/llvm.jpeg&#34;&#xA;      alt=&#34;&#34;&#xA;      &#xA;      loading=&#34;lazy&#34;&#xA;      decoding=&#34;async&#34;&#xA;      data-gallery-src=&#34;http://192.168.1.11:1314/assets/images/llvm.jpeg&#34;&#xA;      data-gallery-alt=&#34;&#34;&#xA;      data-gallery-title=&#34;&#34; /&gt;&lt;/div&gt;&#xA;&#xA;  &lt;/figure&gt;&#xA;대학원 컴파일러 수업에서 ML을 이용하여 unroll and jam을 판별하는 모델을 학습을 하는 term project를 진행하였다.&#xA;unroll and jam pass는 이름에서 알 수 있듯이 loop 최적화에 관련된 pass로 unroll 과 jam을 수행하여 innermost loop body의 병렬성을 증가시켜서 제한된 resource의 utilization을 증가시키는 최적화이다.&#xA;내 기억이 맞다면 O2 이상의 최적화 부터 적용되는데 opt의 debug를 통하여 볼때 생각보다 잘? 사용이 안된다.&#xA;LLVM code를 보면 대부분 loop unroll과 loop fusion pass를 재활용하며 검사 정도만 하는데 이 때문에 da,lcssa,loop simplify가 조건을 만족하여도 unroll and jam pass가 동작되지 않는 경우가 많다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>간단논문 정리 DARTS DIFFERENTIABLE ARCHITECTURE SEARCH (ICLR 2019)</title>
      <link>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-DARTS-DIFFERENTIABLE-ARCHITECTURE-SEARCH-ICLR-2019/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-DARTS-DIFFERENTIABLE-ARCHITECTURE-SEARCH-ICLR-2019/</guid>
      <description>&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH&lt;/p&gt;&#xA;&lt;h1 id=&#34;저자&#34;&gt;저자&lt;/h1&gt;&#xA;&lt;p&gt;Hanxiao Liu,Karen Simonyan,Yiming Yang&lt;/p&gt;&#xA;&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;&#xA;&lt;p&gt;기존 NAS가 상당수의 시간 혹은 cost가 필요(2000 GPU days of reinforcement learning, 3150 GPU days of evolution)이러한 원인 중 하나가  discrete domain, which leads to a large number of architecture evaluations required 때문이라고 분석. 물론 이전에도 filter size와 같은 것들을 연속적으로 학습 했으나 해당 논문은 블록, 그래프 토플로지 까지 학습하는 것을 목표로 함&lt;/p&gt;</description>
    </item>
    <item>
      <title>간단논문 정리 End-to-End Deep Learning of Optimization Heuristics (PACT 17)</title>
      <link>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-End-to-End-Deep-Learning-of-Optimization-Heuristics-PACT-17/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-End-to-End-Deep-Learning-of-Optimization-Heuristics-PACT-17/</guid>
      <description>&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;End-to-End Deep Learning of Optimization Heuristics&lt;/p&gt;&#xA;&lt;h1 id=&#34;저자&#34;&gt;저자&lt;/h1&gt;&#xA;&lt;p&gt;Chris Cummins ; Pavlos Petoumenos ; Zheng Wang ; Hugh Leather&lt;/p&gt;&#xA;&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;&#xA;&lt;p&gt;기존 머신러닝을 이용한 compiler optimizaion 방법에서는  human experts를 이용한 feature engineering 이 필요&lt;/p&gt;&#xA;&lt;h1 id=&#34;contribution&#34;&gt;Contribution&lt;/h1&gt;&#xA;&lt;p&gt;논문에서 제안하는 Source Rewriter &amp;amp; Language model을 이용하여 RAW PROGRAM CODE을 직접 이용하여 compiler optimizaion을 수행 아울어 transfer learning 을 이용하여 small number of program 에서도 학습을 수행&lt;/p&gt;</description>
    </item>
    <item>
      <title>간단논문 정리 Fast and Effective Orchestration of Compiler Optimizations(Zhelong Pan,Rudolf Eigenmann;Purdue University ;CGO’06)</title>
      <link>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Fast-and-Effective-Orchestration-of-Compiler-OptimizationsZhelong-PanRudolf-EigenmannPurdue-University-CGO06/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Fast-and-Effective-Orchestration-of-Compiler-OptimizationsZhelong-PanRudolf-EigenmannPurdue-University-CGO06/</guid>
      <description>&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;Fast and Effective Orchestration of Compiler Optimizations&lt;/p&gt;&#xA;&lt;h1 id=&#34;저자&#34;&gt;저자&lt;/h1&gt;&#xA;&lt;p&gt;Zhelong Pan,Rudolf Eigenmann&lt;/p&gt;&#xA;&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;&#xA;&lt;p&gt;compile-time optimizations 은 전반적으로 프로그램 성능을 향상시키지만 일부 기법은 성능 하락을 야기한다.&#xA;입력프로그램와 target architecture에 대한 불충분한 정보는 컴파일 시간에 정확도 향상을 향상 시키는 모델의 한계를 만든다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>간단논문 정리 TVM An Automated End-to-End Optimizing Compiler for Deep Learning  (OSDI 18)</title>
      <link>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-TVM-An-Automated-End-to-End-Optimizing-Compiler-for-Deep-Learning-OSDI-18/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EA%B0%84%EB%8B%A8%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-TVM-An-Automated-End-to-End-Optimizing-Compiler-for-Deep-Learning-OSDI-18/</guid>
      <description>&lt;p&gt;&#xA;&lt;figure class=&#34;image-figure not-prose my-8&#34; &#xA;        data-lightbox-enabled=&#34;false&#34;&#xA;        data-gallery-type=&#34;auto&#34;&gt;&#xA;  &lt;div class=&#34;image-container&#34;&gt;&#xA;    &#xA;    &lt;img&#xA;      src=&#34;http://192.168.1.11:1314/assets/images/tvm1.png&#34;&#xA;      alt=&#34;&#34;&#xA;      &#xA;      loading=&#34;lazy&#34;&#xA;      decoding=&#34;async&#34;&#xA;      data-gallery-src=&#34;http://192.168.1.11:1314/assets/images/tvm1.png&#34;&#xA;      data-gallery-alt=&#34;&#34;&#xA;      data-gallery-title=&#34;&#34; /&gt;&lt;/div&gt;&#xA;&#xA;  &lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;TVM: An Automated End-to-End Optimizing Compiler for Deep Learning&lt;/p&gt;&#xA;&lt;h1 id=&#34;tvm&#34;&gt;TVM?&lt;/h1&gt;&#xA;&lt;p&gt;해당논문은 머신러닝용 컴파일러중에 대표적인 TVM에 대한 paper입니다. 현재는 apache에서 관리 하고 있으며 graph level IR 을 통한 target-independent optimization,&#xA;autotune을 통한 target-dependent optimization 을 지원하며 llvm 및 vta를 통하여 cpu,gpu뿐만 아니라 FPGA를 backend로 지원합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>논문 정리 Chameleon Adaptive Code Optimization for Expedited Deep Neural Network Compilation(ICLR 2020)</title>
      <link>http://192.168.1.11:1314/posts/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Chameleon-Adaptive-Code-Optimization-for-Expedited-Deep-Neural-Network-CompilationICLR-2020/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-Chameleon-Adaptive-Code-Optimization-for-Expedited-Deep-Neural-Network-CompilationICLR-2020/</guid>
      <description>&lt;p&gt;&#xA;&lt;figure class=&#34;image-figure not-prose my-8&#34; &#xA;        data-lightbox-enabled=&#34;false&#34;&#xA;        data-gallery-type=&#34;auto&#34;&gt;&#xA;  &lt;div class=&#34;image-container&#34;&gt;&#xA;    &#xA;    &lt;img&#xA;      src=&#34;http://192.168.1.11:1314/assets/images/chameleon1.jpg&#34;&#xA;      alt=&#34;&#34;&#xA;      &#xA;      loading=&#34;lazy&#34;&#xA;      decoding=&#34;async&#34;&#xA;      data-gallery-src=&#34;http://192.168.1.11:1314/assets/images/chameleon1.jpg&#34;&#xA;      data-gallery-alt=&#34;&#34;&#xA;      data-gallery-title=&#34;&#34; /&gt;&lt;/div&gt;&#xA;&#xA;  &lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation&lt;/p&gt;&#xA;&lt;h1 id=&#34;저자&#34;&gt;저자&lt;/h1&gt;&#xA;&lt;p&gt;Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh, Hadi Esmaeilzadeh&lt;/p&gt;&#xA;&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;&#xA;&lt;p&gt;The current approaches are oblivious to the patterns in the design space of schedules that are available for exploitation, and causes inefficient search or even converges to solutions that may even be suboptimal.&#xA;Current solutions that rely on greedy sampling lead to significant fractions of the candidate configurations being redundant over iterations(long compilation time)&lt;/p&gt;</description>
    </item>
    <item>
      <title>논문 정리 NeuroVectorizer End-to-End Vectorization with Deep Reinforcement Learning (CGO 20)</title>
      <link>http://192.168.1.11:1314/posts/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-NeuroVectorizer-End-to-End-Vectorization-with-Deep-Reinforcement-Learning-CGO-20/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>http://192.168.1.11:1314/posts/%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC-NeuroVectorizer-End-to-End-Vectorization-with-Deep-Reinforcement-Learning-CGO-20/</guid>
      <description>&lt;p&gt;&#xA;&lt;figure class=&#34;image-figure not-prose my-8&#34; &#xA;        data-lightbox-enabled=&#34;false&#34;&#xA;        data-gallery-type=&#34;auto&#34;&gt;&#xA;  &lt;div class=&#34;image-container&#34;&gt;&#xA;    &#xA;    &lt;img&#xA;      src=&#34;http://192.168.1.11:1314/assets/images/nv1.png&#34;&#xA;      alt=&#34;&#34;&#xA;      &#xA;      loading=&#34;lazy&#34;&#xA;      decoding=&#34;async&#34;&#xA;      data-gallery-src=&#34;http://192.168.1.11:1314/assets/images/nv1.png&#34;&#xA;      data-gallery-alt=&#34;&#34;&#xA;      data-gallery-title=&#34;&#34; /&gt;&lt;/div&gt;&#xA;&#xA;  &lt;/figure&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;제목&#34;&gt;제목&lt;/h1&gt;&#xA;&lt;p&gt;NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning&lt;/p&gt;&#xA;&lt;h1 id=&#34;저자&#34;&gt;저자&lt;/h1&gt;&#xA;&lt;p&gt;Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Sophia Shao, Krste Asanovic, Ion Stoica&lt;/p&gt;&#xA;&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;&#xA;&lt;p&gt;Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions&#xA;The vectorization is critical to enhancing the performance of compute-intensive workloads in modern computers.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
